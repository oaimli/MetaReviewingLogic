{
    "iH1_KBzbwQq": {
        "paper_id": "nips_2021_iH1_KBzbwQq",
        "paper_title": "Learning Transferable Features for Point Cloud Detection via 3D Contrastive Co-training",
        "paper_abstract": "Most existing point cloud detection models require large-scale, densely annotated datasets. They typically underperform in domain adaptation settings, due to geometry shifts caused by different physical environments or LiDAR sensor configurations. Therefore, it is challenging but valuable to learn transferable features between a labeled source domain and a novel target domain, without any access to target labels. To tackle this problem, we introduce the framework of 3D Contrastive Co-training (3D-CoCo) with two technical contributions. First, 3D-CoCo is inspired by our observation that the bird-eye-view (BEV) features are more transferable than low-level geometry features. We thus propose a new co-training architecture that includes separate 3D encoders with domain-specific parameters, as well as a BEV transformation module for learning domain-invariant features. Second, 3D-CoCo extends the approach of contrastive instance alignment to point cloud detection, whose performance was largely hindered by the mismatch between the fictitious distribution of BEV features, induced by pseudo-labels, and the true distribution. The mismatch is greatly reduced by 3D-CoCo with transformed point clouds, which are carefully designed by considering specific geometry priors. We construct new domain adaptation benchmarks using three large-scale 3D datasets. Experimental results show that our proposed 3D-CoCo effectively closes the domain gap and outperforms the state-of-the-art methods by large margins. \n",
        "paper_acceptance": "accept",
        "meta_review": "This work addresses a practically important domain adaptation problem in 3D point cloud detection. The authors establish a sound perspective to this specific problem, and propose a new and effective framework that is not simply an extension of existing UDA algorithms to point cloud detection. The authors also construct new datasets to promote future research on this topic. \n\nAll reviewers agree on the paper\u2019s main contributions. They also recommend further polishing of the paper writing. AC agrees with the reviewers, and recommend acceptance of the paper. \n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "dye4q5qlMwS",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_iH1_KBzbwQq",
                "title": "",
                "comment": "The paper proposed an unsupervised domain adaptation method for point cloud detection. The method is built on several effective strategies, including BEV feature representation, pseudo labels, instance contrastive alignment and hard mining.  Novelty: the instance contrastive joint training is interesting and novel according to my knowledge.\n\nSignificance: It clearly improves the baseline (trained only on source domain) and outperforms the existing unsupervised domain adaptation methods.\n\nPost Rebuttal: My concerns about pseudo labels have been addressed in the rebuttal. My final opinion: this is a valuable paper but not a breakthrough in the field. 1) Details about the pseudo label generation need to be described.  How did you run Eq.(4)?\n\n2) How to process or avoid the possible errors of the pseudo labels? How do such pseudo-label errors influence the final results? It\u2019s good to have some analysis about the relationship between the quality of the pseudo labels and the final performance of the proposed model.\n\n3) Synthetic/simulation-to-real generalization experiments are interesting. If possible, it would be nice to see some tests under this setting.\n",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "ZjyKacXt-d7",
                "writer": "official_reviewer",
                "reply_to": "quGcRNhMRej",
                "title": "Thanks for the response!",
                "comment": " Thanks for the response! All my concerns have been addressed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "quGcRNhMRej",
                "writer": "author",
                "reply_to": "iYYxUdRBsO4",
                "title": "Thanks for the comments!",
                "comment": " Dear reviewer, thanks again for your comments.\n\n1. For a clarification, in Eq.  (4), we search positive sample pairs by minimizing the cosine distance between the features of $I_i^S$ and $I_{j}^T$. Therefore, in Line 144, the cosine distance should be defined as $\\Phi(\\mathbf{x},\\mathbf{y}) = 1 - \\text{CosineSimilarity}(\\mathbf{x},\\mathbf{y})$ instead of $\\Phi(\\mathbf{x},\\mathbf{y}) = \\text{CosineSimilarity}(\\mathbf{x},\\mathbf{y})$. Thank you so much for bringing this to our attention.\n\n\n2. Yes, we agree with that, and we will fix it in the revision.\n\n\n3. Thanks for the suggestion. Indeed, co-training follows the technical details of self-training to generate pseudo-labels. It improves the latter through effective utilization of labeled source data and the hard sample mining method. We will make the comparison more concise and clear in the revision, leaving the space for adding the content included in the rebuttal. \n\n\n4. Admittedly, the current version of the manuscript has a few typoes. We thank all reviewers for pointing out these problems, and we will proofread this paper and fix them carefully.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iYYxUdRBsO4",
                "writer": "official_reviewer",
                "reply_to": "ZuNMzZZSqAy",
                "title": "Thanks for the response!",
                "comment": " Thanks for the response! It solves most of my problems. There are a few minor comments: \n1. Are cosine distance and cosine similarity the same concepts? Please improve the accuracy of the article wording.\n2. A negative sign should be added to each term in Eq. (5). Please refer to [a].\n3. The proposed co-training strategy includes the self-training technique (i.e., pseudo label generation). The use of self-training does not affect the significance of the main contributions. There is no need to spend a lot of space describing the differences between them.\n4. Please carefully proofread the paper to keep the coherence of the context.\n\n\n[a] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. \"A Simple Framework for Contrastive Learning of Visual Representations.\" (2020)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZuNMzZZSqAy",
                "writer": "author",
                "reply_to": "iY10_jtjWSg",
                "title": "Our Response to Reviewer 2DjX",
                "comment": " We thank the reviewer for the constructive comments.\n\nQ1. Description of BEV.\n\nBEV is short for Bird's Eye View. The BEV feature is widely used as a form of representation in 3D detection [14,32,33,34,43], especially for autonomous driving. It represents the scene from a view of a high angle that has the potential to avoid scale ambiguity and occlusions of the objects.\n\n&nbsp;\n\nQ2. (1) 3D features vs. BEV features; (2) Spherical convolutions.\n\n(1) It is a common practice in existing 3D detection methods [14,32,34,43] to convert 3D LiDAR points to BEV features, which have the following properties: \n\n- High efficiency: The use of BEV features greatly accelerates the training process, especially compared with point-based methods that generate proposals from each LiDAR point. In practice, we find that training a detection model based on 3D features is very time-consuming and not affordable for large-scale datasets such as nuScenes [1] and Waymo [26].\n\n- Negligible information loss in cross-domain scenarios: In Fig. 1, the 3D domain shift tackled by this paper is mainly caused by different sensor configurations or environments, instead of the intrinsic geometry of objects. Although the transformation of 3D features into 2D space inevitably leads to the information loss in the height dimension, in real auto-driving scenarios, such a loss would not enlarge the domain shift. It is negligible as the domain shift does not mainly exist in the height dimension of the object geometry, which can be learned and inferred from BEV features. \n\n- Single-domain discriminability: The BEV features can offer more compact and global representations of the scene, making it easier for the model to deal with object occlusion. Therefore, it does not reduce the discriminability of the features, nor single-domain performance.\n\n(2) As a classic technique for learning 3D features, the spherical convolution is based on a spherical kernel that updates the features of each point through a graph network. Although it is effective for learning 3D geometry, we find it too computation-intensive to process datasets used in this paper that have larger amounts of LiDAR points.\n\n&nbsp;\n\nQ3. The idea of separate encoders.\n\nSome existing methods for image domain adaptation also use separate encoders to learn domain-specific features [b,c,d], which will be included as related work in the revision. However, to the best of our knowledge, 3D-CoCo is the first one to use such an architecture in 3D scenarios. Different from the previous literature in this field [35], here, the use of separate encoders is motivated by our key observation that  3D transfer learning is hampered by the lack of transferability in low-level 3D features. Further, the separate 3D encoders serve as the base of the entire co-training framework. From an empirical view, they enable 3D-CoCo to improve both in-domain discriminability and cross-domain transferability of the features.\n\n&nbsp;\n\nQ4. In-domain and cross-domain performance.\n\nIn Section 4.3, after the co-training phase, we evaluate the model, which has separate encoders and a domain-agnostic BEV transformation module, respectively on the source domain test set (in-domain) and the target domain test set (cross-domain). \n\nIn Table 2, we show that the co-training improves the performance on both test sets, which may be counter-intuitive as most domain adaptation methods compromise between the in-domain discriminability and cross-domain transferability of the learned features.  \n\n&nbsp;\n\nQ5. (1) Definition of co-training; (2) Co-training vs. self-training.\n\n(1) Co-training refers to the entire training strategy in Alg. 1 that jointly uses labeled source data and unlabeled target data during the training phase. This is done by matching similar source and target samples and closing their feature distance through the contrastive loss. \n\n(2) Please refer to our response to Reviewer qi8q (Q2). \n\n&nbsp;\n\nQ6. (1) Bi-directional knowledge sharing; (2) Random object scaling (ROS).\n\n(1) Empirically, bi-directional knowledge sharing (Line 112) means that the co-training improves the performance on both source and target test sets. Based on the separate encoders, this is mainly realized by performing contrastive alignment between source and target data, and optimizing the model with data input from different domains under a unified contrast loss term. Thus, the so-called bi-directional knowledge sharing is different from typical multi-task learning methods.\n\n(2) From SN [28], the different object size is an essential part of domain shift. To this end, we use the ROS from ST3D [35] as an alternative technique upon 3D-CoCo, which leads to a tradeoff between in-domain and cross-domain performance. In Table 2, for a comprehensive comparison, we use two different ranges of the ROS factor, one for better adaptation results, while the other for better source performance. We observe that in both settings, 3D-CoCo outperforms the Source Only baseline remarkably on the target test set.\n\n&nbsp;\n\nQ7. (1) The warm-up stage; (2) Notation of augmented target set.\n\n(1) In Fig. 4 (a1), the performance fluctuates in the first 15 epochs and then converges stably before updating the pseudo-labels. Thus, warming-up the model is necessary. It can improve the efficacy of the first pseudo-labels update stage by avoiding a very ill-defined detection model. An empirical parameter for ending the warm-up stage is at half of the total epochs.\n\n(2) Thanks for the suggestion. We'll fix it in the revision.  \n\n&nbsp;\n\nQ8. Expanding $L_\\text{cls}^S$, $L_\\text{loc}^S$, and $L_\\text{reg}^T$.\n\n**Please use Safari or Firefox to display the equations in LaTeX properly.**\n\nWe use CenterPoint [38] as the base detector to predict Gaussian heatmaps at each annotated object center $q_i$ for each class $c_i\\in \\{1\\cdot\\cdot\\cdot |C|\\}$, where $i$ is the object index. For pixel position $p$ in the heatmap of category $k$, we have $Y_{p,k} = \\max_{i:c_i=k}\\exp(-\\frac{{(p-q_i)}^2}{2\\sigma_i^2})$, where $\\sigma_i$ is the size of the object, and $\\max_{i:c_i=k}$ covers all objects under category $k$. We define $L_\\text{cls}^S$ in a form of focal loss:\n\n$$L_\\text{cls}^S =  -\\frac{1}{N^S}\\sum_{p,k} (1-\\hat Y_{p,k}^S)^{\\alpha} \\log(\\hat Y_{p,k}^S) , \\text{if~} Y_{p,k}^S = 1,$$\n\n$$L_\\text{cls}^S = -\\frac{1}{N^S} \\sum_{p,k} (1-Y_{p,k}^S)^{\\beta}(\\hat Y_{p,k}^S)^{\\alpha}\\log(1-\\hat Y_{p,k}^S), \\text{otherwise},$$\n\nwhere $\\alpha=2$, $\\beta=4$, and $N^S$ is the number of objects. \n\nThe other two loss terms regress the size map $Z$, center offset map $O$, and rotation map $A$:\n\n$$L_\\text{loc}^S = \\frac{1}{N^S}\\sum_{i=1}^{N^S}\\lambda_X|\\widehat{X}_{q_i}^S -X_i^S|, X = \\{Z,O,A\\}.$$\n\n$$L_\\text{reg}^T = \\frac{1}{N^T}\\sum_{i=1}^{N^T}\\lambda_X|\\widehat{X}_{q_i}^T -X_i^T|, X = \\{Z,O,A\\}.$$\n\n&nbsp;\n\nQ9. (1) Feature extraction methods; (2) Analyses of $R$.\n\n(1) We use another feature extraction method based on average pooling as a compared method.\n\nOn the nuScenes $\\rightarrow$ KITTI benchmark:\n\n| Method | $\\text{AP}_\\text{BEV}$ (%) | $\\text{AP}_\\text{3D}$ (%)\n| :-----| ----: | :----:\n| Pooling-based | 75.8 | 62.9 \n| Keypoint-based (ours) | 77.1 | 65.6\n\n(2) The sensitivity analysis of $R$: \n\n| Method | $\\text{AP}_\\text{BEV}$ | $\\text{AP}_\\text{3D}$\n| :-----| ----: | :----:\n| R=3 | 78.4 | 64.1\n| R=5 | 75.9 | 62.4\n| R=7 | 77.1 | 65.6\n| Source only | 45.2 | 32.6\n\n&nbsp;\n\nQ10. Notations in Eq. (5).\n\nIn our response to Reviewer qi8q (Q1), we correct the notations of Eq. (5) and provide detailed explanations about the motivation of the contrastive loss. A similar idea of contrastive instance alignment has also been applied in 2D tasks [31].\n\n&nbsp;\n\nQ11. (1) Notation; (2) Sensitivity analyses of $\\tau$ and $\\lambda$.\n\n(1) Thanks for the suggestion. We'll fix it in the revision. \n\n(2) On nuScenes $\\rightarrow$ KITTI, by setting $\\lambda$ to 0.5:\n\n| Method | $\\text{AP}_\\text{BEV}$ | $\\text{AP}_\\text{3D}$ |\n| :-----| ----: | :----:\n| $\\tau$ = 0.01 | 75.7 | 61.7\n| $\\tau$ = 0.07 | 77.1 | 65.6\n| $\\tau$ = 0.2 | 74.7 | 61.4\n|Source only | 45.2 | 32.6\n\nBy setting $\\tau$ = 0.07:\n\n| Method | $\\text{AP}_\\text{BEV}$ | $\\text{AP}_\\text{3D}$ |\n| :-----| ----: | :----:\n| $\\lambda$ = 0.25|76.0|62.6\n| $\\lambda$ = 0.5|77.1|65.6\n| $\\lambda$ = 1.0|76.1|63.5\n\n&nbsp;\n\nQ12. Hard sample mining (HSM). \n\nLine 159: We do not use any specific metric to quantify and differentiate easy/hard samples, but intuitively consider the specific forms of possible geometric domain shift for HSM,. \n\nLines 166/169: We describe how to drop out points and how to select viewpoints for HSM in the supplementary material. \n\nOther related references include point cloud data augmentation methods [32, A], which enhance the data diversity to improve the single-domain performance, while we use the transformed hard samples to further reduce the domain shift.\n\n&nbsp;\n\nQ13. (1) HSM on source domain; (2) HSM vs. the consistency regularization [f,g].\n\n(1) We agree that augmenting the source domain is an effective way to improve generalization ability. However, the proposed HSM specifically focuses on reducing the distribution mismatch caused by the limited diversity of target pseudo-labels. HSM can be combined with any other augmentation methods performed on the source domain.\n\n(2) The consistency regularization introduces data perturbations and forces the model output to remain unchanged. These methods use common augmentation strategies (e.g., flipping and cropping). By contrast, our method creates fictitious hard samples by considering the priors of geometry variations. \n\n&nbsp;\n\nQ14. Pseudo-label update.\n\nPlease refer to our response to reviewer RhUF (Q1).\n\n&nbsp;\n\nQ15. Dataset splits.\n\nIn accordance with the common practice [35], each domain is split into training and validation sets, and we evaluate 3D-CoCo on the validation set of target domains.\n\n&nbsp;\n\n[A] Choi, J.,  Y. Song, and  N. Kwak. \"Part-Aware Data Augmentation for 3D Object Detection in Point Cloud.\" (2020).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "MYo3GS_WG9",
                "writer": "author",
                "reply_to": "dye4q5qlMwS",
                "title": "Our Response to Reviewer RhUF",
                "comment": " We thank the reviewer for the constructive comments.\n\nQ1. (1) Details about pseudo-label generation; (2) How to run Eq. (4)?\n\n(1) We follow the pseudo-label generation process of ST3D [35], which has two parts. First, before the warm-up training stage, we use the source detection model to initialize the pseudo-labels. Second, after the warm-up stage, we progressively refine the pseudo-labels at each training epoch, which specifically has two steps:\n\n- Matching and merging the currently updated pseudo-labels with the previous ones (termed Memory Ensemble by ST3D): This is done by calculating a 3D IoU matrix of the object detection boxes under two adjacent updates of pseudo-labels, and treating pairs that exceed a certain threshold as matching pairs. We then take the boxes with higher confidence as the current pseudo-labels.\n\n- Marking the status of the unmatched pseudo boxes of the first step to determine whether to discard them (termed Memory Voting by ST3D): When a new unmatched box appears, we assume that it is a correct sample caused by the model update, and we thus save the box and record its the number of its unsuccessful matches. If the box is unmatched for a long time after several epochs, it is considered to be an incorrect sample and will be permanently deleted.\n\n(2) Given a source sample $I_i^S$, Eq. (4) searches for its positive target sample $I_{j^\\star}^T$ to optimize the contrastive loss in Eq. (5). Concretely, we calculate the cosine distances, denoted by $\\Phi(\\cdot, \\cdot)$, between $I_i^S$ and all target samples under the same category $c$ (indicated by the same pseudo-labels), among which we select the one with the smallest distance as $I_{j^\\star}^T$, where $j^\\star = \\mathop{\\arg\\min}\\limits_{1\\leq j\\leq N_c^T}\\{\\Phi(I_i^S, I_j^T)\\}$. Please use the Safari or Firefox browser to display the equations in LaTeX properly.\n\n&nbsp;\n\nQ2. (1) How to process or avoid the possible errors of pseudo-labels? (2) Analyses of the relationship between the quality of pseudo-labels and the final performance.\n\n(1) In our domain adaptation scenarios, the errors of pseudo-labels are mainly caused by the distribution shift across domains. In 3D-CoCo, they are processed in three ways:\n\n- The progressive updates of pseudo-labels are helpful to improve the quality of pseudo-labels and eliminate the existing errors (as explained in the above response).\n\n- The proposed co-training framework uses the source labeled data as strong guidance, which has great effects on the feature learning process of the target data. Therefore, the quality of pseudo-labels is gradually improved during the training phase. \n\n- The proposed method of hard sample mining is helpful to enrich the pseudo-labels and can further avoid errors of pseudo-labels caused by the distribution shift across domains. \n\n(2) The visualization in Fig. 4 shows that the three contributions mentioned above are indeed helpful to avoid the errors of pseudo-labels, and the final performance is largely dependent on the quality of pseudo-labels. More specifically,\n\n- We use the ratio of true positive and false positive predictions (#TPs/#FPs) to indicate the quality of pseudo-labels. By comparing the final performance in Fig. 4 (a1) and (b1), as well as the corresponding quality of pseudo-labels in (a2) and (b2), we can see that high-quality pseudo-labels lead to better detection results. \n\n- The progressive refinement of pseudo-labels is helpful to improve the label quality and the detection results. The strong evidence is that at around the 15th training epoch in Fig. 4 (b2) after the warm-up stage, the green curve (i.e., co-training with label update) and the blue curve (i.e., co-training with label update and hard sample mining) grow rapidly, and the model performance in (b1) grows simultaneously. During these periods, the pseudo-labels are progressively refined with hard sample mining in each training epoch. Please refer to our response to Reviewer DPJL for more analyses.\n\n&nbsp;\n\nQ3. Simulation to real. \n\nWe agree that the simulation-to-real is an interesting transfer learning setup. However, due to time constraints, we plan to conduct such experiments in future work and hope to provide results in the revision.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HNIYEta4eeS",
                "writer": "author",
                "reply_to": "_FO5Nv6f-R",
                "title": "Our Response to Reviewer qi8q",
                "comment": " Thank you very much for the encouraging and constructive comments.\n\nQ1. Symbols and terms in equations (4) and (5).\n\nEq. (4) searches for the positive sample pairs and Eq. (5) uses them to optimize the contrastive loss. Below we clarify the symbols and terms in these equations. **Please use the Safari or Firefox browser to display the equations in LaTeX properly.**\n\n- $I_i^S$ ($I_j^T$): a sample of the source (target) domain.\n\n- $N_c^S$ ($N_c^T$): the total number of samples of the source (target) domain from the category $c$.\n\n- $|C|$: the number of categories.\n\n- $\\Phi(\\mathbf{x},\\mathbf{y}) = \\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\left \\|\\mathbf{x}\\right\\|\\left \\|\\mathbf{y}\\right\\|}$: the cosine distance between the features of $I_i^S$ and $I_{j}^T$.\n\nIn Eq. (4), we have $j^\\star = \\mathop{\\mathbf{P}^c(i)} = \\mathop{\\arg\\min}\\limits_{1\\leq j\\leq N_c^T}\\{\\Phi(I_i^S, I_j^T)\\}, ~~ 1\\leq i\\leq N_c^S, ~~ c=1, 2, \\ldots, |C|$. For each $I_i^S$, it finds a positive sample $I_{j^\\star}^T$ from the target domain under the same category of $I_i^S$ (indicated by the same pseudo-labels). \n\nAs for Eq. (5), we correct the typos in the manuscript and re-write the three loss terms as:\n\n$\\mathcal L_\\text{intra-class}(S,T) = \\sum_{c=1}^{|C|}\\sum_{i\\in N_c^S}\\log\\frac{\\exp(I^S_i\\cdot I^T_{j^\\star}/\\tau)}{\\exp(I_i^S\\cdot I_{j^\\star}/\\tau) + \\sum_{j\\in N_{\\tilde{c}}^T} \\exp(I_i^S\\cdot I_j^T)},$\n\n$\\mathcal L_\\text{inter-class}(S) = \\sum_{c=1}^{|C|}\\sum_{i\\in N_c^S, j\\in N_c^S, i\\neq j}\\log\\frac{\\exp(I^S_i\\cdot I^S_j/\\tau)}{\\exp(I_i^S\\cdot I_j^S/\\tau) + \\sum_{j\\in N_{\\tilde{c}}^{S}} \\exp(I_i^S\\cdot I_j^S)},$\n\n$\\mathcal L_\\text{inter-class}(T) = \\sum_{c=1}^{|C|}\\sum_{i\\in N_c^T, j\\in N_c^T, i\\neq j}\\log\\frac{\\exp(I^T_i\\cdot I^T_j/\\tau)}{\\exp(I_i^T\\cdot I_j^T/\\tau) + \\sum_{j\\in N_{\\tilde{c}}^{T}} \\exp(I_i^T\\cdot I_j^T)}.$\n\nThe loss terms have two parts:\n\n- $\\mathcal L_\\text{intra-class}(S,T)$ encourages to close the intra-class distances of samples in the same category cross domains. Note that $j^\\star=\\mathbf{P}^c(i)$ is derived from Eq. (4) and $\\tilde{c}=|C|\\setminus c$ denotes all rest categories except that of the positive pair $(I_i^S, I_{j^\\star}^T)$. All samples in $\\tilde{c}$ are considered as negative samples.\n\n- $\\mathcal L_\\text{inter-class}(S)$ and $\\mathcal L_\\text{inter-class}(T)$ encourage to close the inter-class distances of samples in the same category within same domain. Like $\\mathcal L_\\text{intra-class}(S,T)$, samples in the positive pairs are from the same category and those in the negative pairs are from different categories.\n\n&nbsp;\n\nQ2. Co-training vs. self-training.\n\nIn addition to the comparisons in Lines 294-298 in the manuscript, here we summarize the differences between co-training and self-training planned to be included in the revision.\n\nSelf-training (typically for single-domain semi-supervised learning):\n\n- A teacher model is trained on the labeled data. \n\n- The teacher model generates pseudo-labels on unlabeled data.\n\n- A student model is trained to optimize the loss on human labels and pseudo-labels jointly. \n\nCo-training (for cross-domain transfer learning, also shown in Alg. 1):\n\n- A source model is trained on the labeled data of the source domain. \n\n- A target model initialized with the source model generates (updates) pseudo-labels for unlabeled data of the target domain.\n\n- The pseudo-labels are then used to mine the hard samples to augment the target domain.\n\n- The target model and the source model are co-trained with the detection loss (for source data), a regularization term (for target data), as well as the contrastive loss (for both). \n\n- Go back to the second step and then iterate.\n\nThe most essential difference between the two strategies is whether the source (teacher) model and the target (student) model are jointly learned. The contrastive loss transfers knowledge from the source domain by narrowing the distance between the sample features of different domains under the same category (indicated by pseudo-labels), thus facilitating the learning process on the unlabeled target set. As illustrated by the empirical results in Fig. 4 and the analyses of error bars in the supplementary material, in contrast with self-training, the proposed co-training strategy not only improves the accuracy but also performs more stably.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fdcpYfobJgl",
                "writer": "author",
                "reply_to": "703FMiWz9t",
                "title": "Our Response to Reviewer DPJL",
                "comment": " We thank the reviewer for the constructive comments.\n\nQ1. To combine the proposed approach with PV-RCNN for a fair comparison with ST3D.  \n\nPer the reviewer's request, we build the proposed 3D-CoCo on PV-RCNN (i.e., 3D-CoCo*) to make a fair comparison with ST3D [35]. Due to the time constraint of the rebuttal period, we partly build our method without using the hard sample mining scheme. The experimental result on the nuScenes $\\rightarrow$ KITTI benchmark is represented as follows: \n\n| Method | $\\text{AP}_\\text{BEV}$ (%) | $\\text{AP}_\\text{3D}$ (%)\n| :-----| ----: | :----:\n| Source Only | 60.48 | 49.47 \n| ST3D [35] | 84.29 | 72.94\n| 3D-CoCo* | 85.01 | 74.12 \n| Oracle | 88.98 | 82.50\n\nFrom above, 3D-CoCo is superior to ST3D on the new base detector of PV-RCNN. It is worth noting that the proposed method of hard sample mining has the potential to further improve the adaptation performance. We'll implement the full 3D-CoCo approach based on PV-RCNN in the revision.\n\n&nbsp;\n\nQ2. (1) Sensitivity analyses of 3D-CoCo to the pseudo-label generation thresholds; (2) how to determine the thresholds?\n\n(1) Fig. 4 (c2) provides the sensitivity analyses of the high-pass threshold for generating pseudo-labels (i.e., the confidence score ranging from 0.5 to 0.8). By comparing the results with those of self-training, as shown in Fig. 4 (c1), we can see that: \n\n- 3D-CoCo is robust to different threshold values. It outperforms the Source Only baseline consistently, with all possible values, and significantly, with 50%-100% final performance gains in $\\text{AP}_\\text{3D}$. In contrast, self-training fails to improve the Source Only baseline with some threshold values.\n\n- At different thresholds, the performance of 3D-CoCo is stably increased throughout the training procedure, while the training process of self-training is quite unstable.\n\n(2) We adopt the value of the low-pass threshold (0.2) from the existing work of ST3D [35], because the empirical evidence given by ST3D shows that the sensitivity of the detection model to the low-pass threshold is lower than the high-pass threshold. \n\nBesides, we follow the hyper-parameter tuning strategy of ST3D and set the high-pass threshold to 0.7 on all benchmarks. We observe that:\n\n- In practice, as mentioned above, our method is more robust to different values of the high-pass threshold than the self-training framework of ST3D.\n\n- We only determined the high-pass threshold on the first benchmark, and found it robust on other benchmarks, in the sense that no further tuning on these hyper-parameters is required.\n&nbsp;\n\n&nbsp;\n\nQ3. Analyses and visualizations about pseudo-label adjustment during the training phase to illustrate why and how the proposed method could achieve good results with low-quality initialized pseudo-labels.\n\nThe visualization in Fig. 4 shows that, in 3D-CoCo, the progressive update of pseudo-labels is closely related to the final results, and the key to achieving good results with low-quality initialized pseudo-labels lies in three aspects:\n\n(1) In Fig. 4 (b2), we use the ratio of true positive and false positive predictions to indicate the quality of pseudo-labels.\n\n- At the beginning of the training phase, we have low-quality initialized pseudo-labels due to the large distribution shift across domains. \n\n- At around the 15th training epoch after the warm-up stage (w.r.t. Line 5, Alg. 1), #TPs/#FPs of the blue and green curves grow rapidly, and the model performance in Fig. 4 (b1) grows simultaneously. During these periods, we adjust pseudo-labels in each training epoch (Line 7, Alg. 1).\n\nAnalyses: In 3D-CoCo, the iterative training scheme of (i) pseudo-label adjustment and (ii) model optimization gradually improves the quality of pseudo-labels, which has great effects on the domain adaptation performance.\n\n(2) In Fig. 4 (a2), we validate the effectiveness of the contrastive loss by only using the low-quality initialized pseudo-labels throughout the training phase, without pseudo-label adjustment. We can see that, compared with the self-training method, #TPs/#FPs of 3D-CoCo increases more rapidly during the training phase, and the model performance in Fig. 4 (a1) also grows remarkably. \n\nAnalyses: The contrastive loss contributes to adapting the feature distribution of the target domain to the source domain. Even without high-quality pseudo-labels of the target data, it improves the detection results of target samples under the guidance of source labeled data. \n\n(3) Also shown in Fig. 4 (b1-b2), the proposed method of hard sample mining (HSM) further promotes the pseudo-label adjustment in the training phase, thus improving the domain adaptation performance.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_FO5Nv6f-R",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_iH1_KBzbwQq",
                "title": "",
                "comment": "This work presents an unsupervised domain adaptation for point cloud detection in the autonomous driving scenario, which employs a domain-specific 3D encoder for domain-specific feature extraction, a shared BEV feature transformation module for domain-invariant instance-level feature alignment. It adapts the contrastive instance alignment with geometry priors to align the BEV feature distributions induced by pseudo-labels and the true distribution. The proposed 3D-CoCo works on three transferring settings, effectively closing the domain gaps and can outperform recent SOTA methods.  This study is useful for researchers in both fields of unsupervised domain transfer and point cloud detection in the autonomous driving scenario. The proposed 3D-CoCo tries to utilize the domain knowledge induced by this special task to produce an effective unsupervised domain adaptation, rather than simply adopts the tricks used in 2D unsupervised domain adaptation. To be specific, it is nice to see that 3D-CoCo would like to specifically handle the domain drift caused by geometric variations, and designs a hard negative mining strategy by examining the statistics of pseudo-label quality. The experiments show good results on three datasets collected by heterogeneous LiDAR sensors, and outperform recent UDA methods on point cloud detection. \n\nBut the presentation needs to be polished. At first, symbols and terms in equations (4) and (5) may require a careful definition and remove some careless misuses. And the functionalities of the introduced terms and symbols should be briefly discussed. The authors claimed the effectiveness of co-training beyond self-training, but how they are especially different, or in what aspects co-training is better should be explicitly stated.\n\nOverall, this paper is well written, clearly illustrated, and appropriately structured.  It has included the limitations. The potential negative societal impact of their work  may not be available.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "703FMiWz9t",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_iH1_KBzbwQq",
                "title": "",
                "comment": "This paper proposes a domain adaptation point cloud object detection method termed 3D-COCO to deal with geometry shifts caused by different physical environments or LiDAR sensors. The authors use the shared BEV module to learn the domain-invariant for both source and target domain, while the augmented contrastive alignment schemes are also introduced to discover the hard samples. The mismatch is greatly reduced by 3D-CoCo with transformed point clouds. The experimental results on three benchmarks demonstrate the effectiveness of the proposed method.  Strength\uff1a\n\n+ The paper is well written and easy to follow.\n+ The proposed framework seems simple yet effective.\n+ The thinking and analysis about the hard instance mining are interesting and the corresponding solution achieves a great improvement of the performance.\n\n\n\nWeakness:\n- For fair comparison to ST3D, could the authors further provide the solution to combine the proposed method with PV-RCNN  and further show the performance comparison?\n- Is the proposed method sensitive to the pseudo-label generation threshold? How to determine the high-pass and low-pass threshold in line 210?\n- In line 223, the authors claim that their model achieves good performance even starting with low-quality pseudo labels. Could you please give some analysis and visualization about pseudo label adjustment during the training phase to further illustrate why and how the proposed method could achieve good results with low-quality initialized pseudo labels? The authors have pointed out the limitations of their work.\n\nSince it is just a basic method to deal with 3D point cloud detection, it has nothing to do with the potential negative societal impact.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "iY10_jtjWSg",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_iH1_KBzbwQq",
                "title": "",
                "comment": "This paper contributes as an early study of unsupervised domain adaptation (UDA) on 3D point cloud detection, an important step towards zero cost of labeling. Different from 2D image data that have regular grid structures, 3D point cloud data could have huge geometric variations such as point density and occlusion due to different physical environments or LiDAR sensor configurations; consequently, the low-level features of point cloud representation may be not transferable across domains. Taking it into account, this paper uses two individual encoders for the source and target domains to capture domain-specific characteristics and a shared Bird-Eye-View (BEV) transformation module to learn domain-agnostic BEV features, which have similar grid structures as images and thus are more transferable. Specifically, this paper extends contrastive instance alignment to point cloud detection, based on BEW features of foreground and background proposals. Direct use of contrastive instance alignment can cause the mismatch in point density and occlusion ratio between pseudo-labeled and true samples distributions in the target domain. To address it, this paper proposes to augment positive easy target samples (with dense points and complete geometry) by their artificial counterparts with sparse points and occlusion. Moreover, this paper constructs a new benchmark using three large-scale datasets. Empirical results show that the proposed method is effective and achieves the new state of the art.  Although the contributions and novelty of this paper are considerable, there are still many problems.\n\n1. The full name should be mentioned when BEV is shown for the first time in Section 1, where it would be better to provide a brief description of BEV and its related references for a wider range of audiences.\n\n2. It would be better to compare the high-level 3D features with the BEV features. The high-level 3D features are extracted by the same but deeper network layers than the low-level ones. The 3D domain shift is mapped into the 2D one, causing information loss.  Would it reduce the feature discriminability? Would it affect the single-domain detection performance, e.g., cases of a general detection setup in individual domains? It is also interesting to know how the features extracted by spherical convolutions [a] behave.\n\n3. The idea of separate encoders is similar to that of [b,c,d] for 2D UDA. The authors should consider them as related works and make comparisons and discussions appropriately. \n\n4.  It is interesting to approximately quantify the distribution discrepancy across domains based on the performances of passing source samples through the target encoder and passing target samples through the source encoder. In Section 4.3, the in-domain performance is evaluated on the target domain; however, according to the previous descriptions, the cross-domain performance should be evaluated on the target domain. There exists a contradiction and a clarification is required.\n\n5. Does the \"co-training\" in this paper mean two individual encoders for the source and target domains? Or the training strategy? This paper also adopts the pseudo-labeling technique from self-training. In Lines 56-59, the difference is not clear enough. \n\n6. In Lines 109-112, \"bi-directional knowledge sharing\" is similar to multi-task learning [e]? Why does it hold? Are there any related works that support this argument? In the ablation studies, it seems that the random object scaling has a great effect on the source performance and this has nothing to do with the proposed method. \n\n7. In Algorithm 1, this paper conducts three-stage training. why not two stages? The warm-up stage introduces another set of training hyperparameters. Besides, the augmented target set is denoted by the original one? The notations should be different.\n\n8. It would be more clear to expand the losses of L_cls^S, L_loc^S, and L_reg^T.\n\n9. In Section 3.2, this paper uses a keypoint-based feature extraction approach. Comparisons with other kinds of feature extraction approaches are needed. Besides, the model sensitivity to the hyperparameter R is to be examined.\n\n10. In Eq. (5), some notations are confusing, such as the summation range in the denominator in L_intra and L_inter. A negative sign is missing in L_intra and L_inter? Otherwise, samples of the same classes are pushed away? In L_intra, samples of other classes should be used as negative ones whereas this paper does not make it clear. In L_inter, other samples in the same class are used as negative ones? Is this the typical contrastive learning method of instance discrimination from self-supervised learning? How can this method increase the inter-class distance between different categories within the same domain? Are there any references for contrastive instance alignment? The main component of the proposed algorithm is so ambiguous that the experimental results may be not so convincing. \n\n11. In Eq. (5), the domain \\mathcal{D} is similar to model D; it would be better to use different notations. In Eq. (6), \"arg\" is redundant. The model sensitivity to the temperature \\tao and the hyperparameter \\lambda is to be examined. \n\n12. In Line 159, is there a specific rule to distinguish between easy and hard samples? Please make it clear. In Line 166, how many points are discarded? In Line 169, how to calculate the viewpoint of a certain sample? How many viewpoints do you select to discard? Are there any solid principles to make decisions? Are there any related references? Please make it clear. \n\n13. In Lines 170-173, the proposed augmentation can also be done in the source domain since source samples can be viewed as easy ones. Consequently, the distribution mismatch between the source and target domains can be further reduced. It is interesting to see whether the model performance can be further improved. The proposed augmentation strategy is somewhat similar to consistency regularization [f,g] from semi-supervised learning, which enforces consistent predictions between weakly and strongly augmented versions of the same sample image. Maybe the related works should be considered and discussed.\n\n14. In Line 179, can you describe how to update the pseudo labels in detail? \n\n15. Is each domain split into training and test sets? Is the performance evaluated on the training or test set of the target domain?\n\n\n[a] H. Lei, N. Akhtar and A. Mian, \"Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2020.2983410.\n\n[b] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. 2016. Domain separation networks. In Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS'16). Curran Associates Inc., Red Hook, NY, USA, 343\u2013351.\n\n[c] E. Tzeng, J. Hoffman, K. Saenko and T. Darrell, \"Adversarial Discriminative Domain Adaptation,\" 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2962-2971, doi: 10.1109/CVPR.2017.316.\n\n[d] W. Chang, H. Wang, W. Peng and W. Chiu, \"All About Structure: Adapting Structural Information Across Domains for Boosting Semantic Segmentation,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1900-1909, doi: 10.1109/CVPR.2019.00200.\n\n[e] Caruana, R. Multitask Learning. Machine Learning 28, 41\u201375 (1997). https://doi.org/10.1023/A:1007379606734.\n\n[f] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In Proc. Int. Conf. on Learn. Rep., 2020.\n\n[g] S. Roy, A. Siarohin, E. Sangineto, S. R. Bul\u00f2, N. Sebe and E. Ricci, \"Unsupervised Domain Adaptation Using Feature-Whitening and Consensus Loss,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 9463-9472, doi: 10.1109/CVPR.2019.00970. Annotating point clouds is more difficult and expensive than annotating images for object detection tasks. Good 3D UDA methods are desired to reduce the annotation efforts. On the constructed new benchmark of three autonomous driving datasets, this paper achieves a significant gain in detection performance, one step closer to the goal. As for the limitation, this paper has mentioned that the proposed method takes more memory footprint than existing self-training methods at the training time whereas does not address it. The potential negative societal impact of their work has not been mentioned. The suggestions for improvement are provided in Main Review.",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This work",
                "Sentiment Expression": "addresses a practically important domain adaptation problem",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The authors",
                "Sentiment Expression": "establish a sound perspective",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The authors",
                "Sentiment Expression": "construct new datasets to promote future research on this topic",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper writing",
                "Sentiment Expression": "recommend further polishing",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "recommend acceptance",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "OgCcfc1m0TO": {
        "paper_id": "iclr_2022_OgCcfc1m0TO",
        "paper_title": "Learning to Prompt for Vision-Language Models",
        "paper_abstract": "Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named \\emph{context optimization (CoOp)}. The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17\\% with the highest reaching over 50\\%). CoOp also exhibits strong robustness to distribution shift.",
        "paper_acceptance": "Reject",
        "meta_review": "Given the increasing scale of large models (e.g. CLIP), there's an argument that we need better automated techniques for properly utilizing (prompting) these models. Given the success of prompt learning within pure NLP models, the authors apply the same approach to the V+L domain and show that it also is applicable here.  Generally, reviewers felt that the results were clear and thorough, yet technically limited.  The approach is not novel and the result not surprising.  There is a documentary benefit to having this work out in the community for others to reference and extend.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "GClEN1b4a1M",
                "writer": "official_reviewer",
                "reply_to": "X5WAqRUtXIg",
                "title": "Thanks for the info!",
                "comment": " Hi there --- just confirming that I saw this and am considering your response in my continued capacity as a reviewer for this work; thank you for detailing your thoughts.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UBvArpkLtaq",
                "writer": "author",
                "reply_to": "kQJtX83Zj0",
                "title": "Response",
                "comment": " > \u200b\u200bDespite extensive evaluation, the paper presents in very low quality and lack of description of key methods and the theory behind it. The overall quality is clearly below the threshold of the ICLR standard.\n\nWe hope our answer to your main concern about the theory is resolved. The paper was carefully organized and polished, which is also endorsed by other reviewers who found our paper \u201cgenerally well-written and easy to follow.\u201d Any more advice that you think could improve the paper is welcome. We think the \u201cstrong reject\u201d rating isn\u2019t a fair assessment and hope you can re-evaluate our paper based on whether it has positive impacts on computer vision.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "X5WAqRUtXIg",
                "writer": "author",
                "reply_to": "iclr_2022_OgCcfc1m0TO",
                "title": "Response to the concern about novelty",
                "comment": " Since most reviewers share the same concern about the novelty that the idea of prompt learning has been proposed in NLP, we provide our response in this common area and hope our answer can convince the reviewers to support our paper.\n\nFirst of all, our research deals with *computer vision* problems that are significantly different from those in NLP so we hope our paper can be assessed based on whether the insights and findings are useful to the vision community. To avoid misunderstanding, we\u2019ve changed the word \u201cnovel\u201d to \u201csimple\u201d in the abstract when describing the proposed approach.\n\nSuccessfully applying ideas developed in a different domain like NLP to computer vision is *non-trivial* and could inspire new ideas that might not be possible by only focusing on technical innovation in a single field. A recent example is Vision Transformers [a], which applies the same Transformers [b] architecture developed in NLP to image recognition and has now been extended to many other computer vision problems.\n\nWe are the first to successfully apply the concept of prompt learning to computer vision, specifically to adapting large pre-trained vision-language models, which is an important topic essential for democratizing foundation models [c] (i.e., to make these models more accessible to wider communities that do not have sufficient training data and compute resources for pre-training). Technically, we propose two variants of prompt learning: unified context optimization and class-specific context optimization, which are specifically designed for solving image recognition problems. We believe the insights and findings presented in the paper are novel to computer vision and worth sharing with the community.\n\n[a] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. https://openreview.net/forum?id=YicbFdNTTy\n\n[b] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\n\n[c] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kQJtX83Zj0",
                "writer": "author",
                "reply_to": "CwHrJmanoON",
                "title": "Response",
                "comment": " Thank you for your review. We think designing efficient adaptation methods for large pre-trained vision-language models is an important step to democratize foundation models [a], which is an emerging topic in computer vision. We\u2019d like to emphasize that *our paper is the first to tackle this problem in computer vision using prompt learning*, and hope our paper can be assessed fairly based on whether the insights and findings presented in the paper are novel to the field of computer vision.\n\n[a] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.\n\nWe address your concerns below.\n\n> However, the paper claimed prompt engineering is time-consuming ... The task should be down to exploring more robust models and algorithms that can withstand these minor changes in my opinion.\n\nAutomatic prompt learning has been an established research problem in NLP (see [b] for a comprehensive survey in this topic) where the motivation has been widely recognized: prompt engineering needs labeled data and a significant amount of efforts for words tuning but does not guarantee that the resulting prompts are familiar to what the model has been learned with; so the idea is why not design an efficient algorithm to leverage the labeled data for automatic prompt learning.\n\nWe think the adaptation of large pre-trained vision-language models is a promising research direction, which is of great importance to facilitating deployments of these models in downstream applications. Being *the first to study efficient prompt learning in computer vision*, our paper provides timely insights, and importantly, demonstrates strong empirical performance for a simple approach, which we think is worth sharing with the community.\n\n[b] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2021). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.\n\n> The biggest concern is about the lack of theoretical contribution.\n\nOur prompt learning-based approach has a very simple and intuitive design, i.e., to turn context words in a prompt into a set of learnable vectors. From the learning perspective, our approach is based on supervised learning, or more formally empirical risk minimization, which is backed by the statistical learning theory [c] well known to the machine learning community. Therefore, having a theory seems trivial for prompt learning methods. To our knowledge, none of the existing prompt learning papers in NLP has a theory (please refer to the relevant references in Section 4).\n\n[c] Vapnik, V. (1999). The nature of statistical learning theory. Springer science & business media.\n\n> The main content of the methodology is broken down into two sections. The first section is just a review of CLIP which is already well-known. The key content of context optimisation in 2.2 is just half a page. The description is rough.\n\nThe purpose of putting a review of CLIP is to introduce the model architecture\u2014on top of which our model is built\u2014and the prompting-based zero-shot inference. We think the organization is clear and useful for readers who might not know CLIP\u2019s technical details.\n\nOur approach is quite simple and easy to understand, particularly for readers who are already familiar with CLIP. Therefore, we chose to leave more space for the experiments.\n\n> It is not super clear why adding extra dimensions (claimed as the same number in that of word embedding) to the class token can help the prompt.\n\nAdding more tokens can increase the capacity of context, which is analogous to adding more parameters/layers in a neural network.\n\n> On page 4 above Eq.3, the concept \"unified context\" and \"class-specific context\" are not well explained.\n\nThe explanation for \u201cunified context\u201d appears right before it, \u201cNote that the context here is shared among all classes, which is called unified context ...\u201d\n\nClass-specific context is explicitly explained in the last paragraph of Section 2, \u201cAnother option is to design class-specific context \u2026\u201d\n\n> paragraph under Eq.3, \"Training is performed ...\" is redundant and not informative.\n\nThe paragraph is necessary because its purpose is to discuss how the model is trained as well as to explain the intuition why the design helps learn \u201cmeaningful\u201d context, which we think would give readers a clearer picture of our approach.\n\n> Figure 2 is not helpful in understanding the framework and looks very low-quality.\n\nThe figure is inspired by OpenAI\u2019s sketch of the CLIP model and designed to be as simple as possible. Could you please give more specific comments about which parts you think are unclear or can be improved? We are happy to revise the figure.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dBpQ5TvSpYO",
                "writer": "author",
                "reply_to": "mo5m2Y33ekj",
                "title": "Response",
                "comment": " Thank you for your review. Also thank you for endorsing our paper\u2019s motivation and the empirical results. We are very encouraged by the comments.\n\nAs the concern over the novelty was also raised by other reviewers, we provide a comprehensive discussion in the common area.\n\nWe address your remaining concerns below.\n\n> There are a lack of details for the linear probe --- how was the regularization parameter chosen?\n\nAs mentioned in the paper, we followed the same steps as in Radford et al. (2021). Please see Appendix A.3 in their original paper at https://arxiv.org/pdf/2103.00020.pdf.\n\n> The baseline for comparison to CoOp was the linear probe, which makes sense. However, I would have also liked to have seen a comparison where all the parameters of CLIP are fine-tuned --- how well does that work for few shot learning?\n\nGood question. We did tried this experiment early on where we fine-tuned all parameters in the image and text encoders. We found that this fine-tuning method did not work at all for all CLIP\u2019s (open-source) vision backbones, i.e., RN50, RN101, ViT-B/32 and ViT-B/16. Specifically, the loss initially declined for a few iterations but then quickly went up and stayed at a \u201cdead\u201d mode with zero training accuracy.\n\n> Figure 1 compares a supervised CoOp method to a zero-shot CLIP baseline. While the CoOp method is \"few shot\", in this figure, there are 16 examples per class provided, which, for some datasets may amount to hundreds or thousands of labelled examples. I would have appreciated Figure 1 comparing to the linear probe.\n\nPlease note that Fig.1\u2019s purpose is not to show a comprehensive comparison of all methods (which is detailed in the experiments section, specifically in Sec.3.1 and 3.2) but to highlight the problems of hand-crafted prompts, explaining why we should learn the prompts rather than manually design them.\n\n> Figure 5b has a similar problem with being potentially misleading: I would recommend including not only zero-shot CLIP, but also linear probe CLIP.\n\nPlease note that Fig.5b is part of the ablation studies, which mainly aims to verify whether the learning-based prompts work with different architectures. Sec.3.1 (few-shot learning) and 3.2 (domain generalization) have provided comprehensive comparisons with linear probe CLIP where the results are sufficient to justify the learning-based prompts. So repeating the experiments for linear probe CLIP across all 11 datasets and vision backbones would not give extra new insights but will certainly consume a lot of compute resources and time.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "poAn9cNoHvx",
                "writer": "author",
                "reply_to": "uLmfK84ybVF",
                "title": "Response",
                "comment": " Thank you for your review. Also thank you for endorsing the empirical results. We are very encouraged by the comments.\n\nWe address your concerns below.\n\n> Though effective, the technical novelty of this paper is quite limited. Since the soft prompt tuning approaches (e.g., Prompt Tuning [1] and P-Tuning [2]) have already been proposed in NLP domain. And CoOp adopts a very similar technique. [1] Lester, Brian, Rami Al-Rfou, and Noah Constant. \u201cThe power of scale for parameter-efficient prompt tuning.\u201d EMNLP 2021. [2] Liu, Xiao, et al. \u201cGPT Understands, Too.\u201d arXiv preprint arXiv:2103.10385 (2021).\n\nAs this concern was also raised by other reviewers, we provide a comprehensive discussion in the common area.\n\n> According to Table 4 in the paper, the nearest neighbor words of the learned context vectors rarely have practical semantic meaning. This casts doubt on using the word \u201ccontext\u201d. From this perspective, these soft prompts are just more parameters to improve the model capacity. Hence, can we say that CoOp is just a better version of fine-tuning? In other words, there might be a similar fine-tuning way to utilize additional parameters to get better performance.\n\nAs discussed in the paper, interpreting the learned tokens is difficult and using nearest words from the pre-trained embeddings might be inaccurate (so we do not recommend future work to use this interpretation method). Since the tokens are updated using knowledge (i.e., gradients) distilled from the pre-trained text encoder, it is reasonable to think that the learned tokens still reside within the pre-trained word space but contain more abstract meanings that are useful for downstream recognition.\n\nWe would also like to point out that the interpretation issue is not unique to CoOp but common to broader continuous prompt learning methods developed in NLP (which is discussed in the related work section). To our knowledge, none of the existing continuous prompt learning papers in NLP has solved the interpretation issue. We hope our openness to discuss the issue is encouraged rather than criticized as a weakness.\n\nCoOp is the first prompt learning-based adaptation method for CLIP-like foundation models [a] *in computer vision*, but certainly not the only workable solution. We think our simple design and the findings presented in the paper can be useful for the community to investigate more advanced, and effective, designs for adapting large pre-trained vision-language models\u2014which we view as an important research direction to democratize foundation models [a].\n\n[a] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "10KOSclsdSw",
                "writer": "author",
                "reply_to": "iQ0p4tsDGS",
                "title": "Response",
                "comment": " Thank you for your review. We are very encouraged by your positive comment on the solidity of our paper.\n\nWe address your concerns below.\n\n> Only the results based on CLIP are compared, and there are no more experiments of other visual-language models.\n\nFirst, we would like to clarify that we refer to vision-language models specifically as recent *CLIP-like image recognition models*, which generate classification weights directly from natural language. This concept was used in Jia et al. (2021).\n\nTo our knowledge, only CLIP (Radford et al., 2021) is open-source and has been widely used by the community. We expect our approach to also work for other similar models like ALIGN (Jia et al., 2021) and the more recent DeCLIP [a] and CLOOB [b].\n\n[a] Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., ... & Yan, J. (2021). Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm. arXiv preprint arXiv:2110.05208.\n\n[b] F\u00fcrst, A., Rumetshofer, E., Tran, V., Ramsauer, H., Tang, F., Lehner, J., ... & Hochreiter, S. (2021). CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP. arXiv preprint arXiv:2110.11316.\n\n> It is an improvement of CLIP, and the idea is similar to many existing works such as [1]. [1] Prefix-Tuning: Optimizing Continuous Prompts for Generation.\n\nAs this concern was also raised by other reviewers, we provide a comprehensive discussion in the common area.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iQ0p4tsDGS",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_OgCcfc1m0TO",
                "title": "",
                "comment": "This paper proposes a novel approach named context optimization (CoOp) for prompt engineering of vision-language pre-training models.   The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots. Strengths:\n1. This paper provides a novel approach named CoOp for prompt engineering based on  CLIP. \n2. CLIP-based experiments are sufficient to prove the advantages of CoOp.\n\nWeaknesses:\n1. Only the results based on CLIP are compared, and there are no more experiments of other visual-language models.\n2. It is an improvement of CLIP, and the idea is similar to many existing works such as [1].\n[1] Prefix-Tuning: Optimizing Continuous Prompts for Generation\n This is a somewhat novel but solid work. The comparative experiment based on clip is very sufficient, but it also lacks other important experiments, such as the effect of CoOp on other vision-language models, just as the title of the paper is learning to prompt for vision language models",
                "rating": 6,
                "confidence": 5
            },
            {
                "review_id": "uLmfK84ybVF",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_OgCcfc1m0TO",
                "title": "",
                "comment": "The paper proposes context optimization (CoOp) which learns task-aware continuous prompts to improve CLIP in terms of few-shot image classification. By fixing the pretrained backbone, CoOp performs end-to-end learning to update the learnable context vectors for target domain datasets. The simple yet effective approach substantially beats hand-crafted prompts with a large margin. Meanwhile, CoOp also exhibits better robustness to distribution shift than CLIP. Strengths: \n1) The paper is generally well-written and easy to follow. The author conducts extensive experiments and ablation studies.\n2) CoOp outperforms CLIP on all 11 diverse datasets, especially showing large improvements in specific domains like texture and satellite images.\n3) CoOp is data-efficient and can boost the classification performance with only a few shots of training data.\n4) CoOp demonstrates better robustness to distribution shift than zero-shot CLIP and linear probe CLIP. \n\nWeakness: \n1) Though effective, the technical novelty of this paper is quite limited. Since the soft prompt tuning approaches (e.g., Prompt Tuning [1] and P-Tuning [2]) have already been proposed in NLP domain. And CoOp adopts a very similar technique.\n[1] Lester, Brian, Rami Al-Rfou, and Noah Constant. \u201cThe power of scale for parameter-efficient prompt tuning.\u201d EMNLP 2021.\n[2] Liu, Xiao, et al. \u201cGPT Understands, Too.\u201d *arXiv preprint arXiv:2103.10385* (2021).\n\n2) According to Table 4 in the paper, the nearest neighbor words of the learned context vectors rarely have practical semantic meaning. This casts doubt on using the word \u201ccontext\u201d. From this perspective, these soft prompts are just more parameters to improve the model capacity.  Hence, can we say that CoOp is just a better version of fine-tuning? In other words, there might be a similar fine-tuning way to utilize additional parameters to get better performance. The paper achieves better few-shot image classification performance improvements but lacks enough technical novelty and explanations for learned prompts",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "mo5m2Y33ekj",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_OgCcfc1m0TO",
                "title": "",
                "comment": "The authors demonstrate a more efficient form of few-shot learning\nusing CLIP compared to linear probing for image classification: CoOp.\nInstead of fine-tuning a small linear classifier on the output of\nCLIP, they propose fine-tuning a number of additional embeddings at\nthe input layer; this modification, in theory, allows CLIP to leverage\nmore computation when adapting to tasks, while still only optimizing a\nsmall number of parameters. Experiments across several corpora\ndemonstrate the efficacy of the approach, which generally yields a\nfew accuracy points of gain versus a linear probe trained on the same\namount of data. The paper's motivation is clear, experiments thorough, and results\nconvincing. In addition to the standard few-shot evaluations, I\nappreciated the authors considering the distribution shift scenario:\nin that case, they show that their model can more effectively leverage\nlabelled data from a different dataset versus the linear probes (even\nthough zero-shot CLIP remains somewhat competitive in that\nregime). Additional experiments about the optimal context length,\nplacement of fine-tune-able token embeddings, and vision backbones\nwhere appreciated.\n\nWhile the CoOp method appears to work well, but I had some concerns\nabout novelty. In particular, this method is more-or-less identical to\nLi and Liang (2021) --- that paper came out on arXiv Jan 1 of this\nyear, and was published at ACL earlier this year. In addition, this\nidea has been \"rediscovered\" in the NLP context several times (which\nare also cited, but perhaps cannot be considered contemporaneous,\ngiven their arXiv dates...). While the authors cite the arXiv version,\nI think that, given the timing, the authors should perhaps not pitch\ntheir method as a \"novel\" approach (as is done in the abstract);\nrather, this seems to be applying prefix-tuning to CLIP.\n\nI had a few technical concerns:\n\n1. there are a lack of details for the linear probe --- how was the\n   regularization parameter chosen?\n   \n2. The baseline for comparison to CoOp was the linear probe, which\n   makes sense. However, I would have also liked to have seen a\n   comparison where all the parameters of CLIP are fine-tuned --- how\n   well does that work for few shot learning?\n\nI also had a few presentation concerns:\n\n1. Figure 1 compares a supervised CoOp method to a zero-shot CLIP\n   baseline. While the CoOp method is \"few shot\", in this figure,\n   there are 16 examples per class provided, which, for some datasets\n   may amount to hundreds or thousands of labelled examples. I would\n   have appreciated Figure 1 comparing to the linear probe.\n\n2. Figure 5b has a similar problem with being potentially misleading:\n   I would recommend including not only zero-shot CLIP, but also\n   linear probe CLIP. Overall: the work is generally clear with thorough and convincing\nexperiments. While there were a few presentation/technical concerns,\nthe main drawback of this work is novelty: the proposed idea is\nidentical to Li and Liang (2021) [arxiv in january], Zhong et\nal. (2021) [arxiv in April], and perhaps Lester et al (2021) [arxiv in\nSep, this one is more recent and I haven't read it yet]. While these\npapers consider only the NLP case and not the vision+language case,\nit's still difficult to call this method \"novel,\" as the authors do in\nthe intro.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "CwHrJmanoON",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_OgCcfc1m0TO",
                "title": "",
                "comment": "This paper proposes a context optimisation (CoOp) approach that can automate prompt engineering and allow more efficient and task-specific transfer for pretrained vision-language models. + Extensive experiments\n+ Motivation for the automatic prompt is useful\n\no Investment in Fig 1 is super useful. It is helpful to see how fragile the prompt is affected by word twisting. However, the paper claimed prompt engineering is time-consuming, which I do not disagree with. Yes, human annotators have no way to know whether adding a \"flower\" or changing the sentence order will improve or harm the performance. But the key motivation of the prompt is to reduce the human working load and adding some general description is not super complicated. The task should be down to exploring more robust models and algorithms that can withstand these minor changes in my opinion.\n\n- The biggest concern is about the lack of theoretical contribution. The main content of the methodology is broken down into two sections. The first section is just a review of CLIP which is already well-known. The key content of context optimisation in 2.2 is just half a page. The description is rough.\n\n- It is not super clear why adding extra dimensions (claimed as the same number in that of word embedding) to the class token can help the prompt.\n\n- On page 4 above Eq.3, the concept \"unified context\" and \"class-specific context\" are not well explained.\n\n- paragraph under Eq.3, \"Training is performed ...\" is redundant and not informative.\n\n- Figure 2 is not helpful in understanding the framework and looks very low-quality. Despite extensive evaluation, the paper presents in very low quality and lack of description of key methods and the theory behind it. The overall quality is clearly below the threshold of the ICLR standard.",
                "rating": 1,
                "confidence": 5
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The approach",
                "Sentiment Expression": "not novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the result",
                "Sentiment Expression": "not surprising",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the results",
                "Sentiment Expression": "clear and thorough",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "results",
                "Sentiment Expression": "technically limited",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "JYtwGwIL7ye": {
        "paper_id": "iclr_2022_JYtwGwIL7ye",
        "paper_title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models",
        "paper_abstract": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",
        "paper_acceptance": "Accept (Poster)",
        "meta_review": "I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is presenting an interesting and systematic study of reward hacking [GVMn] that is useful to the research community [bfGN] and targets an important problem [uYeb] in a rigorous way [16uL]. I thus recommend accepting the paper, but I strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular in regards to improving positioning with respect to related work and a better formalization of their work.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "h2n_78mzq4",
                "writer": "official_reviewer",
                "reply_to": "ATfWmZi7gws",
                "title": "Further minor comments",
                "comment": " Thank you for addressing my comments. \n\nSome further minor comments to improve the final paper:\n\n> The value of the benchmark is in allowing other researchers to make progress on reward hacking, particularly on detecting such phase transitions.\n\nWhat I meant is that you could be more specific about how the benchmark provides value. Do you want other researchers to come up with methods that detect the (known) phase transitions in your benchmark without access to the true reward? Do you want other researchers to develop RL algorithms that don't have phase transitions? Overall, I'm not sure if the benchmark in its current form is adding as much value as the rest of the paper, and I could see it being more impactful if you expand it (as other reviewers noted) and made it into its own publication. \n\n> Which concepts would you most like to see formalized?\n\nReward hacking and phase transitions. However, as I noted in my review this might be difficult at this early stage and I don't think this is strictly necessary.\n\nTypo: \"proxies led misspecification\". Further nitpick: optimizing a proxy does not lead to misspecification since the misspecification was already there before the proxy was optimized.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZRzynW2eahu",
                "writer": "official_reviewer",
                "reply_to": "AQmF4QGf7D",
                "title": " ",
                "comment": " Thank you for adding these. It could be interesting to visibly include correlation numbers or plots in the final main paper as the paper's impact depends on showing that the proxies and true rewards are sufficiently similar to be realistic. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "woElwu37xM1",
                "writer": "official_reviewer",
                "reply_to": "qy9hVnfujRw",
                "title": "Thank you for the response",
                "comment": " Thank you for addressing my concerns, these are good points. I think I misunderstood the amount of human feedback required - it's good to hear that only labeling policies is needed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6fedX9uAiFe",
                "writer": "official_reviewer",
                "reply_to": "eLF8f5B1rua",
                "title": "Thanks for the correlation plot",
                "comment": " Thanks you for providing the correlation plots, I find them very curious. While it is of course expected that reward hacking would occur with negative reward correlation, I think it is an important observation that reward hacking occurs even with high correlation between the real and proxy rewards. I think adding such discussion to the introduction or conclusion (in the future) would strengthen the main point of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gkLms4x3PGY",
                "writer": "author",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "Summary of Changes",
                "comment": " - Updated Figure 1 to be more self-contained and simpler\n- Added Appendix A discussing related work, realism of environments, and realism of proxy rewards\n- Added second paragraph in introduction on why proxies are often used in practice\n- Removed Table 1 (There was some overlap between Table 1 and Table 2)\n- Updated Section 3.2 COVID and Section 3.2 Glucose to be more clear and to more accurately reflect the findings of Section 3.1\n- Changed occurrences of \u201csmooth traffic flow\u201d to \u201cminimizing mean commute time\u201d \n- Made claims more specific in abstract\n    - \u201cAchieving lower true reward\u201d \u2192 \u201cAchieving lower true reward than less capable agents\u201d\n    - \u201ccritical thresholds\u201d \u2192 \u201ccapability thresholds\u201d\n- Updated Figure 2c, Figure 7abc with more runs to reduce noise\n- Updated Figures 1, 2, 3, 4, 6 and Table 3 for more clarity\n- Added correlation plots in Appendix B\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eLF8f5B1rua",
                "writer": "author",
                "reply_to": "wBLjeeO7WwC",
                "title": "Thank you for your clarification",
                "comment": " Thanks for your clarification. \n\n*it would still be informative to look at it with a few relevant state distributions, for example, states from a random agent, states from not very expressive agent or states from very expressive agent from this work.*\n\nIn an effort to showcase the correlation between the true and proxy rewards, we have added plots of the correlation between the proxy and true rewards in Appendix B. In short, we find that reward hacking does occur even when the rewards are positively correlated. We expand on this addition in our top-level comment.\n\n*I would prefer if some related work was included in the main paper*\n\nDue to space constraints, we have not updated the PDF to include more related work in the introduction. However, we agree with your suggestion that more related work should be included in the main paper and will update this in the final draft (there was not enough time to rework the introduction)\n\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aOPbHlmG1ny",
                "writer": "author",
                "reply_to": "uyK4aMfrZrD",
                "title": "Added correlation plots ",
                "comment": " Thanks for your feedback. \n\nIn an effort to showcase the correlation between the true and proxy rewards, we have added plots of the correlation between the proxy and true rewards in Appendix B. In short, we find that reward hacking does occur even when the rewards are positively correlated. We expand on this addition in our top-level comment.\n\nWe will look to add more examples of true-proxy reward misspecifications (especially the case when the true reward is difficult to specify), as this is a good suggestion.\n\nThanks for your comment on the framing of the paper. We will add a section addressing this caveat during the introduction.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "AQmF4QGf7D",
                "writer": "author",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "Added correlation plots in Appendix B",
                "comment": " Reviewers bfGN and GVMn both requested correlation plots between the true and proxy rewards for different state distributions. We have added these in Appendix B. For now, only correlation plots for the traffic environment are present, but the full paper will have plot for all environments.\n\nFor a given model size, we obtain two checkpoints: one early in training (less than 1% of training complete) and one which achieves the highest proxy reward during training. We call the former the \"random checkpoint\" and the latter the \"trained checkpoint\". The random checkpoint is plotted in green and the trained checkpoint is plotted in blue. \n\nFor a given model checkpoint, we calculate the correlation $\\rho$ between the proxy reward $P$ and true reward $T$ as the [Pearson correlation coefficient](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) using 30 trajectories sampled from the model checkpoint. The states visited by one of the 30 trajectories are intended to be a rough estimate of the state distribution of the policy.\n\nInterestingly, we see that reward hacking still occurs when there is positive correlation between the true and proxy rewards (bottleneck-misweight and merge-space misspecifications). Unsurprisingly, proxy-true pairs which are highly correlated (merge-misweight misspecification) do not exhibit reward hacking. Finally, proxy-true pairs which are negatively correlated (merge-ontological misspecification) exhibit the most reward hacking. \n\nThanks to the reviewers for this interesting suggestion. We have added these figures and explanation in Appendix B.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wBLjeeO7WwC",
                "writer": "official_reviewer",
                "reply_to": "t0amLms5w2",
                "title": "Thanks for your response and clarifications",
                "comment": " I would like to thank the authors for their responses.\n\n*To clarify, are you asking for an experiment that quantifies the correlation between true and proxy rewards\nTo clarify, are you asking for cases where the true and proxy reward are positively correlated for sufficiently small models, or just cases where the reward functions should intuitively be aligned?*\n\nYes, this is what I had in mind. I understand that it would depend on the state distribution, but it would still be informative to look at it with a few relevant state distributions, for example, states from a random agent, states from not very expressive agent or states from very expressive agent from this work. I understand that intuitively the rewards are supposed to mean the same, but I would like to see their positive correlation. I think it could also help to address the questions of the realism of the setting that other reviewers raised.\n\nRegarding the addition of the related work, I would prefer if some related work was included in the main paper as it would be much appreciated by any reader who is not an expert in this field.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ATfWmZi7gws",
                "writer": "author",
                "reply_to": "LveRouNcuO2",
                "title": "Response to Reviewer bfGN ",
                "comment": " Thanks for the suggestions and for your positive comments on the importance of the topic! We address your concerns below:\n- *\u201cThe paper neglects to formalize some concepts\u201d*\n    - Which concepts would you most like to see formalized? We will attempt to do so in our revision. We agree with the reviewer that our work is a first step and there are still other concepts whose precise definitions would still require a strong research effort from the community. \n- *\u201cSome environments and reward functions could be made more realistic\u201d*\n    - We address this in the common response. In brief: most environments and reward functions are adopted from practitioners and compared to the majority of past work our environments are significantly more complex.\n- *\u201cIn the final paragraph of the introduction, it was unclear to me what is the value proposition for the proposed benchmark and the baselines. This contribution could be better motivated.\u201d*\n    - Our benchmark is motivated by our observations of phase transitions in the severity of reward hacking as agent capabilities increase. Phase transitions suggest that researchers may encounter sudden, unintended consequences during model deployment, which are important to avoid. The value of the benchmark is in allowing other researchers to make progress on reward hacking, particularly on detecting such phase transitions.\n- [Some Figures and Tables could be more self-explanatory] \n    - Thanks a lot for highlighting these issues. We have addressed the issues with Figures 1, 2, 3, 4, 6 and Table 3 in the updated draft. \n- *\u201cincreasing model size is not the same as increasing optimization\u201d*\n    - We agree that these concepts are not similar. We previously used \u201cagent capabilities\u201d instead of \u201coptimization power\u201d, but it was pointed out that \u2018capabilities\u2019 appears more discrete rather than continuous. We would be happy to consider other terms to replace optimization power.\n- *\u201cInstead of adding Gaussian noise, I would expect that you vary the action space, e.g. discretize it at different resolutions.\u201d*\n    - Thank you for your suggestion; this is another approach for decreasing the resolution. We will update once we have experiments with these results. \n- *\u201cThe paper lacks motivation for why it\u2019s hard not to resort to proxy rewards in practice.\u201d*\n    - Thank you for the suggestion. We have updated the introduction with motivation on why designers often rely on proxy rewards.\n- *\u201cIn section 3.2, it was unclear what is your takeaway from the Glucose environment.\u201d*\n    - We observed that reward hacking does occur but did not observe a phase transitions. We have updated Section 3.2 to more explicitly reflect the findings of Section 3.1.\n- *\u201cthis is confusing as in other places the true objective is said to be minimizing the mean commute time.\u201d*\n    - Thanks; we have clarified this in the draft. \n\nWe will reply again after updating the draft with the rest of your feedback.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "t0amLms5w2",
                "writer": "author",
                "reply_to": "PTO4B7EzBrN",
                "title": "Response to Reviewer GVMn",
                "comment": " Thanks for the suggestions and feedback on the paper! We address your concerns below. \n- *\u201cthe paper does not provide any extensive related work overview\u201d*\n    - Thank you for the suggestion. We address this concern in the common response.\n- *\u201csome information about the correlation between the true and the proxy rewards would be very beneficial to the reader without prior experience\u201d*\n    - To clarify, are you asking for an experiment that quantifies the correlation between true and proxy rewards? We would be happy to compute this, although it would depend on the reference distribution chosen. \n- *\u201cA more interesting observation would be that a reward hacking occurs even when the objectives are mostly aligned and only sometimes diverge.\u201d*\n    - To clarify, are you asking for cases where the true and proxy reward are positively correlated for sufficiently small models, or just cases where the reward functions should intuitively be aligned? If the latter, we note that several proxy reward functions were adopted from practitioners (see common response for details): for instance, in the traffic environment \u201caverage velocity\u201d is the reward used in the traffic simulator from Wu et al. (100+ citations), and we ourselves only noticed the problem with it when using large neural net policies.\n- *\u201cI would like the paper to be a bit more specific in its claims.\u201d*\n    - Thank you for those pointers; we have clarified the claims you mentioned in the abstract.\n- *\u201cThe results often seem to be quite noisy, for example, see Figure 2. How many experiments are conducted?\u201d*\n    - We run our experiments with 3 seeds. We have updated Figure 2c after running with 5 seeds to reduce noise. \n- *The results would be more informative if they included the performance of the agent that optimises the true reward directly to provide an upper bound on the agent\u2019s performance.\u201d*\n    - We will add the performance of an agent trained on the proxy reward in the final draft.\n- *\u201cI am not completely convinced by the example of diabetic risk and cost of insulin. I think this example does not take into account the ethics of such policy and the long-term costs of losing health due to acute hypoglycemic episodes.\u201d*\n    - Yes, we agree that there are certainly ethical considerations to incorporate. Although the \u201ctrue reward\u201d has a morally positive connotation, we only adopt this terminology for convenience and are not making normative statements. Furthermore, there is prior work in the medical community [1-2] that studies income-related insulin underuse, where patients ration insulin (often at the cost to their own health) as a result.\n\n### References\n[1] Herkert D, Vijayakumar P, Luo J, et al. Cost-Related Insulin Underuse Among Patients With Diabetes. JAMA Intern Med. 2019;179(1):112\u2013114. doi:10.1001/jamainternmed.2018.5008\n\n[2] Fralick M, Kesselheim AS. The U.S. Insulin Crisis - Rationing a Lifesaving Medication Discovered in the 1920s. N Engl J Med. 2019 Nov 7;381(19):1793-1795. doi: 10.1056/NEJMp1909402. PMID: 31693804.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uyK4aMfrZrD",
                "writer": "official_reviewer",
                "reply_to": "GfCsJon7cqO",
                "title": "Remaining concerns about reward functions",
                "comment": " Thank you. My concern about the proxies used will be partially addressed by including the above motivations in the paper. However, I have not increased my score (6/10) because my previous evaluation already took into account that your proxies are motivated by real-world examples or prior work. (This may differ for other reviewers). I still feel that the paper could be improved by only using proxies that \n1) a smart practitioner would actually use for training their agent \n2) are more similar to the true reward or correlate with the true reward in most situations. \nEven better would be if \n3) the true reward is actually difficult to specify and so we are forced to use a proxy. (As no true reward is given, reward hacking would have to be studied qualitatively.)\n\n(A further minor issue is that the paper is written for ML practitioners but some of the examples (Covid policy, glucose) the reward is likely \"designed\" by other people.)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "78UDwG-oUUe",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "",
                "comment": "This paper targets the very important problem of reward-hacking that occurs when the objectives optimized by intelligent agents are misaligned with respect to the tru objectives of the algorithm designer. The paper presents an empirical study across a range of different settings including a simple driving simulator, covid modeling, and a single atari game. The experiments show evidence of reward hacking as a function of modeling power of the agent and the size of the state-space. The paper concludes with some ideas and initial directions on how to potentially mitigate reward hacking.  This paper targets the very important problem of misaligned AI models, under the specific lens of reinforcement learning based optimization. This problem has been widely studied in the AI safety / alignment community but has not received equivalent attention in the machine learning community. The experiments presented in the paper shed light on different settings where the problem of reward-hacking might happen, and under controlled variations of the agent's modeling power, and size of the state-space, shows different forms of reward-hacking in these environments. The paper is well-written and easy to follow, and has a thorough coverage of related works in AI alignment. \n\nMy main concerns with the paper are as follows:\n\n1. None of the experiments in the paper are on sufficiently realistic settings. The paper tries to present empirical evidence of reward-hacking on a number of environments, but doesn't dig deeper into any of them. The specific challenges of each setting are different, and it would be useful to focus on a few settings and perform the evaluations keeping real-world considerations in mind. For example, in the traffic simulator, it would be useful to consider pixel-observations (which can be obtained easily for example in the CARLA simulator) and study the same problem. \n\n2. The proxy reward functions considered are very \"incorrect\" in all the settings. In my understanding, the proxy reward functions are trying to show what happens when a reward function different from the true reward function is optimized by the agent. If so, then every attempt must be made to construct reward functions that are *very* close to the true reward function. This is similar to shaped reward functions in typical RL pipelines - it wis important to construct properly shaped functions. An example of this for the traffic example could be a weighted combination of the 4 proxy functions. \n\n3. Missing experiments on environments where the true reward function cannot be easily constructed. The main challenge of misaligned models is in settings where the underlying reward function is unknown and cannot be queried. As a simple example, consider the task of pick and place of a puck with a robot arm. The end result of \"puck in goal location\" can be used to check task success but there is no underlying ground-truth dense reward function. In these settings, different papers come up with different shaped rewards for RL - it would be interesting to show experiments on these settings where the underlying reward function is not known, and demonstrate how different shaped rewards (similar to proxy rewards in the paper) fail, and draw inferences from them. This relates back to my first point above about realistic settings. \n\n4. Question about atari experiments: why not evaluate on a large number of the atari games, as done by most RL papers that show results on atari? \n\n5. Section 4 is very vague without clear takeaways. The notion of a trusted policy is introduced in section 4, as a way to mitigate the reward hacking problem, but the conclusions from Table 4 are unclear. What exactly is helpful in mitigating reward-hacking? Which metrics should we use for checking the extent of model mis-specification?\n\n6. What is the number of training steps experiment measuring? Is the conclusion of this experiment that an agent that has been optimized for more time-steps will likely succumb to more reward-hacking? If so, then this requires addition experimentation because \"not optimizing enough\" cannot be a solution to reward-hacking, as that would also lead to lower true rewards as seen in the experiment.\n\n7. The addition of action noise experiments (which the paper calls \"resolution\") seem to have a trivial takeaway? Adding more noise (i.e. higher variance) causes both the true and proxy rewards to decrease. I request the authors to explain how this non-trivial. \n\n8. An additional dimension of agent capabilities to check would be the \"type of training algorithm\" used. This would likely be very important because different variants of algorithms (e.g. on-policy / off-policy, model-based / model-free) would have different failure modes with respect to reward hacking. This might provide more useful empirical takeaways for future work, compared to action resolution, and training steps variations considered. \n\nIn summary, I believe this paper tackles a very important and timely problem through a purely empirical lens. Since, this is a purely empirical paper, it is important to have more comprehensive experiments that are reflective of real-world settings, and have better motivated design choices. The paper in its current form does not provide useful takeaways that are practically significant and would require significant revision. \n\n I believe this paper tackles a very important and timely problem through a purely empirical lens. Since, this is a purely empirical paper, it is important to have more comprehensive experiments that are reflective of real-world settings, and have better motivated design choices. The paper in its current form does not provide useful takeaways that are practically significant and would require significant revision. ",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "aWnD2QyeUM",
                "writer": "official_reviewer",
                "reply_to": "ZMiglg6ERL3",
                "title": "Thanks for the response. I have updated my score.",
                "comment": " Thanks a lot for the detailed response to reviewers' comments. My main concerns have been addressed, and I have revised the score. I would strongly encourage the authors to take into account all the reviewer suggestions for updating their paper, in particular those of reviewer bfGN. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "GfCsJon7cqO",
                "writer": "author",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "Response to all reviewers",
                "comment": " Thanks to all the reviewers for their detailed and helpful feedback. We have updated the draft with a few changes (noted in the responses) and we reply when we update the draft with all requested changes.\n\nSeveral reviewers have noted that Figure 1 with the caption is \u201cnot self-explanatory\u201d and we have included a simpler version of it and improved the caption. We welcome any further feedback. \n\nThere were some concerns on how realistic our chosen environments and rewards are. To address those comments, we will:\n- Detail prior works in reward hacking, which we will also add to the paper.\n- Show that the design choices behind our environments and reward functions are motivated by real world applications and more complex than prior work.\n\n## Related Work\nPrevious works in RL that demonstrate examples of reward hacking have often used grid-world type environments or game playing agents. \n- Hadfield-Menell et al. [1] (NeurIPS oral) exhibit an example of reward hacking in a grid-world (Lavaland - 10x10) where the reward misspecification arises because of incorrect sensor reading at test time.\n- Leike et al. [2] show two examples of reward gaming in grid-world environments. First, in a 3x3 boat race, the agent learns to move back and forth to collect points instead of finishing the race. Second, in a 5x7 tomato watering environment, the agent learns to put a bucket on its head so that plants appear watered.\n- Toromanoff et al. [3] exhibit reward gaming examples in several Atari games (Elevator Action, Kangaroo, Bank Heist) where the agent keeps looping in a sub-optimal trajectory to obtain a repeated small reward.\n- Baker et al. [4] study a multi-agent hide-and-seek environment. They show that, in the absence of a penalty for leaving the play area, the hiders learn to run endlessly to prevent the seekers catching them.\n- Christiano et al. show reward hacking in the Pong game [5]. The agent learns to hit the ball back and forth instead of winning the point.\n- Separate from grid-world or game playing applications, Stiennon et al. show reward hacking when learning to summarize Reddit posts [6]. An agent over-optimizes the learnt reward model and produces lower quality summarizations.\n\n## Our Environments vs. Related Work\nOur environments are more complex than the gridworld environments used in most related work, e.g. [1], [2], [3]. Our subjective judgment is that they are about as complex as the Atari environments in [4] and [5], although ours have more real-world grounding. For instance:\n- The blood glucose environment relies on an FDA-approved simulator [7] used in medical research for testing glucose administration strategies.\n- The traffic environment is based on a microscopic traffic simulator [12] (800+ citations) for urban planning. The simulator uses cars that are controlled by the Intelligent Driver Model, which is a widely-accepted approximation of human driving behavior [13] (3000+ citations). \n\n\n## Realism of Proxies\nMost of our proxies were either taken directly from practitioner code, or motivated by practical trade-offs. For instance:\n- Our blood glucose proxy is taken from a previous work [8], which adapts the medical community\u2019s measure of glycemic risk [9] into a reward function to train a continuous glucose controller.  Similarly, our true reward (economic cost of treatment) is motivated by studies [10-11] showing that low-income patients ration insulin to save money, i.e., patients may opt to emphasize monetary benefits over health benefits.\n- For traffic, the \u201caverage velocity reward\u201d is the reward used in the traffic simulator from Wu et al. (100+ citations), and we ourselves only noticed the problem with it when using large neural net policies.\n- For COVID, different actors might emphasize political vs. medical costs. An economics paper [18] argues that \u201cofficials face incentives in making their pronouncements\u2026 overly pessimistic forecasts will grab media headlines and steer policy responses\u2026 [we] stress that the politics of economic policy cannot be ignored\u201c. Similarly, a medical paper [19] uses ICU usage as an approximation of direct healthcare costs of the COVID-19 pandemic.\n\nIn particular, the traffic control, blood glucose, and COVID environments have misspecifications that may arise in practice (due to different stakeholders possessing misaligned incentives). \n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cHPz2M6PmJ3",
                "writer": "author",
                "reply_to": "78UDwG-oUUe",
                "title": "Response to Reviewer uYeb",
                "comment": " Thanks for your detailed suggestions and feedback! We address your concerns below:\n- *\u201cNone of the experiments in the paper are on sufficiently realistic settings.\u201d*\n   - We address this in the common response. In brief: most environments and reward functions are adopted from practitioners, and compared to the majority of past work our environments are significantly more complex.\n- *\u201cIn the traffic simulator, it would be useful to consider pixel-observations (which can be obtained easily for example in the CARLA simulator) and study the same problem.\u201d*\n    - Using pixel observations would be an interesting environment to study. As mentioned in the common response, the traffic simulator we used in our work is widely used by practitioners. We believe this is evidence for its realism.\n- *\u201cThe proxy reward functions considered are very \"incorrect\" in all the settings.\u201d*\n    - In the common response, we provide justifications of our choice of true and proxy rewards. We do not agree that the proxies are very \u201cincorrect\u201d, since in many cases they were adopted by practitioners. For instance, the average velocity proxy in the traffic simulator is the default reward in Wu et al., which has over 100 citations. \n- *\u201cevery attempt must be made to construct reward functions that are very close to the true reward function.\u201d*\n    - In practice, reward designers do not often construct reward functions that are very close to the true reward function. For example, reward designers have tried to optimize YouTube\u2019s recommender system for user engagement, using proxies such as clicks and watch-time. The difference between true and proxy reward functions in this instance is arguably more drastic than our traffic control example of mean velocity vs. mean commute time. Studies of YouTube have observed several unintended consequences including decreasing user happiness [1] and promoting conspiratorial behaviors [2]. We believe this shows that greatly misspecified reward functions do cause harm in practice and are worthy of study.\n- *\u201cAn example of this for the traffic example could be a weighted combination of the 4 proxy functions.\u201d*\n    - We do consider a misweighting, which looks at a combination of the 4 proxy functions. This appears in Appendix A, Figure 7a and 7b.\n- *\u201cWhy not evaluate on a large number of the atari games, as done by most RL papers that show results on atari?\u201d*\n    - Unlike traditional RL papers, we would need to construct a suitable true-proxy pair for each environment. For many games it is not obvious what reward to use other than game score, while other games run into technical difficulties because training hyperparameters from prior work were over-optimized for a particular model size. As a result, we focused on one game where we could overcome both difficulties. \n-*\u201cthe conclusions from Table 4 are unclear. What exactly is helpful in mitigating reward-hacking? Which metrics should we use for checking the extent of model mis-specification?\u201d*\n    - In Section 4, we create an anomaly detection task using the trusted policy. We then propose three baseline anomaly detectors that measure the distance between the trusted policy and the potentially anomalous policy. Table 4 shows that none of the baselines perform well on all of the environments. We leave the task of designing a better anomaly detector for future work.\n- *\u201cWhat is the number of training steps experiment measuring?\u201d*\n    - Similarly to the other three settings, we vary training steps as one proxy for agent capabilities --- agents with limited resources can optimize the objective to a lesser amount than those with larger resources. Not optimizing enough is indeed not a solution to reward hacking, and that isn't what we were showing with these experiments. Instead, we propose that one solution would be to possibly regularize the optimized policy towards a safe policy, as indicated by our benchmarks. \n- *\u201cThe addition of action noise experiments (which the paper calls \"resolution\") seem to have a trivial takeaway?\u201d*\n    - We agree that this result is unsurprising. As noted by reviewer bfGN, our goal was to avoid picking only the most convincing results. In the broader context of our work, these experiments were designed to measure one notion of agent capabilities.\n- *\u201cAn additional dimension of agent capabilities to check would be the \"type of training algorithm\" used.\u201d*\n    - This is an interesting idea as a notion of agent capabilities. It may be tricky to implement in practice, because most training algorithms are not stable on most environments [3].\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qy9hVnfujRw",
                "writer": "author",
                "reply_to": "ZNXNRvWRBkH",
                "title": "Response to Reviewer 16uL",
                "comment": " Thank you for your suggestions! We are glad that you appreciate the impact of our work. We address your concerns below:\n- *\u201cit would be good to see results on more than one Atari environment\u201d*\n    - Unlike traditional RL papers, we would need to construct a suitable true-proxy pair for each environment. For many games it is not obvious what reward to use other than game score, while other games run into technical difficulties because training hyperparameters from prior work were over-optimized for a particular model size. As a result, we focused on one game where we could overcome both difficulties. \n- *\u201cThe choice of proxy and true rewards for some of the environments seemed a bit arbitrary\u201d*\n    - We address this in the common response.\n- *\u201cI have the impression that the proposed benchmark requires a large amount of human feedback\u201d*\n    - Our current benchmark actually does not require any human feedback. The benchmark we plan to release will consist of model checkpoints, their labels, and the trusted policy. In practice, one could use human feedback to create a trusted policy, but in our case we mark one of our trained policies as trusted and manually check that it does not exhibit reward hacking.\nIf the benchmark were to be expanded to environments not considered in the paper, this would require humans to label which policies are anomalous and also label one of the trained policies to be a trusted policy. Labeling policies is very cheap (one only needs to check whether or not there is reward hacking), and would roughly cost: \u201c1 minute / trajectory * 5 trajectories / policy = 5 minutes per policy\u201d. \n- *\u201cI also wonder whether using the distance from the (suboptimal) trusted policy would result in labeling a policy with superhuman performance on the true reward as aberrant\u201d*\n    - Your example of AlphaGo\u2019s Move 37 is very nice. We agree that using distance between policies may not lead to an optimal detector, especially when faced with aberrant actions. For this specific task, we envision a pipeline where such a move would indeed be flagged by the detector as an outlier but after some manual evaluation, humans do figure out the importance of this move and mark this as a \u201csafe move\u201d. With this, one can start exploring better policies which stay around these new safe policies. \n\n### References\n[1] Christiano, P.F., Leike, J., Brown, T.B., Martic, M., Legg, S., & Amodei, D. (2017). Deep Reinforcement Learning from Human Preferences. NIPS.\n\n[2] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D.M., Lowe, R.J., Voss, C., Radford, A., Amodei, D., & Christiano, P. (2020). Learning to summarize from human feedback. ArXiv, abs/2009.01325.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZMiglg6ERL3",
                "writer": "author",
                "reply_to": "cHPz2M6PmJ3",
                "title": "References for response to Reviewer uYeb",
                "comment": " ### References\n[1] Stray, J. Aligning AI Optimization to Community Well-Being. Int. Journal of Com. WB 3, 443\u2013463 (2020). https://doi.org/10.1007/s42413-020-00086-3\n\n[2] Manoel Horta Ribeiro, Raphael Ottoni, Robert West, Virg\u00edlio A. F. Almeida, and Wagner Meira. 2020. Auditing radicalization pathways on YouTube. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 131\u2013141. DOI:https://doi.org/10.1145/3351095.3372879\n\n[3] Henderson P, Islam R, Bachman P, Pineau J, Precup D, Meger D. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence 2018",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "TW96q3Z9iCP",
                "writer": "author",
                "reply_to": "GfCsJon7cqO",
                "title": "References for common response",
                "comment": " ## References\n[1] Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S.J., & Dragan, A.D. (2017). Inverse Reward Design. NIPS.\n\n[2] Leike, J., Martic, M., Krakovna, V., Ortega, P.A., Everitt, T., Lefrancq, A., Orseau, L., & Legg, S. (2017). AI Safety Gridworlds. ArXiv, abs/1711.09883.\n\n[3] Toromanoff, M., Wirbel, \u00c9., & Moutarde, F. (2019). Is Deep Reinforcement Learning Really Superhuman on Atari? Leveling the playing field. arXiv: Artificial Intelligence.\n\n[4] Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., & Mordatch, I. (2020). Emergent Tool Use From Multi-Agent Autocurricula. ArXiv, abs/1909.07528.\n\n[5] Christiano, P.F., Leike, J., Brown, T.B., Martic, M., Legg, S., & Amodei, D. (2017). Deep Reinforcement Learning from Human Preferences. NIPS.\n\n[6] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D.M., Lowe, R.J., Voss, C., Radford, A., Amodei, D., & Christiano, P. (2020). Learning to summarize from human feedback. ArXiv, abs/2009.01325.\n\n[7] Man CD, Micheletto F, Lv D, Breton M, Kovatchev B, Cobelli C. The UVA/PADOVA Type 1 Diabetes Simulator: New Features. J Diabetes Sci Technol. 2014;8(1):26-34. doi:10.1177/1932296813514502\n\n[8] Magni L, Raimondo DM, Bossi L, Man CD, De Nicolao G, Kovatchev B, Cobelli C. Model predictive control of type 1 diabetes: an in silico trial. J Diabetes Sci Technol. 2007 Nov;1(6):804-12. doi: 10.1177/193229680700100603. PMID: 19885152; PMCID: PMC2769684.\n\n[9] BorIs. P. Kovatchev, Martin Straume, Daniel J. Cox & Leon.S Farhy (2000) Risk analysis of blood glucose data:Az quantitative approach to optimizing the control of insulin dependent diabetes, Journal of Theoretical Medicine, 3:1, 1-10, DOI: 10.1080/10273660008833060\n\n[10] Herkert D, Vijayakumar P, Luo J, et al. Cost-Related Insulin Underuse Among Patients With Diabetes. JAMA Intern Med. 2019;179(1):112\u2013114. doi:10.1001/jamainternmed.2018.5008\n\n[11] Fralick M, Kesselheim AS. The U.S. Insulin Crisis - Rationing a Lifesaving Medication Discovered in the 1920s. N Engl J Med. 2019 Nov 7;381(19):1793-1795. doi: 10.1056/NEJMp1909402. PMID: 31693804.\n\n[12] P. A. Lopez et al., \"Microscopic Traffic Simulation using SUMO,\" 2018 21st International Conference on Intelligent Transportation Systems (ITSC), 2018, pp. 2575-2582, doi: 10.1109/ITSC.2018.8569938.\n\n[13] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical observations and microscopic simulations. Physical review E, 62(2):1805, 2000.\n\n[14] \"Flow: Architecture and Benchmarking for Reinforcement Learning in Traffic Control\", C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, A. Bayen, arXiv preprint arXiv:1710.05465, 2017,\n\n[15] Buliung, R. N., & Kanaroglou, P. S. (2002). Commute minimization in the Greater Toronto Area: applying a modified excess commute. Journal of Transport Geography, 10(3), 177-186.\n\n[16] He, S., Peng, Y. & Sun, K. SEIR modeling of the COVID-19 and its dynamics. Nonlinear Dyn 101, 1667\u20131680 (2020). https://doi.org/10.1007/s11071-020-05743-y\n\n[17] Gary E. Weissman, Andrew Crane-Droesch, Corey Chivers, et al. Locally Informed Simulation to Predict Hospital Capacity Needs During the COVID-19 Pandemic. Ann Intern Med.2020;173:21-28. [Epub ahead of print 7 April 2020]. doi:10.7326/M20-1260\n\n[18] Boettke, P, Powell, B. The political economy of the COVID-19 pandemic. South Econ J. 2021; 87: 1090\u2013 1106. https://doi.org/10.1002/soej.12488\n\n[19] Sarah M. Bartsch, Marie C. Ferguson, James A. McKinnell, Kelly J. O'Shea, Patrick T. Wedlock, Sheryl S. Siegmund, and Bruce Y. Lee The Potential Health Care Costs And Resource Use Associated With COVID-19 In The United States \nHealth Affairs 2020 39:6, 927-935",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LveRouNcuO2",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "",
                "comment": "This paper studies reward hacking, a common but understudied phenomenon, across a set of environments. Reward hacking emerges in several tasks, meaning that the resulting policy has a high proxy reward but a low true reward. A key finding is that reward hacking increases with agent capabilities so that increasing capability lowers the true reward. This holds across several ways of increasing capabilities (model size, training steps, action space, etc). The authors also find \u2018phase transitions\u2019 where a small increase in capability results in qualitatively new reward hacking behavior, a phenomenon that may require novel monitoring strategies. One such strategy is anomaly detection, for which the authors introduce a benchmark and baselines.\n The paper\u2019s findings are clearly useful to the research community as reward hacking has been a poorly understood but ubiquitous problem, broadly relevant for many practical settings. If the paper only brings extra attention to this topic, I think it has had some impact. The most important findings in my view are that:\n\n1) Reward hacking can not only stop the true reward from increasing, but often actively lowers it \n2) Increasing capability can increase reward hacking and thereby lower reward. This will be unsurprising to some readers but I believe it is still somewhat controversial and it helps to have demonstrations across several environments and several ways to increase capability, which makes these results more certain. This results in several interesting hacking behaviors.\n3) Increasing capability can lead to phase transitions, a tricky new problem for that should be brought to the attention of RL practitioners as it may necessitate new monitoring strategies (such as the ones proposed in this paper).\n\nSome readers won't find all of the above particularly surprising or novel. However, given how common and understudied reward hacking is, I believe that they deserve the more systematic study and exposition this paper provides.\n\nThe paper neglects to formalize some concepts. To some extent this is necessary for the first paper that investigates an important concept (reward hacking) which so far has no definition - I therefore think the lack of formality can be forgiven. What makes rigor more challenging here is that reward hacking and phase transitions can happen through multiple means such as increasing the size of the model and action space. I think the paper is a strong attempt at an important and hard problem, and will provide a good basis on which others can study reward hacking with increased formalization and rigor.\n\nI agree with the other reviewers that some of environment and their proxy-true reward pair are not particularly convincing. I've lowered my score accordingly as I think this could detract from the impact this paper has on the research community. But I also recognize that it is difficult to design realistic pairs of reward functions. In reality, the true reward is typically too complex to specify (hence we need a proxy). Because a complex reward is not available in practice, in research papers we need to develop examples that are somewhat artificial.\n\nWhile the paper is mostly well motivated, well written, and easy to follow, I do find that the presentation could be more precise and clear at times, as noted below.\n\nIn the final paragraph of the introduction, it was unclear to me what is the value proposition for the proposed benchmark and the baselines. This contribution could be better motivated.\n\nI appreciate that in several places, such as Figure 4b,  the authors chose to display negative results where reward hacking and phase transitions were not observed. As researchers tend to cherry-pick the most convincing results, it is good to see that the authors avoided this.\n\n\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Detailed comments \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\n\n\nI\u2019ll start by giving detailed feedback on the figures because most readers will look primarily at these.\n\n- Figure 1: \n\u2014The figure with caption is not self-explanatory. E.g. who is the agent in this decision problem and what is their action space? (controlling a traffic light?) \n\u2014It\u2019s not immediately obvious why the proxy and true rewards lead to different behavior here and it is not explained either. For an illustrative figure like this, I\u2019d recommend using a more obvious example.\n\u2014The meaning of arrow colors is unclear\n\u2014It\u2019s unclear what \u2018optimization ability\u2019 means here. If you\u2019re referring to model size, you may want to use the same model size symbols you\u2019re using in Fig 1c instead.\n\nFigure 2: nice figure.\n\nFigure 3: this figure is not self-explanatory, it needs more detail. The testing rate and the meaning of 16/112 are not explained. This figure uses a different format than figure 2 (separate plots for true/proxy rewards) which confused me at first sight. You could have one plot per model size to fix this, or move everything into one plot.\n\nFigure 4: the takeaway could be clarified in the caption, especially for 4b (I believe it\u2019s that no reward hacking occurred).\n\u2014 Minor point: Figure 4a has a lower resolution than Figure 4b.\n\nFigure 6 and Table 3 could also be improved to be more self-explanatory.\n\n\nMinor suggestions:\n\n\u201cOverfitting their objectives\u201d - unclear how overfitting is defined here, it seems to be a non-standard use of the word.\n\n\u201cwe study how increasing optimization power affects reward hacking, by training RL agents with varying resources such as model size \u2026\u201d - increasing model size is not the same as increasing optimization. \u201cOptimization power\u201d makes me as the reader expect that you are talking about doing more/better gradient descent. Perhaps there is no better term here, in which case this is a necessary evil.\n\nIn Figure 4, it is unclear what action noise has to do with misspecification of the action space. Instead of adding Gaussian noise, I would expect that you vary the action space, e.g. discretize it at different resolutions. This does resemble Gaussian noise but it\u2019s not the same thing.\n\nThe paper lacks motivation for why it\u2019s hard not to resort to proxy rewards in practice. Some readers will already know this, others won\u2019t.\n\nCOVID results (section 3.2): I get the result here and it is interesting - the policymaker only regulates once infections are high, but this saves little economic costs because it only delays the restrictions once while increasing the number of ICU patients indefinitely. But I found this not very well explained.\n\nIn section 3.2, it was unclear what is your takeaway from the Glucose environment. That reward hacking happens? That there was a phase transition?\n\n\u201cThe objective of the RL agent is to promote a smooth traffic flow within the highway network\u201d - this is confusing as in other places the true objective is said to be minimizing the mean commute time.\n The paper\u2019s findings are clearly useful to the research community as the reward hacking problem has been a poorly understood but ubiquitous. Three specific findings about reward hacking (see above) had to my knowledge not been empirically demonstrated in RL environments or only mentioned in passing but not studied explicitly. Before this paper, we therefore did not know if the findings correspond to generally applicable phenomena and through this paper we can gain some confidence. However, some environments and reward functions could be made more realistic.\n",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "PTO4B7EzBrN",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "",
                "comment": "This paper provides a systematic study of \u201creward hacking\u201d in the environments with the misspecified rewards. The authors conduct a set of experiments with 4 environments, several types of reward misspecification in each of them and several agents of different expressivity (model capacity). They notice that often the agents that are more capable end up obtaining high proxy reward, but low real reward. Besides, often the transition to the low real reward happens very quickly and authors call this phenomenon \u201cphase transition\u201d. Finally, they propose a baseline for anomaly detection to identify this phase transition. The paper provides an interesting study of reward hacking behaviour where different parameters of the problem are varied to demonstrate the phenomenon. The authors chose a set of diverse and relevant environments. An interesting observation that might have large implications in practice is that training more capable agents might result in the behavior that achieves high proxy reward, but low true reward. While this behaviour was noticed before, I am not aware of a systematic study of the phenomenon where various parameters of the environment, reward and agent are controlled. Unfortunately, the paper does not provide any extensive related work overview that seems to be an important missing part given that the main contribution of the paper is the systematic study of reward hacking.\n\nOne concern that I have is that when the authors study misspecified rewards, the specification of the reward is only motivated by intuition. It is hard to appreciate how reasonable such a specification is without the full knowledge and experience with a particular environment. To me, a reasonable reward specification, even if it is misspecified, would still have significant correlation with the true reward. Sometimes (like in Figure 2) it seems that true reward goes down when the proxy reward goes up and it would be no surprise that a reward hacking occurs when the true objective is the opposite of what is optimised. A more interesting observation would be that a reward hacking occurs even when the objectives are mostly aligned and only sometimes diverge. To understand this better, it would be useful to look at some plots depicting the dependency of proxy and true reward on a broad set of sampled trajectories. \n\nFinally, the authors provide a new baseline of anomaly detection for identifying the \u201cphase transition\u201d in the agent\u2019s behaviour. Despite being small, such problem formulation could be useful for sparking more research in this direction. \n\nOther comments and concerns:\n\n- I didn\u2019t find the Figure 1 very informative on its own, it is only possible to understand the bottom row after reading the main text and at that point the figure does not bring any new information. I would recommend trying to make the figure with its caption more self-contained\n\n- Often rewards are manually crafted in continuous control tasks such as, for example, robotics. Would it be possible to provide another environment with, for example, control of a simulated robotic arm performing the manipulation tasks? The rewards might be assigned (and misspecified) based on the distances or positions of the objects. \n\n- I would like the paper to be a bit more specific in its claims. For example, when the paper talks about more capable agents archiving \u201clower true reward\u201d, lower than what? When the paper talks about \u201ccritical threshold\u201d, it is the threshold of what?\n\n- The results often seem to be quite noisy, for example, see Figure 2. How many experiments are conducted?\n\n- The results would be more informative if they included the performance of the agent that optimises the true reward directly to provide an upper bound on the agent\u2019s performance.\n\n- I am not completely convinced by the example of diabetic risk and cost of insulin. I think this example does not take into account the ethics of such policy and the long-term costs of losing health due to acute hypoglycemic episodes.\n I appreciate a systematic study of the reward hacking phenomenon where the parameters of the problem are manually varied. I think the related work overview should be extended to justify the \u201csystematic\u201d aspect of the paper. Besides, some information about the correlation between the true and the proxy rewards would be very beneficial to the reader without prior experience with the studied environments.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "ZNXNRvWRBkH",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_JYtwGwIL7ye",
                "title": "",
                "comment": "This paper investigates the phenomenon of reward hacking as a function of agent capabilities. They introduce four diverse RL environments with nine misspecified rewards and demonstrate that more capable agents are better at exploiting the misspecification. They find instances of phase transitions where a small increase in agent capability produces a large change in behavior that sharply decreases the true reward.\n\nTo mitigate the reward hacking problem, they propose to set up an anomaly detection task, given a trusted model with moderate performance on the true reward, where the anomaly detector's task is to identify whether policies from a different model are satisfactory for the true reward. They provide several baseline anomaly detectors and show how they perform on different tasks.  This paper does a great job at investigating an important problem in AI safety in a concrete and rigorous way. There is an informal assumption in AI safety that the reward hacking problem will tend to get worse as the agent becomes more capable. To my knowledge, this paper is the first to demonstrate this effect quantitatively by varying different agent capabilities (such as model capacity) and showing that the true reward decreases as a result. The paper is clearly written and well-motivated. \n\nThis paper systematically investigates the reward hacking phenomenon across a variety of environments, agent capabilities and forms of misspecification. The environments used in the paper are diverse and complex, which helps to show that reward hacking occurs in a wide variety of domains (though it would be good to see results on more than one Atari environment). The environments were thoughtfully chosen to include tradeoffs between several desiderata that have to be managed by the agent. \n\nThe authors introduce a taxonomy of misspecification (misweighting, ontology and scope) and design a variety of proxy rewards illustrating the different forms of misspecification. The choice of proxy and true rewards for some of the environments seemed a bit arbitrary (e.g. average speed vs mean commute time), so it would be great to see more justification for this. \n\nI found the section on mitigating reward misspecification to be a somewhat weaker point of the paper. I have the impression that the proposed benchmark requires a large amount of human feedback - it would be great to include the human time cost for the anomaly detection task. The anomaly detection task is mostly illustrated for the traffic environment - it would be useful to include more of the other environments as well. I also wonder whether using the distance from the (suboptimal) trusted policy would result in labeling a policy with superhuman performance on the true reward as aberrant (e.g. AlphaGo's Move 37, which is very unlikely for a human player). \n\n This paper does a great job at investigating an important problem in AI safety in a concrete and rigorous way, and to my knowledge is the first to do so. I am in favour of accepting this paper. ",
                "rating": 8,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper is presenting an interesting and systematic study of reward hacking",
                "Sentiment Expression": "liked",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "pTZ6EgZtzDU": {
        "paper_id": "iclr_2021_pTZ6EgZtzDU",
        "paper_title": "Meta-Reinforcement Learning With Informed Policy Regularization",
        "paper_abstract": "Meta-reinforcement learning aims at finding a policy able to generalize to new environments. When facing a new environment, this policy must explore to identify its particular characteristics and then exploit this information for collecting reward.  We consider the online adaptation setting where the agent needs to trade-off between the two types of behaviour within the same episode. Even though policies based on recurrent neural networks can be used in this setting by training them on multiple environments, they often fail to model this trade-off, or solve it at a very high computational cost. In this paper, we propose a new algorithm that uses privileged information in the form of a task descriptor at train time to improve the learning of recurrent policies.  Our method learns an informed policy (i.e., a policy receiving as input the description of the current task) that is used to both construct task embeddings from the descriptors, and to regularize the training of the recurrent policy through parameters sharing and an auxiliary objective. This approach significantly reduces the learning sample complexity without altering the representational power of RNNs, by focusing on the relevant characteristics of the task, and by exploiting them efficiently.  We evaluate our algorithm in a variety of environments that require sophisticated exploration/exploitation strategies and show that it outperforms vanilla RNNs, Thompson sampling and the task-inference approaches to meta-reinforcement learning.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "This paper is borderline, as evidenced by all of the reviewer's scores.\n\nThe pros are:\n- important and relevant topic\n-  IMPORT is a reasonable, technically sound approach\n- paper is relatively clear\n\nThe cons all lie in the experimental evaluation, and whether the experiments sufficiently back the claim that IMPORT can learn sophisticated exploration strategies and validate IMPORT's merits compared to prior algorithms. In particular:\n- The choice of benchmarks does not sufficiently test the ability to explore in a sophisticated manner\n- Lack of comparisons to PEARL and MANGA, which can readily be applied to the online setting\n- The empirical improvements are relatively modest.\n\nOverall, the cons slightly outweigh the pros of the paper. Indeed, no reviewer was willing to champion the paper's acceptance.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "1wZ_1msAUp",
                "reply_to": "iclr_2021_pTZ6EgZtzDU",
                "title": "Proposal of the advanced usage of the task descriptor for meta RL",
                "comment": "Summary\\\nWhen the task descriptor is available as the privileged information, the authors propose a novel method to learn the policy that can benefit from privileged information. It is reward-driven learning and yet can make use of privileged information for efficient exploration. The advantage of the proposed method is verified in the experiments.\n\nComments on the paper\\\nI think the authors show an advantage of the proposed method by some experiments, but I\u2018d like to further request the following things to make the paper more convincing.\n\n1. Because the proposed method needs the task descriptor, it would be good to explain what kind of tasks we can apply the proposed method. The wider the applicability of the proposed method is, more valuable the proposed method would be. \n2. In the experiments, the authors compare with TS, TI and AuxTask. But I would like to see the comparison with another task embedding method, such as Pearl, K. Rakelly, et al., \"Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,\" in ICML, 2019. I guess RNN makes the training of the exploitation policy more difficult because its latent code dynamically changes especially when the state space is large such as Maze3D environment. On the other hand, the task descriptor would not change. So RNN may sometimes makes the training difficult. We can use other embedding architecture for $f_H$ such as used in Pearl. I also would like to note that when the parameter of the dynamics is used as the task descriptor, it becomes similar to Homanga Bharadhwaj et al., \u201cMANGA: Method Agnostic Neural-policy Generalization and Adaptation\u201d, in ICRA 2020.\n\n----------------------------------------------------------------\nUpdate\nThank you for the comments. But there is a misunderstanding. MANGA as well as PEARL are online methods. They just need the observed data during the episode. It can encode the observation data in online manner. I think it is not evident whether IMPORT performs better than MANGA or PERAL. I agree that RNN is general, but on the other hand, I am afraid that the internal state of RNN does not converge and usually fluctuate from time to time. It may be difficult to get a persistent policy during the episode in the same environment. I would like to encourage the authors to perform more convincing experiment and make the claim of the paper consistent with the experimental findings.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "N7cQQ3JAylF",
                "reply_to": "iclr_2021_pTZ6EgZtzDU",
                "title": "Review",
                "comment": "Summary: This paper studies the exploration in meta-RL problem, where a meta-RL agent must both explore (to reduce uncertainty about the task) and then solve the task. Prior RNN approaches can theoretically learn the optimal policy, but optimization can be challenging. Instead, this paper opts to leverage additional information in the form of task-descriptors that specify the task, which make the learning process easier, because the task-descriptor provides the information normally discovered via exploration. In contrast to other task-descriptor approaches, which use Thompson Sampling for exploration (and can be arbitrarily sub-optimal in certain tasks), or auxiliary losses for predicting the task-descriptor (which may try to predict task-irrelevant information), this paper instead proposes to condition on the task-descriptor as an input to the policy during meta-training. Then an RNN policy is trained in two ways: first, conditioned on the task-descriptor, which enables learning to solve the task without requiring exploration; and second, conditioned on its own hidden state, which eventually replaces the task-descriptor at meta-test time. This approach outperforms prior approaches on a suite of tasks.\n\nStrengths:\n- Clarity. Generally, this paper presents a clear and coherent narrative. The motivation for the approach is clear (better leveraging task-descriptors compared to prior approaches to more easily learn informed policies). And the approach itself is also quite understandable.\n- Technical soundness. Furthermore, the approach appears to be technically sound. Leveraging the task-descriptor to learned informed policies, which are easier to optimize, can clearly produce some benefits. And then addressing the issue that the task-descriptor is unavailable at meta-test time by leveraging the hidden state also seems reasonable. The additional auxiliary loss to keep the recurrent state and task-descriptor embedding close also seems technically sound.\n\nWeaknesses:\n- Experiments. My main concern is that the experiments do not clearly substantiate the claims made in the main text:\n    - The introduction of the paper claims that IMPORT \u201cadapts faster to unknown environments, showing better generalization capabilities.\u201d From the learning curves in Figure 4, this is not clearly the case. In 4a), 4b) and 4c), other methods seem to be learning in roughly as many samples as IMPORT requires. Notably, 4d) does show some impressive improvements, but the general claim of faster learning isn\u2019t clearly supported by the current experiments. I was also unable to determine if the train / valid / test task splits overlapped at all, to evaluate the generalization to unknown environments part.\n    - The paper also states: \u201cWe evaluate IMPORT against the main approaches to online adaptation on environments that require sophisticated exploration/exploitation strategies.\u201d However, the environments used in the experiments, beyond the Maze3D environment don\u2019t appear to require sophisticated exploration. I would find the experiments more compelling if IMPORT was evaluated on more complex environments requiring sophisticated exploration, like Maze3D. In a similar vein, it\u2019s unclear to me what the takeaways from the results on the current environments should be: i.e., what does it mean about IMPORT if it performs slightly better than other approaches on CartPole or a tabular MDP? To be clear, I find evaluation on simpler environments to be useful if they clearly illustrate a point about the method, but it\u2019s unclear to me what that point is.\n    - One of the contributions of the approach, the auxiliary loss (C) is not clearly evaluated / ablated in the experiments. It appears that all of the experiments use the same value of $\\beta$? But it\u2019s not explicitly stated.\n    - Finally, it would be nice if the experiments substantiate the claim that IMPORT outperforms TI by avoiding \u201creconstructing features in the task descriptor that are irrelevant for learning.\u201d However, it\u2019s not clear to me that this is occurring in the experiments. Concretely, when both approaches are given task identifiers, this problem doesn\u2019t seem to exist, since only recovering the learning-relevant aspects is sufficient for predicting the identifier with TI. Similarly, it seems like the $\\mu$\u2019s used in the experiments contain mostly learning-relevant information.\n    - In addition to the concerns about supporting the paper\u2019s main claims, I have two additional concerns: First, the performance improvement from IMPORT is generally fairly modest. There\u2019s only a 3-5% gain over prior approaches in CartPole, and the TabularMDP, and the results on the bandits are mixed, although the results on Maze3D are impressive.\n    - Second, the TS baseline seems strangely implemented, as it\u2019s based on \u201cmaximizing the log-likelihood of $\\mu$.\u201d It\u2019s challenging to verify the details, because the paper states that they are in the appendix, but there is no appendix. In particular, for the bandits setting, it seems like TS should be updating a beta distribution over each arm, which I would expect to lead to stronger performance. Alternatively, it would be good to use prior approaches for TS, such as PEARL [1].\n\nGenerally, I find the proposed approach to be quite promising. This work convincingly states reasons why prior approaches (e.g., TS and task inference) sub-optimally leverage task-information. Yet, I find that the experiments insufficiently support the paper's claims, so I initially lean toward rejection.\n\nAdditional minor comments that did not affect my score:\n- Related works:\n    - The point about conditioning on a belief state with an RNN policy should probably cite [2].\n    - Thompson Sampling as an exploration policy in meta-RL should probably cite [1].\n    - [3] is also relevant to the exploration problem.\n- Several key details are missing from this paper, such as the details of the environments (e.g., what is $\\mu$ in the Maze3D task), which makes evaluating the experiments challenging. As mentioned above, these are reported to be in appendix, but there is no appendix.\n- It would be nice to know the failure mode of other approaches on the Maze3D environment.\n- The end of the setting section (Section 2) somewhat conflates the problem statement with the solution: i.e., the problem is maximizing returns, while the _proposed solution_ is \u201cto find an architecture for $\\pi$ that is able to express strategies that perform the best according to Eq. 1\u201c\n- \u201cthey approaches are sensitive\u201d \u2014> these approaches...\n\n[1] Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. Mar. 2019. https://arxiv.org/abs/1903.08254\n\n[2] Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and Whiteson, S. Varibad: A very good method for bayes-adaptive deep RL via meta-learning. Oct. 2019. https://arxiv.org/abs/1910.08348\n\n[3] Liu, E. Z.; Raghunathan, A.; Liang, P.; and Finn, C. Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning. June 2020. https://openreview.net/forum?id=La1QuucFt8-\n\n======= UPDATE ========\n\nI appreciate the authors' efforts during the rebuttal period, but I still retain my initial assessment of the work.\n\nOverall, I find the proposed approach promising and easy to understand, but believe that the experiments can be improved to better substantiate the claims in this work. In particular, I believe that the benchmarks can still be more carefully chosen to better evaluate IMPORT's ability to perform sophisticated exploration. I find the 3D Maze experiment to be quite nice, as it clearly highlights a shortcoming of TS exploration, but I would find the experiments more compelling if there were additional benchmarks testing such exploration. The authors commented that exploration in meta-RL is about inferring the task to solve, which I agree with, but I think such exploration can still be made more \"sophisticated\" by requiring careful sequences of actions to lead to distant states, which reveal this task information.\n\nIn addition, several issues were raised regarding the TS baseline during the discussion period. The results in the bandits setting appear to be lower than those reported in other works, and it still seems like PEARL can be adapted to be a drop-in replacement for the TS baseline. I agree with the authors' assessment that the basic form of PEARL explores the setting with multiple episodes, but PEARL could just resample from the posterior every few timesteps, which is already what happens in the TS baseline.\n\nI do think this work should be published in the future with a more careful selection of experiments.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "AEUI9bYWjWB",
                "reply_to": "u0y8EZnGlQc",
                "title": "Author response to R3",
                "comment": "We thank you for the active discussion and multiple questions. To answer to your different points:\n1. RL^2 considers a different setting (which is different to ours and to the one in PEARL) where one trial is composed of multiple episodes: between each episode the agent returns to the first state (\u201cepisode always starts on the first state\u201d for T-MDP in [1]). IMPORT is only evaluated on one episode (without reset) which is a different setting (and more challenging since it does not allow \u2018retries\u2019). Moreover, in our setting, we are evaluating using train/test/validation tasks, evaluating the ability of the different models to generalize, which is not the setting of RL^2.  \n2. In the bandit setting, as far as we understand, one episode is just one single step and RL^2 is exactly our RNN baseline. Moreover their bandit setting is easier than the one we propose (uniform sampling of the reward probability on the arm): since the reward probability is sampled uniformly, multiple arms provide a reasonably good reward while in our setting, only one arm provides a good amount of reward. In the first case, one just has to identify a \u2018good\u2019 arm, while our setting needs the algorithm to identify the best arm. This is the reason why all the models we have compared perform less than UCB. \nWe have quickly relaunched experiments today on the MAB setting following the description provided in RL^2 (uniform sampling of the probability of the arms) for 5 and 10 arms with 100 timesteps. Our preliminary results are in line with the RL^2 paper in terms of numbers, with the RNN on par with UCB. IMPORT achieves a similar performance but seems to have a faster training. We are currently running more grid searches to finalize the results. \n3. In addition to the differences listed above, our T-MDP configuration is also different from RL^2 in the way we sample the rewards making the performance reported in the RL^2 paper and our article not directly comparable. But note that RL^2 is close to the RNN baseline, (just having the additional reset that our baseline does not use) and we show that IMPORT is competitive over RNN.\n\n[1] https://arxiv.org/abs/1611.02779",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "u0y8EZnGlQc",
                "reply_to": "usM10FPGz0N",
                "title": "R3 Response",
                "comment": "Thanks for the additional clarifications and updated draft.\n\nTo summarize, I generally like this work: the paper is clear and I find the proposed approach to be promising.\n\nMy main concern is still about whether the claims about IMPORT learning sophisticated exploration strategies are empirically substantiated. For reference, the original RL^2 paper [1] also evaluates on the bandits task and tabular MDP task (with more states). The results may not be directly comparable between this work and [1], but [1] already shows that RL^2 outperforms UCB in the bandits task for both k = 10 and k = 50. Furthermore, RL^2 performs quite well in the tabular MDP task, although the evaluation in [1] uses 10 episodes, rather than just 1, as in this work. Consequently, regardless of whether these tasks are considered to require \"sophisticated exploration,\" it's unclear if IMPORT is learning better exploration behaviors than RL^2, which does not even require $\\mu$. This is my main hesitation for raising my score to recommend acceptance.\n\n[1] https://arxiv.org/abs/1611.02779",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oSoenKPAUZu",
                "reply_to": "PRd2urm85hT",
                "title": "Notification of PDF Update ",
                "comment": "We have submitted a new version of the paper where each modification is highlighted in blue. In addition, we have provided an ablation study on the auxiliary loss in the main paper. Appendix is included in the new PDF file.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hcSFAelbqs6",
                "reply_to": "bqsEOPprZ0q",
                "title": "Notification of PDF Update",
                "comment": "We have submitted a new version of the paper where each modification is highlighted in blue. In addition, we have provided an ablation study on the auxiliary loss in the main paper. Appendix is included in the new PDF file.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "lDBvKROuLaD",
                "reply_to": "N7cQQ3JAylF",
                "title": "Notification of PDF update ",
                "comment": "We have submitted a new version of the paper where each modification is highlighted in blue. In addition, we have provided an ablation study on the auxiliary loss in the main paper. Appendix is included in the new PDF file.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1HbXwjL22a",
                "reply_to": "1wZ_1msAUp",
                "title": "Notification of PDF update",
                "comment": "We have submitted a new version of the paper where each modification is highlighted in blue. In addition, we have provided an ablation study on the auxiliary loss in the main paper. Appendix is included in the new PDF file.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "usM10FPGz0N",
                "reply_to": "Sp1BV-Y9_fz",
                "title": "Author response to R3",
                "comment": "**Sophisticated exploration.**  The exploration (probing) we are talking about is not about the exploration of the states of the MDP during the training process,  but the fact the agent needs to choose actions that allow exploring the state-space to identify the task to solve. Solving the probing/exploitation trade-off online is a POMDP problem (since mu is hidden) and is difficult even if there are only a few mu-MDP states and actions. More precisely, the difficulty of the exploration (or probing) in the Tabular MDP setting comes from the fact that the task is defined by the transition probabilities and reward over states (resulting in potentially large mu vectors, even with few states). T-MDP corresponds to a case where the probing is particularly challenging: transitions are stochastic, so in order to perform optimally, the agent needs to probe relevant state-action pairs enough times before committing (similarly to the bandits task).\nWe will be more clear about this in the updated version.\n \n**TI baseline.**  Since TI is learned to approximate the whole mu vector (while IMPORT learns small size embeddings through the informed policy), it may take a large number of samples that will be used to approximate redundant or irrelevant mu features and reduce the sample efficiency. For instance, the RNN underlying TI may spend part of its capacity to approximate irrelevant features which is not the case with IMPORT. This is what we show in Figure 4c: when information is spread over multiple dimensions of mu, TI has difficulties to learn. It is also shown in Fig. 9 (in the appendix) where the capacity of the RNN is regulated by the embedding size: when embeddings are small, TI has more difficulties to select the relevant information than IMPORT which can rely on smaller task embeddings. \n\n\n**Thompson Sampling baseline.** As defined in the literature, Thompson Sampling maintains a posterior distribution on the parameters of the system it is facing (and not upon a latent variable). The originality of PEARL is to maintain such a distribution over a latent space, but this is not how Thompson Sampling is defined.  Maintaining a posterior over \\mu also enables to leverage available information about \\mu using a supervised objective instead of the sparse (and stochastic) reward signal. In the next version, as said previously, we will add the results on bandits using a Beta distribution as a \\mu belief distribution, as described in Section 3 of [1]. Note that, if it increases the performance of TS on bandits, it still does not change our conclusions.\nAt test time, the policy conditions on a sample from the posterior distribution: we re-sample from the posterior every k timesteps, with k an hyperparameter. As stated in Section 5, \u201cFor TS, an estimated \\mu is re-sampled from the posterior every k steps, k \u2208 {1, 5, 10, 20}\u201d and k is selected through the described protocol (using validation tasks, then reported results on the test tasks). \n\n[1] Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen. A Tutorial on Thompson Sampling. Jul. 2020.\n https://arxiv.org/pdf/1707.02038.pdf\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Sp1BV-Y9_fz",
                "reply_to": "Fs43DgRTVxz",
                "title": "R3 Response",
                "comment": "Thanks for the detailed response and for pointing out the Appendix in the supplementary, which I had missed, but have now read.\n\n**Generalization to new task.** The author's response addresses my concerns here.\n\n**Sophisticated exploration.** I agree that the bandits and tabular MDP tasks require _some_ exploration. However, from the Appendix, the largest tabular MDP considered only consists of *5 states,* and the bandits task effectively only consists of a single state. This contrasts the literature on exploration in RL, which typically considers problems with many more states, or problems that require multiple time steps of exploration to reach distant states, (e.g., the classic chain MDP example in tabular exploration).\n\n**TI baseline.** Figure 4c does provide compelling empirical results. It's still not clear to me why uninformative or redundant dimensions are so problematic for TI, though. If a dimension x is redundant with dimension y, then predicting x well seems like it should also predict y well. If a dimension z is uninformative, it's clear that changing the hidden state representation to make it predictive of z shouldn't help at all, but it's not clear to me why it should harm performance\n\n**Thompson Sampling baseline.** The implementation of this baseline still seems somewhat odd to me. Rather than directly maintaining a posterior over $\\mu$'s , it seems like this baseline should maintain a posterior over a latent $z$, like PEARL. Generally, this baseline doesn't seem to type check, either. Typically, TS should involve resampling from the posterior every episode, and it's not clear when this baseline is resampling from the posterior, since there is only a single episode. With respect to the bandits task, it seems like TS in the bandits task should also update via the conjugate posterior of the beta distribution (itself another beta distribution), rather than just using a prior of the beta distribution. Using this, [1] reports significantly better results for TS in this bandits task.\n\n[1] https://arxiv.org/pdf/1707.02038.pdf",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "p4jEmtKcNeX",
                "reply_to": "tsQc82EXoAx",
                "title": "Author Response to Reviewer1 ",
                "comment": "In our experiments, we are using task seeds to define the train, validation and test environments. Given one task seed X, the train/validation/test tasks are generated with respectively seeds X, X+100, X+1000. We use three different task seeds, i.e. X in {0,1,2} for each experiment. We use different task seeds to show that our method is robust across different sets of tasks.\nNow, for each task seed and method, we explore multiple hyperparameter values and follow the classical model selection schema: we select the best hyperparameter values on the validation tasks, and compute the performance of this selected policy on the test tasks.\n\nAt last, for each experiment (e.g Cartpole with N=10 and task identifiers) and methog (e.g. IMPORT, AuxTask\u2026), we report the average performance of the 3 best sets of HPs that have been selected on the 3 task seeds. Said otherwise, on the supervised learning problem, it would be like having 3 different train/test/validation datasets (e.g ImageNet, CIFAR, MNIST), doing a separate model selection over each dataset, and then reporting an average performance, which seems a realistic setting. \nWhile this selection method allows us to average performance obtained with different hyperparameter values for different task seeds, it appears that, in practice, given an experiment and a method, the value of the best hyperparameters is actually the same on all the task seeds. \n\nWe will make this more clear in the next version of the paper submitted before the end of the rebuttal period.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ddNsGrtnZ2Z",
                "reply_to": "aJt5WdbuyvN",
                "title": "Clarification about the setting to R3",
                "comment": "\u2192  Yes, this is correct. The reported reward is the reward cumulated over the whole episode, thus including reward collected while identifying the task. This  \u201czero-shot generalization\u201d setting is particularly relevant for tasks where the agent is needed to perform well straight from the beginning, e.g being able to drive a new car. Note that results are reported by averaging the performance over multiple episodes and test tasks, but starting \u2018from scratch\u2019 at each new episode.  \n\n\n\u2192 In the bandits environment, an episode is 100 arm pulls. An arm probability vector is sampled at the beginning of the episode and remains constant during the episode.  The agent is allowed to pull an arm in [1, K] at each timestep and observes the resulting reward. It can thus identify the best arm (like UCB) by pulling them multiple times, and then pull the best arm, resulting in a policy which is more efficient than a random one. This is what IMPORT (and other baselines) is doing: discovering an efficient exploration policy that does not \u2018consume\u2019 too much reward. \n\nOur evaluation setting is a perfect fit for environments as bandits as the trade-off between probing and exploitation is of utmost importance. The setting from PEARL, DREAM, MANGA would make bandits trivial following a random policy to gain information during the free episode then exploiting the inferred best arm during other episodes.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tsQc82EXoAx",
                "reply_to": "3n7XM1NsT6",
                "title": "question about the model selection strategy",
                "comment": "I understand that multiple policies are trained on training tasks and the best one is selected based on the performance on the validation tasks. Then, I have a question about the hyperparameters of the trained policies.  Each trained model uses different set of hyperparameters? Or, multiple models are trained with the same set of hyperparameters? \nI'm asking this because the purpose of model selection in supervised learning is to select the model with the best set of hyperparameters.  Meanwhile, in RL, the performance of the trained policies varies for different random seeds. Is the purpose of the model selection to select the best policy among the different random seeds or to select the policy trained with the best set of hyperparameters?",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aJt5WdbuyvN",
                "reply_to": "MLSPpoZIi9w",
                "title": "Clarification about the setting",
                "comment": "Thanks for clarifying the setting. I have a further clarification question about the author's description of the setting:\n\n> In our setting, at evaluation time, a task is sampled and the performance of an algorithm is measured as the cumulative reward during a single episode. At the end of the episode, a new task is sampled.\n\nIf I am understanding this correctly, this means that performance is evaluated on only a single episode in each test task. Is that correct?\n\nIf so, how can any of the methods perform well on the bandits task? If there is only a single arm pull in the bandits task, it seems like none of the approaches have any information, and cannot do better than random. Perhaps I'm missing something? I'd love to hear the authors' thoughts.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "MLSPpoZIi9w",
                "reply_to": "iclr_2021_pTZ6EgZtzDU",
                "title": "General answer to all reviewers.",
                "comment": "\nWe first thank all reviewers for their valuable comments. \n\nA common request is to clarify our 'online adaptation setting' (R1) and compare it to PEARL/DREAM/MANGA [3,4,5] (R2, R3). In our setting, at evaluation time, a task is sampled and the performance of an algorithm is measured as the cumulative reward during a single episode. At the end of the episode, a new task is sampled. This is what we call the 'online  adaptation' setting, because the policy must explore as little as possible to maximize the reward on a single episode. This setting is standard in Meta-RL  literature [1,2], including Task Inference, which we compare to.\n\nIn contrast, PEARL, DREAM and MANGA have been developed and evaluated in a setting where at evaluation time, a task is sampled, and then 1) the learner is given a few \u201cfree\u201d' episodes or transitions form this task (off- or on-policy, depending on the paper), followed by 2) one or several episodes. The test performance is defined as the cumulative reward collected on step 2) only. Such meta-learning settings are different from ours because stage 1) allows the policy to perform system identification while ignoring the task reward. We did not compare to these algorithms because of this discrepancy in problems addressed.\n\n[1] Wenhao Yu, Jie Tan, C. Karen Liu, Greg Turk. Preparing for the Unknown: Learning a Universal Policy with Online System Identification. May 2017. https://arxiv.org/abs/1702.02453\n\n[2] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, Nicolas Heess. Meta reinforcement learning as task inference. Oct 2019.  https://arxiv.org/abs/1905.06424\n\n[3] Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy meta-reinforcement learning via probabilistic context variables. Mar. 2019. https://arxiv.org/abs/1903.08254\n\n[4] Liu, E. Z.; Raghunathan, A.; Liang, P.; and Finn, C. Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning. June 2020. https://openreview.net/forum?id=La1QuucFt8-\n\n[5] Homanga Bharadhwaj, Shoichiro Yamaguchi, Shin-ichi Maeda. MANGA: Method Agnostic Neural-policy Generalization and Adaptation. Nov. 2019. https://arxiv.org/abs/1911.08444\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Fs43DgRTVxz",
                "reply_to": "N7cQQ3JAylF",
                "title": "Author Response to Reviewer3",
                "comment": "We thank you for the constructive comments. We would like to point out that the appendix contains additional experiments and is available in the supplementary material file (the .zip file) submitted and available at the time of review.   We will update the paper to move important points of the current supplementary material  into the main paper. \n\nTo be more precise on the points you raised:\n\na. We consider the problem of generalization to new tasks, that is why the set of train/validation and test tasks are different without overlap between the tasks (except in the Maze3d experiment where only two goal locations are possible - the objective of the Maze3d experiment was not to test the generalization ability, but the capacity of the model to scale to high dimensional input spaces where the mapping between pixel inputs and \\mu is complicated).\n\nFigure 4 does not show an improvement in terms of sample efficiency for the Cartpole environment, because CartPole needs a very simple exploration strategy (basically, doing random transitions is enough to identify the task to solve). We included CartPole to showcase that IMPORT\u2019s learned representations lead to better generalization than other methods (Table 1). We would like to point out the results obtained on problems where the exploration is more complex like Bandits (Figure 13 of the appendix), tabular MDP (Figure 16) and Maze3d (Figure 15) where the improvement in terms of sample efficiency is clear. Again, these results are in the appendix for sake of space, but we will resubmit an updated version at the end of the discussion period with the additional page containing some of these curves to support our claims.\n\nb. The Bandits and Tabular-MDP (Appendix C.3 and C5) environments are two typical examples of problems where the exploration is complicated. For instance, in the Bandit environment, since IMPORT needs to address the exploration/exploitation trade-off, it needs to discover strategies like UCB, which is far from being simple to learn just from interactions. For the tabular MDP, the agent has to estimate both the reward and transition probabilities while trying to stay in high-reward states which is also a complicated strategy. Control problems (i.e CartPole and Acrobot) are mainly used to demonstrate the generalization ability of our method and we agree that they involve simpler exploration strategies.\n\nc. Following your suggestion, we performed an ablation study for the auxiliary loss. It shows that in all environments (CartPole, Acrobot, Bandits) except Tabular MDP, IMPORT benefited from the supervised auxiliary objective (in terms of sample efficiency and final performance).   The curves will be added in the appendix of the next version of the paper, which we will submit at the end of the discussion period.\n\nd. First of all, one of the \u2018structural\u2019 differences between TI and IMPORT is that TI tends to reconstruct the whole \\mu information, while IMPORT is focused on the relevant part of mu discovered by learning the informed policy. Figure 4c describes an experiment where relevant information is spread over multiple features. In this case, some of the created features are almost uninformative (or redundant with other features) and the results show that IMPORT outperforms TI. \n\ne. In addition to results on Maze3d,  IMPORT  clearly outperforms all the other methods on Tabular MDP (Fig.  16  in the current version of the appendix), both in terms of sample efficiency and final performance.\n\nf. The appendix was provided in the supplementary material file together with the submission. The way we implemented TS is the following: the informed policy is used to sample episodes on which a) the log-likelihood of \\mu is maximized (and computed using a RNN) b) the policy is updated to maximize the task reward.  As explained in the related work section of the paper, such training does not allow the agent to learn probing as it always acts according to an informed policy.  To circumvent this, we implemented a new version that samples from the posterior \\mu distribution even at train time, yielding better results (though not matching IMPORT, AuxTask and TI\u2019s results). We also ran TS, AuxTask and TI on bandits with a prior beta distribution: results are slightly better but still IMPORT is better when K=20. We will add those to the appendix in the next version of the paper. \n\ng. As we discuss in the comment to all reviewers, PEARL is using a different evaluation protocol. \n\nAn updated version of the paper will be resubmitted ASAP. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "k0TTsc15fb",
                "reply_to": "1wZ_1msAUp",
                "title": "Author Response to Reviewer2",
                "comment": "We thank you for the constructive comments.\n\n1. Our algorithm, IMPORT,  learns representations of tasks to speed up learning for any type of task descriptors (\\mu). We performed experiments with two types of task descriptors: \na) a set of task features, \nb) the weakest possible information about the task, i.e. a task identifier / index of the train task.  \nAccess to such a \\mu during training is a common assumption in the MTRL literature  and captures a large variety of  concrete problems: \nType a) informative \\mu at train time  is a common assumption in the literature of domain randomization [1, 2] and multi-task [3].  It is particularly relevant for problems when we have prior knowledge on physical parameters of the environment and/or the agent (robotics) or the reward function (e.g. the speed target in half cheetah). \nType b) (See [4, 5])) mu information is less restrictive and corresponds to a large number of problems: learning in a set of N training levels in a video game, learning to drive on N different vehicles, learning to interact with N different users, learning to control N different robots, etc\u2026 \nIn the two cases, IMPORT can use these tasks identifiers to generalize well. Moreover,. experimental results on  CartPole (Fig. 6) and Tabular MDP (Fig. 16) suggest that the type a) improves sample efficiency, however it does not change the final performance. Equivalent final performances of IMPORT on both types of privileged information is a desirable property and shows our method is agnostic to task descriptors.\n\n2. As explained in the comment to all reviewers, the comparison with PEARL, MANGA, DREAM is not trivial because these models have been developed in a different setting. For instance,  MANGA uses \u201cfree\u201d off-policy data for task identification, whereras IMPORT in an online method where the identification is made during the episode. The collected reward that we report in our paper considers the \u2018price\u2019 of the task identification, while it is ignored in the MANGA approach. Thus, MANGA does not need to address the exploration/exploitation dilemma that IMPORT addresses.\nWe agree with the reviewer that using  the factored Gaussians architecture of PEARL could be an alternative to our RNN. Nonetheless, this is orthogonal to the main idea of IMPORT, and the RNN is a more general architecture with which we obtained good results, so we decided to keep it in the paper.\n\nAn updated version of the paper will be resubmitted ASAP. \n\n[1] Bhairav Mehta, Manfred Diaz, Florian Golemo, Christopher J. Pal, Liam Paull. Active Domain Randomization.  Jul. 2019. https://arxiv.org/abs/1904.04762\n\n[2] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. Mar. 2017. https://arxiv.org/abs/1703.06907\n\n[3] Wenhao Yu, C. Karen Liu, Greg Turk. Policy Transfer with Strategy Optimization. Dec. 2018. https://arxiv.org/abs/1810.05751\n\n[4] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, Nicolas Heess. Meta reinforcement learning as task inference. Oct 2019.  https://arxiv.org/abs/1905.06424\n\n[5] Samuel P. M. Choi, Dit-Yan Yeung, Nevin L. Zhang.  Hidden-Mode Markov Decision Processes for Nonstationary Sequential Decision Making. Dec. 2001. https://link.springer.com/chapter/10.1007/3-540-44565-X_12\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rBcclgsvzIs",
                "reply_to": "bqsEOPprZ0q",
                "title": "Author Response to Reviewer4",
                "comment": "Thank you for your review and comments. \n\nConcerning the significance to the community, we consider that, even if the model can be seen as an architecture change w.r.t TI or AuxTask, it is in our opinion much more than just an architectural contribution. Indeed, the main novelty of our model is to simultaneously learn an informed policy and a recurrent policy.  Training the informed policy is fast as it solves a fully-observable MDP.  The informed policy helps the discovery of the recurrent policy that efficiently manages the exploration/exploitation trade-off.  Moreover it helps in two ways:\na) transfer to the recurrent policy through weight sharing: previous approaches did not leverage the privileged information during trajectory rollouts.\nb) providing a task embedding that is focused on the information minimally relevant for the task to solve while approaches like TI and AuxTask are based on an auxiliary supervised objective whose target can contain irrelevant or misleading information. Good learned representations enable better generalization on unseen tasks.\n\n An updated version of the paper will be resubmitted ASAP. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3n7XM1NsT6",
                "reply_to": "PRd2urm85hT",
                "title": "Author Response to Reviewer1",
                "comment": "We thank R4 for their review.\n\n- To clarify the experimental protocol: train, test and validation tasks are sampled from the same distribution, but are not overlapping (except for the Maze3d environment since only two tasks are possible, but this environment is used to demonstrate the ability of the model to deal with large input spaces and complex exploration policies). The task information (\\mu) is available only at train time. The informed policy is learned and used only at train time: a) it is used to build a relevant task embedding for training tasks that will guide the RNN to build relevant  embeddings at test and validation time b) the informed policy shares weight with the final policy, allowing the final policy to faster converge to a good solution. Again, at test and validation time, only the recurrent policy is used, the informed policy being only updated on the training environments where the task information or task identifier is known. \u201cOnline adaptation\u201d is done thanks to the internal state of the RNN being updated at each time-step (contrary to weight updates in MAML approaches).\n\n- All curves in the paper correspond to the performance obtained on testing tasks, thus showing the ability of IMPORT to generalize to unseen tasks. \n\n- To correctly evaluate generalization to unseen environments, we use the following model selection strategy, as done in the supervised learning  paradigm:\nWe train policies on training tasks, and collect their performances on validation and test tasks. To select the policies that we expect to be the best on the test tasks (i.e to avoid overfitting on the training task), we select the final policies on another set of unseen tasks (the validation tasks). Then we report the performance of these selected policies on the test tasks.\n\n- You are right. Thanks, we will describe the expectation explicitly in the next version of the paper (that we will submit before the end of the revision period)\n\n- Thanks again, the right sentence is \u201cNote that vanilla RNN does not use \\mu at train time\u201d\n\n- In practice, actions are encoded by their one-hot encoding. At timestep = 0, the RNN is initialized with a fixed internal state (a vector of zeros), and the last action is considered to be a vector full of zeros (which does not correspond to a \u201creal\u201d action). \n\nAn updated version of the paper will be resubmitted ASAP. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PRd2urm85hT",
                "reply_to": "iclr_2021_pTZ6EgZtzDU",
                "title": "Nice method, but the paper needs some more work",
                "comment": "This paper presents a method that leverage task descriptors for multi-task learning. In the proposed method, an informed policy that takes in the task descriptor and the state is trained to maximize the expected return. At the same time, a RNN policy based on the history of states and actions is trained such that the RNN layers imitates the behavior of the feature extraction layers of the informed policy. In this way, the RNN policy is trained as if the task description is available. The experimental results show that the proposed method outperforms baseline methods that leverages the tasks descriptor.\n\nThe proposed method seems novel and the experimental results show its benefits. However, there are some unclear points. Especially, \u201conline adaptation\u201d described in the introduction is not clear.  I would like to ask the authors to clarify the following points:\n\n- The experiment procedure is not clear to me. I understand that the informed policy and the RNN policy are trained on training tasks, but I\u2019m not sure how the policy is adapted for test tasks. Both informed and RNN policies are further trained on test tasks? \n\n- Does Figure 4 shows the learning curve during the training on the training tasks? If so, I recommend to add the learning curve on the test tasks to show the performance of adaptation. \n\n- In page 7, I do not clearly understand this sentence: \u201cEach model is trained on the training tasks, and the best model is selected on the validation tasks.\u201d Were several models trained on training tasks? If so, how many models were trained? \n\n- If I understand correctly, the policy is trained to maximized the expected return across the training tasks. If so, for clarify, I recommend to describe the expectation explicitly in Eq. (3), e.g, E_{\\mu \\sim p(\\mu)} [ E_{s \\sim p(s\u2019|s, a), a \\sim \\pi(a|s)} [ \u2026 ] ] \n\n- Caption of table 1 \u201cNote that RNN does not \\mu at train time.\u201d <- something is wrong?\n\n-\tHow to sample action at the initial step in the test tasks? The RNN policy seems to require the previous action a_{t-1} to generate actions, but a_{t-1} is not available in the first time step. \n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "bqsEOPprZ0q",
                "reply_to": "iclr_2021_pTZ6EgZtzDU",
                "title": "Simple end-to-end approach works well.",
                "comment": "The authors propose an alternative architecture to handle explore/exploit tradeoffs in RL environments where each task instance may change in such a way that the policy needs to change in order to be optimal. Rather than using an explicit task inference process, and rather than relying on an RNN to slowly learn the distribution implicitly, the task id is observed during training instances and both an embedding and an RNN are trained, such that the two are interchangeable. Thus, during testing, the task id is not needed and only the RNN state is used to condition the policy. It is a straightforward way to include privileged information during training without imposing the burden of reconstruction.\n\nThe paper is clearly written and Fig 1 is very helpful to understanding the details of the architecture. The experiments are clearly explained.\nThe main question as a reviewer is whether the paper has significance to the community. Although it is only a small architectural contribution, the method works impressively well. It is faster to learn than Task Inference and achieves higher scores than Thompson Sampling. \n\nIt would be nice to know if the method could work in combination with other methods to quickly adapt in dynamic environments, given some labels for different features of the environments. In general, using an interchangeable embedding and RNN state is a good way to avoid the challenges of conditional architectures. The paper could be stronger if the method was framed more generally and it was shown that it could be useful on a broader range of domains that require adaptation or exploration/exploitation strategies.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "- important and relevant topic - IMPORT is a reasonable, technically sound approach - paper is relatively clear",
                "Sentiment Expression": "The pros are",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "experimental evaluation",
                "Sentiment Expression": "The cons all lie in",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The choice of benchmarks",
                "Sentiment Expression": "does not sufficiently test the ability to explore in a sophisticated manner",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Lack of comparisons to PEARL and MANGA",
                "Sentiment Expression": "can readily be applied to the online setting",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The empirical improvements",
                "Sentiment Expression": "are relatively modest.",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the cons",
                "Sentiment Expression": "slightly outweigh the pros of the paper.",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper's acceptance",
                "Sentiment Expression": "no reviewer was willing to champion",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "r1gIdySFPH": {
        "paper_id": "iclr_2020_r1gIdySFPH",
        "paper_title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
        "paper_abstract": "Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing the entropy of the goal distribution together with goal reaching performance, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. Skew-Fit enables self-supervised agents to autonomously choose and practice reaching diverse goals. We show that, under certain regularity conditions, our method converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that it can learn a variety of manipulation tasks from images, including opening a door with a real robot, entirely from scratch and without any manually-designed reward function.",
        "paper_acceptance": "reject",
        "meta_review": "This paper tackles the problem of exploration in RL. In order to maximize coverage of the state space, the authors introduce an approach where the agent attempts to reach some self-set goals. The empirically show that agents using this method uniformly visit all valid states under certain conditions. They also show that these agents are able to learn behaviours without providing a manually-defined reward function.\n\nThe drawback of this work is the combined lack of theoretical justification and limited (marginal) algorithmic novelty given other existing goal-directed techniques. Although they highlight the performance of the proposed approach, the current experiments do not convey a good enough understanding of why this approach works where other existing goal-directed techniques do not, which would be expected from a purely empirical paper. This dampers the contribution, hence I recommend to reject this paper.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SJxzk8sitS",
                "reply_to": "iclr_2020_r1gIdySFPH",
                "title": "Official Blind Review #4",
                "comment": "The paper introduces SKEW-FIT, an exploration approach that maximizes the entropy of a distribution of goals such that the agent maximizes state coverage. \n\nThe paper is well-written and provides an interesting combination of reinforcement learning with imagined goals (RIG) and entropy maximization. The approach is well motivated and simulations are performed on several simulated and real robotics tasks.\n\nSome elements were unclear to me:\n- \"We also assume that the entropy of the resulting state distribution H(p(S | p\u03c6)) is no less than the entropy of the goal distribution H(p\u03c6(S)). Without this assumption, a policy could ignore the goal and stay in a single state, no matter how diverse and realistic the goals are.\" How do you ensure this in practice?\n- In the second paragraph of 2.2, it is written \"Note that this assumption does not require that the entropy of p(S | p\u03c6) is strictly larger than the entropy of the goal distribution, p\u03c6.\" Could you please clarify?\n\n\nThe experiments are interesting, yet some interpretations might be too strong (see below):\n- In the first experiment, \"Does Skew-Fit Maximize Entropy?\", it is empirically illustrated that the method does result in a high-entropy state exploration. However, it is only compared to one very naive way of exploring and it is not discussed whether other techniques also achieve the same entropy maximization. The last sentences seems to imply that only this technique ends up optimizing the entropy of the state coverage, while I believe that the claim (given the experiment) should only be about the fact it does so faster.\n- On the comments of Figure 6, the paper mentions that \"The other methods only rely on the randomness of the initial policy to occasionally pick up the object, resulting in a near-constant rate of object lifts.\" I'm unsure about the interpretation of this sentence given Figure 6 because other methods do not seem to fail entirely when given enough time.\n- In the experiment \"Real-World Vision-Based Robotic Manipulation\", It is written that \"a near-perfect success rate [is reached] after five and a half hours of interaction time\", while on the plot it is written 60% cumulative success after 5.5 hours and it is thus not clear where this \"5.5 hours\" comes from.",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HJlTYAfnoS",
                "reply_to": "rJgogsu-jB",
                "title": "Additional Experiments Added",
                "comment": "As suggested, we have added experiments that study the importance of the reinforcement learning algorithm used with Skew-Fit. Specifically, we replaced soft actor critic (SAC) with twin delayed deep deterministic policy gradient (TD3) and reran the simulated, vision-based experiments with this new combination. The results show that Skew-Fit performs well with both TD3 and SAC, with both versions achieving approximately the same final error. These results suggest that the benefits of Skew-Fit are not specific to SAC, and can instead be combined with other reinforcement learning algorithms.\n\nWe have also added an experiment that explicit tests for exploration, by using a maze environment with long corridors and measuring the state coverage. These experiments are shown in Section B.1, and show that Skew-Fit significantly accelerates exploration. We have also added a simulated robot quadruped experiment that requires a robot to explore a narrow box-shaped corridor. In this experiment, we again see that Skew-Fit results in faster exploration than prior methods (see Section B.1).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJlYx0G3jB",
                "reply_to": "Bkl-PnTtir",
                "title": "Additional Experiments Added",
                "comment": "As suggested, we have also added additional exploration experiments on a simple maze setup in Section B.1. The maze and action spaces are designed so that random actions are unlikely to result in fast exploration and instead require goal-directed exploration. In these new experiments, we see that Skew-Fit significantly accelerates exploration.\n\nWe have also added a simulated robot quadruped experiment that requires a robot to explore a narrow box-shaped corridor. In this experiment, we against see that Skew-Fit results in faster exploration than prior methods.\n\nWe note that prior work in the field have similarly tested their algorithms on 3 simulated domains [1,2]. We believe that with our real-world robot experiments, as well as the additional experiments described above, our evaluation provides a similar level of rigor.\n\n[1] Andrychowicz, Marcin, et al. \"Hindsight experience replay.\" Neural Information Processing Systems. 2017.\n[2] Nair, Ashvin, et al. \u201cVisual Reinforcement Learning with Imagined Goals. Neural Information Processing Systems. 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bkl-PnTtir",
                "reply_to": "SJgBwsTYsr",
                "title": "Re: Clarifications, further comments... from AnonReviewer1 (2/2)",
                "comment": "(continued from previous comment)\n\n> What do you exactly mean by a policy to be reusable?\nBy reusable, we mean that a policy can reach user-defined goals after performing exploration. In other words, it is a goal-conditioned policy rather than a policy that only performs exploration. For example, imagine you left a robot in a large office. If it performs exploration well, then the robot will autonomously visit every room in the building, regardless of whether it is goal-oriented or not. The difference between a goal-conditioned and non-goal-conditioned exploration policy is based on what happens next: After exploration, you can tell a goal-conditioned policy, \u201cPlease go to room A.\u201d and it will know how to go to room A since it has already practiced reaching every possible goal. In other words, we can *reuse* the exploration policy to achieve these user-specified goals. However, if the exploration policy is not goal-conditioned but instead trained with an exploration-reward bonuses, then there is no way to control the policy. It may have visited every room during exploration, but at any given time, it only knows how to reach one location. In particular, it will always go to the last state that was deemed novel by the exploration-reward bonus.\n\n\n> What do you exactly mean by a distribution over terminal states? Why not the normalized discounted weighting of states?\nBy terminal state, we mean the last state of each episode. So, the distribution over terminal states is the distribution of states where the policy will be located at the end of each episode. For example, will the policy always end at position X? Or will its final position have a Gaussian distribution? Uniform distribution?\n\nFor our analysis, we found it more natural to study the distribution over terminal states, since in the goal-conditioned setting, we would like our goal-conditioned policy to *end* at a goal state when the episode is complete. However, we believe that our analysis could be applied to normalized discounted weighting of states by allowing an agent to constantly set new goals rather than waiting until the beginning of a new episode to set the next goal.\n\n\n> Does \u201cno user-specified rewards\u201d mean a sparse reward?\nNo, and we understand that this may have been a source of confusion. By \u201cno user-specified rewards\u201d we mean that the user does not need to manually engineer a reward for each task. Instead, as described in Section 4, we use the same reward as the one used in RIG, which is an approximation of the log probability of the goal given the current state. Overall, this means that the same code and generic reward is used for the real-world door task and all of the simulated tasks. For example, there is no reward that specifically tells the robot to open the door, nor is there a reward that specifically tells the robot to pick up or move the objects. Instead, the same Skew-Fit objective encourages the robot to learn to manipulate these objects into as many configurations as possible (by setting diverse goals and then reaching those goals) regardless of the environment.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJgBwsTYsr",
                "reply_to": "S1lF1vROjS",
                "title": "Re: Clarifications, further comments... from AnonReviewer1 (1/2)",
                "comment": "Thank you for the additional response. We answer your remaining questions, and are happy to continue discussing if there are still points of confusion. In particular, we explain the large empirical and conceptual differences between Skew-Fit and prior methods, both of which we believe would be of interest to the ICLR community.\n\n\n> How does Skew-Fit relate to hindsight experience replay (HER) and similar approaches? How is Skew-Fit different?\nWe note that there are significant empirical differences between Skew-Fit and HER. In Figure 5, we see that HER does not perform well without access to an oracle uniform goal distribution (more on this below). Specifically, HER has a final distance that is 400%, 200%, and 150% higher than the final distance when using of Skew-Fit, on the door, pickup, and pushing tasks, respectively. Similarly, Skew-Fit outperforms other prior methods across all tasks.\n\nA major conceptual difference between Skew-Fit and many prior methods, such as HER, is that we *learn* the goal distribution, whereas many prior methods *assume* that a uniform goal distribution is provided. When using images as observations, this amounts to assuming that the agent knows the distribution of natural images -- an unreasonable assumption in most cases. For example, in the object-pushing task of the hindsight experience replay (HER) paper, an XY goal-position for the object is sampled uniformly from within the workspace of the robot and given to the robot for exploration. While this procedure is simple to implement in a simulated domain and when the goal corresponds to an XY-Cartesian position in the plane, it is much more challenging when goals correspond to images, as in the environments that we tested. Randomly sampling an image will result in an image of static noise. So, Skew-Fit learns a goal distribution that corresponds to the uniform distribution over the set of valid states, instead of assuming that we are given access to this goal distribution.\n\nBecause most prior goal-conditioned RL methods assume access to an oracle uniform goal distribution, most of the methods have only been applied to simulated domains, where defining such a goal distribution by hand is easy. As our experiments show, without access to a uniform distribution, prior methods such as HER perform poorly. Moreover, many of these methods (AutoGoal GAN, Rank-Based Prioritization, HER) assume access to ground-truth state information for computing the reward, which is readily available only in simulation. Enabling goal-conditioned RL to be applied to domains where the goal-space and reward function are unknown a priori, such as image-based domains, is important if we want to use these methods outside of simulation and for real-world applications. We believe that Skew-Fit is a useful step, both empirically and conceptually, towards this objective. Note that we evaluate Skew-Fit on a real-world image-based robotic manipulation task to demonstrate this (see Figure 1 and 7).\n\nAs far as we know, the only other goal-conditioned methods that have been developed for goal images are RIG and DISCERN, but neither of these methods address the important question of how goals should be sampled for exploration. Applying Skew-Fit results in considerable performance gains over these prior methods: Figure 5 shows that RIG and DISCERN have final distances that are about 100% higher than that of Skew-Fit. Lastly, the only method that has been applied to real-world robot domains from images is RIG, and we found that Skew-Fit outperformed it not only in simulation, but also on the real-world door task.\n\n\n> How are these goals determined in the first place?\nWe first describe how we generate goals chosen for evaluation. For the simulated tasks, we use an \u201coracle\u201d sampling procedure that exploits the fact that the task is simulated. Note that this procedure is only used for evaluation and never used by the algorithms. To generate an evaluation goal, we sample a ground-truth state uniformly distribution from the entire state space, set the environment to this state, capture an image corresponding to this state, and use the resulting image as the goal. We then return the environment state back to its original state and instruct the policy to reach the captured goal image. This procedure based on ground-truth state information is only used for evaluation and in simulation. For the real-world door task, we took images of the door at 5 different angles, evenly spaced from 0 to 45 degrees. Like before, these goals are only used for evaluation and not be the algorithms.\n\nWe now describe how goals for exploration are generated. At the very beginning of exploration, the agent takes random actions in the environment to collect a set of states. From thereon, the exploration goals are generated by the agent by sampling from the learned goal distribution, which is learned with Skew-Fit.\n\n(continued in next comment)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1lF1vROjS",
                "reply_to": "BygXShuZsS",
                "title": "Clarifications, further comments and doubts on technical novelty. It seems like the contribution is somewhat marginal. Not fully convinced about the usefulness of this approach.",
                "comment": "Thank you for your detailed response. It is certainly very helpful. However, I have few other comments and questions : \n\n- It seems that this approach is useful in the case where there are no user-specified rewards (my understanding is you mean a sparse reward setting). However, as you mention, that p(s) in your case represents the distribution over terminal states for a finite horizing setting - I find it difficult to understand what this distribution actually means in an episodic setting. \n\nWhat do you exactly mean by a distribution over terminal states? My understanding is this would be the normalized occupancy measure, where the occupancy is only over the terminal states? Given this, why should we even consider this type of p(s) and why not the normalized discounted weighting of states - and maximize this distribution accordingly? I do agree that simply maximizing this might not be sufficient, which is why you include the H(s|g) term - but that raises further concerns in my opinion, as follows : \n\nWith the current formulation, we would now require distributing goals across the state space - this is similar to Hindsight Experience Replay and related approaches? How are these goals determined in the first place - is it more like a random sampling of states that you consider as goals - and then encouraging the agent to reach these goals?\n\nIsn't this method then a very slight modification of HER and other related papers? To me, it seems like that makes the contribution very marginal - since instead of HER, now we have a clever way of maximizing a MI term, but it is indeed doing something very similar to HER from a technical contribution? \n\nFurther comment : I don't necessarily agree, or fully understand this from a technical perspective - \"We instead would like for this policy to be reusable, by, e.g., being able to control what state it reaches\". \n\nWhat do you exactly mean by a policy to be reusable? Is it more like a transfer learning setting? If so, are there any experiments that justify that using the trained policy in a new task is useful? \n\nOverall, as you mention \"first uniformly set goals over the state space (maximize H(g)) and then separately learn to reach those goals (minimize H(g | s))\" - this seems to me like a very minor modification to several other tons of papers related to HER - where we randomly sample goal states and encourage the agent to reach these goal states. This paper seems like a very marginal contribution compared to that.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BygXShuZsS",
                "reply_to": "rkgaGhdWjr",
                "title": "Citations for: Re: Official Blind Review #1",
                "comment": "Due to space constraints, the citations are included in this separate comment:\n\n[1] Hazan, Elad, et al. \"Provably Efficient Maximum Entropy Exploration.\" International Conference on Machine Learning. 2019.\n[2] Bellemare, M, et al.. Unifying count-based exploration and intrinsic motivation. NeurIPS. 2016.\n[3] Tang, H., et al. #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. NeurIPS, 2017.\n[4] Burda, Y., et. al. Large-scale study of curiosity-driven learning. ICLR. 2019.\n[5] Burda, Y., et. al. \"Exploration by random network distillation.\" ICLR. 2019.\n[6] Nair, Ashvin, et al. \u201cVisual Reinforcement Learning with Imagined Goals. Neural Information Processing Systems. 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkgaGhdWjr",
                "reply_to": "HyeleOROFS",
                "title": "Re: Official Blind Review #1",
                "comment": "Thank you for the suggestion and detailed review. As suggested, we have modified the introduction to expand our discussion around H(s). We also answer the questions about the use of H(s) and H(s|g) below, and describe how the experimental results do in fact show that Skew-Fit substantially outperforms prior methods. We believe that these clarifications address the major criticisms raised in your review, but we would be happy to address any other points or discuss this further.\n\nQ: Why should H(s) used as an exploration objective?\n\nThe goal of our method is to learn a policy that can reach any possible goal state, in the absence of a single user-specified task reward. Prior work has already argued that the entropy of the state distribution H(s) is a suitable exploration objective [1]. In our case, p(s) represents the distribution over terminal states in a finite horizon task, though we believe extensions to infinite horizon stationary distributions should also be possible. Unfortunately, maximizing H(s) by itself does not necessarily provide for a useful policy in the absence of a user-specified reward. For example, if we maximize state coverage by using reward bonuses based on state novelty [2,3,4,5], then, in the absence of user-specified rewards, the resulting policy will only reach the latest states deemed novel.  We instead would like for this policy to be reusable, by, e.g., being able to control what state it reaches. This observation motivates the inclusion of the second term -- H(s|g) -- which amounts to training the policy to effectively (with high probability) reach the commanded goal, while being able to visit as many states/goals as possible. Our overall objective is therefore to maximize H(s) - H(s | g), since maximizing only H(s) does not result in a useful policy. As you pointed out, this has the added benefit that the corresponding algorithm is tractable by using Equation 1, whereas directly maximizing H(s) is difficult. We have accordingly modified the introduction to (1) discuss prior work, (2) raise the concern with directly maximizing H(s), and (3) include a more specific definition of H(s).\n\nQ: What\u2019s the intuition behind the new objective MI(S;G)?\n\nThe mutual information provides an equivalent interpretation of our new objective: the new objective changes the exploration objective from \u201cuniformly visit all the states,\u201d as prior work has advocated, to a two stage process: first uniformly set goals over the state space (maximize H(g)) and then separately learn to reach those goals (minimize H(g | s)). At the optimum, the exploration policy will uniformly visits all states, and has the added benefit that we obtain a goal-conditioned policy that can be reused to reach goals.\n\n\nExperiments\nWe understand that there were concerns over the significance of the results. We find this concern surprising, as there is a clear difference between Skew-Fit and the next best prior work in Figure 5. Specifically, for the pickup task, Skew-Fit is the only method that makes significant progress: no prior method consistently picks up the object (Figure 6), and Skew-Fit\u2019s final distance is approximately half that of the next best method. For the pushing tasks, the next best method results in a final distance that is 1.5 times worse than that of Skew-Fit, with an average score that is 3-4 standard deviations away from the average score of Skew-Fit. On the door task, some prior methods perform only slightly worse than Skew-Fit. However, we note that this task is much easier than the other tasks (the x-axis more than 4x shorter than the other tasks), as prior work [6] using these environments has also observed. Lastly, the difference on the real-robot experiments are particularly pronounced, with a final success rate double that of the prior method. While we acknowledge that the presentation of the results in the plots could be improved, the results themselves show that Skew-Fit is substantially better than all prior methods that we compared with.\n\nWe agree that it is informative to include a simplified experiment that does not directly jump to using goal-conditioned policies nor images. Therefore, Figure 3 of Section 6 analyzes a simplified 2D navigation task. While we did not have room to include in the main paper, Figure 9 of the appendix provides an \u201cin between\u201d experiment that does not contain images, but does include goal-conditioned policies.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgUssdWjr",
                "reply_to": "SJxzk8sitS",
                "title": "Re:  Official Blind Review #4",
                "comment": "Thank you for the review and suggestions. We have adjusted the experimental discussion to clarify a few points of confusion and to avoid possibly overstating the results.\n\n\n> \"We also assume that the entropy of the resulting state distribution H(p(S | p\u03c6)) is no less than the entropy of the goal distribution H(p\u03c6(S)). Without this assumption, a policy could ignore the goal and stay in a single state, no matter how diverse and realistic the goals are.\" How do you ensure this in practice?\nWe found that using RIG performed quite well. In particular, we found that using hindsight experience replay with the dense latent-distance ensured that the goal-conditioned policies consistently paid attention to the goal, and eventually learned to reach them.\n\n\n> In the second paragraph of 2.2, it is written \"Note that this assumption does not require that the entropy of p(S | p\u03c6) is strictly larger than the entropy of the goal distribution, p\u03c6.\" Could you please clarify?\nWe mean that the entropy of p(S | p\u03c6) and p(\u03c6) can be equal. It is unnecessary for the entropy to increase during exploration, since we increase it by changing the goal-distribution.\n\n\n> The last sentences seems to imply that only this technique ends up optimizing the entropy of the state coverage, while I believe that the claim (given the experiment) should only be about the fact it does so faster.\nWe agree that other methods can also eventually maximize the entropy. We have modified the sentence to clarify that we mean that Skew-Fit results in higher entropy faster.\n\n\n> I'm unsure about the interpretation of this sentence given Figure 6 because other methods do not seem to fail entirely when given enough time.\nThank you for pointing out this unclear phrasing. Since we are plotting the cumulative pickups, the success rate is given by the slope of the curves. While some prior methods do perform better than others, most curves have constant slopes after the first 40k iterations, meaning that their success rate does not increase over time. We have modified the text to clarify this.\n\n\n> it is thus not clear where this \"5.5 hours\" comes from.\nWe have corrected the text to say 6 hours. Thank you!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgogsu-jB",
                "reply_to": "BylJ3YZntB",
                "title": "Re: Official Blind Review #2",
                "comment": "Thank you for the review and suggestions. Below, we address a number of questions asked and are happy to continue the discussion.\n\n> I would also like to see how Skew-Fit works with different goal-conditioned RL algorithms\n\nWe are currently running experiments that replace SAC with TD3. The only image-based, goal-conditioned RL algorithm other than RIG that we are aware of is DISCERN, which we found never learned. We are happy to take suggestions for alternate image-based, goal-conditioned RLs algorithm to try.\n\n\n> More elaboration on this point is necessary.\n\nThank you for the suggestion. We have updated Section E to clarify, and include the new text here for convenience:\n\u201c...one can see that goal-conditioned RL generally minimizes H(G | S) by noting that the optimal goal-conditioned policy will deterministically reach the goal. The corresponding conditional entropy of the goal given the state, H(G | S) would be zero, since given the current state, there would be no uncertainty over the goal (the goal must have been the current state since the policy is optimal). So, the objective of goal-conditioned RL can be interpreted as finding a policy such that H(G | S) = 0. Since zero is the minimum value of H(G | S), then goal-conditioned RL can be interpreted as minimizing H(G | S).\u201d\n\n\n> Appendix has several broken references.\n\nThank you. We have fixed the references.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyeleOROFS",
                "reply_to": "iclr_2020_r1gIdySFPH",
                "title": "Official Blind Review #1",
                "comment": "Summary : \u000b\n\nThe paper proposes an exploratory objective that can maximize state coverage in RL. They show that a formal objective for maximizing state coverage is equivalent to maximizing the entropy of a goal distribution. The core idea is to propose a method to maximize entropy of a goal distribution, or a state distribution since goals are full states. They show that the proposed method to maximize the state or goal distribution can lead to diverse exploration behaviour sufficient for solving complex image based manipulation tasks. \n\n\nComments and Questions : \n\n\t- The core idea is to maximize the entropy of the state visitation frequency H(s). It is not clear from the paper whether the authors talk about the normalized discounted weighting of states (a distribution) or the stationary distribution? The entropy of the state visitation distribution only deals with valid states - but I am not sure what it means to maximize the entropy of this term exactly in terms of exploration, since it is neither the discounted weighting of states or the stationary distribution for an infinite horizon task? \n\t- The authors do mention that maximizing the entropy of H(s) is not sufficient - so instead suggests for maxmizing entropy of H(s|g). But why is this even sufficient for exploration - if I do not consider new tasks at test time but only the training task? How is this a sufficient exploration objective? Furthermore, since it is the conditional entropy given goal states, the fundamental idea of this is not clear from the paper. \n\t- Overall, I am not convinced that an objective based on H(s|g) is equivalent to an maximizing H(s), and why is this even a good objective for exploration? The meaning of H(s) to me is a bit vague from the text (due to reasons above) and therefore H(s|g) does not convince to be a good exploration objective either?\n\t- The paper then talks about the MI(S;G) to be maximized for exploration - what does this MI formally mean? I understand the breakdown from equation 1, but why is this a sufficient exploration objective? There are multiple ideas introduced at the same time - the MI(s;g) and talking about test time and training time exploration - but the idea itself is not convincing for a sufficient exploration objective. In light of this, I am not sure whether the core idea of the paper is convincing enough to me. \n\t- I think the paper needs more theoretical insights and details to show why this form of objective based on the MI(s;g) is good enough for exploration. Theoretically, there are a lot of details missing from the paper, and the paper simply proposes the idea of MI(s;g) and talks about formal or computationally tractable ways of computing this term. While the proposed solutuon to compute MI(s;g) seems reasonable, I don't think there is enough contribution or details as to why is maximizing H(s) good for exploration in the first place.\n\t- Experimentally, few tasks are proposed comparing skew-fit with other baselines like HER and AutoGoal GAN - but the differences in all the results seem negligible (example : Figure 5). \n\t- I am not sure why the discussion of goal conditioned policies is introduced rightaway. To me, a more convincing approach would have been to first discuss why H(s) and the entropy of this is good for exploration (discounted weighting or stationary state distribution and considering episodic and  infinite horizon tasks). If H(s) is indeed a difficult or not sufficient term to maximize the entropy for, then it might make sense to introduce goal conditioned policies? Following then, it might be convincing to discuss why goal conditioned policies are indeed required, and then tractable ways of computing MI(s;g). \n\t- Experimentally, I think the paper needs significantly more work - especially considering hard exploration tasks (it might be simple setups too like mazes to begin with), and then to propose a set of new experimental results, without jumping directly to image based tasks as discussed here and then comparing to all the goal conditioned policy baselines. \n\nOverall, I would recommend to reject this paper, as I am not convinced by the proposed solution, and there are lot of theoretical details missing from the paper. It skips a lot of theoretical insights required to propose a new exploration based objective, and the paper proposes a very specific solution for a set a very specific set of experimental setups. \n\n\n\n",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BylJ3YZntB",
                "reply_to": "iclr_2020_r1gIdySFPH",
                "title": "Official Blind Review #2",
                "comment": "This paper introduced a very interesting idea to facilitate exploration in goal-conditioned reinforcement learning. The key idea is to learn a generative model of goal distribution to match the weighted empirical distribution, where the rare states receive larger weights. This encourages the model to generate more diverse and novel goals for goal-conditioned RL policies to reach.\n\nPros:\nThe Skew-Fit exploration technique is independent of the goal-conditioned reinforcement learning algorithm and can be plugged in with any goal-conditioned methods. The experiments offer a comparison to several prior exploration techniques and demonstrate a clear advantage of the proposed Skew-Fit method. It is evaluated in a variety of continuous control tasks in simulation and a door opening task on a real robot. A formal analysis of the algorithm is provided under certain assumptions.\n\nCons:\nThe weakest part of this work is the task setup. The method has only been evaluated on simplistic short-horizon control tasks. It\u2019d be interesting to see how this method is applied to longer-horizon multi-stage control tasks, where exploration is a more severe challenge. It is especially when the agent has no access to task reward and only explores the environment to maximize state coverage. It is unclear to me how many constraints are enforced in the task design in order for the robot to actually complete the full tasks through such exploration.\n\nI would also like to see how Skew-Fit works with different goal-conditioned RL algorithms, and how the performances of the RL policy in reaching the goals would affect the effectiveness of this method in exploring a larger set of states.\n\nSection E: it seems that there\u2019s a logic jump before the conclusion \u201cgoal-conditioned RL methods effectively minimize H(G|S)\u201d. More elaboration on this point is necessary.\n\nMinor:\nAppendix has several broken references.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "theoretical justification and algorithmic novelty",
                "Sentiment Expression": "lack of and limited",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the current experiments",
                "Sentiment Expression": "do not convey a good enough understanding",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the contribution",
                "Sentiment Expression": "dampers",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "HFPTzdwN39": {
        "paper_id": "iclr_2022_HFPTzdwN39",
        "paper_title": "Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing",
        "paper_abstract": "Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing, provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts (e.g., \"red apple'') instead of just individual attributes (\"red'' and \"apple'' independently). Finally, we propose to use supervised classifiers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights. Code at: https://github.com/iro-cp/ssl-qrp.",
        "paper_acceptance": "Accept (Poster)",
        "meta_review": "This paper proposes a method for inspecting and interpreting the visual representations learned by self-supervised methods. \nThe method is conceptually simple and intuititive, the authors assume that concept labels for the images are available, and then go on to learn a mapping between the learned image vectors and the human-provided descriptions of the images. The key insight is to learn a reverse mapping, i.e., to map label vectors to representation vectors. Specifically, feature vectors are quantized using k-means to obtain clusters;  images are labeled (automatically) with a diverse set of concepts from expert models trained with supervision on\nexternal data sources, and  a linear model is trained  to map concepts to clusters, measuring the mutual information between the representation and human-interpretable concepts.\n\nReviewers raised some questions regarding the relation of the approach to topic models, the difference between reverse probing and linear probing, implementation details and computation. The authors addressed reviewers comments convincingly with additional experiments and/or explanations.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "j_ElWaOaApl",
                "writer": "author",
                "reply_to": "2ggVaAPswh2",
                "title": "Follow-up",
                "comment": " Dear reviewer phRe, \n\nThank you again for your valuable feedback on our work. Based on your comments, we have added a discussion on topic models in the appendix, categorized methods under a taxonomy, clarified our findings, and included additional experiments with varying K to verify that this choice does not actually lead to different conclusions. We hope that our revised version has addressed your concerns and we would appreciate it if you could engage with us with feedback on the revision. We are also happy to answer any further questions and clarify any remaining concerns. If you are satisfied by our response, we would appreciate it if you could consider raising your score.\n\nThank you.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "bBJD-DhSr-K",
                "writer": "author",
                "reply_to": "R-r7IArc27",
                "title": "Follow-up",
                "comment": " Dear reviewer UJ5M,\n\nWe thank you again for your feedback on our work. We have addressed your concerns and believe that has clarified several misunderstandings. We would appreciate it if you could let us know whether our response has addressed your concerns and provide an updated assessment of our revised paper. We would be also glad to answer any further questions and clarify any remaining concerns. If you are satisfied by our response, we would appreciate it if you could consider raising your score.\n\nThank you.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SbKJpUMgpee",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_HFPTzdwN39",
                "title": "",
                "comment": "The paper proposes a method for characterizing the \"meaning\" of the representation learned by a given model (interpretability). Towards this goal, it proposes reverse linear probing, a post-hoc method that aims at predicting a quantized version of the internal representation (as observed in specific examples) from semantic label.\n\nSpecial emphasis is given in the capability of the proposed method on enabling the interpretation of models learned in a self-supervised manner.\n\nPROs\n+ Code will be released upon acceptance.\n+ Evaluation covers a good variety of models.\n\nCONs \n- Missing related literature\n- Unclear how interpretability is gained. On the stronger side, the proposed method aims at addressing a task, post-hoc interpretation of pre-trained models, that had received much reduced attention compared to its explanation counterpart. As such it could help push research in this direction further.\nIn addition, the proposed method is evaluated considering a good amount of self-supervised learning methods.\nFinally, code related to the paper will be released upon the publication of the manuscript which is always from the point of view of reproducibility.\n\nOn the weaker side,\n\nWhile the idea of having the proposed probing in reverse order is, to best of my knowledge novel to me, it is not clear to what extent the proposed method does provides insight in the representation learned by the model being probed. Is is by providing semantic means (via y(x)) to describe some modes (f_K(x)) in the representation f(x) ? Perhaps it should be made more explicit the way in whic the proposed method makes the interpretation, of the model being analyzed, possible.\n\nI find the way in which the paper approaches the idea of finding relationships between the internal representations learned by a model and a set of concepts very similar to [Escorcia et al., 2015], that did it at the attribute level, and to [Oramas et al., 2019], that further explore it towards interpretation of learned representations. \nUnfortunately, there is no descriptive nor quantitative comparison with respect to these methods. Given the similarity, I would suggest properly positioning against them.\n\nAt this point it is not clear how the quantized representation $f_K(x)$ is actually computed. For instance, is the internal representation f(x) taken at the layer level, fed to the clustering algorithm and the its result concatenated with that from other layers. Or perhaps you feed the concatenation of all the internal activations to the clustering components, or only use a specific [set] of layers, e.g. the last convolutional layers? Are there any normalization steps involved?\n\nIn Section 3, it is mentioned that \"we can further and cheaply increase the coverage of the label space by predicting the labels y(x)\nautomatically via a battery of expert classifiers learned using full manual supervision.\" While this indeed may seem as a positive and cheap means to further get annotations, as properly noted by the paper, it indirectly requires the existence of the expert predictors and the annotated data on which they were trained. While applicable in the more general imageNet like testing set considered in the experiments, this requirement might not in place to more specific/niche applications, which will limit the applicability of this practice.\n\nAt the end of Section 4.2, by using the proposed method, it is observed and stated that methods that use a clustering mechanism during training have generally more interpretable representations. Doesn't this observation originates from the fact that the proposed method relies on an intermediate clustering step and as such favors methods that rely in similar clustering steps as well. If this indeed the case, perhaps there is some bias indirectly injected by the method that favors some types of methods over others.\n\nIn Section 4.4, it is observed that confusion between different clusters decreases when additional concept groups are added. This observation is used to further stress the fact that there could be interpretable properties in the network that may remain undetected since they are not annotated a-priori in the benchmark data and that this further motivates the need for reverse linear probing.\nIn this regard, this is a known limitation of standard linear probing methods whose interpretation capabilities are bounded by the set of pre-defined annotated concepts. In my opinion, this is also a limitation of the proposed method which, as shown in Section 4.4. does require additional pre-defined concepts in order to decrease the confusion of the identified clusters. Therefore I do not agree with the statement that the proposed reverse linear probing is a solution to the problem.\n\nFinally, it might be good to provide some details on the linear models mentioned at the end of Section 4.1. Is this a single [fully-connected] layer neural network trained for several epochs or perhaps something more elaborated?  The proposed idea is sound and, to the best of my knowledge, novel. I believe the proposed method can perfectly complement the somewhat reduced amount of efforts along the line of interpretation of pre-trained models. However, I believe several aspects (please see my review) that should be polished/clarified before the paper is ready for publication.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "x5StL3G9Mq8",
                "writer": "author",
                "reply_to": "89OY10YLOma",
                "title": "Re: Re: Response to Reviewer AZhK",
                "comment": " Thank you for your response! \n\nWe are glad that we have addressed your concerns and we would appreciate it if you raised your rating to reflect this. \n\nWe have also addressed all points raised by reviewer phRe and added the relevant discussions in the revised manuscript that we have uploaded (e.g., we have included a lenghty discussion on topic models in the appendix, categorized methods under a taxonomy and evaluated more methods with varying K to verify that this choice does not affect the ranking). ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "89OY10YLOma",
                "writer": "official_reviewer",
                "reply_to": "NDX4EHO8Pi",
                "title": "Re: Response to Reviewer AZhK",
                "comment": " Thanks for addressing my review.\n\nTo a good extent the review has addressed my concerns.\nHaving said that, there are very valid concerns pointed out by reviewer phRe, that I would advise explicitly addressing in a revised version of the manuscript.\n\nIn light of the above I am considering increasing my initial rating.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IOp8LjESf4b",
                "writer": "author",
                "reply_to": "W0Oji1CuO-8",
                "title": "Note on correlation with downstream tasks",
                "comment": " Ultimately, we did not observe any significant significant p-values when measuring correlations with downstream tasks. This is likely due to the fact that (a) sample size is small: we do not have enough data points (models) that report performance on other tasks, (b) often there are incosistencies w.r.t. performance reports from paper to paper, and (c) for example on VOC detection, most methods reach AP50 between 82% and 83% (i.e. just one point difference), while the measured interpretability varies more significantly. Due to these reasons, we refrain from making any conclusive comments for now, but will keep investigating this point.  ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Tgzww0rgA6_",
                "writer": "author",
                "reply_to": "iclr_2022_HFPTzdwN39",
                "title": "Revision Summary",
                "comment": " We have revised our submission according to the reviewers\u2019 comments; thank you again for the constructive feedback. For convenience, major edits and added content are shown with green font. \n\nSummary of changes:\n\n* Some key insights are summarized in the introduction and more added to the qualitative analysis **[RSYr]**\n* We have revised the related work to include [Escorcia et al. 2015] and [Oramas et al. 2019] **[AZhK ]**\n* We have added a longer description of Fig. 2 to better highlight the difference between standard linear probes and our approach and what makes reverse probing more suitable for the task we are considering **[UJ5M, phRe]**\n* We introduced icons to categorize self-supervised methods as suggested by **[phRe]**\n* We discussed the relation to topic models and topic coherence evaluation in the appendix **[phRe]**\n* Missing implementation details and computation times are added in the appendix. **[AZhK, UJ5M]** \n* We included more comprehensive evaluation over varying K by adding more methods in Fig. 7 and relevant discussion **[phRe]**\n* We have specifically discussed self-supervised methods based on clustering in the appendix **[AZhK, UJ5M]**",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rn2OWD4Lqqz",
                "writer": "author",
                "reply_to": "SbKJpUMgpee",
                "title": "Response to Reviewer AZhK [1/2]",
                "comment": " We thank the reviewer for the valuable feedback.  \n\n>  **It is not clear to what extent the proposed method provides insight in the representation learned by the model being probed. Is it by providing semantic means (via y(x)) to describe some modes (f_K(x)) in the representation f(x)?** \n\nThis is exactly right. Measuring the mutual information between $f_K(x)$ and semantic labels $y(x)$ provides a quantitative figure that represents the overall semanticity of the representation space of a model. This makes it convenient to probe and compare a number of models. More than that, considering different groups of concepts in isolation (objects, scenes, materials and texture), tells us the extent to which concepts of different nature are \u201cpresent\u201d in the representation. For example, a key insight is that more performant models\u2019 clusters are less likely to contain low level attributes (e.g. color and textures), which is a desirable property when the goal is to learn highly semantic representations. \n\nLast but not least, there are qualitative ways to obtain insights into a learned representation. By examining the linear model\u2019s coefficients, one can find _which_ concepts are the most important for each cluster in the representation space, which can be used to answer questions such as: is it a purely semantic concept or does the cluster carry lower-level information (e.g. indicated by high weights on texture and material attributes)? Or is the cluster related to a combination of semantic concepts, e.g. people and wind instruments (Fig. 5)? We show a number of examples in the appendix (Fig. 8-11), with an emphasis on pairs of clusters that share the same ImageNet label(s), but are visibly different, suggesting that ImageNet labels alone are not enough to understand the representation, despite the fact that it was learned on ImageNet. The difference is better \u201cexplained\u201d with the inclusion of additional concepts in the evaluation, which are shown as word clouds. However, it is important to note that due to the high number of clusters (typically K=1000), an exhaustive qualitative analysis is not possible, which makes the use of a quantitative metric necessary to \u201csummarize\u201d the findings. \n\n> **The way in which the paper approaches the idea of finding relationships between the internal representations learned by a model and a set of concepts is similar to [Escorcia et al., 2015], that did it at the attribute level, and to [Oramas et al., 2019], that further explore it towards interpretation of learned representations.**\n\nThank you for the references! We are happy to discuss these in our revised related work section. In fact, [Escorcia et al., 2015] is a good example for the _forward probe_ formulation, i.e. studies whether intermediate representations are predictive of attributes. [Oramas et al., 2019]  identify features relevant for the prediction of a given class, which is likely not applicable to unsupervised models.\n\n> **It is not clear how the quantized representation fK(x)  is actually computed.**\n\nRather than analyzing all intermediate layers, we focus on the penultimate layer representation of a model. On ResNet-50 models this is the output of the average pooling layer, resulting in a 2048-d vector. On ViT models we evaluate the [CLS] token of the last self-attention layer (768-d vector). Prior to clustering we standardize the features to zero mean and unit standard deviation. We will include these details in the revision.\n\n> **\"We can further and cheaply increase the coverage of the label space by predicting the labels y(x) automatically via a battery of expert classifiers learned using full manual supervision.\" \u2014 indirectly requires the existence of the expert predictors and the annotated data on which they were trained. While applicable in the more general imageNet like testing set considered in the experiments, this requirement might not in place to more specific/niche applications, which will limit the applicability of this practice.**\n\nWe agree that our approach relies on some form of annotated data used to train expert predictors with supervision. However, we still think this is more feasible than annotating all possible concepts on any given dataset. In addition, the concept base could be seamlessly expanded as new datasets and/or expert models become available in the future. \n\nImageNet is still the most common choice for self-supervised pre-training and, due to the large number of pre-trained models available, our efforts were focused in this direction. As self-supervised learning reaches various specific domains, e.g. the medical domain, it is likely that specialized probing methods will be necessary. However, we believe that we need to develop suitable algorithms for understanding and evaluating models, _before_ we apply self-supervised learning to downstream tasks or other domains.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "NDX4EHO8Pi",
                "writer": "author",
                "reply_to": "SbKJpUMgpee",
                "title": "Response to Reviewer AZhK [2/2]",
                "comment": " > **It is observed that methods that use a clustering mechanism during training have generally more interpretable representations. Doesn't this observation originates from the fact that the proposed method relies on an intermediate clustering step and as such favors methods that rely in similar clustering steps as well.**\n \nThis is not necessarily the case, as clustering seems to work well even for contrastive approaches; this is also noted by (Zheltonozhskii et al. 2020). As a matter of fact, the highest ranked models are MoCo-v3 (1k epochs) and OBoW (200 epochs). MoCo-v3 uses a contrastive objective, while OBoW follows a different approach from previous methods with a bag-of-words prediction task that puts emphasis on contextual information and local feature statistics. Another example, SeLa, uses Sinkhorn-Knopp optimization to compute the cluster assignments and DeepCluster-v2 uses spherical K-means. Moreover, SeLa and DeepCluster-v2 train with $K=3000$ prototypes, and there is no reason to believe that their representations are optimal for all values of $K$; yet we find that the relative ranking of methods does not depend on the choice of $K$ (Figure 7).\nIn all cases the expectation is that representation space contains semantically meaningful parts and our approach evaluates the extent to which this is true for all methods. Under our metric, we found that clustering-based methods do generally better than contrastive ones with a similar linear classification accuracy, meaning that using some notion of grouping during self-supervised training likely leads to more interpretable representations. \n\n> **The observation [that confusion between different clusters decreases when additional concept groups are added] is used to stress the fact that there could be interpretable properties in the network that may remain undetected since they are not annotated a-priori in the benchmark data and that this further motivates the need for reverse linear probing. This is a known limitation of standard linear probing methods \u2014 this is also a limitation of the proposed method which, as shown in Section 4.4. does require additional pre-defined concepts in order to decrease the confusion of the identified clusters. Therefore I do not agree with the statement that the proposed reverse linear probing is a solution to the problem.**\n\nWe would like to clarify that there are two crucial aspects here. The first one is the importance of expanding the effective label set, so that it is possible to perform analysis over concepts that are not annotated a-priori in the benchmark data, _for the sake of interpretation._ A characteristic example here is that several clusters contain (prominently) the category \u201cperson\u201d, which is not an annotated class in ImageNet. However, without bringing this concept into existence in the first place, it is not possible to understand its role in the representation. Expanding the label set can be realized for reverse probing, _as well as_ for standard linear probing. For example, the Broden dataset (Bau et al., 2017) has been created with a similar goal in mind. We see this as a vital step to interpretation rather than a limitation. \n\nThe second aspect is the specific advantage of reversing the probing direction, which we have illustrated with an example in Figure 2 of the paper. In our revised version, we have made the discussion of this figure more comprehensive to better explain why the concept-to-representation direction handles combinations of individual attributes more effectively. We also clarify that what motivates the need for reverse linear probing in Section 4.4 is the observation that clusters in the representation space often contain such attribute combinations and not the availability (or lack) of annotations. \n\n\n> **Provide some details on the linear models mentioned at the end of Section 4.1. Is this a single [fully-connected] layer neural network trained for several epochs or perhaps something more elaborated?**\n\nFor the sake of interpretability, we use a single fully-connected linear layer in all experiments and the training details for this model are provided in the appendix.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fSa84b8CZo3",
                "writer": "author",
                "reply_to": "d5oGu_Z4QJd",
                "title": "Response to Reviewer UJ5M [1/2]",
                "comment": " We thank the reviewer for the thorough reading of our submission and constructive feedback, but we think some points might have been misunderstood. Below we address the reviewer\u2019s concerns and kindly ask them to reconsider their rating. In the revision we will also rephrase the claims that the reviewer has pointed out to provide more clarity with respect to standard linear probing and the reasons why we think (a) expanding the concept space and (b) reversing the probing direction may provide additional insights into self-supervised representation learning.  \n\n> **\u201cA limitation of this type of approaches is that one can only discover in the representation meanings that are represented in the available data annotations\u201d. I\u2019m afraid both linear probing and the proposed method have this problem, the proposed method relies on pseudo labels generated by the expert models that were trained with data annotations.**\n\nWe have not claimed our approach to be annotation-free. In fact, how could one study the alignment of features and human-interpretable concepts _without_ human-provided labels? What we actually meant to point out is the opposite, i.e. our goal is to exploit _additional_ annotations from _diverse_ sources in order to provide a more holistic evaluation metric than the ones that have been, to date, considered specifically in self-supervised representation learning (SSL). For example, the linear classification accuracy metric in SSL only evaluates representations against the 1000 ImageNet categories. However, as these models are trained without supervision, there is little to no reason why their representation vectors should align perfectly with a specific label distribution. It is thus important to augment the label set to provide good coverage over a number of diverse concepts that extend well beyond ImageNet categories.  \n\nAs stated in the sentence after the one quoted by the reviewer, \u201cin order to maximize the semantic coverage of  the  analysis,  it  is  thus  customary  to  combine  annotations for several types  of  attributes\u201d, e.g. as done for the Broden dataset (Bau et al., 2017). This is true for existing studies, as well as ours, and _it is a necessary step for post-hoc interpretations of features, rather than a limitation._ \n\n> **\u201cThe representation might be predictive of combinations of attributes, i.e. it might understand the concept of \u2018red apple\u2019 without necessarily understanding the individual concepts, e.g. \u2018red\u2019 and \u2018apple\u2019.\u201d This statement is correct. Unfortunately, neither linear probing nor the proposed method can resolve this problem, as they both are linear functions, so score(\u2018red apple\u2019) == score(\u2018red\u2019) + score(\u2018apple\u2019) . In order to recognize \u201cred apple\u201d, both methods should have high scores on \u201cred\u201d and \u201capple\u201d. In other words, the claim that the proposed method can better handle the compositions of the concepts is incorrect from the methodology perspective.**\n\nWe believe that this is a misunderstanding. We would like to point to **Figure 2** of the paper, which we have presented with exactly this example in mind and the goal of highlighting the difference between our approach and forward linear probes. Forward probes can be trained as binary classifiers for each attribute, mapping a feature vector to the corresponding value of the attribute. Instead, reverse probes map a binary vector that represents a combination of attributes (color, shape) to a categorical cluster in feature space. The decision boundaries shown in the figure are those of the forward linear probes. If _all_ attributes are linearly separable in the representation space (Fig. 2a), then both forward and reverse probes do well at separating the attributes, which may be thought of as identifying the concepts \u201cred\u201d, \u201cblue\u201d, \u201csquare\u201d and \u201ccircle\u201d. \n\nHowever, in Fig. 2b this is not the case: here, the color attribute is not linearly separable, highlighting a failure case of the forward probe. Due to the linear inseparability, the color probe ($h_1$) results in chance accuracy (50%). In this case, score(\u201cred square\u201d) == score(\u201cred\u201d) + score(\u201csquare\u201d) is low for the forward probe, because score(\u201cred\u201d) is low. However, please note that the clusters are easily interpretable/nameable by a human observer. The linear inseparability of individual attributes does _not_ affect the reverse probe, which will perform well as long as the clusters are meaningful (as is the case here), and this is precisely what we wish to measure. The reverse probe gets a high score on \u201cred square\u201d without requiring the concept \u201cred\u201d to be _also_ encoded in the features. In this case, we can say that the model has discovered the concept of a \u201cred square\u201d without identifying, in isolation, the concept \u201cred\u201d.\n   \nIn our revision, we will discuss this example in more detail to prevent misunderstandings. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "R-r7IArc27",
                "writer": "author",
                "reply_to": "d5oGu_Z4QJd",
                "title": "Response to Reviewer UJ5M [2/2]",
                "comment": " > **\u201cThe proposed method provides a single principled figure characterizing the entire representation instead of a separate figure for each predicted attribute\u201d. With the proposed linear mapping, \u201cthis principled figure\u201d is obtained by simply averaging the edge weights between the concepts and the clusters. I don\u2019t see why linear probing cannot obtain such a figure by averaging the per-category scores.**\n\nWe should note that our measure is not the average of edge weights between concepts and clusters, but the mutual information between concepts and quantized representations (clusters), which is quantified via the predictive capability of the reverse probe. This is fundamentally different from averaging class probabilities.  \n\nWe agree that it is of course _possible_ to compute a single number for forward linear probes by averaging per-category scores. Our point here is that this score then represents simply that: an average performance of individual attributes. In the previous comment, we discussed a possible issue of this approach in interpreting the feature space. \n\nWe will revise the writing to improve clarity regarding this difference between linear probes and our approach.\n\n> **\u201cOur measure can be computed much faster than linear probes\u201d. Please elaborate more on this, as at first glance I thought the proposed method is slower as it involves clustering.**\n \nGiven a dataset (e.g. ImageNet) and a set of experts (e.g. the ones listed in Table 1), all labels can be pre-computed for all data points. Feature vectors for a given model can be also pre-computed and stored for the whole dataset. Our method computes cluster assignments using the fast K-means implementation of faiss (typically less than 5min for 2048-d vectors on 4 GPUs*). We then train a linear model to predict these from binary input vectors, which converges in less than 100 epochs in a matter of minutes on a single GPU (1 epoch takes 2sec). On the other hand, standard linear probing on ImageNet typically requires full images (including online augmentations) and multiple forward passes through the frozen feature extractor, which makes it significantly slower to train.     \n\n*The GPUs used in our experiments are NVIDIA RTX A4000. \n\n> **From the experiments, it is still not obvious to me that the proposed method is better than linear probing. Some observations listed in the paper are as expected or already pointed out by previous literature\u2026**\n\nOur work focuses on the post hoc interpretation of the _raw representations_ that self-supervised pre-training yields, which differs significantly from evaluating downstream task performance. As a result, it provides complementary insights to existing literature and is not meant to replace standard protocols (e.g. linear classification or finetuning on downstream tasks).   \n\n> **Deepcluster-v2 performs better as the optimization objective of deepcluster-v2 is pretty similar to the optimization objective of the proposed metric**\n\nPlease see our response to reviewer AZhK regarding methods that use clustering mechanisms. We will include this discussion in the paper. \n\n> **The evaluated self-supervised methods have very similar performance so their rankings may be sensitive to the metrics**\n\nThe evaluated self-supervised methods have often even more similar performance in terms of linear classification accuracy on ImageNet or transfer to downstream tasks (e.g. VOC detection), so the same argument can be made to question _existing_ protocols, as well as whether methods have started to \u201coverfit\u201d these protocols. As most methods train similar contrastive mechanisms, this is not very surprising. Our approach measures the interpretability of a learned representation instead of downstream task performance which is a conceptually different evaluation. Further, we derive additional insights about these models (lower performant models rely more on low level cues than better methods) that cannot be obtained from down-stream task evaluation or linear probing.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2ggVaAPswh2",
                "writer": "author",
                "reply_to": "xBfxqhLgrDX",
                "title": "Response to Reviewer phRe [2/2]",
                "comment": " > **Ability to draw insights. Is there a better way to present the findings than table 2? Grouping approaches based on some form of taxonomy may make it easier to identify differences.** \n\nGrouping approaches to improve readability and insights is a great idea, thank you for suggesting this. Unfortunately, the provided survey paper [2] has very little overlap with the methods that we have investigated (i.e. in our case, most are state-of-the-art methods of the year 2020 or more recent).  [2] discusses mostly supervised methods and the only self-supervised methods are based on generative or context-based pretext tasks, which is mostly not applicable here, so we cannot use the proposed taxonomy. Although there is currently no official taxonomy, perhaps the most common categorization of the methods in our investigation is: \n\n1. **context-based pretext tasks** \u2014 typically training via tasks such as colorization or puzzles \n2. **contrastive learning** \u2014 training by contrasting positive examples (different views of the same image) to negative examples (other images)\n3. **positive-only** \u2014 training using only positive examples (also known as teacher-student)\n4. **clustering-based** \u2014 training via cluster assignments or, usually, combining contrastive learning and clustering\n\nHowever, we note that the majority of current approaches are contrastive with a recent trend towards positive-only methods. Some, like SwAV and PCL combine contrastive learning with clustering. We have updated Table 2 in the paper to highlight the group to which each method belongs. \n\nRegarding drawing insights, as we discuss in the paper, we have observed that clustering-based methods generally produce more interpretable representations. In Table 2, rows within each epoch group are sorted by ascending NMI and it is easy to see that methods which use some form of clustering appear in the last rows (highest NMI). However, we should note that it is hard to use the taxonomy alone to draw conclusive insights, as performance of models is largely dependent on other factors such as the choice of augmentations or, in some cases, number of training epochs. \n\nAnother important insight, as also noted by reviewer RSYr, is that models (trained on ImageNet) capture more than just purely semantic labels and more than just ImageNet labels. They capture information about material, textures, scenes, and their combinations. This is true for all models, however we observe that more performant models recover more of the original label distribution and rely less on other lower-level concepts, such as textures (Figure 4, Table 3). \n\n\n> **Choice of K: Would the authors draw different conclusions if K was larger?**\n\nWhile this is indeed an important point, the goal of Fig. 7 in the Appendix was actually to show that the choice of K does not matter, i.e. the ranking is mostly robust to the choice of K, using some selected methods. Since ImageNet consists of 1000 classes, we found K=1000 to be a sensible choice for the experiments. Nevertheless, for exhaustiveness, we are currently evaluating more methods over a range of K values and will update the figure with the new results.\n    \n\n> **Choice of K: In section 4.5 the authors drop K to 500, why?**\n\nSince in Section 4.5 we present experiments on Places-365, which has fewer classes, we drop K to 500 to reflect this. The exact choice of K does not matter, as long as it is used consistently for all methods under evaluation. In fact, although it is possible to determine K using known methods, such as the silhouette score, this could result in a different number of clusters for each model, which would not allow for relative comparisons between models. For completeness, we will include more values of K. \n\n> **Limitations of a linear probe. The authors outline how the proposed approach allows more complex relationships between image attributes to be captured. Yet they only present results for a linear model (predicting the cluster in representation space). How well would a non-linear model do in this task, which would be able to capture more elaborate relationships between image attributes?**\n\nThe choice of linear models is _essential_ to interpretability studies. While, technically, it is very much possible to use more complex non-linear models,  it is important to ensure that the probe does not \u201cdo the job\u201d instead, i.e. a concept should be captured by the _raw representation_ rather than some complex mapping operating on top of the representation vector, as it would not be possible to determine whether the original model is responsible for the performance or the probe.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WuEroT8Phb6",
                "writer": "author",
                "reply_to": "xBfxqhLgrDX",
                "title": "Response to Reviewer phRe [1/2]",
                "comment": " Thank you for the valuable feedback on our work!\n\n> **Related work: topic models. The authors propose to measure the quality of \"topics\" (i.e. clusters in this work), which is a well studied area in NLP where it is usually referred to as \"topic coherence\".**\n\nThank you for pointing out the analogy to topic models! This is rather interesting. We agree that there is a similarity between topic coherence and the way we evaluate models. However, this analogy breaks down rather quickly. To discuss this, we can directly map the terms of topic models (topics, documents and words) to our scenario (clusters, images and concepts) in this order. The semantic coherence of a topic (cluster) is often measured by the co-occurrence of words (concepts) across documents (images). One immediate difference that arises is that in topic modeling the original space (corpus) is authored by humans and can be thus considered interpretable. Measures of coherence are introduced to understand whether the topics found by a model are also interpretable, i.e. evaluating the model producing the topics. In our case, we do not know the degree to which the original space (representation) is interpretable and this is precisely what we aim to quantify with our method.\n\nAnother conceptual difference is that topic models operate on a higher level of abstraction: a topic is a collection of words that may describe a higher-level idea defined by this collection of words. This is sensible as the goal in topic modelling is to discover which words describe higher-level ideas and can be likely grouped together vs. irrelevant words. In our case, this is not necessary, as the expert annotations already define the relevant concepts. A further abstraction is unnecessary and in fact it may even be undesirable because we are interested in the minimum number of concepts that may explain a cluster. For example, a topic of \u201canimals\u201d may be coherent, yet a cluster of \u201canimals\u201d lacks specificity because it may mix a multitude of categories, instead of purely containing a single species.\n\nFinally, the main mechanism in evaluating topic coherence is co-occurrence of words in the general corpus (documents). In our case, this translates to visual concepts co-occuring in the same image and models relationships between concepts in the images/the real world, whereas we are interested in the relationship between clusters and concepts as they have been learned by the model.\n\nIf the reviewer was thinking of a different analogy to topic modelling we are open to discuss. In our understanding, the one presented above fits best but shows that both fields have somewhat different goals and mechanisms to measure performance. We are happy to discuss this connection in the appendix.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "W0Oji1CuO-8",
                "writer": "author",
                "reply_to": "v2_Um2cjW-",
                "title": "Response to Reviewer RSYr",
                "comment": " We thank the reviewer for the positive and motivating feedback. We really appreciate the suggestion to surface the paper's findings in the abstract/introduction and we have revised the paper accordingly to reflect this.\n\n> **I wish the choice of K-means as the clustering algorithm was questioned. For example, if you used hierarchical agglomerative clustering, would that lead to clusters where different levels of the hierarchy could represent compositions of children cluster concepts.**\n\nWe experimented with other clustering algorithms, such as BIRCH, but found K-means to be a reasonable choice. The difficulty in choosing the clustering algorithm is that it is not possible to know which algorithm works best in this case and in absence of ground truth. We really like the idea of hierarchical clustering and are currently experimenting with it. However, besides qualitative findings, we should note that it is not clear how one could use hierarchies for quantitative evaluation, especially since measuring the mutual information over different $K$ yields incomparable results.  \n\n\n>  **Which concepts are not captured by any of these models?**\n\nThank you for suggesting this! It is possible to find concepts that are \u201cwell-learned\u201d or \u201cnot captured\u201d by examining the linear model\u2019s coefficients and the predictive performance for each cluster. Concepts not captured by most models are certain scene attributes (e.g. \u201cstudying\u201d, \u201cresearch\u201d, \u201cstressful\u201d which may require a certain degree of reasoning), as well as small, less common household objects (e.g. can opener, paper cutter, cooking spray), which are likely not salient regions in a dataset like ImageNet. Clusters with high weights on such categories have a lower prediction score (e.g. NMI or accuracy) and are far less coherent.   \n\n  \n>  **Which models are capable of decoupling individual concepts (identifying each concept in isolation) versus only compositions of concepts?**\n  \nGenerally, more performant models are better at decoupling individual concepts; these often also cluster more \u201cpure\u201d IN-1k categories in their representation space (Fig. 4, blue bars). However, we have observed that, even then, _context_ plays a prominent role in the representation (e.g. Fig. 9c: whale in _man-made environment_ vs. whale in the _ocean_, Fig.10c: snake on a _hand_ vs. snake on the _ground_, Fig. 11a: citrus _slices_ vs. the whole fruit). This is true even for models that do well at separating IN-1k categories, i.e. the true underlying label distribution. This likely also explains why in [1] ImageNet self-supervised models do as well as or better than the supervised baseline at context, counting and gestalt tasks. Finally, we should note that for _all_ models, compositions are very common with human-related concepts, such as faces or hands (Fig. 5, Fig. 8c&d, Fig. 10c).\n\n>  **How does interpretability correlate with transfer to other downstream tasks like object detection, segmentation, etc.**\n \nThis is indeed a very good point, which we have also considered, but performance reports for transfer tasks are not always consistent across the literature (e.g. evaluation protocols are changing, or different tasks are evaluated); so it has been more difficult to obtain reliable data points to measure the correlation. Upon your suggestion, however, we computed the correlation of interpretability and transfer performance of models on VOC Detection (one of the more standardized tasks) and found it to be more correlated than that of standard linear probing with the transfer task. We will revise the appendix with the relevant plots.  \n\n-----\n[1] Van Horn, et al. \"Benchmarking Representation Learning for Natural World Image Collections.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1qsljRBQEl",
                "writer": "author",
                "reply_to": "iclr_2022_HFPTzdwN39",
                "title": "General response to all reviewers",
                "comment": " We would like to thank all reviewers for their valuable feedback. We are happy that reviewers appreciated the valuable insights [RSYr] and the amount of experimentation and evaluated models [phRe, UJ5M, AZhK]. We also appreciate the constructive comments and suggestions for strengthening our work. Below we respond to the reviewers' comments and we will soon also upload a revision to our submission based on their feedback. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "v2_Um2cjW-",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_HFPTzdwN39",
                "title": "",
                "comment": "This paper studies how human-interpretable are the concepts learned within the representation space of self-supervised models. It argues that linear probing methods are unable to identify whether a representation space contains a specific concept because the input might contain multiple confounding concepts (\u201cred apple\u201d) and might encode them in a manner that renders \u201cred\u201d and \u201capple\u201d individually linearly inseparable. Instead, it proposes a \u201creverse linear probe\u201d moving from which maps combinations of binary concept labels to the representation space, which is clustered using k-means.\n\nThe paper uses this linear probing method to propose a normalized mutual information metric to measure how well human-interpretable concepts are encoded in the representation space. Even though they find that their metric is correlated with linear probe, there are some interesting insights. First, the paper finds that models trained on ImageNet images capture more than just semantic class labels. They capture information about textures, scenes, objects, etc. Second, they found that additional training epochs dont lead to more interpretable representations even though it increases end-task performance. They also identified that OBoW, does really well even with only 200 epochs (though the reasons for why is still left to be investigated). Clustering-based approaches generally produce more interpretable representations. Fourth, ViT is a better architecture. Finally, qualitative evaluation of clusters seem to justify their utility of the reverse probe since NMI increases as when accounting for multiple categories for objects that occur together (ex, french horn and people). \n I actually really enjoyed reading this paper and found the contribution to be compelling. My main suggestion is to surface your insights and findings in the abstract and introduction. These sections, currently, only describe your method and not your findings. The abstract and introduction made me ask \u201cso what?\u201d. The findings are buried; some in the appendix!\n\nI wish the choice of K-means as the clustering algorithm was questioned. For example, if you used hierarchical agglomerative clustering, would that lead to clusters where different levels of the hierarchy could represent compositions of children cluster concepts.\n\nWhat I think is missing:\n1) Which concepts are not captured by any of these models?\n\n2) Which models are capable of decoupling individual concepts (identifying each concept in isolation) versus only compositions of concepts?\n\n3) How does interpretability correlate with transfer to other downstream tasks like object detection, segmentation, etc. I know the paper includes a transfer experiment to Places but there could have been a more thorough exploration of transfer learning.\n Overall, I am still quite satisfied with where the paper currently stands. I listed things I would like to see improved or discussed. I also listed some potential questions for investigation. But these questions do not detract from my overall positive impression of the paper.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "xBfxqhLgrDX",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_HFPTzdwN39",
                "title": "",
                "comment": "This paper presents an approach to investigate the semanticity of representations learned via self-supervision applied to images. The approach measures (via mutual information) how well a linear model can map a vector indicating image attributes (e.g. objects, texture, etc) to clusters within the representation space. The authors apply this method to recent methods from the literature and interpret the resulting ranking, aiming to learn more about which methods produce representations that map well to human judgements of similarity. I enjoyed reading the paper - it is well written and the proposed approach is simple. In general, interpretability or maybe rather explainability of representations is a subject worth studying, and I appreciate the effort that went into experimentation in this work. However, there are also issues that I will describe below that relate to 1) positioning with respect to related work, and 2) the ability for the reader to draw insights from the analysis, among more minor aspects, that would significantly strengthen the work. In the current state I believe the paper is not yet ready for publication at ICLR.\n\n**Related work: topic models**\n\nIn essence, the approach proposed here boils down to a linear model that predicts an output class for inputs in the shape of vectors that indicate image attributes (e.g. object presence, texture, etc). While this type of approach might be new in the assessment of semanticity of image representations, it is strikingly similar to classic topic models in NLP (e.g. a model identifying topics in documents represented as bag-of-words). The authors propose a measure to essentially measure the quality of those \"topics\" (i.e. clusters in this work), which is a well studied area in NLP where it is usually referred to as \"topic coherence\", e.g. [1]. All sorts of measures have been proposed to measure topic coherence, including those based on (pointwise) mutual information. I would therefore encourage the authors to take a deeper look into this relationship - it may strengthen the paper if the relationship of this approach to other methods can be understood more thoroughly.\n\n**Ability to draw insights**\n\nThe authors run plenty of experiments using recent methods to assess their semanticity, which I think should be the focus point of this paper. While I appreciate the effort that went into this, I think how these results are currently presented (is there a better way to present the findings than table 2?), and discussed falls short of what I expected given the focus of this work. While the authors provide some insights w.r.t. to different types of approaches (e.g. clustering vs contrastive) it would be useful to link this to more established taxonomies (see e.g. [2]). Grouping approaches based on some form of taxonomy may make it easier to identify differences. This is an area where this paper could shine and provide actionable insights for the practitioner but at the moment falls short of that expectation.\n\n**Choice of K**\n\nThe choice of the number of clusters seems a bit arbitrary to me, particularly as it impacts the (relative) performance of different methods. The authors outline in the appendix (B.2, in particular Fig 7) how BYOL's performance improves a lot with much larger K, essentially being en-par with the best performing methods. Yet in the main text the authors mention specifically that more recent methods (like BYOL) don't perform as well as others - Would the authors draw different conclusions if K was larger? Again, these differences may be alleviated with a more adequate grouping of approaches across experiments. Also in section 4.5 the authors drop K to 500, why?\n\n**Limitations of a linear probe**\n\nThe authors outline how the proposed approach allows more complex relationships between image attributes to be captured. Yet they only present results for a linear model (predicting the cluster in representation space). How well would a non-linear model do in this task, which would be able to capture more elaborate relationships between image attributes?\n\n[1] Roder et al. (2015) Exploring the Space of Topic Coherence Measures \n\n[2] Jing et al (2020) Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey The paper is well written and the proposed approach is simple. In general, interpretability of representations is a subject worth studying, and I appreciate the effort that went into experimentation in this work. However, there are also issues that relate to 1) positioning with respect to related work, and 2) the ability for the reader to draw insights from the analysis, among other aspects, that would significantly strengthen the work. In the current state I believe the paper is not yet ready for publication at ICLR.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "d5oGu_Z4QJd",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_HFPTzdwN39",
                "title": "",
                "comment": "The paper aims to measure the interpretability of the visual representations learned by the recent self-supervised models. In doing this, the paper formulates the \u201cinterpretability\u201d as the mutual information between the feature clusters and a set of visual concepts that can be interpreted by humans. The mutual information is approximated by the proposed Quantized Reverse Probing, a linear function that maps the concept vectors to the feature clusters. The proposed method is claimed to be complementary to linear probes, with further advantages: (1) a single principled score rather than individual prediction scores for different labels in linear probes (2)  handle combinations of concepts better than linear probes. (3) Faster than linear probes. Strengths\n\n[S1] The paper tries to answer a very important question: to what extent the self-supervised visual representations can be interpreted by humans. This question is important and interesting as these representations were learned without human interventions but showed to be very effective for many downstream recognition tasks. While there are many task-oriented evaluations, systematic analysis of the interpretability of the representations is missing.\n\n[S2] Formulating the \u201cinterpretability\u201d as the mutual information between the representations and the semantic concepts is technically sound.\n\n[S3] The analysis covers a wide range of visual concepts and pretrained self-supervised models.\n\n[S4] The paper is well written.\n\n\u2014------------------------------------------------------------------------------------------------------------------------\n\nWeaknesses\n\nThe proposed Quantized Reverse Probing is claimed to be complementary to linear probes with further advantages. At this point, I don\u2019t think this claim is supported by the method or the experiments. To me, the proposed method is too similar to linear probes that they share the same drawbacks.\n\n[C1] \u201cA limitation of this type of approaches is that one can only discover in the representation meanings that are represented in the available data annotations\u201d. I\u2019m afraid both linear probing and the proposed method have this problem, the proposed method relies on pseudo labels generated by the expert models that were trained with data annotations.\n\n[C2] \u201cThe representation might be predictive of combinations of attributes, i.e. it might understand the concept of \u2018red apple\u2019 without necessarily understanding the individual concepts, e.g. \u2018red\u2019 and \u2018apple\u2019.\u201d This statement is correct. Unfortunately, neither linear probing nor the proposed method can resolve this problem, as they both are linear functions, so score(\u2018red apple\u2019) == score(\u2018red\u2019) + score(\u2018apple\u2019) . In order to recognize \u201cred apple\u201d, both methods should have high scores on \u201cred\u201d and \u201capple\u201d. In other words, the claim that the proposed method can better handle the compositions of the concepts is incorrect from the methodology perspective.\n\n[C3] \u201cThe proposed method provides a single principled figure characterizing the entire representation instead of a separate figure for each predicted attribute\u201d. With the proposed linear mapping, \u201cthis principled figure\u201d is obtained by simply averaging the edge weights between the concepts and the clusters. I don\u2019t see why linear probing cannot obtain such a figure by averaging the per-category scores.\n\n[C4] \u201cour measure can be computed much faster than linear probes\u201d. Please elaborate more on this, as at first glance I thought the proposed method is slower as it involves clustering.\n\n[C5] From the experiments, it is still not obvious to me that the proposed method is better than linear probing. Some observations listed in the paper are as expected or already pointed out by previous literature: deepcluster-v2 performs better as the optimization objective of deepcluster-v2 is pretty similar to the optimization objective of the proposed metric; also the evaluated self-supervised methods have very similar performance so their rankings may be sensitive to the metrics; ViT + Self-supervised-learning performs better on KNN classifications (and clustering ) was already observed by DINO, etc.\n\n\u2014------------------------------------------------------------------------------------------------------------------------\n\nMinor comments:\n\nPage 2, \u201ceither only cluster (Yan et al., 2020a) or cluster and further train and \u2026\u201d\n\n While the problem studied in the paper is very interesting, at this moment, I don't see there are any obvious advantages of the proposed method over a simple linear probing, as discussed in the \"weaknesses\". Therefore, I'm leaning toward a rejection.",
                "rating": 3,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The authors addressed reviewers comments",
                "Sentiment Expression": "convincingly",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "CMcptt6nFaQ": {
        "paper_id": "nips_2022_CMcptt6nFaQ",
        "paper_title": "Structure-Aware Image Segmentation with Homotopy Warping",
        "paper_abstract": "Besides per-pixel accuracy, topological correctness is also crucial for the segmentation of images with fine-scale structures, e.g., satellite images and biomedical images. In this paper, by leveraging the theory of digital topology, we identify pixels in an image that are critical for topology. By focusing on these critical pixels, we propose a new \\textbf{homotopy warping loss} to train deep image segmentation networks for better topological accuracy. To efficiently identify these topologically critical pixels, we propose a new algorithm exploiting the distance transform. The proposed algorithm, as well as the loss function, naturally generalize to different topological structures in both 2D and 3D settings. The proposed loss function helps deep nets achieve better performance in terms of topology-aware metrics, outperforming state-of-the-art structure/topology-aware segmentation methods. ",
        "paper_acceptance": "Accept",
        "meta_review": "The paper proposes a topology-aware learning objective for semantic segmentation models based upon warping masks. The loss is used for training satellite data and medical segmentation datasets and provides benefits in these domains. Reviewers acknowledge that the approach is simple, intuitive, has thorough empirical evaluation and ablations. Reviewers note a few presentation issues, e.g. related to terminology. These must be fixed in a final revision of the paper. Overall all reviewers vote for acceptance and so do I.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "4ElPuKji35C",
                "writer": "author",
                "reply_to": "1pWsj1Xp6pa",
                "title": "Thanks very much for your positive feedback",
                "comment": " Dear Reviewer RT7y,\n\nThanks very much for your positive feedback! We'll make the modifications accordingly in the final version.\n\nBest,\n\nAuthors of Paper #11428",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tMEUu9kmYXb",
                "writer": "author",
                "reply_to": "NIDX5126YUx",
                "title": "Thanks for your response ",
                "comment": " Dear Reviewer 6qMj,\n\nThanks very much for your confirmation and suggestions! We'll include a separate limitation section/paragraph in the final version as you suggested.\n\nBest,\n\nAuthors of Paper #11428",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Rf0dPVVswk_",
                "writer": "author",
                "reply_to": "atuoRm_XcB-",
                "title": "Thanks for your response",
                "comment": " Dear Reviewer xhKW,\n\nThanks very much for your confirmation!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "NIDX5126YUx",
                "writer": "official_reviewer",
                "reply_to": "eQmWP6-zyp",
                "title": "Response",
                "comment": " Thank you authors for the response.  I have no further questions and will maintain my postive rating.  Should the paper get accepted it would be helpful to comment a bit more on the limitations of the method as this may seed future research.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1pWsj1Xp6pa",
                "writer": "official_reviewer",
                "reply_to": "SfoRQsHTvnY",
                "title": "Reviewer Interaction 2",
                "comment": " Dear authors, \nI do think that the term \"topologically critical pixel\" is a better fit for the given method. I have now increased my rating. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "atuoRm_XcB-",
                "writer": "official_reviewer",
                "reply_to": "hCGmqQ_JBt2",
                "title": "Response to authors' update",
                "comment": " Thank you for answering my questions. I have no further questions and will maintain my rating.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SfoRQsHTvnY",
                "writer": "author",
                "reply_to": "k3FI5O6ePoW",
                "title": "Thanks for your response",
                "comment": " Dear Reviewer RT7y,\n\nThanks very much for your time and additional comments! Below we are trying to resolve your further questions/concerns.\n\n> I am still unsure about the terminology, to me a \"topologically critical location\" would be a unique pixel location. Maybe the authors are able to come up with a better terminology along the lines of warping.\n\nThanks for your suggestion. In the paper, _topologically critical locations_ refer to the set of identified critical points/locations which are relevant to topology. Will _topologically critical pixel set_ a better term for you? \n\n> Experimentation and reproducibility.\n\nAs we have mentioned in the Sec.11 (Datasets section) of the updated version, for the RoadTracer and DeepGlobe datasets, we follow the data splits of [4] and [5], respectively. And for the other datasets, such as Massachusetts, DRIVE and CREMI, we conduct a three-cross validation, which follows the settings in DMT [26]. \n\nAnd in Tab.7, we provide the loss weights $\\lambda$ (hyperparameter) for each dataset.\n\nBut as you suggested, we'll provide all the necessary details in a public github repo upon acceptance. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "k3FI5O6ePoW",
                "writer": "official_reviewer",
                "reply_to": "Lh_c_Opp0bK",
                "title": "Discussion of Rebuttal",
                "comment": " Dear authors, \n\nthank you for replying to my review and considering my comments. Many of my quesitons are addressed and I am willing to increase the rating. Please see additional comments here: \n\n1) (Review Q 2) I am still unsure about the terminology, to me a \"topologically critical location\" would be a unique pixel location. Maybe the authors are able to come up with a better terminology along the lines of warping.  \n\n2) (Review Q 5)  Experimentation and reproducibility: the authors state in their checklist that they provide all details, data, code and hyperparameters to reproduce the experimental results (checklist 3 a, b). To me the current supplementary material does not specify all of these. E.g. which images are in the train/test/val sets and which parameters were chosen for which ablation study in order to reproduce the numbers in the experimentation. I would encourage the authors to release all of the above in a github repository. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "lPeVhsBIvdp",
                "writer": "author",
                "reply_to": "Lh_c_Opp0bK",
                "title": "Follow up",
                "comment": " Dear Review RT7y,\n\nWill you be able to kindly spend some time to have a look at our response? \n\nWe'll try to resolve your further questions/concerns (if any) during the discussion period.\n\nBest,\n\nAuthors of Paper #11428\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "bcBmj9Bd2Yq",
                "writer": "author",
                "reply_to": "jVWsUlHExwJ",
                "title": "Thank you",
                "comment": " Thanks very much for your time!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jVWsUlHExwJ",
                "writer": "official_reviewer",
                "reply_to": "JmIn9j8Dq_j",
                "title": "Thank you",
                "comment": " Thank you for the quick response! I have no further questions for now, and, having read all the reviews and responses, I am maintaining my positive rating of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JmIn9j8Dq_j",
                "writer": "author",
                "reply_to": "JDPXhw_VeSV",
                "title": "Further response to Reviewer 4Rx6",
                "comment": " Thanks very much for your further clarifications! We conduct the additional ablation studies as you suggested. We'll include these additional results in the final version.\n\n---\n\n**Q6**: My suggestion was to try binarizing the mask differently: not by thresholding at probability 0.5, but by sampling the binary value of each pixel from the output distribution and fixing it for the whole duration of the warping procedure. This binarization is stochastic and may vary between iterations.\n\n**A6**: Thanks for your clarification. It\u2019s easy to implement and we report the performances on the RoadTracer dataset:\n\n| Method | DICE $\\uparrow$ | ARI $\\uparrow$ | Warping $\\downarrow$ | Betti $\\downarrow$ |\n|:------:|:------:|:------:|:------:|:------:|\n|   Multinomial  |  0.599 |    0.571   | 9.017 $\\times 10^{-3}$| **1.245** |\n|  **Warping**  |  **0.603** |   **0.572**   |  **8.853** $\\times 10^{-3}$| 1.251 |\n\nFrom the above table, we see that the \u2018Multinomial\u2019 alternative achieves very close results to the proposed method. The reason is that after going through softmax, the predicted probabilities will be close to 0 or 1 (for binary labels). As torch.multinomial returns indices based on the probability distribution, it is very similar to the proposed thresholding strategy.\n\n---\n\n**Q7**: What I was proposing was to test specifically the effect of the distance heuristic: still flip only non-simple pixels but removing the heuristic that uses the distance transform to choose which non-simple pixels to flip.\n\n**A7**: In our understanding, you mean only flipping the simple pixels but removing the heuristic that uses the distance transform, and then use the remaining non-simple pixels as our critical pixel set. Please correct us if we misunderstood. And we report the performances on the RoadTracer dataset:\n\n| Method | DICE $\\uparrow$ | ARI $\\uparrow$ | Warping $\\downarrow$ | Betti $\\downarrow$ |\n|:------:|:------:|:------:|:------:|:------:|\n|   w/o DT  |  0.586 |    0.547   | 10.256 $\\times 10^{-3}$| 1.473 |\n|  **Warping**  |  **0.603** |   **0.572**   |  **8.853** $\\times 10^{-3}$| **1.251** |\n\nAs we can see from the above table, by removing the heuristic that uses the distance transform, the performances of the method drop significantly. The reason is that without the heuristic, we only flip the wrongly predicted pixels which are near the boundaries. In this way, the extracted critical pixel set will be very noisy and most of the pixels are irrelevant to the topology. \n\n---\n\n**Q8**: There are high-resolution remote sensing datasets with both roads and waterways (which can have similar appearances) and ones that separate roads from other kinds of impervious surfaces (buildings).\n\n**A8**: Thanks for your suggestion, and we\u2019ll leave this as a future work.\n\n---\n\nPlease let us know if you have further questions/concerns.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JDPXhw_VeSV",
                "writer": "official_reviewer",
                "reply_to": "V2sets02lUZ",
                "title": "Clarification to questions",
                "comment": " Thank you for the detailed answer. This answers most of my questions, but there are a few things I'd like to clarify, in decreasing order of importance.\n\n>> What would happen if each pixel's on/off state is sampled from the output distribution instead of bring thresholded?\n>\n> We can infer topology only on the binarized mask, and so we cannot directly apply warping on the output distribution (continuous likelihood map).\n\nMy suggestion was to try binarizing the mask differently: not by thresholding at probability 0.5, but by sampling the binary value of each pixel from the output distribution and fixing it for the whole duration of the warping procedure. This binarization is stochastic and may vary between iterations. It seems to require just changing `torch.argmax(pre, dim=1)` to `torch.multinomial(pre, dim=1).squeeze(1)` in lines 119 and 150 of `warping_loss.py`. This is not essential, but I would like to see it if it is easy to run.\n\n>> Importance of the distance sorting heuristic: should drops in performance (accuracy or computation time) be expected if a simpler method of warping the masks is used, such as iteratively selecting a random non-simple point that borders the current mask?\n> \n> We conduct an additional experiment by randomly choosing a set of pixels for each iteration instead of identifying all the topologically relevant pixels. Specifically, we randomly choose 1/10 of the whole wrongly predicted pixels as the pixel set.\n\nThanks for sharing the updated results, which are interesting and support the main claims of the paper. However, what I was proposing was to test specifically the effect of the distance heuristic: still flip only non-simple pixels -- not random wrongly predicted ones -- but removing the heuristic that uses the distance transform to choose which non-simple pixels to flip. It's also not essential, but would be interesting to see in the final version.\n\n>> For what other kinds of data could this approach be useful? How could the proposed method be generalized to multiclass problems?\n>\n> To the best of our knowledge, we haven\u2019t seen datasets with multiclass curvilinear structures. We note that it\u2019s always doable to convert the multiclass segmentation task to a number of binary class segmentation tasks.\n\nThis is more of a suggestion for future work than a comment on this paper, but there are high-resolution remote sensing datasets with both roads and waterways (which can have similar appearances) and ones that separate roads from other kinds of impervious surfaces (buildings).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "V2sets02lUZ",
                "writer": "author",
                "reply_to": "F9M9-kjX7MY",
                "title": "Response to the comments of Reviewer 4Rx6",
                "comment": " Thanks for your constructive feedback. Below we address specific concerns one-by-one.\n\n---\n\n**Q1**: There are no comparisons of outputs of models trained using different critical point selection methods that would show the impact of a noisy point selection on the model output. Importance of the distance sorting heuristic: should drops in performance (accuracy or computation time) be expected if a simpler method of warping the masks is used, such as iteratively selecting a random non-simple point that borders the current mask? As another, simpler ablation study, I would suggest to evaluate warping only in one direction (ground truth to prediction or prediction to ground truth).\n\n**A1**: We conduct an additional experiment by randomly choosing a set of pixels for each iteration instead of identifying all the topologically relevant pixels.  Specifically, we randomly choose 1/10  of the whole wrongly predicted pixels as the pixel set. Here we report the performances on the RoadTracer dataset:\n\n| Method | DICE $\\uparrow$ | ARI $\\uparrow$ | Warping $\\downarrow$ | Betti $\\downarrow$ |\n|:------:|:------:|:------:|:------:|:------:|\n|  UNet | 0.587 | 0.544 | 10.412 $\\times10^{-3}$ |1.591|\n| Random select | 0.584 | 0.547 | 10.261 $\\times 10^{-3}$ | 1.672|\n| Warping (GT-> Pred) | 0.594 | 0.567 | 9.171 $\\times 10^{-3}$ | 1.290 |\n| Warping (Pred -> GT) | 0.598 | 0.562 | 9.124 $\\times 10^{-3}$ |1.315 |\n| **Warping**| **0.603** | **0.572** | **8.853** $\\times 10^{\u22123}$ | **1.251**|\n\nAs we can see from the above table, \u2018Random select\u2019 strategy achieves comparable results to the vanilla UNet, which is not surprising as the model is topology agnostic.\n\nAlso, we conduct further ablation studies as suggested. The two variations (warping only in one direction: ground truth to prediction or prediction to ground truth) achieve reasonably better while still slightly inferior results to the proposed version. The reason might be that the proposed critical point selection strategy contains more complete topologically challenging locations. \nBoth ablation studies demonstrate the effectiveness of the proposed critical point selection strategy.\n\n---\n\n**Q2**: Similarly, I would have liked to see some examples of the failure modes of the proposed algorithm, as well as images of outputs on all the datasets studied.\n\n**A2**: We add a few of the failure cases in the updated version (Fig.12 in Sec.17 of the Supplementary Material) as you requested. Note that inferring topology given an image is a very difficult task, especially near challenging spots, e.g., blurred membrane locations or weak vessel connections. Current methods can help to improve the topology-wise accuracy, while they are far from perfect.\n\n---\n\n**Q3**: For what other kinds of data could this approach be useful? How could the proposed method be generalized to multiclass problems?\n\n**A3**: In the paper we focus on the segmentation of curvilinear structures, and try to improve the topology-aware accuracy. To the best of our knowledge, we haven\u2019t seen datasets with multiclass curvilinear structures. We note that it\u2019s always doable to convert the multiclass segmentation task to a number of binary class segmentation tasks.\n\n---\n\n**Q4**: L185: This line is somewhat confusing or misleading, since the proposed heuristic only finds a mask that is locally optimal, i.e., no simple points can be flipped to decrease the Hamming distance. (Is it known whether finding the global optimum simply slow (cf. L239-242) or computationally intractable?)\n\n**A4**: L185: In this part, we are talking about the possible \u2018best warping\u2019 which reaches the minimal Hamming distance. And you are right that in practice, by using the proposed algorithm, we only find a mask that is locally optimal and the \u2018perfect\u2019 minimal distance is not always guaranteed. We\u2019ve \n\nRegarding finding the global optimum, the search space is very large. The reason is that some non-simple points might become simple as the warping algorithm continues (since their neighboring pixels get flipped in previous iterations).  Thus there are too many degrees of freedom, and finding the global optimum is computationally intractable. Moreover, we\u2019d like to emphasize that it\u2019s fine to miss some critical locations in one specific epoch, as we are not expecting to fix all the topological errors within one epoch.\n\n---\n\n**Q5**: L219: What would happen if each pixel's on/off state is sampled from the output distribution instead of bring thresholded?\n\n**A5**: We can infer topology only on the binarized mask, and so we cannot directly apply warping on the output distribution (continuous likelihood map). \n\n---\n\n\n\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Lh_c_Opp0bK",
                "writer": "author",
                "reply_to": "mRD_xuF-GDr",
                "title": "Response to the comments of Reviewer RT7y",
                "comment": " Thanks for your constructive feedback. We will improve our manuscript accordingly, especially for the presentation issues. Below we address specific concerns one-by-one.\n\n---\n\n**Q1**: Terminology\n\n**A1**: Thanks for pointing out all of them.\n\n* I.24-26: We change it to \"pixels at the boundary of the object of interest can generally be challenging, but not relevant to topology\".\n\n* \"topologically critical location\": Yes, you are right. Based on our algorithm, the identified locations are not unique. We use the term \u2018topologically critical location\u2019 for simple understanding of the paper.\n\n*  l. 34 \"to force the neural network to memorize them\": Thanks for your suggestions. We changed \"to force the neural network to memorize them\" to \"to force the neural network to focus on them\".\n\n---\n\n**Q2**: The identified locations are more likely to be relevant to topological errors. --> this statement should be statistically supported. Compared to what exactly? Does this rely on the point estimate for any pixel? Or given a particularly trained network?\n\n**A2**: It is compared to the whole set of wrongly predicted locations. We mean that not all the wrongly predicted locations are related to topology, and thus we use the proposed warping strategy to extract the topologically relevant locations. The identified locations are independent of the network, and are obtained  from the definition of non-simple points.\n\n---\n\n**Q3**: Theorem 1: The presentation of a well known definition from Kong et al. is trivial and could be presented in a different way.\n\n**A3**: Thanks for your suggestion. We changed \u2018Theorem 1\u2019 to \u2018Definition 1\u2019, which is referred to as the definition of simple points in the 2D case.  We wanted to highlight this definition, as it is essential for the understanding of our warping strategy/loss.\n\n---\n\n**Q4**: It would be important to understand how and if the parameters of the baselines were chosen and experimented with. (I understand that the authors cannot train ablation studies for all baselines etc.) However, it is an important information to understand the results in Table 1.\n\n**A4**: This is a very good question. \n\nFor the Mass, DRIVE and CREMI dataset, we follow the settings in DMT [26]. Specifically, we generate the numbers of \u2018Warping Error\u2019 for all the methods, and for the remaining metrics we copy the numbers from the original paper. We believe the parameters have been fine-tuned and carefully chosen. \n\nFor the RoadTracer and DeepGlobe datasets, we tune the parameters of the baselines (though it may not be exhaustive) as well as the proposed method, and report the best performances with the appropriate parameters. \n\n---\n\n**Q5**: How does the filtering using the DT compare to the calculation of image persistence computationally?\n\n**A5**: We\u2019ve compared the efficiency of different methods in Tab.4 of the main paper, including the algorithm complexity and training time in practice. We discuss the same in L389.\n\n---",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eQmWP6-zyp",
                "writer": "author",
                "reply_to": "GRrzwUVeb4d",
                "title": "Response to the comments of Reviewer 6qMj",
                "comment": " Thanks for your constructive feedback. Below we address specific concerns one-by-one.\n\n---\n\n**Q1**: The segmentation performance obtained from this method mainly relies on the standard UNet results. If the segmentation output from UNet is noisy, the warping operation may take many iterations, perhaps more so initially as the network is starting to train.  \n\n**A1**: Our basic motivation is to identify the topologically challenging locations and then force the neural networks to focus on/correct them. In practice, the output of the standard UNet is usually reasonable (since it\u2019s optimized with per-pixel loss), and so the number of epochs we observed was decent. Also, the warping is operated on the binarized mask instead of the noisy likelihood map. In this way, the proposed method can always efficiently identify and focus on the topologically challenging locations. \n\n---\n\n**Q2**: Experiments with other architectures is left for future work. \n\n**A2**: We add experiments by using FCN as the backbone, and we report the performances on the RoadTracer dataset.\n\n| Method | DICE $\\uparrow$ | ARI $\\uparrow$ | Warping $\\downarrow$ | Betti $\\downarrow$ |\n|:------:|:------:|:------:|:------:|:------:|\n|   FCN  |  0.567  |    0.518   | 14.515 $\\times 10^{-3}$| 2.311 |\n|  FCN + Warping  |  **0.581** |   **0.543**   |  **11.126** $\\times 10^{-3}$| **1.918** |\n\nFrom the results, we observe that the proposed method also improves the topology-aware segmentation accuracy, regardless of the network backbones.  \n\n---\n\n**Q3**: While results are improved, the improvement may be subtle; for example 1% improvement in Dice score for most experiments in Table 1.\n\n**A3**: In Sec.10 of the original version (which corresponds to Sec.15 in the revised version) and Tab.6 of the supplementary material, we provide stddev besides mean, and use t-test (95% confidence interval) to determine the statistical significance (highlighted with bold) for the RoadTracer dataset. The quantitative results show that the proposed method performs significantly better than baselines. Also, we believe that the topology-relevant metrics, such as ARI, Warping Error and Betti Error are better metrics for topology/structure-aware image segmentation tasks.\n\n---\n\n**Q4**: It wasn\u2019t clear to this reviewer if or how the method generalizes to problems where there are more segmentation classes (e.g. three or more).\n\n**A4**: In the paper, we mainly focus on curvilinear structure segmentation tasks, and try to improve the topology-aware accuracy. To the best of our knowledge, we haven\u2019t seen datasets with multiclass curvilinear structures. Additionally it\u2019s always doable to convert the multiclass segmentation task to a number of binary class segmentation tasks.\n\n---\n\n**Q5**: What if some False Positives or False Negatives exist at the edges of segmentation regions?\n\n**A5**: The proposed method mainly focuses on the topological errors of curvilinear structures. The logic is that the standard segmentation methods (such as vanilla UNet optimized with dice loss) can deal with most of the pixel level segmentation. If False Positives/Negatives exist at the edges of segmentation regions, they won\u2019t be identified as topologically challenging locations and corrected  by the proposed method, and actually they will not induce topological errors.\n\n---\n\n**Q6**: Would the method be useful for datasets with more irregular shapes as a region of interest\u2014for example, the Kvasir SEG - Simula dataset?\n\n**A6**: In this paper we mainly test our method on fine-scale curvilinear structure datasets. However we believe it could be extended to more general cases. As for the Kvasir SEG - Simula dataset you mentioned, the proposed method should help to improve the topology-wise accuracy. The reason is that the proposed method can always help to capture the topological errors (if any) between the prediction and ground truth masks, and force the neural networks to focus on/correct these topological errors. For this specific dataset, if the predicted mask has a different number of connected components compared to the ground truth mask (which means topological errors), our proposed method will identify those pixels which induce the topological errors and force the neural network to focus on and correct them.\n\n---\n\n**Q7**: What are the limitations of the proposed method?\n\n**A7**: One major limitation might be that currently we only test the proposed method on curvilinear structure datasets, and we\u2019ll try to explore more general cases in the future.\n\n---",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hCGmqQ_JBt2",
                "writer": "author",
                "reply_to": "VAZv3DQYD4T",
                "title": "Response to the comments of Reviewer xhKW",
                "comment": " Thanks for your constructive feedback. Below we address specific concerns one-by-one.\n\n---\n\n**Q1**: The proposed distance-ordered homotopy warping algorithm is possible to miss some simple pixels during the flipping although very rare.\n\n**A1**: Yes, and we had already mentioned this in L277-282. Moreover, we\u2019d like to emphasize that it\u2019s fine to miss some simple points during flipping in one specific epoch, as we are not expecting to fix all the topological errors within one epoch.\n\n---\n\n**Q2**: In the previous work TopoNet, Variation of Information (VOI) is used as a metric to measure topological correctness. It would be better if VOI is also compared in the experiments.\n\n**A2**: We omitted the VOI in the original version because of space limitation, and we report the VOIs of the RoadTracer dataset here.\n\n| Method | VOI $\\downarrow$| \n|:------:|:------:|\n|   UNet  |  2.318  | \n|  RoadTracer  |  2.542 |\n|    VecRoad  |    2.419 |\n|    iCurb  |    2.251 |\n|    VGG-UNet  |   2.109 |\n|    TopoNet  |    2.088 | \n|    clDice  |    2.185 |\n|    DMT  |    1.967 |\n|    **Warping**  |    **1.896** | \n\nFrom the table, we see that the proposed method also achieves the best VOI compared with the other baseline methods. We\u2019ll include the VOIs for the other datasets in the final version.\n\n---\n\n**Q3**: Presentation issues.\n\n**A3**: Thanks for your suggestions. We\u2019ve fixed the presentation issues in the revised version.\n\n---",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "o8dDHZoysm2",
                "writer": "author",
                "reply_to": "nips_2022_CMcptt6nFaQ",
                "title": "General response",
                "comment": " We thank all the reviewers for their valuable feedback! We will improve our presentations accordingly. We are encouraged that all reviewers appreciated the novelty of the contribution and the performances on challenging benchmarks. We have updated the revised version and added the supplementary material (which was a separate pdf file for the original submission) at the end of the main paper for friendly reading.\n\nBelow we address specific concerns one-by-one.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "VAZv3DQYD4T",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_CMcptt6nFaQ",
                "title": "",
                "comment": " This paper proposed a new method for segmenting images with better topological accuracy. Specifically, a new algorithm utilizing distance transform is used to identify the topologically critical locations, based on which a new homotopy warping loss is used to measure the difference between the predicted and ground-truth topology and train the segmentation network. Experiments on four 2D datasets and one 3D dataset validated the efficacy of the proposed method. Besides pixel-wise accuracy, better topology accuracy is obtained. Strengths\n1. This paper introduced an efficient heuristic algorithm to identify topologically critical locations. Compared with a previous work on topology-preserving image segmentation method which runs in cubic time complexity, the proposed algorithm's complexity is linear to image size, thus making the training more efficient.\n\n2. A new homotopy warping loss is proposed to train the segmentation network to preserve the topology. The homotopy warping loss penalizes errors on topologically critical locations and results in models with better topological accuracies.\n\n3. Comparisons with existing topology-preserving segmentation methods show that the proposed method achieves the best performance. Ablation studies provide some guidance regarding the choices of loss weights and loss functions.\n\nWeaknesses\n1. The proposed distance-ordered homotopy warping algorithm is possible to miss some simple pixels during the flipping although very rare.\n\n2. In the previous work TopoNet, Variation of Information (VOI) is used as a metric to measure topological correctness. It would be better if VOI is also compared in the experiments.\n\n3. Grammar/spelling errors:\na) Line 92: \"Except for the methods mentioned above, UNet has also been one of the most popular methods for image segmentation\". It seems \"besides\" is the proper preposition, not \"except for\".\nb) Line 131: \"efficiently identity\" -> \"efficiently identify\"\nc) Line 382: \"we'd like the investigate\" -> \"we investigate\"\n Please see the comments above N/A",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "GRrzwUVeb4d",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_CMcptt6nFaQ",
                "title": "",
                "comment": " This paper presents a new method to improve the topology of image segmentation.  The method finds topologically critical locations by comparing the predicted segmentation mask with the ground truth, and proposes a novel homotopy warping loss which guides the training to improve topological accuracy. Experiments are conducted in both 2D and 3D settings to segment satellite and biomedical images, and results show the proposed method outperforms other existing methods. Strengths:\n\n1.\tAlthough previous work such as TopoNet also emphasize the importance of focussing on topologically critical locations, the warping approach taken in this paper appears to be novel and more efficient, computationally.\n\n2.\tThe warping loss is intuitive and seeks to transform the topology of the predicted segmentation mask to match the ground truth (and vice versa).  The proposed algorithm focusses on simple points, which are described clearly in terms of their connectivity. \n\n3.\tThe experimental results demonstrate the method is capable of improving segmentation not just at the critical points but overall in the image.\n\nWeaknesses:\n\n1.\tThe segmentation performance obtained from this method mainly relies on the standard UNet results. If the segmentation output from UNet is noisy, the warping operation may take many iterations, perhaps more so initially as the network is starting to train.  Experiments with other architectures is left for future work.  \n\n2.\tWhile results are improved, the improvement may be subtle; for example 1% improvement in Dice score for most experiments in Table 1.  Nonetheless, visual results in Figure 6 show good improvement.\n\n3.\tIt wasn\u2019t clear to this reviewer if or how the method generalises to problems where there are more segmentation classes (e.g. three or more).  All the experiments shown in the paper are binary segmentation problems.  However, the paper makes it clear that it is focussing on binary segmentation.\n 1.\tAll the used cases in the paper address the segmentation error in between the segmented areas. What if some False Positives or False Negatives exist at the edges of segmentation regions? \n\n2.\tThe datasets used in the proposed study have straight or curved lines as an area of interest. Would the method be useful for datasets with more irregular shapes as a region of interest\u2014for example, the Kvasir SEG - Simula dataset?  https://arxiv.org/abs/1911.07069 \n\n3.\tHow might it be possible to apply the method for image segmentation problems with more than two classes (e.g. a problem with two foreground classes and one background)?\n\n4.\tWhat are the limitations of the proposed method?\n Limitations are not discussed in the paper.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "mRD_xuF-GDr",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_CMcptt6nFaQ",
                "title": "",
                "comment": " The authors implement the Warping Error metric, introduced by Jain et al. (CVPR 2010) as a loss for topology aware segmentation. To be able to efficiently calculate the warping, the authors introduce a heuristic algorithm based on the distance transform to choose the pixels to \"flip\" or warp to achieve a better topology.  \n**Strengths:** \n\nOverall I find the proposed warping idea to be innovative and clever. It improves topology awareness in segmentation models in an efficient manner. It is significantly faster to compute than methods based on barcode-matching and persistent homology and achieves competitive performance to such methods. \n\n\n**Weaknesses:**\n\n1) Terminology: Introduction l. 24-26 \"pixels near the peripheral of the object of interest can generally be challenging, but not relevant to topology.\" I think this statement is problematic. When considering the inverse e.g. in the case of a surface or vessel, that a foreground pixel changes to background. Such a scenario would immediately lead to a topology mismatch (Betti error 1). \n\n2) Terminology: \"topologically critical location\" -->  I find this terminology to be not optimally chosen. I agree that the warping concept appears to help with identifying pixels which may close loops or fill holes. However, considering the warping I do not see a guarantee that such \"locations\" (as in the exact location) which I understand to refer to individual or groups of pixels are indeed part of the real foreground, nor are these locations unique. A slightly varying warping may propose a set of different pixels. \n\n3) The identified locations are more likely to be relevant to topological errors. --> this statement should be statistically supported. Compared to what exactly? Does this rely on the point estimate for any pixel? Or given a particularly trained network? \n\n4) Theorem 1: The presentation of a well known definition from Kong et al. is trivial and could be presented in a different way. \n\n5) Experimentation, lack of implementation details: In Table 2 and a dedicated section, the authors show an ablation study on the influence of lamda on the results. Lamda is a linear parameter, weighting the contribution of the new loss to the overall loss. Similarly, the studied baseline methods, e.g. TopoNet [24], DMT [25], and clDice [42] have a loss weighting parameter. It would be important to understand how and if the parameters of the baselines were chosen and experimented with. (I understand that the authors cannot train ablation studies for all baselines etc.) However, it is an important information to understand the results in Table 1.\n\n6) Terminology: l. 34 \"to force the neural network to memorize them\" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact \"critical point\" as such in TopoNet [24]. \n\n\n\n**Minor:** \n\n- I find the method section to be a bit wordy, it could be compressed on the essential definitions.\n\n- There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l. 271 \"This lemma is naturally generalized to 3D case.\"\n\n- l. 52 \"language of topology\" I find this to be an imprecise definition or formulation. \n\n**Note:** \n\nAfter rebuttal and discussion I increased the rating to 5.  How does the filtering using the DT compare to the calculation of image persistence computationally?  yes.",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "F9M9-kjX7MY",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_CMcptt6nFaQ",
                "title": "",
                "comment": " A topology-aware learning objective is proposed for semantic segmentation models. This objective relies upon a warping operation that transforms a predicted mask to one that is close to the ground truth mask in pixel space without changing the topology, and vice versa. The objective is a pixelwise cross-entropy computed on the points of disagreement between the target and warped source masks. Evaluation on 2D and 3D satellite and medical imagery segmentation datasets shows the algorithm is efficient and delivers strong results. Strengths:\n- Simple, intuitive algorithm and clear writing. The algorithm is well-motivated and mathematically sound, and care is taken to make it efficient using the heuristic for computing the points of interest.\n- Strong experimental evaluation on diverse datasets where preservation of connectivity structure has been an important challenge.\n- Ablations and comparisons are appropriately selected.\n\nWeaknesses:\n- The paper would benefit from more discussion about what may be responsible for the better performance of the algorithm relative to baseline methods of selecting critical points. The comparison with TopoNet's points of interest in Figure 7 gives some intuition about the question, but there are no comparisons of outputs of models trained using different critical point selection methods that would show the impact of a noisy point selection on the model output.\n- Similarly, I would have liked to see some examples of the failure modes of the proposed algorithm, as well as images of outputs on all the datasets studied. - For what other kinds of data could this approach be useful? How could the proposed method be generalized to multiclass problems?\n- Importance of the distance sorting heuristic: should drops in performance (accuracy or computation time) be expected if a simpler method of warping the masks is used, such as iteratively selecting a random non-simple point that borders the current mask? As another, simpler ablation study, I would suggest to evaluate warping only in one direction (ground truth to prediction or prediction to ground truth).\n- L185: This line is somewhat confusing or misleading, since the proposed heuristic only finds a mask that is locally optimal, i.e., no simple points can be flipped to decrease the Hamming distance. (Is it known whether finding the global optimum simply slow (cf. L239-242) or computationally intractable?)\n- L219: What would happen if each pixel's on/off state is sampled from the output distribution instead of bring thresholded? Please see the first two points of Weaknesses above.",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the approach",
                "Sentiment Expression": "simple, intuitive, has thorough empirical evaluation and ablations",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "presentation issues",
                "Sentiment Expression": "a few",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "These",
                "Sentiment Expression": "must be fixed",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "Overall all reviewers vote for acceptance",
                "Sentiment Expression": "and so do I",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "3vmKQUctNy": {
        "paper_id": "nips_2022_3vmKQUctNy",
        "paper_title": "Washing The Unwashable : On The (Im)possibility of Fairwashing Detection",
        "paper_abstract": "The use of black-box models (e.g., deep neural networks) in high-stakes decision-making systems, whose internal logic is complex, raises the need for providing explanations about their decisions. Model explanation techniques mitigate this problem by generating an interpretable and high-fidelity surrogate model (e.g., a logistic regressor or decision tree) to explain the logic of black-box models. \nIn this work, we investigate the issue of fairwashing, in which model explanation techniques are manipulated to rationalize decisions taken by an unfair black-box model using deceptive surrogate models. More precisely, we theoretically characterize and analyze fairwashing, proving that this phenomenon is difficult to avoid due to an irreducible factor---the unfairness of the black-box model. \nBased on the theory developed, we propose a novel technique, called FRAUD-Detect (FaiRness AUDit Detection), to detect fairwashed models by measuring a divergence over subpopulation-wise fidelity measures of the interpretable model. \nWe empirically demonstrate that this divergence is significantly larger in purposefully fairwashed interpretable models than in honest ones. \nFurthermore, we show that our detector is robust to an informed adversary trying to bypass our detector. The code implementing FRAUD-Detect is available at https://github.com/cleverhans-lab/FRAUD-Detect.",
        "paper_acceptance": "Accept",
        "meta_review": "The reviewers were split about this paper: on one hand they appreciated the motivation and the comprehensive experiments in the paper, on the other they were concerned about the clarity of the paper, even worried about a potential flaw. I have decided to vote to accept given the clear and convincing author response. I urge the authors to take all of the reviewers changes into account (if not already done so). Once done this paper will be a nice addition to the conference!",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "XBoo6J6ju2v",
                "writer": "author",
                "reply_to": "4Nud5BnYKNWn",
                "title": "Thank you! ",
                "comment": " Dear reviewer vBkK,\n\nThank you very much for the very positive feedback and insightful suggestions! We would be happy to answer any further questions you may have before the response period ends today.\n\nWarm regards,\nPaper3526 Authors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "VGcKQ_53iM",
                "writer": "author",
                "reply_to": "RvaaiY86OmZB",
                "title": "Thank you!",
                "comment": " Thank you very much for your time reading our responses, upgrading your score and very insightful suggestions. Indeed, for the theorem to hold, the surrogate model does not need to be an interpretable model even if in practice it is almost always the case in the context of post-hoc explanation as the objective is precisely to explain a black-box. We will incorporate your suggestion in the final version of our manuscript by changing the first sentence of our theorem to\n\n\u2018\u2019Assume $\\hat{Y}_{B}$ are the black-box model $B(\\cdot)$ predictions, $\\hat{Y}_I$ are the predictions of a surrogate (interpretable) model $I(\\cdot)$ trained on  $B(\\cdot)$ outputs and $A$ is a sensitive attribute:\u2019\u2019",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SAN-XrEa3ZD",
                "writer": "author",
                "reply_to": "IyDrOyPPKNzm",
                "title": "Thank you!",
                "comment": " Thank you very much for your time reading our responses. Sure we will add these additional results in the appendix of the final version of our manuscript.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WXZA0P2N-xH",
                "writer": "author",
                "reply_to": "nBSewmI3tEP",
                "title": "Response to the remaining concerns (part 1 of 2) ",
                "comment": " Thank you for your time reading our responses and posting new comments. We respond to them inline.\n\n> **Accessibility to B.\nI'm still confident that the accessibility to B is problematic. First, the theoretical analyses shown in Section 3 are dependent on the accessibility to the model B. As mentioned in the initial comments, since we can calculate the unfairness directly because of the accessibility to B, the detectability is obvious. Second, if we can obtain the suing dataset labeled by B, we can calculate the unfairness from the labeled dataset, meaning that we can easily get the unfairness of the original model. Hence, the detectability is trivial even in this case.** \n\nRegarding the accessibility to B, we would like to highlight that we did not invented a new problem setting, rather we have followed the problem setting that was previously introduced in the seminal papers of the fairwashing literature. See for instance : \n\n- [ICML-2019] Ulrich A\u00efvodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, Alain Tapp: Fairwashing: the risk of rationalization. ICML 2019: 161-170.\n\n- [NeurIPS-2021] Ulrich A\u00efvodji, Hiromi Arai, S\u00e9bastien Gambs, Satoshi Hara: Characterizing the risk of fairwashing. NeurIPS 2021: 14822-14834.\n\nWe agree with the reviewer that we can compute the unfairness of the black-box model and interpretable model on the suing set. However, these unfairness values (or their differences) cannot be used to detect whether the interpretable model is adversarially manipulated to rationalize decisions taken by the black-box (i.e. fairwashing). This is due to the fact that the unfairness value alone provides little insight on which design choice (e.g., a fairness constraint was added to the optimization objective) introduced the bias (with respect to the sensitive attribute). In addition to this, we show both empirically and theoretically that the unfairness of the black-box model and the unfairness of its surrogate model are always different. We mentioned this in lines 58-61 of our introduction as:\n\n*\u201cmeasuring the differences between the fairness of the black-box model and the explainable model cannot help to detect fairwashing as there are several design choices (including fairwashing) that could lead to different fairness, thus non-intentional fairwashing.\u201d*\n \nHereafter, we illustrate why this is the case with an example. \n\n1. A company uses a black-box model to provide personalized services or make decisions on users queries, for example deciding based on the application of a user if they can get a loan. Here, the black-box model\u2019s prediction would reflect the company\u2019s decision in response to the loan application. \n\n2. Upon receiving the company\u2019s decision, some users may deem that the decision they received is biased. For example, they may believe that it is unfair with respect to the \u201cgender\u201d sensitive attribute. As a consequence, users report this issue to an auditor by sending their data along with the decision they received. As multiple user complaints accumulate, a suing set is built (e.g., for a class action).\n\n3. We agree with the reviewer that the auditor can compute the black-box model\u2019s unfairness on the suing set. However, the unfairness value alone provides little insight on which design choice introduced the bias (with respect to the sensitive attribute). For instance, in the scenario considered, evaluating the unfairness of predictions made on the suing set alone does not characterize the impact of \u201cgender\u201d on the predictions of the black-box model. To achieve this, the Explainable AI community has developed post-hoc explanation techniques as a way to assess the fairness of models by explaining the \u201creasoning\u201d behind the predictions of the model as well as its reliance on sensitive attributes. Therefore, in the problem setup proposed by previous papers on fairwashing, the auditor asks for a model explanation from the company to determine whether the \u201cgender\u201d attribute has a direct impact on the model predictions. As our detector is non-cooperative so it does not need further query access to the black-box model than what was already collected by users on the suing set (put another way, our detector does not need to perform any additional adaptive queries to the black-box beyond the initial suing set).    \n\n4. In order to evade penalties from the auditor, the company trains a fairwashed interpretable model in which the impact of \u201cgender\u201d on the model prediction is fairwashed (i.e., decreased) and sends this model to the auditor. \n\n5. If the company successfully fairwashed their model, upon inspecting the interpretable model, the auditor gets a false impression that \u201cgender\u201d is not impacting the prediction. Instead, the auditor finds that other attributes such as \u201coccupation\u201d strongly affect the output. The company has successfully evaded accusations of unfairness with respect to the \u201cgender\u201d attribute. \n\nThis is precisely what we detect with our work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IXwM19pJ77D",
                "writer": "author",
                "reply_to": "nBSewmI3tEP",
                "title": "Response to the remaining concerns (part 2 of 2)",
                "comment": " > **Sufficiency\nI understand the definition of \"completely eliminating fairwashing.\" However, it is still unclear about the definition of sufficiency. In the authors' analysis, \u03b3 involves terms other than the false- and true-positive rates. The authors claim in Line 209 that such other terms are constant. However, without a mathematically rigorous definition of sufficiency, I cannot confirm that these terms are actually independent of sufficiency.**\n\nRegarding the definition of sufficiency, what we mean by sufficiency is simply that $\\tilde{\\delta}$ and $\\delta\u2019$ are sufficient for identification of fairwashing (i.e., an auditor does not require a greater amount of information than $\\tilde{\\delta}$ and $\\delta\u2019$ to determine whether fairwashing has occurred). We note that as the other components of the fairwashing gap $\\gamma$ are irreducible ( $\\Gamma_B (1 + F_1^{+} - T_1^{+})$ ), these cannot be helpful in the determination of fairwashing and thus are excluded from our sufficiency statement. As $\\tilde{\\delta}$ and $\\delta\u2019$ rely on the black-box and interpretable model while $\\Pr\\left[\\hat{Y}_{B}=y \\mid A=0\\right], y\\in\\{0,1\\}$ is exclusively related to the black-box model. We will clarify in the paper by stating that $\\tilde{\\delta}$ and $\\delta\u2019$ are sufficient information from the interpretable model to determine fairwashing, as the other terms are constant from the perspective of the interpretable model. Since we are attempting to determine fairwashing of the interpretable model, we discuss sufficiency w.r.t. that model. We intend sufficiency as in logical implication such that certain values of $\\tilde{\\delta}$ and $\\delta\u2019$ alone provide determination for fairwashing.\n\nGiven the above, we propose the following definition of sufficiency:\n\n**Definition.** *We define sufficiency in the context of determination of fairwashing as the dependence of fairwashing on particular variables \u2013 i.e. the values taken by particular variables form a sufficient condition for the determination of fairwashing. In our case, if the values of $\\tilde{\\delta}$ and $\\delta\u2019$ exceed a threshold, this is a sufficient condition for fairwashing.*",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nBSewmI3tEP",
                "writer": "official_reviewer",
                "reply_to": "XrA1G8PKrup",
                "title": "Thanks for your response",
                "comment": " I thank the authors for your response.\n\n> Accessibility to $B$\n\nI'm still confident that the accessibility to $B$ is problematic. First, the theoretical analyses shown in Section 3 are dependent on the accessibility to the model $B$. As mentioned in the initial comments, since we can calculate the unfairness directly because of the accessibility to $B$, the detectability is obvious. Second, if we can obtain the suing dataset labeled by $B$, we can calculate the unfairness from the labeled dataset, meaning that we can easily get the unfairness of the original model. Hence, the detectability is trivial even in this case. \n\n> Sufficiency\n\nI understand the definition of \"completely eliminating fairwashing.\" However, it is still unclear about the definition of sufficiency. In the authors' analysis, $\\gamma$ involves terms other than the false- and true-positive rates. The authors claim in Line 209 that such other terms are constant. However, without a mathematically rigorous definition of sufficiency, I cannot confirm that these terms are actually independent of sufficiency.\n\nDue to these concerns, I'd like to keep my score unchanged.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Yootz_DbIsW",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_3vmKQUctNy",
                "title": "Discussion with Authors",
                "comment": " Dear Reviewers! Thank you so much for your time on this paper so far.\n\nThe authors have written a detailed response to your concerns. How does this change your review?\n\nPlease engage with the authors in the way that you would like reviewers to engage your submitted papers: critically and open to changing your mind. Thank you Reviewers 53g2 and WPPT for your initial engagement!\n\nLooking forward to the discussion!\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IyDrOyPPKNzm",
                "writer": "official_reviewer",
                "reply_to": "P3_6qS6JpI4",
                "title": "Thank you for the response",
                "comment": " I want to thank the authors for the response which addresses my question. I hope the authors can include this additional results on equalized odds in the updated appendix (probably as an extra section).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RvaaiY86OmZB",
                "writer": "official_reviewer",
                "reply_to": "8Q9zgDstwLV",
                "title": "Thanks for the comments",
                "comment": " Thanks for getting back, and this helps clarify the paper considerably for me.\n\nI'm still about confused about the statement of Theorem 1. If it doesn't matter that I is an \"interpretable model\" why not just say a surrogate model trained to predict B's predictions? This is much more explicit than having terms like \"interpretable\" and \"explain\" in the theorem statement, which doesn't necessarily have concrete definitions, and you can describe later on the theorem applies in case I is a very simple interpretable model. I think using an argument like this would help clarity here.\n\nI think the paper is in better shape overall, however, so I upgraded my score to weak accept.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LEl_WYTIZg7",
                "writer": "author",
                "reply_to": "nips_2022_3vmKQUctNy",
                "title": "Thank you! we are available for discussion",
                "comment": " Dear reviewers,\n\nThank you very much for your time serving as the reviewer for our paper! We would be happy to answer any further questions you may have before the response period ends on Aug 9.\n\n\n\nWarm regards,\n\nPaper3526 Authors\n \n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "P3_6qS6JpI4",
                "writer": "author",
                "reply_to": "EvJ4GagR51c",
                "title": "Response to Reviewer WPPT",
                "comment": " We thank the reviewer for the comments, and for highlighting that our paper is well-written, well-motivated, comprehensive in its experiments as well as for recognising the importance of the evaluation of our method against an informed adversary.  Below, we will answer your comment.\n\n> **The method currently only applies to one fairness definition. However, in practice, other definitions like equality of opportunity are also commonly used. For other fairness criteria such as equality of opportunity, how easy is it to adapt the current method?**\n\nWe thank the reviewer for the suggestion. As discussed in Section 7, our main reason for focusing on demographic parity in this work is that it does not assume knowledge or even the existence of true labels (see Section 2.1). Nevertheless, following the reviewer\u2019s suggestion, we have evaluated our detector on other fairness metrics. In particular, we have considered the equalized odds fairness metric which in addition to constraining the true positive rates (as would equality of opportunity) also constrains the false positive rates across groups. We choose the Adult dataset (black-box model: DNNs and interpretable model: Decision trees) as it displays an inherent equalized odds unfairness. The results below show that, similarly to our demographic parity results, we observe a relationship between the equalized odds gap of interpretable models and the associated $C_KL$ values: as fairwashing becomes more severe ( $1-\\epsilon \\downarrow$), the equalized odds decreases while $C_{KL}$ increases.\n\n|$1-\\epsilon$:|0.6|0.8|0.9|0.95|0.99|\n|-------|--------|--------|--------|--------|--------|\n|Eodds:|.077345|.07616|.06650|.04473|.02191|\n|$C_{KL}$:|.12300|.13329|.13883|.14118|.14131|\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8Q9zgDstwLV",
                "writer": "author",
                "reply_to": "CnNvY-PmgLS",
                "title": "Response to Reviewer 53g2",
                "comment": " > **Overall, I think this paper needs to much more rigorously provide the problem setting and explicit definitions of what is going on. Could you help clarify the problem setup a bit more, per the weaknesses section?**\n\nWe clarified our assumptions, problem and solution based on the below points:\n\nAssumptions:\nWe follow the standard assumptions made in the fairwashing literature:\nCompany uses a black-box model $B$; \nBinary classification tasks, $B:\\mathbb{R}^M \\mapsto \\{0,1\\}$;\nBinary-valued sensitive attributes, $A \\in \\{0,1\\}$.\n\n\nProblem statement:\nBelow, we clarify the problem setup in 3 steps:\nThe company trains $B$  on their training set $(X_{\\text{Tr}}, Y_{\\text{Tr}}, A)$, in which $A$ represents a sensitive attribute;\nThe company makes decisions based on the prediction of $B$  about users' queries;\nAfter receiving decisions that they deem unfair, users from the suing set requires the company to be investigated in terms of the effect of $A$ on the decisions by an auditor;\nGiven the black-box model $B(\\cdot)$ and the suing set $X_{\\text{sg}}, B(X_{\\text{sg}},)$, company solves the fairwashing optimization (Definition 4) defined as learning an interpretable model $I(\\cdot)$ from $B(\\cdot)$ on $X_{sg}$ such that the interpretable model has 1) high fidelity with respect to the black-box model and 2) is less unfair than this black-box model.\n\n\nOur solution:\nTo distinguish between the fairwashed and honest interpretable model in a non-cooperative way we propose to compute the divergence between subgroup confusion matrices $C_0$ and $C_1$ using Kullback\u2013Leibler (KL) as $\\mathcal{C}_{\\text{KL}}=\\text{KL}(C_0,C_1)$.\n\n\n\n\n> **Figure 3 is a bit hard to follow. Could you help clarify this figure? Where does the dotted line come from? Why are there multiple fidelity values for every \u0394?**\n\nThe dotted line is the unfairness of the black-box model computed on the suing set data. Figure 3 displays the results of solving the constrained optimization problem in Equation 9. More precisely, the constraints in Equation 9 are related to the fidelity (defined based on loss) and $\\Delta$. For each value of $\\Delta$, we consider different values for fidelity because our objective is to assess the evasion power of fairwashing attacks on the Rashomon Set of interpretable models. This is designed to characterize the damage an adversary can achieve given a constraint on $C_{KL}$. Therefore, the multiple fidelity values for every $\\Delta$ in Figure 3 show the performance of the detector when facing different high-fidelity interpretable models. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CnNvY-PmgLS",
                "writer": "author",
                "reply_to": "USsylUR6O0pZ",
                "title": "Response to Reviewer 53g2",
                "comment": " > **While the empirical results about the detection algorithm are pretty complete, it seems tricky to choose this threshold \u0394 that determines whether fair washing is going on. It would useful to provide more guidance here.**\n\n\nWe thank the reviewer for the insightful comment. The question of choosing a threshold is a pertinent one. We provide two answers, one based on the theory we have presented in Section 3, and an empirical solution. \n\n### Theory Solution\nIn Appendix I, we provide the following bounds for the KL divergence measure of FRAUD-detect:\n\n$\\kappa := \\operatorname{KL}\\left(\n        \\begin{bmatrix} F^+_0 \\\\\n                        F^-_0 \n        \\end{bmatrix}, \n        \\begin{bmatrix} F^+_1 \\\\\n                        F^-_1 \n        \\end{bmatrix}\n        \\right)$ \n\nis bounded from below and above by:\n\n$\n  F_0^+ \\log\\left(\\frac{F_0^+}{F_1^-} \\cdot \\frac{\\gamma_0}{\\gamma_1}\\right) \n                        \\leq  \\kappa \\leq  \n  F_0^- \\log\\left(\\frac{\\gamma_0}{\\gamma_1} \\cdot \\frac{F_0^-}{F_1^+}\\right)\n$\n\nAny choice between \n$\\kappa_{min} := F_0^+ \\log\\left(\\frac{F_0^+}{F_1^-} \\cdot \\frac{\\gamma_0}{\\gamma_1}\\right)$ \nand \n$\\kappa_{max} := F_0^- \\log\\left(\\frac{\\gamma_0}{\\gamma_1} \\cdot \\frac{F_0^-}{F_1^+}\\right)$ is valid. \n$\\kappa = \\kappa_{min}$ is the most conservative choice which increases the risk of false discovery (of fairwashing), while $\\kappa = \\kappa_{max}$ is a permissive choice that minimizes the risk of false discovery at the expense of increase in the false negatives (not detecting fairwashing). A balanced choice for threshold  could be the intermediate point:\n\n$\\bar{\\kappa}=\\frac{F_{0}}{2}\\left(\\Delta_{l}\\left(F^{+}\\right)+\\Delta_{l}\\left(F^{-}\\right)\\right)+\\frac{\\gamma_{0}}{2 \\Gamma_{B}} \\log \\left(\\frac{\\gamma_{0}}{\\Gamma_{B}}\\right)-\\frac{1}{2} \\frac{\\gamma_{0}}{\\gamma_{1}} \\frac{1}{\\Delta\\left(F^{-}\\right)}$\n\nwhere $\\Delta(R) = R_0/R_1$ and $\\Delta_l(R) = \\log \\Delta(R)$. The above point is accurate up to $O\\left(\\frac{\\Gamma_B^2}{\\gamma_1^2}\\left(\\frac{F^-_1}{F^-_0}\\right)^2\\right)$.\n\n### Empirical Solution\nWe note that the threshold is a hyper-parameter. Assuming access to additional (possibly public) data $\\mathcal{D}$, one can calibrate the threshold using a separate \u201ccalibration\u201d dataset: \n\n1. Train a state-of-the-art black-box model using $D_{train} \\sim \\mathcal{D}$\n2.Train an explainable model $M_{honest}$ on $D_{train} \\sim \\mathcal{D}$ without using any additional constraints on the gap between the black-box and interpretable model\n3. Train an explainable model $M_{fairwashed}$ on $D_{train} \\sim \\mathcal{D}$ using the Informed Adversary Optimization of Definition 5 in order to minimize the fairness gap\n4. Measure the KL divergence of $D_{sg} \\sim \\mathcal{D}$ on $M_{honest}$ and $M_{fairwashed}$ to form $X_{honest}$ and $X_{fairwashed}$ datasets. Assign labels $y = 1$ to $X_{fairwashed}$ and $y = 0$ to $x_{honest}$. \n5. $X= X_{honest} \\cup X_{fairwashed}$, and $Y = Y_{honest} \\cup Y_{fairwashed}$ form a univariate regression model with the following loss function $\\ell$:\n$\\ell(x, y, T)=\\sum_{i} \\frac{1}{2}\\mathbb{I}\\left(x_i \\leq T, y=1\\right)+\\frac{1}{2}\\mathbb{I}\\left(x_i>T, y=0\\right)$. And the optimal threshold $T^* = \\arg\\min_T \\ell(x, y, T)$.\nNote that this assumes equal weights for type I and type II error of the detector. Finally, we note the state-of-the-art in calibration is [1]. However, a SOTA method is likely not necessary because if there are enough samples for calibration, central limit theorem ensures both  $X_{fairwashed}$ and $X_{honest}$ are normally distributed at which point the optimal threshold is simply $T^* = \\frac{1}{2}(\\mu_{fairwashed} +\\mu_{fairwashed})$.\n\n\n[1] R. Sahoo, A. Chen, S. Zhao, and S. Ermon, \u201cReliable Decisions with Threshold Calibration,\u201d p. 14.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "USsylUR6O0pZ",
                "writer": "author",
                "reply_to": "eeK51LFm3H",
                "title": "Response to Reviewer 53g2",
                "comment": " We thank the reviewer for the comments and for highlighting that our paper is interesting and comprehensive in its experiments. \n\n\n> **The initial problem setup is not defined explicitly enough. It seems like the setting is a classifier with a sensitive attribute, but the specifics here are lacking. For example, in equation (1), is this a multiclass or binary class setting? Is the protected attributed discrete or continuous? If it's continuous, is it allowed to take on multiple values? These details are a bit hazy right now making it hard to figure out where the claims apply exactly.**\n\nFollowing the standard assumptions made in the fairwashing literature ([ICML-2019] and [NeurIPS-2021]), we consider binary classification tasks, and binary-valued sensitive attributes. We clarified and formalized this in the revised version of the paper in Section 2.2 in which we introduce the black-box model and sensitive attributes. We have uploaded the revised version of the paper.\n\n-[ICML-2019] Ulrich A\u00efvodji, Hiromi Arai, Olivier Fortineau, S\u00e9bastien Gambs, Satoshi Hara, Alain Tapp: Fairwashing: the risk of rationalization. ICML 2019: 161-170.\n\n-[NeurIPS-2021] Ulrich A\u00efvodji, Hiromi Arai, S\u00e9bastien Gambs, Satoshi Hara: Characterizing the risk of fairwashing. NeurIPS 2021: 14822-14834.\n\nOur analysis can be extended to multi-class and multi-valued sensitive attributes but as written in Section 7 we leave this as future work, as it involves multi-class calibration results that are out of the scope of the current paper.\n\n> **What are the specifications of the interpretable model? Does this model mirror the predictions across multiple classes or just a single class? Could you clarify what you mean by interpretable model? The construction of interpretable model I is a bit hard to follow as well, and I is introduced as just \"a simple interpretable model I\". I think what is exactly meant by the interpretable model needs to be stated more explicitly, because right now, it is a bit hazy and hard to follow**\n> **This makes it difficult to follow the statement of Theorem 1, where the theorem specifices and interpretable model, without this really being defined.**\n\nWe thank the reviewer for attention to detail. Indeed, Theorem 1 is quite broad because it is model-agnostic. Consider two machine learning models trained on the same dataset. One model is deemed to be more \u201cinterpretable.\u201d This could be because of its tree-like construction that facilitates observing a decision boundary, or it could be a linear model with weights that signify the importance of each feature. The other model, deemed the black-box model, does not necessarily enjoy these characteristics\u2014but instead may be more performant. \n\nRegardless of the particular architecture of models, Theorem 1 says that as long as we can characterize the false positive rate and true positive rate of the interpretable model\u2019s decisions (with the reference points defined as black-box model decisions); we can fully characterize the fairwashing gap provided we know the unfairness of the black-box model.  Furthemore, the theorem shows that this gap cannot be reduced to zero and provides the lower bound $\\gamma = \\Gamma_B (1 + F^+_1 - T^+_1)$. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "XrA1G8PKrup",
                "writer": "author",
                "reply_to": "A2txta97HE",
                "title": "Response to Reviewer bYAU",
                "comment": " > **This paper lacks the comparison with the following studies:\n[AAAI-2020] K. Fukuchi, et al. \"Faking fairness via stealthily biased sampling.\" AAAI2020.\n[AIES-2020] D. Slack, et al. \"Fooling lime and shap: Adversarial attacks on post hoc explanation methods.\" AIES2020.\nBoth papers investigate the risk of deceiving fairness. Remarkably, the first paper discusses the detectability of malicious modification for deceiving unfairness under situations where the detector can access the labeled benchmark dataset.** \n\nThank you for these suggestions. After verification, both of the suggested papers share indeed key similarities with our own in the sense that they consider deceptions of fairness. These papers also further highlight the importance of conducting further research in fairwashing, model auditing, as well as model explanation. We added the following discussion in the Related Work (Section 2.1) and Appendix B.\n\nTo summarize, the first paper [AAAI-2020], similar to us, tries to detect fairwashing but with a different setting. The second paper [AIES-2020] shows an alternate method of fairwashing that evades model explanation techniques in a setting that is comparable to ours but does not attempt to detect the fairwashing. Next, we highlight some of the differences in settings, assumptions and solutions:\n\n\n- Setting: We rely on explainer models while they use a published data subset for fairness auditing. In particular, they reveal some subset of the training data and their predictions. We follow the setting considered in the literature introducing fairwashing ([ICML-2019] and [NeurIPS-2021]). [AIES-2020] explanations require input perturbations while ours require the model owner to provide an interpretable model.\n\n\n- Assumptions: [AIES-2020]  assume that fairness auditing is performed via model-agnostic local explanations (e.g., LIME and SHAP). Both [AIES-2020] and [AAAI-2020] assume an ideal detector with knowledge of the underlying training distribution of the model; and query access to the black-box model. However, we only assume access to black-box model predictions on the suing set. We stress that this dataset is available before any formal audit takes place (in the form of asking for model explanations). This does _not_ constitute query access to the black-box model; in fact it is only dataset access, which here is the predictions on the suing set. \n\n\n- Solutions: They attempt to detect fairwashing using a Kolmogorov-Smirnov (KS) test over the underlying and company-provided subset distribution to determine if the data subset was honestly provided. More precisely, they show that detection is difficult when fairwashing is conducted by minimizing the Wasserstein distance between the distributions while subject to fairness; this optimization also minimizes the upper-bound of the advantage (i.e., distinguishability) of the KS test. This differs from our setting in which an informed malicious company must optimize the fairness subject to both fidelity (loss) and the detection threshold.\n\nExplanation methods like LIME and SHAP perturb inputs to gauge feature relevance, inadvertently querying with synthetic data that may be detected with out-of-distribution detection. Queries determined to originate from explainers are fed to a fair model whereas in-distribution points are given to the biased model. In contrast, our detection method is non-cooperative (we require no additional information from the black-box model as the auditor already has the predictions on the suing set) and therefore does not rely on input perturbations. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "A2txta97HE",
                "writer": "author",
                "reply_to": "Zh34qCft8o2",
                "title": "Response to Reviewer bYAU",
                "comment": " We thank the reviewer for the comments and for highlighting that we are the first to propose a detection method for fairwashing. \n\n\n> **The problem setting looks weird. The authors assume the detector can query the original model B in a black-box way. However, if we can query B, we can also calculate the unfairness score of B, such as the difference of the conditional probabilities in the demographic parity, by which we can confirm the fairness of the original model B. Hence, under the assumption of having accessibility to B, the defender easily detects the unfairness of the original model \nB. In other words, there is no risk of fairwashing in this context. While this paper adequately validates the fact that their method can detect fairwashing in this context, the detectability is obvious due to accessibility to B.**\n\nPlease note that the Algorithm 1 never accesses the black-box model after the first query step which is solely done on the suing set samples. We stress that this dataset is available before any formal audit takes place. This does _not_ constitute query access to the black-box model; in fact it is only dataset access, which here is the predictions on the suing set. Under this problem setting, there is a risk of fairwashing.\n\nWe acknowledge a mistake in Algorithm 1\u2019s input that mentioned query access (despite not using it on any sample other than the suing set samples). This has been fixed in the revised submission and is uploaded. We thank the reviewer for the insightful comment. \n\n> **The statement of Theorem 1 is not rigorously defined. What mean by \"completely eliminating fairwashing\"? Also, what mean by \"sufficient\"?** \n\nBy \u201ccompletely eliminating fairwashing\u201d we mean to reduce the fairwashing gap $\\gamma$ (defined in Equation 3) to zero. We discuss this phenomenon in the Irreducibility paragraph (line 202 and Equation 6) where we show that $\\gamma = \\Gamma_B (1 + F^+_1 - T^+_1) > 0$. We have adapted the revision text to highlight $\\gamma > 0$.\n\nBy sufficiency of measuring false-positive and true-positive rates of the interpretable model w.r.t black-box model for detecting fairwashing, we mean that only these two rates (plus the black-box model unfairness which is not a characteristic of the detector), fully characterize the fairwashing gap. That is, measuring any other metric (for instance, to create a new detector) is unnecessary. We show this in the Sufficiency paragraph (line 209). \n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4Nud5BnYKNWn",
                "writer": "author",
                "reply_to": "9tqLesugn61",
                "title": "Response to Reviewer vBkK",
                "comment": " We thank the reviewer for the comments and for highlighting the importance of our analytical and empirical analysis. \n\n> **The logic of the paper is clear but the expressions in the paper are kind of obscure and not easy for readers to follow. The motivation and the algorithm in the proposed detection model are simple and clear, but the authors use a lot of long sentences to describe them, which makes the simple things complicated. It would be better if the authors could use more short and succinct sentences, which will improve the presentation and readability of the paper.**\n\nThanks for this suggestion. We have implemented it in the revised version of the paper, in particular by improving the presentation and splitting long sentences into short and succinct sentences. The revised version of the manuscript has been uploaded. \n\n\n> **The hyperparameter \\delta (the fairwashing detection threshold) is a key parameter in this proposed detection algorithm. More discussions on how to choose this hyperparameter in practical use are needed.\nHow should we choose the hyperparameter \\delta in different scenarios (e.g. different tasks, different data)?**\n\nWe thank the reviewer for the insightful comment. The question of choosing a threshold is a pertinent one. We provide two answers, one based on the theory we have presented in Section 3, and an empirical solution. \n\n### Theory Solution\nIn Appendix I, we provide the following bounds for the KL divergence measure of FRAUD-detect:\n\n$\\kappa := \\operatorname{KL}\\left(\n        \\begin{bmatrix} F^+_0 \\\\\n                        F^-_0 \n        \\end{bmatrix}, \n        \\begin{bmatrix} F^+_1 \\\\\n                        F^-_1 \n        \\end{bmatrix}\n        \\right)$ \n\nis bounded from below and above by:\n\n$\n  F_0^+ \\log\\left(\\frac{F_0^+}{F_1^-} \\cdot \\frac{\\gamma_0}{\\gamma_1}\\right) \n                        \\leq  \\kappa \\leq  \n  F_0^- \\log\\left(\\frac{\\gamma_0}{\\gamma_1} \\cdot \\frac{F_0^-}{F_1^+}\\right)\n$\n\nAny choice between \n$\\kappa_{min} := F_0^+ \\log\\left(\\frac{F_0^+}{F_1^-} \\cdot \\frac{\\gamma_0}{\\gamma_1}\\right)$ \nand \n$\\kappa_{max} := F_0^- \\log\\left(\\frac{\\gamma_0}{\\gamma_1} \\cdot \\frac{F_0^-}{F_1^+}\\right)$ is valid. \n$\\kappa = \\kappa_{min}$ is the most conservative choice which increases the risk of false discovery (of fairwashing), while $\\kappa = \\kappa_{max}$ is a permissive choice that minimizes the risk of false discovery at the expense of increase in the false negatives (not detecting fairwashing). A balanced choice for threshold  could be the intermediate point:\n\n$\\bar{\\kappa}=\\frac{F_{0}}{2}\\left(\\Delta_{l}\\left(F^{+}\\right)+\\Delta_{l}\\left(F^{-}\\right)\\right)+\\frac{\\gamma_{0}}{2 \\Gamma_{B}} \\log \\left(\\frac{\\gamma_{0}}{\\Gamma_{B}}\\right)-\\frac{1}{2} \\frac{\\gamma_{0}}{\\gamma_{1}} \\frac{1}{\\Delta\\left(F^{-}\\right)}$\n\nwhere $\\Delta(R) = R_0/R_1$ and $\\Delta_l(R) = \\log \\Delta(R)$. The above point is accurate up to $O\\left(\\frac{\\Gamma_B^2}{\\gamma_1^2}\\left(\\frac{F^-_1}{F^-_0}\\right)^2\\right)$.\n\n### Empirical Solution\nWe note that the threshold is a hyper-parameter. Assuming access to additional (possibly public) data $\\mathcal{D}$, one can calibrate the threshold using a separate \u201ccalibration\u201d dataset: \n\n1. Train a state-of-the-art black-box model using $D_{train} \\sim \\mathcal{D}$\n2.Train an explainable model $M_{honest}$ on $D_{train} \\sim \\mathcal{D}$ without using any additional constraints on the gap between the black-box and interpretable model\n3. Train an explainable model $M_{fairwashed}$ on $D_{train} \\sim \\mathcal{D}$ using the Informed Adversary Optimization of Definition 5 in order to minimize the fairness gap\n4. Measure the KL divergence of $D_{sg} \\sim \\mathcal{D}$ on $M_{honest}$ and $M_{fairwashed}$ to form $X_{honest}$ and $X_{fairwashed}$ datasets. Assign labels $y = 1$ to $X_{fairwashed}$ and $y = 0$ to $x_{honest}$. \n5. $X= X_{honest} \\cup X_{fairwashed}$, and $Y = Y_{honest} \\cup Y_{fairwashed}$ form a univariate regression model with the following loss function $\\ell$:\n$\\ell(x, y, T)=\\sum_{i} \\frac{1}{2}\\mathbb{I}\\left(x_i \\leq T, y=1\\right)+\\frac{1}{2}\\mathbb{I}\\left(x_i>T, y=0\\right)$. And the optimal threshold $T^* = \\arg\\min_T \\ell(x, y, T)$.\nNote that this assumes equal weights for type I and type II error of the detector. Finally, we note the state-of-the-art in calibration is [1]. However, a SOTA method is likely not necessary because if there are enough samples for calibration, central limit theorem ensures both  $X_{fairwashed}$ and $X_{honest}$ are normally distributed at which point the optimal threshold is simply $T^* = \\frac{1}{2}(\\mu_{fairwashed} +\\mu_{fairwashed})$.\n\n\n[1] R. Sahoo, A. Chen, S. Zhao, and S. Ermon, \u201cReliable Decisions with Threshold Calibration,\u201d p. 14.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9tqLesugn61",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_3vmKQUctNy",
                "title": "",
                "comment": " This paper focuses on the problem of fairwashing detection in black-box models. To be specific, fairwashing is a technique used to fool model explanation methods to produce deceptive outputs for unfair models. In this paper, the authors argue the importance of studying the faiwashing issue, and propose a novel framework FRAUD-Detect to detect fairwashed models, and present empirical results. In addition, theoretical analysis is provided to show that fairwashing is unlikely to be avoided. And the robustness of the proposed detection method towards an informed adversary is also discussed. Strengths:\n1. This paper investigates a novel and important problem. Model explanation is a popular and promising approach for auditing a model with critical usage to avoid unfairness in automated decision-making. But such model explanation approaches are also vulnerable to being attacked, which has been overlooked. The problem is also challenging, a thorough study on it is needed.\n\n2. The paper provides a simple but effective method for detecting dishonest interpretation models that take advantage of the fairwashing technique.\n\n3. Theoretical conclusion is provided to show that fairwashing is unlikely to be completely avoided. Empirical results are provided to validate the effectiveness of the proposed detection method and also its robustness towards an informed attack.\n\nWeaknesses:\n1. The logic of the paper is clear but the expressions in the paper are kind of obscure and not easy for readers to follow. The motivation and the algorithm in the proposed detection model are simple and clear, but the authors use a lot of long sentences to describe them, which makes the simple things complicated. It would be better if the authors could use more short and succinct sentences, which will improve the presentation and readability of the paper.\n\n2. The hyperparameter \\delta (the fairwashing detection threshold) is a key parameter in this proposed detection algorithm. More discussions on how to choose this hyperparameter in practical use are needed. 1. How should we choose the hyperparameter \\delta in different scenarios (e.g. different tasks, different data)? No potential negative societal impact.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "Zh34qCft8o2",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_3vmKQUctNy",
                "title": "",
                "comment": " The authors deal with the problem of detecting fairwashing, a maliciously generated explanation such that the model looks fair even if it is actually unfair. They investigated fairwashing in the global explanation and found that elimination of fairwashing is difficult, and fairwashing can be detected using false- and true- positive rates. Then, their proposed detection algorithm for fairwashing is to check the deviation of the false/true positive/negative rates between the original and explanation models using KL divergence. To evaluate the robustness of their algorithm, they introduce a fairwashing algorithm with a mechanism evading the proposed detection method. The empirical evaluations demonstrate the high detectability and robustness of their proposed detection algorithm. (Strengths)\n- The fairwashing is an urgent risk. Hence, the detection of the fairwashing is well motivated.\n- To the best of my knowledge, this is the first to propose a detection method for fairwashing.\n\n(Weakness)\n- The authors make an impractical assumption about the defender's knowledge.\n- Lacks some important references.\n\nThe authors deal with the interesting and demanding task of detecting fairwashing, and most of the results are technically sound. However, I found a fundamental flaw in the problem setting, by which they deal with the situation where there is no risk of fairwashing. Due to such a considerable flaw, I recommend the rejection of this paper.\n\nThe problem setting looks weird. The authors assume the detector can query the original model $B$ in a black-box way. However, if we can query $B$, we can also calculate the unfairness score of $B$, such as the difference of the conditional probabilities in the demographic parity, by which we can confirm the fairness of the original model $B$. Hence, under the assumption of having accessibility to $B$, the defender easily detects the unfairness of the original model $B$. In other words, there is no risk of fairwashing in this context. While this paper adequately validates the fact that their method can detect fairwashing in this context,  the detectability is obvious due to accessibility to $B$.\n\nThe statement of Theorem 1 is not rigorously defined. What mean by \"completely eliminating fairwashing\"? Also, what mean by \"sufficient\"? \n\nThis paper lacks the comparison with the following studies:\n- K. Fukuchi, et al. \"Faking fairness via stealthily biased sampling.\"  AAAI2020.\n- D. Slack, et al. \"Fooling lime and shap: Adversarial attacks on post hoc explanation methods.\" AIES2020.\n\nBoth papers investigate the risk of deceiving fairness. Remarkably, the first paper discusses the detectability of malicious modification for deceiving unfairness under situations where the detector can access the labeled benchmark dataset.  See strengths and weaknesses. The accessibility to $B$ makes the situation no fairwashing risk. Hence, I recommend the authors reconsider the situation to be meaningful.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "eeK51LFm3H",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_3vmKQUctNy",
                "title": "",
                "comment": " This paper presents a theoretical analysis of fair washing and demonstrates that completely reducing fair washing in certain cases is not possible. In addition, they present a simple technique that looks at the KL between surrogate model confusing matrices to detect fairwashing. They evaluate their technique and find it helps detect fairwashing issues. Strengths:\n- Interesting problem \n- The empirical evaluation of fraud detect is comprehensive, considering many different models and settings.\n\nWeaknesses:\nThe main weakness is that the theory is missing explicitness in several places and there are gaps in what the exact setting is. Specifically, here are the main gaps I see:\n- The initial problem setup is not defined explicitly enough. It seems like the setting is a classifier with a sensitive attribute, but the specifics here are lacking. For example, in equation (1), is this a multiclass or binary class setting? Is the protected attributed discrete or continuous? If it's continuous, is it allowed to take on multiple values? These details are a bit hazy right now making it hard to figure out where the claims apply exactly.\n- What are the specifications of the interpretable model? Does this model mirror the predictions across multiple classes or just a single class?\n- The construction of interpretable model $I$ is a bit hard to follow as well, and $I$ is introduced as just \"a simple interpretable model I\". I think what is exactly meant by the interpretable model needs to be stated more explicitly, because right now, it is a bit hazy and hard to follow. This makes it difficult to follow the statement of Theorem 1, where the theorem specifices and interpretable model, without this really being defined. \n\nWhile the empirical results about the detection algorithm are pretty complete, it seems tricky to choose this threshold $\\Delta$ that determines whether fair washing is going on. It would useful to provide more guidance here.\n\nOverall, I think this paper needs to much more rigorously provide the problem setting and explicit definitions of what is going on.\n\n  - Figure 3 is a bit hard to follow. Could you help clarify this figure? Where does the dotted line come from? Why are there multiple fidelity values for every $\\Delta$?\n- Could you help clarify the problem setup a bit more, per the weaknesses section?\n- Could you clarify what you mean by interpretable model?\n The authors have done a sufficient job.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "EvJ4GagR51c",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_3vmKQUctNy",
                "title": "",
                "comment": " The paper characterizes the problem of fairwashing for interpretable models. It proposes a fairwashed model detection method based on measuring the difference over subpopulations in true-positive and false-positive rates of the interpretable model w.r.t. the black-box model. It shows the method\u2019s sufficiency theoretically and effectiveness empirically even under the assumption of an informed attacker. The evaluation is conducted on two interpretable models of four classification models on three widely used fairness datasets. ## Strengths\n1.The problem is well-motivated. Fairwashing can be a practical threat in reality.\n\n2.The theoretical analyses look correct.\n\n3.The proposed method is comprehensively evaluated and shows decent performance for practical usage. In particular, the setting of an informed adversary strengthens the generalizability of the proposed method.\n\n4.The paper is well-written, well-organized, and easy to follow.\n\n## Weaknesses\n1.The method currently only applies to one fairness definition. However, in practice, other definitions like equality of opportunity are also commonly used.\n 1.For other fairness criteria such as equality of opportunity, how easy is it to adapt the current method? They are well addressed.",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the motivation and the comprehensive experiments in the paper",
                "Sentiment Expression": "appreciated",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the clarity of the paper",
                "Sentiment Expression": "were concerned",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "a potential flaw",
                "Sentiment Expression": "even worried about",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not at all"
            }
        ]
    },
    "QevkqHTK3DJ": {
        "paper_id": "iclr_2022_QevkqHTK3DJ",
        "paper_title": "Compressing Transformer-Based Sequence to Sequence Models With Pre-trained Autoencoders for Text Summarization",
        "paper_abstract": "We proposed a technique to reduce the decoder\u2019s number of parameters in a sequence to sequence (seq2seq) architecture for automatic text summarization. This approach uses a pre-trained AutoEncoder (AE) trained on top of a pre-trained encoder to reduce the encoder\u2019s output dimension and allow to significantly reduce the size of the decoder. The ROUGE score is used to measure the effectiveness of this method by comparing four different latent space dimensionality reductions: 96%, 66%, 50%, 44%. A few well-known frozen pre-trained encoders (BART, BERT, and DistilBERT) have been tested, paired with the respective frozen pre-trained AEs to test the reduced dimension latent space\u2019s ability to train a 3-layer transformer decoder. We also repeated the same experiments on a small transformer model that has been trained for text summarization. This study shows an increase of the R-1 score by 5% while reducing the model size by 44% using the DistilBERT encoder, and competitive scores for all the other models associated to important size reduction.",
        "paper_acceptance": "Reject",
        "meta_review": "The paper proposes to incorporate an autoencoder to transformer-based summarization models in order to compress the model while preserving the quality of summarization. The strengths of the paper, as identified by reviewers, are in extensive experiments presented in the paper and in a relatively clear write-up. However, the reviewers identify several weaknesses, including missing state-of-the-art summarization baselines and missing relevant compression/knowledge distillation baselines. Although the author response have addressed some of reviewers' concerns, all the reviewers agree that the draft is not yet ready for publication.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "590ILX4-5OL",
                "writer": "official_reviewer",
                "reply_to": "FBxab1DF_ci",
                "title": "Unchanged review",
                "comment": " I thank the authors for their responses to my questions which have clarified some of the confusions.  However, I remain of the opinion that there is insufficient novelty and practical significance in this paper to warrant acceptance.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4P0ulAowfQE",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_QevkqHTK3DJ",
                "title": "",
                "comment": "The paper proposes a new autoencoder-based seq2seq model for text summarization tasks. The paper tries to find the best trade-off between compression ratio and model performance. The paper conducts extensive experiments by evaluating the loss of accuracy with ROUGE. Strength\n1. The paper is clearly written. The paper tries to discover the trade-off between the compression ratio and model performance. The paper shows experiment results by comparing different types of autoencoders as well as different compression rates. The paper also presents the generation results and another related experiment in the appendix. \n\nWeakness\n1. How is the compression rate determined? It would be better to show why 32, 128, 384,512 are chosen for the decoder size. The scale of those dimensions is not even linear. It might be more reasonable for readers to see dimensions with 32, 64, 128, 256, 512. The paper needs to include more comparison between other compression methods such as distillation, information bottleneck, pruning. The paper fails to show insight qualitative analysis for the Table. 3 and Table 4. Table 4 seems to be incomplete. In Section 3.4, why not use all training data to train the autoencoder. \n\n2. The rouge scores might not be good  It would be better to incorporate other metrics such as BLEU, BERTscore.  Overall, this paper presents an interesting experiment to discover the trade-off between the compression rate and the performance. The experiment seems a little bit naive without detailed analysis.  I recommend rejection for this paper.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "FBxab1DF_ci",
                "writer": "author",
                "reply_to": "fn2Udw6u9pU",
                "title": "Response to Reviewer 1LU5",
                "comment": " Thank you for reviewing our paper and pointing out possible improvement. It seems that there are some misunderstandings due to lack of clear explanation in the paper. We hope that the below description and the changes to the submission can clear out some confusions.\n\n---\n\n\n* **Q** The way these are used is not very clear but I assume that the linear AE maps each symbol independently whereas the LSTM and CNN see the whole sequence? This point should be clarified. Also, what motivated the choice of a purely linear network. A standard MLP bottleneck autoencoder seems to me to be a more obvious choice than a CNN. Why wasnt this used?\n\n  * **4.1**   The autoencoder models are trained to compress the embedding size of different pre-trained encoders (for example; BERT) from [batch_size, seq_len, 768] to [batch_size, seq_len, compression_size]. (Added some explanation to the paper as well) Also, our linear autoencoder is not a single linear layer network. It is an MLP with 6 layers. (as stated in section 3.2 and figure 3)\n\n\n* **Q** please explain what motivates the choice of 60% and confirm that no test material is used for pre-training the autoencoder.\n\n  * **4.2**  The idea is to hold back on some training data for the summarizer model to ensure that our proposed method can also compress the encoder\u2019s representation on unseen data. Also, the test set is selected from the CNN/DM dataset and it has not been used while training neither the AE nor the summarizer model.\n\n\n* **Q** There appears to be a problem with Table 4 - perhaps the labelling in the left hand column? In any event, I cant find the comparisons between AE, AE S and LL. It's really not obvious why AE S should be worse than AE. Please comment on this.\n\n  * **4.3**  Unfortunately, the labels were not correct in Table 4. We fixed the issue and appreciate that you point out the mistake.\n\n    The AE models are trained independently to compress the data to a smaller latent space and reconstruct it using the said representation. They learn how to keep the most useful information from the mentioned data using an MSE loss. However, the AE S model is basically a 3-layer MLP on top of the summarizer model\u2019s encoder (for example, BERT) that tries to project the data to a smaller space and uses the summarizer model\u2019s loss function (CrossEntropy). So, it appears to be difficult to train the summarizer model and the AE S jointly.\n\n\n* **Q** The results show that you can reduce the decoder size by up to 40% without sacrificing too much in performance. However, this does nothing to change the size of the pre-trained encoder and it is the problem of the ever-increasing size of the pre-trained encoder which is highlighted in the introduction to the paper.\n\n  * **4.4** We are focusing on the text summarization task where the number of parameters in both encoder and decoder will add up. In this experiment, we solely focused on the decoder component, and it was not our intention to reduce the encoder size.\n\n\n* **Q** It seems to me that what you are really doing here is adjusting the size of the decoder to best match the amount of training data for the downstream task. In your case, text summarization. A different task with much more training data might give a different result. Please comment on this.\n\n  * **4.5**   For training the autoencoder, we can have access to a lot of data because the training is unsupervised. Once the autoencoder model (that can do the dimensionality reduction) is trained, the amount of supervised data for fine-tuning the model could be increased/decreased as desired by the users. Of course, in general, more data will always help get better results in deep learning, but it will not have a negative effect.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "bxvgo3PS63v",
                "writer": "author",
                "reply_to": "gD93-pF793",
                "title": "Response to Reviewer wmem",
                "comment": " Thank you for reviewing our paper and pointing out possible improvement. It seems that there are some misunderstandings due to lack of clear explanation in the paper. We hope that the below description and the changes to the submission can clear out some confusions.\n\n---\n\n\n* **Q** The results of the transformer baseline is far from previous literatures, such as [Li et al., 2018; Lewis et al., 2019; Wei et al., 2020] that achieves at least 33.42 R-L score, while this paper report a 31.2 R-L score with the transformer baseline.\n\n  * **3.1**  Our transformer model is a small vanilla transformer model with 6 encoder/3 decoder layers. It has only 70M parameters which is significantly smaller than other models. It is just a baseline to evaluate the proposed method\u2019s effectiveness in comparison to the effectiveness of a custom small model and is not designed to reproduce any specific paper\u2019s results. We add more explanation about this on the paper to reduce the confusion.\n\n\n* **Q** There is no any comparisons with existing compression methods, including pruning and distillation.\n\n  * **3.2**  Please refer to Answer 1.3 with [response to reviewer Lsjg](https://openreview.net/forum?id=QevkqHTK3DJ&noteId=TGTFlOq_SDG).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xFXyTlvjy2a",
                "writer": "author",
                "reply_to": "4P0ulAowfQE",
                "title": "Response to Reviewer vaDv",
                "comment": " Thank you for reviewing our paper and pointing out possible improvement. It seems that there are some misunderstandings due to lack of clear explanation in the paper. We hope that the below description and the changes to the submission can clear out some confusions.\n\n---\n\n* **Q** How is the compression rate determined? It would be better to show why 32, 128, 384,512 are chosen for the decoder size. The scale of those dimensions is not even linear. It might be more reasonable for readers to see dimensions with 32, 64, 128, 256, 512.\n\n  * **2.1**  We did more experiments on the BERT model with three more compression rates. (Appendix A.2) The complete list of compression sizes is 32, 64, 128, 256, 384, 448, and 512. Our results showed the same trend in all experiments.\n\n\n* **Q** The paper needs to include more comparison between other compression methods such as distillation, information bottleneck, pruning. \n\n  * **2.2**  Please refer to Answer 1.3 with [response to reviewer Lsjg](https://openreview.net/forum?id=QevkqHTK3DJ&noteId=TGTFlOq_SDG).\n\n\n* **Q** The paper fails to show insight qualitative analysis for the Table. 3 and Table 4. Table 4 seems to be incomplete.\n\n  * **2.3**  Unfortunately, the labels were not correct in Table 4. We fixed the issue and appreciate that you point out the mistake.\n\n\n* **Q** In Section 3.4, why not use all training data to train the autoencoder.\n\n  * **2.4**  The idea is to hold back on some training data for the summarizer model to ensure that our proposed method can also compress the encoder\u2019s representation on unseen data. We tried to make it more clear in the paper on the latest revision.\n\n\n* **Q** The rouge scores might not be good It would be better to incorporate other metrics such as BLEU, BERTscore.\n\n  * **2.5**  While the ROUGE score is the dominant metric for evaluating text summarization tasks, we added the BERTScore metric evaluation to the paper as well. Thanks for the suggestion.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "TGTFlOq_SDG",
                "writer": "author",
                "reply_to": "2RRapThRckD",
                "title": "Response to Reviewer Lsjg",
                "comment": " Thank you for reviewing our paper and pointing out possible improvement. It seems that there are some misunderstandings due to lack of clear explanation in the paper. We hope that the below description and the changes to the submission can clear out some confusions.\n\n---\n\n* **Q** The motivation for using autoencoder is not quite clear. Actually, BART is already pre-trained with a denoising loss which should be better than autoencoder loss. How about a more detailed comparison between these two losses?\n\n  * **1.1** The Autoencoder has been used as a dimensionality reduction method, where its encoder projects the input to a smaller latent space, and the decoder part tries to recreate the input. It is responsible for reducing a pre-trained encoder\u2019s (e.g. BERT) output dimension, resulting in a smaller decoder size in our summarizer model. The reasoning for this size reduction is that the summarizer model decoder will have to process smaller tensors.\n\n\n* **Q** It's not clear whether the encoder is frozen or not. Is the encoder further optimized when fine-tuning? If not, it's not clear why not optimize the encoder. If optimized, then pre-training losses from BART can also be adopted.\n\n  * **1.2**  We have two different encoders in this experiment:\n    1.\tThe Auntoencoder\u2019s encoder ($ AE_{enc} $): Takes care of dimensionality reduction. We train an autoencoder for each pre-trained transformer model (for example, BERT), then freeze the Autoencoder\u2019s encoder and use it in our summarizer model. The reason for freezing it is to reduce the number of trainable parameters. Our experiment showed that training it jointly with the summarizer model will reduce the ROUGE score. (AE S) \n    2.\tTransformer encoder ($ T_{enc} $): It is chosen from a number of well-known pre-trained models. It is also frozen in our summarizer model. We chose to do it to eliminate an additional fine-tuning factor from the experiments. It could have raised questions like if different models based on their sizes should be treated differently. The focus of this paper is to show that a Linear Autoencoder can reduce a pre-trained encoder representation size while not losing critical information for the decoder to generate a meaningful summary.\n\n    A number of changes have been done to the paper and figures to better clarify this for readers.\n\n\n* **Q** Missing some knowledge distillation baselines, such as Noisy Self-Knowledge Distillation for Text Summarization and PRE-TRAINED SUMMARIZATION DISTILLATION\n\n  * **1.3**  While there are numerous techniques available to reduce the network size, we believe these methods are orthogonal to each other and can be used together. We used the combination of knowledge distillation (DistilBERT) and our proposed AE method to reduce the network size even more and increase the R-1 score by 5%. The same concept can be applied to other distilled models.\n\n\n* **Q** Figures 1-3 have been shown in many papers. It would be better to emphasize more on the novel part.\n\n  * **1.4**  We rearranged the figures in the paper to better describe our approach and removed the unnecessary ones.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2RRapThRckD",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_QevkqHTK3DJ",
                "title": "",
                "comment": "The authors propose to add an autoencoder on top of a pre-trained encoder to reduce the encoder\u2019s output dimension and allow to significantly reduce the size of the decoder. Weaknesses\n1. The motivation for using autoencoder is not quite clear. Actually, BART is already pretrained with a denoising loss which should be better than autoencoder loss. How about a more detailed comparison between these two losses? \n2. It's not clear whether the encoder is frozen or not. Is the encoder further optimized when fine-tuning? If not, it's not clear why not optimize encoder. If optimized, then pre-training losses from BART can also be adopted.\n3. Missing some knowledge distillation baselines, such as Noisy Self-Knowledge Distillation for Text Summarization and PRE-TRAINED SUMMARIZATION DISTILLATION\n4. The paper writing can be further improved. Figures 1-3 have been shown in many papers. It would be better to emphasize more on the novel part.\n Overall, the proposed loss is not novel enough and needs to have a further comparison with other pertaining losses such as the losses from BART. And also missing some strong Seq2Seq baselines with knowledge distillation losses.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "gD93-pF793",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_QevkqHTK3DJ",
                "title": "",
                "comment": "This paper proposes to compress the transformer-based summarization models with well pre-trained autoencoders (AE). In this architecture, AE produces an intermediate states that is obtained by compressing the encoder\u2019s final representation and pass it to the decoder. Strengths\n\n1. The paper was written clearly enough to understand the basic ideas.\n2. The paper conducted extensive experiments to find the ideal trade-off between the compression ratio and model\u2019s text generation capability.\n\nWeaknesses:\n\n1. The results of the transformer baseline is far from previous literatures, such as [Li et al., 2018; Lewis et al., 2019; Wei et al., 2020] that achieves at least 33.42 R-L score, while this paper report a 31.2 R-L score with the transformer baseline.\n\n2. There is no any comparisons with existing compression methods, including pruning and distillation.\n\n[Li et al., 2018] Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling.\n[Lewis et al., 2019] BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.\n[Wei et al., 2020] Multiscale Collaborative Deep Models for Neural Machine Translation.  In this paper, the baseline system is far from literatures and is lack of comparisons to existing methods.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "fn2Udw6u9pU",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_QevkqHTK3DJ",
                "title": "",
                "comment": "This paper investigates the use of a pre-trained autoencoder to reduce the output dimensionality of a pre-trained transformer (such as BERT or BART) so that a lower dimensionality decoder can be used.  The results show that for a summarisation task, the decoder size can be reduced by 40% without significant loss of accuracy as measured by rouge.  This paper investigates the use of a pre-trained autoencoder to reduce the output dimensionality of a pre-trained transformer (such as BERT or BART) so that a lower dimensionality decoder can be used.  \n\nThree kinds of AE are investigated: Linear, LSTM and CNN.  The way these are used is not very clear but I assume that the linear AE maps each symbol independently whereas the LSTM and CNN see the whole sequence?  This point should be clarified.  Also, what motivated the choice of a purely linear network.  A standard MLP bottleneck autoencoder seems to me to be a more obvious choice than a CNN.  Why wasnt this used?\n\n\nThe AE is trained using \"60% of these combinations of dataset\" - please explain what motivates the choice of 60% and confirm that no test material is used for pre-training the autoencoder.\n\nThere appears to be a problem with Table 4 - perhaps the labelling in the left hand column?  In any event, I cant find the comparisons between AE, AE S and LL.  It's really not obvious why AE S should be worse than AE.  Please comment on this.\n\nThe results show that you can reduce the decoder size by up to 40% without sacrificing too much in performance.  However, this does nothing to change the size of the pre-trained encoder and it is the problem of the ever-increasing size of the pre-trained encoder which \nis highlighted in the introduction to the paper.\n\nIt seems to me that what you are really doing here is adjusting the size of the decoder to best match the amount of training data for the downstream task.  In your case, text summarisation.  A different task with much more training data might give a different result.  Please comment on this. This paper describes a simple approach to reducing the dimensionality of the decoder to be used for a pre-trained encoder.  The experimental results may be of interest to practitioners concerned with reducing model sizes.  However, the original contribution is rather small and the restriction to a single downstream task leaves questions over its general applicability.",
                "rating": 3,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "extensive experiments presented in the paper and in a relatively clear write-up",
                "Sentiment Expression": "strengths",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "missing state-of-the-art summarization baselines and missing relevant compression/knowledge distillation baselines",
                "Sentiment Expression": "weaknesses",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the draft",
                "Sentiment Expression": "is not yet ready for publication",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "SJgVHkrYDH": {
        "paper_id": "iclr_2020_SJgVHkrYDH",
        "paper_title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
        "paper_abstract": "Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. \n      Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path.\n      Experimental results show state-of-the-art results in three open-domain QA datasets, showcasing the effectiveness and robustness of our method. Notably, our method achieves significant improvement in HotpotQA, outperforming the previous best model by more than 14 points.",
        "paper_acceptance": "accept-poster",
        "meta_review": "The paper proposed a multi-hop machine reading method for hotpotqa and squad-open datasets. The reviewers agreed that it is very interesting to learn to retrieve, and the paper presents an interesting solution. Some additional experiments as suggested by the reviewers will help improve the paper further. ",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "S1gd_SqAKr",
                "reply_to": "iclr_2020_SJgVHkrYDH",
                "title": "Official Blind Review #2",
                "comment": "Summary\n========\nThis paper introduces a graph-based recurrent retrieval model for retrieving evidence documents in a multi-hop reasoning question answering task. The main idea is that (1) the graph formed by Wikipedia links between passages can be used as constraint for constructing reasoning chains, and (2) the joint encoding of the question and current passage can be used to retrieve a subsequent passage in the reasoning chain. The paper describes a model for implementing the above retrieval system, and how they jointly train with a reading comprehension model. They demonstrate the effectiveness of the system on HotPotQA, showing improvements over previously published models, and SQuaD-Open, showing competitive results.\n\nOverall Comments\n===============\nThe paper is an interesting, but incremental, improvement to the area of question answering. Overall, there are two main concerns about this work. First, while the results are somewhat strong, the ideas presented are small variations on existing systems. For example, Godbole et al 2019 and Ding et al. 2019 both explore using graphical structural to constraint iterative, multi-hop, retrieval. Also, Feldman et al 2019, describe an encoder based approach to encode question and paragraph context for iterative retrieval. Asides from smaller modeling differences (choice of RNN, training regime, BERT reader, etc.) to account for the difference in results, the main difference seems to be the joint training of the retrieval system with the reader. Secondly, the paper lacks clarity on some formal definitions and definition of the graph, making it hard to understand the content precisely.\n\nDetailed Comments\n================\nBelow are some detailed comments about specific parts of the paper, in order of importance:\n\n1. One important limitation of this technique is the reliance on a linked documents for constructing the retrieval system. It is not clear from the paper how much of the results are obtained from constraining the set of retrieved passages (after the initial retrieval) to Wikipedia links. And whether, for example, substituting Wikipedia links with links derived from an off-the-shelf entity linking system would suffice.\n\n2. Given that the retrieval model is restricted to link structure in Wikipedia that induces the proposed retrieval graph, I assume that there are \u201creasoning paths\u201d that do not exist in the graph, given Wikipedia\u2019s policy of avoiding adding redundant links within a Wikipedia page. It would have been informative to conduct an \u201cOracle\u201d experiment: that is, given the initial  set of retrieved nodes and the graph structure, are there *any* paths that provide the correct answer and reasoning chain? That is to say, what is the upper-bound performance on the proposed system given the currently induced Wikipedia graph?\n\n3. In Section 3, and even later on in the paper, it was not clear what \u201cE\u201d denotes. It never seems to be defined, and is used interchangeably with \u201cgraph node\u201d, \u201cwikipedia page\u201d, \u201cwikipedia paragraph\u201d and \u201creasoning path\u201d. Are these the same thing? It would be much clearer to define what E means, and perhaps separate the different concepts (node, passage, reasoning path) properly.\n\n4. In Section 3, it seems that \u2018q\u2019 is not defined. Is it the question?\n\n5. In Section 3.1, it is not clear what the graph actually contains. Does it contain all the paragraphs from Wikipedia? Just the paragraphs with links? The first paragraph of every Wikipedia page? What granularity of the wikipedia page becomes an individual node in the graph?\n\n6. In Section 3.1.1., the representation of the starting retrieval (i.e., time-step = 0), h_0, is not defined. Later in the section, the paper mentions the use of TF-IDF for the initial set of nodes, instead of the learned retrieval model. This seems a bit unusual design decision without further explanation. Particularly when taking the results in Table 4, showing TF-IDF based retrieval performs worse that the learned retrieval system from the proposed model.\n\n7. In Section 3, C_{t} (the candidate set of paragraphs) is not defined. This is an important set to define. Is it the set of paragraphs derived from Wikipedia links, starting from the current node?\n\n8. In Section 3.1.2, \u201cLoss function\u201d, the term g_{r} is not defined.\n\n9. In Section 4.4 \u201cAnalysis on reasoning path length\u201d, it would have been useful to see the performance of the model with different path lengths. This analysis is somewhat common on multi-hop reasoning tasks, and should be included.\n\n10. Typo in Section 4.4: \u201c..., and out model is likely too terminate \u2026\u201d  should be \u201c likely to terminate \u201c\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BJegSqBYsH",
                "reply_to": "HkglHuj3FS",
                "title": "Updates on empirical results on query independent encoding",
                "comment": "[For the response to individual weaknesses, please read our response to Official Blind Review #3 (part 1 and part 2).]\n\nTo show the importance of the query-dependent encodings in terms of accuracy, we conducted an experiment again with a query-independent variant of our approach on the HotpotQA development set. More specifically, our retriever model encodes paragraphs independently from their corresponding queries and sequentially retrieves paragraphs in the same manner as our proposed approach. For a fair comparison, we use the same reader model (BERT wwm) with the path re-ranking. We train the alternative model without using the data augmentation technique for quick experiments. For evaluation, we provide results on both the distractor and full wiki settings. \n\nThe results are summarized in the table below:    \n\n\n  encoding method   | full wiki QA F1  | full wiki QA EM  | distractor QA F1  | distractor QA EM \n--------------------------   |----------------------|-----------------------|------------------------|-----------------------\n query-dependent     |          64.1           |         52.6             |           81.2              |           68.0 \n query-independent |          47.3            |         37.8             |           80.0              |           66.4  \n\nFor our query-dependent approach, the full wiki results correspond to \u201cretriever, no link-based negatives\u201d in Table 6, and the distractor results correspond to \u201cOurs (Reader: BERT wwm)\u201d Table 1. As seen in this table, the QA F1 and EM performance significantly deteriorate on the full wiki setting, which demonstrates the importance of the query-dependent encoding for complex and entity-centric open-domain question answering. This is the reason why we employ our query-dependent approach, and we achieve competitive results on the three datasets.\n\nWe also found that the performance drop on the distractor setting is much smaller than that on the full wiki setting. This is due to its closed nature. In the distractor setting, we are given ten paragraphs and the two gold paragraphs are always included, which makes the retrieval task much easier than that in the full wiki setting. We have only 10 paragraphs for each question, and thus the number of the possible reasoning paths is quite limited. Therefore, our recurrent retriever model is likely to discover the gold reasoning paths by the beam search, and our reader model can select the gold paths by the robust re-ranking approach. To verify this assumption, we checked the P EM score as a retrieval accuracy in the distractor setting. If we only consider the top-1 path from the beam search, the P EM score of the query-independent model is 12% lower than that of our query-dependent model. However, if we consider all the reasoning paths produced by the beam search, the coverage of the gold paths is almost the same. As a result, our reader model can perform similarly with both the query-dependent/independent approaches. This additionally shows the robustness of our re-ranking approach.\n\nWe are planning to add these experimental results in our next revision.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkeehtADiH",
                "reply_to": "SklkStAPjH",
                "title": "Response to Official Blind Review #3 (part 2)",
                "comment": "# Clarification on Figure 2 (re: Weakness 5, Update 6)\nSorry for the confusion caused by our current figures. We first would like to clarify that our reader reads all of the eight reasoning paths in parallel and jointly predicts P_i^{start}, P_i^{end} and P(E|q) for each of the reasoning path. To determine the final answer, the reader selects a span (i,j) from E, whose P(E|q) is the highest (See Equation 7). In our previous version, we call the reasoning path re-ranking (\u201csecond stage of re-ranking\u201d in your words) as \u201canswer re-ranking\u201d, which might cause confusion. In our updated version, we re-name this module as \u201creasoning path reranking\u201d for clarification and update all of the figures and relevant section titles. \n\n\n# Performance and running-time comparison with query-independent encoding (re: Weaknesses 6)\nAppendix A.2 discussed the motivation of using our query-dependent paragraph encodings. In our preliminary experiments, we started from a query-independent model with the RNN, but we found that the retrieval accuracy was very low even on the HotpotQA distractor setting. This motivated us to use the query-dependent representations, with the help of the initial TF-IDF retrieval. Lee et al. (2019) also showed that their query-independent retrieval model performs poorly on datasets requiring entity-centric retrieval such as SQuAD Open. The query-independent encodings save computational costs on the BERT encodings, while introducing other engineering efforts like how to store and retrieve pre-computed representations (Seo et al., 2019). By contrast, we primarily put more weight on improving the accuracy, keeping our model scalable by introducing our efficient inference strategies such as the beam search.\nFor an additional experiment about this, please also refer to another thread \"Updates on empirical results on query independent encoding.\"\n\n# Typo liked -> linked (Sec 4.3, line 5)\nThank you for pointing it out. We have fixed the typo. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HkglHuj3FS",
                "reply_to": "iclr_2020_SJgVHkrYDH",
                "title": "Official Blind Review #3",
                "comment": "This paper proposes a method to find a sequence of reasoning paragraphs in Wikipedia to answer queries requiring multi-hop reasoning. They make the key observation that answering multi-hop queries might require retrieving evidence that have very less lexical overlap with the question. Given a query, the proposed method starts from a set of initial paragraphs retrieved by a tf-idf retriever and uses the outgoing Wikipedia anchor link to hop to the next evidence. They propose a simple recurrent neural network that takes in the current paragraph (and the hidden state) and decide which paragraph to hop to in the next step. Because of the available supervision for the paragraphs (in HotpotQA), they can train a supervised path selector. They also add a special EoE token that denotes the end of the reasoning path, thereby having the ability to produce reasoning paths of different lengths. After training the retriever a beam of reasoning paths is sent to the reader module. The reader module re-ranks the reasoning paths again and then use a standard BERTQA model and the top re-ranked chain of paragraphs to find the evidence.\n\nOverall, the paper presents a well-designed system for handling multi-hop queries and the explicit recurrent state is a nice contribution and addition to the IR model proposed in Godbole et al., 2019. The paper is clearly written for the most part.\n\n===Update (11/12/2019)===\nThe authors have addressed all my comments and have improved the results since. I am recommending acceptance. Nice work.\n\nStrengths:\n\u2014 The proposed method has demonstrated strong results on 2 datasets in challenging open-domain settings. The ablation results are helpful.\n\u2014 The paper is clearly written and was straightforward to follow\n\nWeaknesses:\n\n1.  The paper mentions that it studies the interplay between the retriever and reader. It is unclear how it is doing so, since the retriever and the reader are not explicitly interacting with each other. Cant the retriever and the reader be trained separately? \n2.  It is unclear / not motivated, why there is an extra step of re-ranking required in the reading stage? In other words, what kinds of extra inductive bias is this additional step of re-ranking providing since the same kind of supervision was used while training the retriever model. I do note that the ablation study is helpful and it is clear that it is effective, but it would be nice to see a discussion regarding why this second step of re-ranking helps.\n3. Since the reader model (BERT reader) takes the top scoring chain of paragraphs concatenated together, that would imply that it is currently limited by the number of positional embeddings in the BERT model (512 tokens). I think this limitation should be explicitly mentioned and possible remedies discussed.\n4. The current approach is heavily dependent on Wikipedia graph and will not work if the hyperlink graph is not provided. It would have been nice to have an entity linker component that could also create the graph structure. I believe concurrent work such as Godbole et al., 2019 has addressed this and the paper should mention this while contrasting with their work. \n5. From figure 2, I got an impression that since the reader scored the span in \"Top 2 reasoning path\u201d higher, that was selected. But after section 3.2, I was left confused because it looks like the reader model consumes the top scoring chain after the second stage of re-ranking. This is not clear from the figure and should be fixed.\n6. Discussion on scalability: Although the retriever is clearly very effective for such questions, the running time would be prohibitive (for open domain QA) as at test time, query dependent context representations is constructed for each of the paragraph in the reasoning chain. I would like to see a discussion / some running time comparison where query independent paragraph representations are constructed and the network just encodes the query independently at test time.\n\nMinor: Typo liked -> linked (Sec 4.3, line 5)",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HJxL380PiS",
                "reply_to": "HJlHgIAPiB",
                "title": "Response to Official Blind Review #2 (part 2)",
                "comment": "Clarity.\n\nThank you for pointing out several unclear descriptions in our method, and in this revision, we did our utmost best to clarify some details and also add additional experimental results to make our results more convincing (See the details from the response on \u201cThe clarity on the definition\u201d below). \n\n================\n\nWe list our response to your Detailed Comments below.\n\n# On the reliance on hyperlinks and performance evaluation with an off-the-shelf entity linking system (re: Detailed Comments 1)\nOur updated manuscript presents a comparison of our framework with and without the given hyperlinks. In place of the hyperlinks, we used an off-the-shelf entity linking system. Please refer to the details of the experiments in Section 4.4 \u201cThe performance with an off-the-shelf entity linking.\u201d Table 7 shows marginal performance drop even without the hyperlinks, still achieving the state of the art on HotpotQA full wiki. We would also like to mention that the existence of hyperlinks is common, especially when using documents on the Web, and our results suggest that using hyperlinks as well as entity links is promising. The most recent work (Anonymous, 2019; Nie et al., 2019) also relies on the hyperlinks, while our results perform better.\n\n\n# The clarity on the definition of the Wikipedia graph, reasoning paths, paragraph candidates C_{t}, and search spaces (re: Detailed Comments 2, 3, 5, 7)\nWe have updated Section 3 (Overview) and Section 3.1 and 3.2 to clarify the definitions.\n\n- The Wikipedia Graph \\mathcal{G}: each node of \\mathcal{G} is \u201ca paragraph\u201d. A paragraph is represented as p_i in our paper. Thus, an edge connects two paragraphs, and it can be either a hyperlink or a with-in article link (See Section 3.1). By default we consider all of the paragraphs in English Wikipedia, but for HotpotQA, we only consider introductory paragraphs, following all the previous work using the dataset. This is described in Section 4.1 \u201cEvidence Corpus and the Wikipedia graph\u201d. The whole graph is constructed in advance, and we reuse the same graph for training and inference, instead of dynamically building entity graph everytime from the TF-IDF retrieval results as in Ding et al. (2019) or Godbole et al. (2019).  \n\n- A reasoning path E = [p_i, \\ldots p_k]: a reasoning path contains one or more paragraphs that are together used by out reader model to answer a given question. Our framework learns to retrieve a reasoning path for a given question from the entire Wikipedia. \n\n- Top B reasoning paths \\mathbf{E}=\\{E_1, \\ldots, E_B\\}: our retriever employs a beam search with the beam size of B for decoding, and thus, the top B distinct reasoning paths \\{E_1, \\ldots, E_B\\} are returned. Our reader further re-ranks these reasoning paths to determine an answer. Our reader jointly encodes all of the paragraphs in each reasoning path and then re-ranks the retrieved reasoning paths, fully capturing the paragraph interactions in E.  \n\n- The candidate paragraph set C_{t}: we construct C_{t} as a set of the paragraphs to be considered for retrieval at each time step t. At t=1, the candidates are initialized with the F paragraphs with the highest TF-IDF scores with respect to the question. Then, C_{t+1} includes (i) paragraphs linked from the previously selected paragraph p_{t} or (ii) a few of top ranked paragraphs from the previous step. C_{t} is not limited to the initially retrieved F paragraphs, and thus, our retriever dynamically expands the paragraph candidates over the graph.   \n\n\n# On the upper-bound performance of the proposed system (re: Detailed Comments 2)\nWe expect that your suggestion about \u201cupper-bound performance\u201d is to calculate how many of the ground-truth reasoning paths can be found in our initial TF-IDF retrieval and the graph. As shown in our experimental results on HotpotQA, a retrieved reasoning path consists of up to three paragraphs. Thus, we consider if a ground-truth reasoning path can be found in the subgraph within three steps from the initial TF-IDF retrieval.  In particular, we calculate the upper-bound paragraph EM as follows:\n(the number of questions whose ground-truth reasoning paths can be found in the sub-graph) / (the total number of questions).\n\nWe have checked the upper bound in our preliminary experiments on the development set in HotpotQA. The coverage of the gold reasoning paths is 75.4% when the initial TF-IDF retrieval size is 20. The coverage is considered as an upper bound of the P EM score of our method in Table 5. For reference, the coverage is 84.1% and 89.2% with the initial TF-IDF size of 100 and 500, respectively. This analysis is now reported in Appendix C.1. [...continued in next post]",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bkl-WmRviB",
                "reply_to": "iclr_2020_SJgVHkrYDH",
                "title": "Summary of general updates",
                "comment": "We thank all of the reviewers for providing such insightful and valuable feedback. Based on the feedback, we made substantial updates on our paper. \n\nOur updates are summarized below:\n\n[Update 1] New experimental results on Natural Questions Open:\nWe added new experimental results on Natural Questions Open (Lee et al., 2019) to show our method\u2019s robustness and scalability. Natural Questions Open has three unique features as an open-domain QA benchmark. The questions are written by actual users independently from existing corpora. Some questions in this dataset require multi-hop reasoning (e.g., how tall is the actor who plays hagrid in harry potter), but the multi-hop reasoning annotations are not provided. This dataset requires a system to search *all* paragraphs in Wikipedia articles, and thus a system needs to be truly scalable. Our results are competitive with the state of the art (See Section 4.2 and Table 4), which demonstrates the scalability and robustness of our framework. We also provide a qualitative example which shows that our system learns to conduct multi-hop retrieval on Natural Questions Open without original multi-hop reasoning path annotations (See Appendix C.5 and Table 13). We do not add any architectural design changes for this experiment.  \n\n[Update 2] Updated results on HotpotQA (distractor, full wiki) and SQuAD Open:\nWe found that our reader model was under-tuned on HotpotQA and SQuAD Open during our experiments on Natural Questions Open. In particular, it is effective to use larger mini batches for training our reader model with BERT and distant supervised examples extracted from our training data for the retriever model (See Section 3.2 and Appendix B.3). Consequently, we advance our state-of-the-art results from our initial submission on HotpotQA (both fullwiki and distractor) by around 4 points and also outperforms the state of the art (multi-passage BERT) on SQuAD Open by 3 points (See Section 4.2 and Table 1,3). We re-submitted to the HotpotQA full wiki leaderboard on November 6th ( https://hotpotqa.github.io/ ), and our model ranks first, outperforming all of the published and up-to-date unpublished work (See Table 2).  \n\n[Update 3] Performance using an off-the-shelf entity linking system vs. hyperlinks (review#2, review#3):\nWe added an experiment by replacing the Wikipedia hyperlinks with entity links given by an off-the-shelf system, and observed a slight performance drop (2.3 F1 and 2.2 EM), still achieving the state of the art (See Section 4.4 \"The performance with an off-the-shelf entity linking,\" Table 7, and Appendix B.6 for details). This also suggests that using hyperlinks is promising, considering that hyperlinks are commonly used on the Web.\n\n[Update 4] Clarification of the method (review#2):\nWe added clarification about the definition of the Wikipedia graph, reasoning path and search space by our retriever in Section 3 Overview and Section 3.1.1. \n\n[Update 5] Additional analysis of experimental results (review#2, #3):\nWe added analysis on (1) the performance comparison with different reasoning path length (See Section 4.4 \u201cThe performance of different path length\u201d and Table 8) and (2) qualitative and quantitative analysis to understand the importance of the reader-side reasoning path re-ranking (See Section 4.4 \u201cThe effectiveness of the interplay between retriever and reader\u201d, Table 9, Figure 4). \n\n[Update 6] Rename \u201canswer re-ranking\u201d to \u201creasoning path re-ranking\u201d (review#3): \nWe rename \u201canswer re-ranking\u201d to \u201creasoning path re-ranking\u201d to reflect our framework\u2019s actual behavior and to avoid confusion. \n\n[Update 7] Clarify the experimental settings and results on HotpotQA distractor (review#1): \nIn our first version, we briefly described the experiments on HotpotQA distractor. We added descriptions of the experimental settings and results for this setting in Section 4.1 and 4.2. \n\nIt should also be noted that Godbole et al. (2019) presented their work in Machine Reading for Question Answering workshop in EMNLP 2019 on November 4th, and the original manuscript was submitted to arXiv on September 17th (one week before ICLR 2020 deadline). Nevertheless, we did our utmost efforts to provide a careful comparison, and empirically our approach yields more than 20 F1 and 30 P EM improvements over this work. In this revision, we also added comparison with the most up-to-date work on HotpotQA posted after the ICLR submission deadline (Qi et al., 2019; Anonymous, 2020; Nie et al., 2019). \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklkStAPjH",
                "reply_to": "HkglHuj3FS",
                "title": "Response to Official Blind Review #3 (part 1)",
                "comment": "We really appreciate your supportive comments on our paper and your detailed feedback. Below, we address the weakness.\n\n# On the training of reader and retriever (re: Weaknesses 1)\nOur reader and retriever are separately trained, and this paper does not explore joint learning. We used the term \u201cinterplay\u201d to represent our reasoning path re-ranking framework where our reader verifies the retrieved reasoning paths produced by the beam search, instead of finalizing the path selection only by the retriever. Our training strategy for our reader uses not only ground-truth paragraphs but also negative examples to simulate irrelevant paths produced by our retriever. Joint learning is interesting future work; nevertheless, such a two-stage training strategy is worth investigating, considering our strong empirical results. In practice, another advantage is that the framework is flexible; for example, when better reader models are made available, we can easily leverage the advance, without re-training the retriever model. For this revision, we re-train our reader models for SQuAD Open and HotpotQA and leveraging these new models further advances the state-of-the-art results on the two datasets.\n\n\n# On the inductive bias and the differences in supervision (re: Weaknesses 1, 2) \nThere are practical differences in training our retriever and reader models. \n\nThe first difference is in paragraph interactions. Our retriever learns to capture the paragraph interactions through the BERT\u2019s [CLS] representations, after independently encoding the paragraphs along with the question; this makes our sequential retrieval scalable to the open-domain scenario. By contrast, our reader model fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths; this is especially crucial for multi-hop reasoning as discussed in recent work (Wang et al., 2019a).\n\nThe second difference is in supervision signals. Our retriever is trained to predict plausibility of the reasoning paths, without learning to answer the question. Our reader model also learns to predict the plausibility with the stronger paragraph interactions, and jointly learns to answer the question.  \n\nIn summary, our retriever is scalable, but the top-1 prediction is not always enough to fully capture multi-hop reasoning to answer the question. Therefore, we use our reader model for the additional re-ranking process to mitigate the uncertainty and make our framework robust. Table 9 shows the statistics of the re-ranking results, and one interesting observation is that our reader model prefers longer paths. We added an example in Figure 4 (and also in Table 12 in Appendix) where the re-ranking finds more convincing reasoning paths.\n\n\n# The token length limitations by BERT (re: Weaknesses 3)\nWe investigated the statistics of the token length of the concatenated paragraphs in the selected reasoning path for HotpotQA full wiki. In summary, only 0.2% of the examples exceed 512 tokens (based on the BERT tokenization), and thus we expect that the influence of BERT\u2019s maximum length limitation is marginal. We believe this is another benefit of our framework. By selecting the reasoning path, our model can effectively avoid handling many paragraphs in the encoding steps.\n\n# Reliance on hyperlink information and experiments with off-the-shelf entity linking components (re: Weaknesses 4, Update 3)\nOur updated manuscript presents a comparison of our framework with and without the given hyperlinks. In place of the hyperlinks, we used an off-the-shelf entity linking system. Please refer to the details of the experiments in Section 4.4 \u201cThe performance with an off-the-shelf entity linking.\u201d Table 7 shows marginal performance drop even without the hyperlinks, still achieving the state of the art on HotpotQA full wiki. We would also like to mention that the existence of hyperlinks is common, especially when using documents on the Web, and our results suggest that using hyperlinks as well as entity links is promising. The most recent work (Anonymous, 2019; Nie et al., 2019) also relies on the hyperlinks, while our results perform better.\n\n[...continued in next post]",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HylTvDCvsr",
                "reply_to": "HJxL380PiS",
                "title": "Response to Official Blind Review #2 (part 3)",
                "comment": "# On the effectiveness of TF-IDF-based initial paragraph candidates (re: Detailed Comments 6)\nWe initialize the candidate paragraph set C_1 with TF-IDF-based top F paragraphs, and start using our RNN retriever from the candidate set. There are several reasons for this strategy. \n\nFirst, processing millions of paragraphs with neural networks is computationally infeasible, especially for non-industry scale computational resources, as discussed in previous work (Lee et al., 2019; Seo et al., 2019).\n\nSecond, the previous work shows that fully trainable retrieval without using any term-based features performs poorly for entity-centric questions (e.g., SQuAD), as compressing specific entity information into a fixed dimensional vector is challenging (Seo et al., 2019, Lee et al., 2019). In particular, a recently proposed end-to-end retriever, ORQA (Lee et al., 2019) shows significantly lower performance than a TF-IDF retriever (DrQA proposed by Chen et al., 2017) on SQuAD Open (See Related Work and Table 3). \n\nFor the reasons we listed above, we bootstrap the retrieval with the TF-IDF retriever. We clarified these points in Section 3.1.1 in our updated manuscripts.  \n\n\n# The definition of q (re: Detailed Comments 4)\nWe have updated the manuscript to clear define q as a question.\n\n\n# The initialization of h_0 (re: Detailed Comments 6)\nOur RNN\u2019s initial state is h_1, which is used to predict the first paragraph in each reasoning path. h_1 is based on an independent parameterized vector, and please also refer to Appendix A.1 for more details.\n\n\n# The definition of \u201cLoss function\u201d, the term g_{r}  (re: Detailed Comments 8)\nWe have found that ``_{r}\u2019\u2019 was missing from the definition of ``g_{r}\u2019\u2019 in the sentence: ``In particular, we add a new training path g = [pr, p1, . . . , p|g| ] by \u2026\u2019\u2019 in Section 3.1.2. We have revised this part to precisely define the term.\n\n\n# The performance with different path lengths (re: Detailed Comments 9)\nWe have added the performance comparison with the settings where we use the same model but set the length of the reasoning paths to a fixed number (i.e., 1, 2, 3, 4). We can see that our adaptive approach performs the best, even though the HotpotQA\u2019s gold reasoning path length is always 2 (See Table 8).\n\nWe further present the QA performance of the model with different lengths (averaged QA F1 and EM scores on the questions whose retrieved reasoning path length is {1,2,3}) on HotpotQA (See Table 9). \n\n\n# A typo in Section 4.4 (re: Detailed Comments 10)\nThank you for pointing it out. We fixed this typo in our updated submission. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJlHgIAPiB",
                "reply_to": "S1gd_SqAKr",
                "title": "Response to Official Blind Review #2 (part 1)",
                "comment": "We thank you for your helpful feedback. We have substantially updated our manuscript to address all the concerns you kindly raised as much as we can.\n\nFirst of all, we would like to address the two weaknesses you mention in your overall comments. \n\nOriginality.\n\n# On the difference with other graph-based approaches\nOur work has several significant originalities in its system design, training and inference time strategies from Ding et al. (2019) and Godbole et al., (2019), which leads to more than 20 point improvements over these previous approaches on HotpotQA full wiki.\n\n1)  System design: We formulate the retrieval as reasoning path search over the Wikipedia graph, instead of dynamically constructing an entity-graph for each question based on compiled document lists as in the previous work; the recurrent module dynamically updates and expands candidate paragraphs from the initial TF-IDF-based candidates at each time step. In addition, our work also studies the interplay between our retrieval model and the reader model (See the response of \u201cReasoning path retrieval and the interplay between our reader and retriever\u201d below for details).\n\n2) Training strategy: To train our recurrent module to learn to retrieve the path, leveraging the graph structure, we train our model with negative sampling and multiple reference paths (See Section 3.1.2 and the summary by review#1). \n\n3) Inference strategy: We introduce beam-search based decoding to make the framework more scalable (See Section 3.1.1; also summarized by review#1 and #2), and the beam search with our reasoning path re-ranking is more effective than a greedy search. As in Table 6, replacing beam search with greedy search deteriorates F1 by 3.7. Also, our method does not need to encode all possible nodes like the previous studies, and instead each path only encodes its corresponding paragraphs.\n\nThe HotpotQA dataset used in the previous work is based on introductory paragraphs only. By contrast, our method is applied not only to HotpotQA but also to the Natural Questions (See Update 1) and SQuAD Open datasets. These two datasets are not restricted to the introductory paragraphs, and our method achieves state-of-the-art results. This is made possible by our search-based decoding strategy. One interesting observation on our Natural Questions experiments is that our model learns to retrieve multi-hop reasoning paths with our training strategy, even without multi-hop gold path annotations as in HotpotQA (See Appendix C.5 and Table 13). This demonstrates the robustness and scalability of our approach.  \n\n\n# On the difference with other multi-step retrieval approaches\nPrevious multi-step approaches such as Das et al. (2019), Qi et al. (2019), Godbole et al. (2019) and Feldman and El-Yaniv (2019) do not accommodate arbitrary steps of reasoning. As review#1 and review#3 summarize, our RNN approach uses the EOE symbol to produce reasoning paths with different lengths. This allows our model to be easily applicable to both multi-hop and single-hop questions without specifically changing the model architecture. Table 8 demonstrates the effectiveness of this adaptive retrieval process. In practice, it is not obvious if a question requires single-hop or multi-hop retrieval (e.g., some of the Natural Questions Open are clearly answerable based on single paragraph, while in some questions, multi-hop reasoning helps.), and thus this flexibility is another significant advantage. \n\n\n# Reasoning path retrieval and the interplay between our reader and retriever \nOur framework benefits from the interplay between our retriever and reader. Our retriever encodes the candidate paragraphs independently for scalability, and iteratively selects a paragraph at each time step conditioned by the prediction history. Each of the resulting K reasoning paths (K=beam size) includes one or more paragraphs. Our reader encodes the paragraphs in the paths jointly and predicts probabilities of each reasoning path E containing an answer span. By encoding the paragraphs jointly, our reader model fully leverages the self-attention mechanism across the concatenated paragraphs in the retrieved reasoning paths; this is especially crucial for multi-hop reasoning as discussed in recent work (Wang et al., 2019a). The additional reasoning path re-ranking makes our overall framework robust, leading to large performance improvement (See Section 4.4 and Table 8,9). This reasoning path re-ranking is one of the novel points in our work.\n\nThese significant differences together demonstrate state-of-the-art performance on the four experimental settings in the three datasets, HotpotQA (full wiki, distractor), SQuAD Open and Natural Questions Open (Update 1). Notably, our method outperforms all the previous graph-based or multi-step retrieval methods by more than 20 points on HotpotQA full wiki and 15 points on SQuAD Open (See Table 1,2,3). \n\nWe added discussions in Section 2 (Related Work) to clarify these points. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgyjVRPir",
                "reply_to": "ryg3CRRRYS",
                "title": "Response to Official Blind Review#1",
                "comment": "Thank you for reading our paper thoroughly and providing encouraging feedback. \n\n# The results on HotpotQA distractor setting\nRegarding the Hotpot distractor setting, we have evaluated our method on the settings, and the scores on the development set are reported in the first version of our manuscript (See Table 1, columns 6-9); due to the time constraints, we did not submit our model to the leaderboard. The results show that our method achieves state-of-the-art scores on the distractor setting, outperforming the previous best-published model by more than 10 points. Our work is the first to demonstrate the state-of-the-art performance on both the distractor and full wiki settings of HotpotQA. We revised our manuscript to make the distractor evaluation clear (See Update 7 and Section 4.1 \u201cHotpotQA\u201d and  Section 4.2 in our updated manuscript). We have a qualitative example in Appendix C.6 and Table 14, which shows how our sequential reasoning path process also helps in distractor setting. \n\nWe added new experimental results on Natural Questions Open (Table 4) and additional analysis on selected reasoning paths (Section 4.4) in our updated version. We hope it will be helpful in evaluating the effectiveness of our method. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryg3CRRRYS",
                "reply_to": "iclr_2020_SJgVHkrYDH",
                "title": "Official Blind Review #1",
                "comment": "The paper is proposing a multi-hop machine reading method tested on hotpotqa in the Full Wikipedia setting and squad-open datasets.\nFor hotpotqa, It could also have been interesting to evaluate the method of the distractor ones.\nFirst, the proposed method constructs a graph over the Wikipedia pages represented by their respective summary paragraphs.\nIn this representation, the hyperlinks among pages represent the edges.\nThen, the authors trained a normalized RNN model to retrieve the candidate reasoning paths from the question.\nThe model is bootstrap using TF-IDF page retrieval techniques.\nThen, a Beam-search decoding strategy is used to retrieve \"reasoning path\" which is then pass through a BertQA model using a simple question-reasoning-path concatenation technique.\nOne originality of the method is the negative sampling strategy that includes negative TF-IDF retrieval as starting points to robustify the sequential extraction process.\nThe detailed experiments and ablation tests give to illustrate the experimental relevance of the proposed method.",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HJgZPKrFOS",
                "reply_to": "r1g-3abtdr",
                "title": "thanks for your clarification",
                "comment": "Huge thanks for your detailed and valuable information, they are extremely helpful, much appreciated!\n\nWith your explanation, all numbers make sense to me now, thanks a lot for your time!",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1g-3abtdr",
                "reply_to": "BkxGvEnBuS",
                "title": "Re: question about table 4 (Retrieval evaluation)",
                "comment": "Hi Siqi, \n\nThank you for your interest in our work and such an encouraging comment. We agree that our current description of paragraph recall (PR) might be not clear enough, and we will update our manuscript once the discussion phase begins and edits are allowed. In summary, our PR evaluates the percentage of questions for which at least one gold paragraph is retrieved. We additionally contrast our PR metric with PEM, which evaluates the percentage of questions for which both gold paragraphs are retrieved. \n\n> For us we just use PR = number of retrieved gold paragraph /  total number of gold paragraph, then average PR across different questions.\n>  (1) The PR for TF-IDF on dev set is way too high. The hits@10 for TF-IDF retriever in HotpotQA paper is 56.06 on dev, while your TF-IDF achieves 66.9 recall on top 2...  We also implemented our own TF-IDF retriever and achieved 55.71 recall if we retrieved 10 paragraphs per question, which is similar to original HotpotQA paper\n\nBy PR, we evaluate if at least one of the ground-truth paragraphs for each question is included among the retrieved paragraphs. In particular, the score is calculated as follows: \n\nPR = (the number of questions where a retriever finds at least one of the gold paragraphs) / (the total number of questions in the development set).\n\nWe assume that the PR would be estimated higher than the recall score calculated by you or the hits@10 calculated by the HotpotQA authors. \n\n> (2) We ran the CogQA retriever and achieved 69.98 PR (in table 4 the number is 87.6). Since the retrieved documents should be almost the same for different runs, can you help us find out what might be the reason that we failed to replicate the results?\n\nWe expect that this happens due to the same reason we described above. \n\nIn addition, we would like to mention the motivations behind the metric design. Here, we aim at evaluating (1) if a retriever can find at least one paragraph of the gold paragraphs (corresponding to PR), and (2) if a retriever can find all of the gold paragraphs  (corresponding to P EM). We expect that even a non-parameterized retriever (e.g., TF-IDF retriever) can find one of the gold paragraphs based on lexical matching, but it is likely to fail to find one or more of the gold paragraphs consisting of little lexical overlap or semantic relationship to the original questions. \n\nAs in Table 4, the PR is relatively high across several retrievers, but the TF-IDF retriever or the Re-rank show low P EM, as it cannot access the paragraphs that are ranked lower by TF-IDF but entailed or the relationships between paragraphs. \n\nWe will add more detailed explanation about how we calculate the scores and why we design the metrics in the way in our next version. We also consider changing the names (PR and P EM). \n\nAgain, thank you so much for your interest and insightful comments.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxGvEnBuS",
                "reply_to": "iclr_2020_SJgVHkrYDH",
                "title": "question about table 4 (Retrieval evaluation)",
                "comment": "First of all, thanks for posting your amazing work here! The paper's novelty, presentation and results are all excellent to me!\n\nHowever we do have one question about your results in table 4, may I ask how do you compute the AR and PR (especially PR) in table 4 (some rows are copied below) ? Note that we absolutely trust your results because they are evaluated on a hidden test set, we just couldn't reproduce the numbers in this specific table and hope you could help if possible :)\n  \nModels                        AR       PR      PEM       EM\n--------------------------------------------------------------------\nTF-IDF                         39.7    66.9    10.0       18.2\nCognitive Graph       76.0    87.6     57.8      37.6\n\nFor us we just use PR = number of retrieved gold paragraph /  total number of gold paragraph, then average PR across different questions. The numbers in table confuse me because\n(1) The PR for TF-IDF on dev set is way too high. The hits@10 for TF-IDF retriever in HotpotQA paper is 56.06 on dev, while your TF-IDF achieves 66.9 recall on top 2...  We also implemented our own TF-IDF retriever and achieved 55.71 recall if we retrieved 10 paragraphs per question, which is similar to original HotpotQA paper (of course, way below your number, especially considering your model only retrieves top 2)\n\n(2) We ran the CogQA retriever and achieved 69.98 PR (in table 4 the number is 87.6). Since the retrieved documents should be almost the same for different runs, can you help us find out what might be the reason that we failed to replicate the results?\n\nThanks again for your time.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "a multi-hop machine reading method for hotpotqa and squad-open datasets",
                "Sentiment Expression": "proposed",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the paper presents an interesting solution",
                "Sentiment Expression": "very interesting",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "additional experiments",
                "Sentiment Expression": "will help improve the paper further",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "ByeadyrtPB": {
        "paper_id": "iclr_2020_ByeadyrtPB",
        "paper_title": "Learning Deep-Latent Hierarchies by Stacking Wasserstein Autoencoders",
        "paper_abstract": "Probabilistic models with hierarchical-latent-variable structures provide state-of-the-art results amongst non-autoregressive, unsupervised density-based models. However, the most common approach to training such models based on Variational Autoencoders often fails to leverage deep-latent hierarchies; successful approaches require complex inference and optimisation schemes. Optimal Transport is an alternative, non-likelihood-based framework for training generative models with appealing theoretical properties, in principle allowing easier training convergence between distributions. In this work we propose a novel approach to training models with deep-latent hierarchies based on Optimal Transport, without the need for highly bespoke models and inference networks. We show that our method enables the generative model to fully leverage its deep-latent hierarchy, and that in-so-doing, it is more effective than the original Wasserstein Autoencoder with Maximum Mean Discrepancy divergence.",
        "paper_acceptance": "reject",
        "meta_review": "The paper received 6, 3, 1. The main criticism is the lack of quantitative evaluation/comparison. The rebuttal did not convince the last reviewer who strongly argues for a comparison. The authors are encouraged to add additional results and resubmit to a future venue.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "rJgzs5lwtr",
                "reply_to": "iclr_2020_ByeadyrtPB",
                "title": "Official Blind Review #1",
                "comment": "The paper aims to develop a deep generative model, which -unlike VAEs or GANs- comprises a hierarchy of latent variables rather than a direct map from the stochastic latent manifold to the observation space. To this end, the paper builds a training objective based on nesting the Wasserstein distance between the data distribution and its estimation arbitrarily many times. The generated objective corresponds naturally to a deep hierarchical generative model.\n\nThe principled approach followed to achieve the objective is solid and elegant. It is also intuitive and matches nicely with some valid observations highlighted in the paper such as insufficiency of by-passing intermediate latent variables (sentence above the Sec 2.3 title).\n\nOne major weakness of the paper is that it lacks a sufficient argumentation about how it differentiates from earlier attempts to nest Wasserstein distances. For instance,\n\nY. Dukler et al., \"Wasserstein of Wasserstein Loss for Learning Generative Models\", ICML, 2019\n\nApart from the theoretical argumentation, the paper should also compare their solution to this prior work on a number of benchmarks.\n\nAnother major weakness is that the paper lacks a quantitative evaluation scheme for its success. The experiments section starts with the claim that the proposed method \"significantly\" improves on the WAE, which I fail to see on the plots. \n\nLastly, Having said that the proposed method is novel and elegant, it is still a straightforward extension of the existing and well-known Wasserstein Auto-Encoder (WAE) approach. It extends WAEs by repetitively applying the tricks proposed by this earlier work, putting aside some minor additional adjustments.\n\nMinor on style: The abstract does not give any single hint about the methodological novelty of the work.\n\n---\nPost-rebuttal: Thanks to authors for their effort for clarifications. Yet, I'm afraid the author response does not touch at all to any of the concerns I have raised. There are well-known ways to compare the success of generative models, FID being one of them as the authors point out. Another could be the test log-likelihood of a synthetic data set the true distribution of which can be predesigned. I understand the issues the authors raise about the difficulties in comparing generative models, but I kindly disagree with the attitude that there are no ways to compare, so we are obliged to live with qualitative comparisons. If a one-score comparison is not enough, the right way to go is to provide multiple scores. If direct metrics are not feasible, one should go for indirect ones, but should still provide outcomes a reader can reproduce.",
                "rating": 1,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rJlkhDzwjB",
                "reply_to": "BJx5f3iAKr",
                "title": "On the motivation behind training deep latent hierarchical generative models.",
                "comment": "We appreciate the reviewer detailed feedback.\n\nThere are many differences between our work and that of [1], but they stem predominantly from a significant difference in motivation. In [1], the authors train a single-latent-layer generative model (in evident contrast to our work) with a bespoke architecture for the encoder and decoder aiming at capturing hierarchical structure in the data and learning disentangled representations. In our work on the other hand, the goal was to show that using the Stacked WAE objective, a deep-hierarchical-latent model can be trained, in principle improving generative capacity over shallower generative models. \n\nWhile we acknowledge that one of the motivations behind using hierarchical latent-variable models is the discovery of hierarchical representations (as [1] sought to do), we focus on improving the ability of generative models to learn deep latent hierarchies (similarly to [2], [3]). That is, our motivation is to methodologically enable the training of deep latent hierarchies. Indeed, as explained in Section 2.2 and shown in Section 3.1, the Stacked WAE method allows for better training of deep hierarchical generative models than the original WAE framework. More specifically, it is able to learn an approximate posterior over all the latent layers as opposed to the WAE, and without the need for skip connections and weight sharing in the encoder and decoder networks unlike VAE methods ([2], [3]). \n\nWe admit that this leads to debatable choices for the generative models considered. For example, in the MNIST experiment in Section 3.1, we trained a generative model that is surely too deep for this simple data set (we use 5 latent layers while [1] have only 3 levels in their hierarchy). The intention of our work was not to learn MNIST well, but to show that a 5-layer latent-variable model can actually be trained on MNIST (a feat that requires significant architecture and optimisation design in the VAE setting, see Figure 6 of [2]). This is why we did not, for example, carefully interpret our latent hierarchies, despite that being an interesting question.\n\nTo make our motivation clearer to readers, in particular in contrast to [1], we have added a short discussion to the introduction. We believe that the motivational differences between our work and that of [1] justify the shortcomings pointed out by the reviewer, and in this context hope that the reviewer would agree to amending their rating to a \"weak accept\".\n\n[1]: S. Zhao, J. Song, and S. Ermon.  Learning hierarchical features from deep generative models. In International Conference on Machine Learning, 2017.\n[2]: C. K. S\u00f8nderby, T. Raiko, L. Maal\u00f8e, S. K. S\u00f8nderby, and O. Winther. Ladder variational autoencoders. In Advances in neural information processing systems, 2016.\n[3]: L. Maal\u00f8e, M. Fraccaro, V. Li\u00e9vin, and O. Winther. BIVA: a very deep hierarchy of latent variables for generative modeling. In Advances in neural information processing systems, 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkl9gazPjB",
                "reply_to": "rJgzs5lwtr",
                "title": "On the comparison with prior works.",
                "comment": "We thank the reviewer for their feedback.\n\nThe reviewer identified 3 areas on which they felt the draft could be stronger: (i) incrementality versus[1], (ii) quantitative evaluations of Stacked WAE versus other methods, and (iii) purported lack of significance in addition to the standard WAE framework. We believe that, unfortunately, this review has misunderstood our work. In particular:\n\n(i) the comparison with [1] misunderstands the difference between our approach to \"stacking\" WAE losses and the \"nesting\" of Wasserstein distances in [1],\n\n(ii) the lack of quantitative comparisons is a result of our work using a novel non-likelihood based objective, which prohibits natural single-metric comparisons, and\n\n(iii) our work is a mathematically clean and qualitatively incremental contribution on top of the existing WAE approach, enabling the training of latent-variable models that the WAE outright fails to train.\n\nWe address each of these points in turn.\n\n[1] propose to nest Wasserstein distances in the sense that they use the Wasserstein distance in the space of the images pixels as their $\\textit{ground metric}$. They then use the dual formulation of the 1-Wasserstein distance (in the image space) to derive an adversarial objective for training generative models. This differs from our work in 2 paradigmatic ways. Firstly, while [1] use the \"nested\" Wasserstein distance as their ground metric for the Wasserstein distance, we use the \"nested\" Wasserstein distance as a regularisation term on the space of latent distributions in the formulation of the WAE objective. Secondly, the objective in[1] is trained using an adversarial scheme and thus, no encoder network allows for the mapping from the observation space to the latent space. In our work, we are interested in training deep-hierarchical generative models in the autoencoder framework, with an encoder network allowing us to learn a meaningful latent manifold. The role of the \"nested\" Wasserstein distance in these two works is thus only the same in nomenclature: [1] actually $\\textit{nest}$ a Wasserstein distance in the pixel space as their ground metric, while we $\\textit{stack}$ a Wasserstein distance as a latent regulariser in the WAE objective.\n\nWe agree with the reviewer that a rigorous comparison with existing methods is important. That said, the form of the WAE loss makes such a comparison hard. Indeed, in the WAE, the relaxation of the hard constraint on the coupling of the data distribution and the generative distribution introduces a hyperparameter that will be tuned for each experiments. Moreover, the WAE objective is a likelihood free method, making it hard to compare with the common likelihood based methods. A good metric that enables the comparisons between likelihood and non-likelihood methods remains to be discovered. One attempt at comparing generative models trained with non-comparable objectives is to use sample-based metrics such as the FID score ([2]). However, given the data sets considered in our work, we felt that such metric would not be relevant. Despite this, we do perform a qualitative comparison with the original WAE method when training deep hierarchical models. We intuitively explain why WAEs would fail in training deep hierarchical latent models in section 2.2 (see Equation (7)) and then show empirically in section 3.1 that it is indeed the case (see Figure 4). While the 5-layer generative model trained as WAE achieved good reconstructions (Figure 4a), the samples are significantly worse than those obtained using our Stacked WAE (Figure 4b versus Figure 2b) and no structure was learnt in the deep latent space (Figure 4c versus Figure 2c).\n\nFinally, while our Stacked WAE method is indeed built on the well-known WAE objective and consists of stacking WAE modules on the top of each other, the novelty resides in the way we unroll the original WAE objective, using WAEs as latent regularisers at each layer, enabling the hierarchical model to leverage all of its deep layers. This allows for the propagation of information from the observation space all the way to the deepest latent layer in fully factorised Markov models, and by doing so, it captures the data structure all along the hierarchy. This result, which we clearly demonstrate, is something that both WAEs and VAEs outright fail at. In this sense we do not consider our work to be an insignificant contribution on top of the pre-existing WAE framework.\n\nWe hope that this review might either be amended significantly given that it seems to have misunderstood both our work and the relevant literature.\n\n[1]: Y. Dukler,  W. Li, A. Lin and G. Montufar. Wasserstein of Wasserstein Loss for Learning Generative Models. In International Conference on Machine Learning, 2019.\n[2]: M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information processing systems, 2017.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyxkyrGvsH",
                "reply_to": "BJllJxEH5B",
                "title": "Thank you",
                "comment": "We thank the reviewer for their positive feedback.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJx5f3iAKr",
                "reply_to": "iclr_2020_ByeadyrtPB",
                "title": "Official Blind Review #2",
                "comment": "In this paper, a hierarchical extension to Wasserstein Autoencoders (WAE) is proposed, where the latent variables are stacked in a multi-layer structure. In the proposed model, the divergence function in WAE is viewed as a relaxed WS distance. Therefore, another layer of WAE can be stacked to minimise the WS distance. In this way, a hierarchical model can be built to learn hierarchical representations.\n\nI think the idea of viewing the divergence in WAE as a relaxed WS distance and then minimising it with another WAE structure is interesting, intuitive and straightforward. However, the advantages of the proposed model over WAE and VLAE (S.Zhao et.al 2017) are less obvious to me. It is a bit hard for me to tell whether the hierarchical latent variables help to improve quantitative results, generate better images, or learn intuitive hierarchical representations, which is the main reason that I go to mild rejection.\n\nFor example, I would expect to see similar things as in VLAE, where the representations in different layers capture hierarchical structures or disentanglements. But in the proposed model, it seems to be hard to see the differences between the hierarchical representations such as in Figure 3(b). Also in the two-dimensional visualisation of Figure 3(a), it is a bit hard for me to intuitively understand what the representations really capture. \n\nFrom the graphical model point of view, the proposed model is a hierarchical Gaussian model and the inference (although with WAE) is in the flavour of Gibbs sampling, which propagates information layer-wisely from bottom up. Conventionally, a hierarchical Gaussian model is hard to work with many layers such as 5. Therefore, I may suggest improving in case of fewer layers.",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BJllJxEH5B",
                "reply_to": "iclr_2020_ByeadyrtPB",
                "title": "Official Blind Review #3",
                "comment": "This paper presents a deep, latent variable model for unsupervised data modeling problems. The problem with such latent, deep generative models is that they are difficult to train reliably. In this paper, the authors provide an approach based on stacked Wasserstein autoencoders to train deep latent variable models. Experimental results are demonstrated on various image datasets and the latent codes are demonstrated to have an interpretable meaning. \nI like the inference techniques in the paper and like the ideas presented in this paper.",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "quantitative evaluation/comparison",
                "Sentiment Expression": "lack of",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "mk0HzdqY7i1": {
        "paper_id": "iclr_2022_mk0HzdqY7i1",
        "paper_title": "What\u2019s Wrong with Deep Learning in Tree Search for Combinatorial Optimization",
        "paper_abstract": "Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard Maximum Independent Set problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality.",
        "paper_acceptance": "Accept (Poster)",
        "meta_review": "I would like to thank the authors for having managed a thorough discussion despite the complexity of the task at hand (e.g. BEvM). during discussion, the reviewers clearly converged to accepting the paper, praising the importance of the problem tackled and the setup put in place to effectively tackle the challenge at hand.\n\nAll this makes the paper an important contribution and a clear accept (and an enjoyable read), for which I can only recommend a further polish before camera ready to follow the latest inclusions.\n\nAC.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "Hjlw7L-HF1O",
                "writer": "official_reviewer",
                "reply_to": "hJsdHeucxFv",
                "title": "Thank you for the response",
                "comment": " I thank the authors for their response and for revising the paper to address the concerns. I have increased my score from 5 to 6.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EYQ5agHzK8r",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_mk0HzdqY7i1",
                "title": "",
                "comment": "The paper presents an evaluation of deep learning-based tree serch solutions (that are based on graph neural networks) for solving combinatorial optimization problems. They present an open-source benchmark suite for the maximum independent set (MIS) problem (both weighted and unweighted) that includes instances from multiple random graph models, known benchmark suites (e.g., SATLIB) and other graphs from the literature. They conduct experiments on different configurations of neural-guided tree search and show that the results by Li et al. [2018] are not reproducible. They also show that general and tailored classical solvers outperform deep learning solutions. Strengths:\n- I think the paper addresses interesting and important questions. Understanding what work and what does not work, and which parts of a solution actually contribute to the performance, is important.\n- The paper provides interesting insight on the performance and reproducibility of a previous work (Li et al. [2018]) based on thorough experiments.\n- The paper presents a new benchmark suite for MIS that includes a large number of benchmark instances and implementations of several popular approaches.\n\nWeaknesses:\n- All experiments are done on one problem type, MIS, while there is a lot of work on other graph-related problems such as TSP, VRP, etc. For example, Li et al. [2018] that is discussed in this work have considered other problems. It is hard to draw conclusions on combinatorial optimization from one problem.\n- The paper focuses on a single work (Li et al., 2018) that the authors were unable to reproduce and a single work that showed promising results (Ahn et al. [2020]). A study of a larger sample of deep learning solutions would be useful to support claims about the value of GNNs, neural-guided tree search, or reinforcement learning for combinatorial optimization.\n- The result that specialized solvers and even classical solvers are often better than deep learning solutions, especially on larger problems, has been reported for other computational problems (e.g., TSP [1]). Ahn et al. [2020] already reported that KaMIS outperforms Li et al. [2018] and that reduction and local search lead to improvements. Further, other works have looked at generalization of GNNs (e.g., [2]). These need to be cited and the similarities and differences with this work should be discussed.\n- While providing open-source benchmark suite, including implementations of several popular approaches, is important, as far as I understand this suite is primarily a collection of existing problems and randomly-generated graphs and does not introduce new benchmark datasets. I am not sure this is an important contribution of the work.\n\n\n\n[1] Joshi, C. K., Cappart, Q., Rousseau, L. M., Laurent, T., & Bresson, X. (2020). Learning TSP requires rethinking generalization. arXiv preprint arXiv:2006.07054.\n\n[2] Xu, K., Zhang, M., Li, J., Du, S. S., Kawarabayashi, K. I., & Jegelka, S. (2020). How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848.\n Overall, I think this type of works is important and can lead to important insight. However, I think experiments with more problem types and more solutions are needed to draw general and interesting conclusions. Also, the paper needs to discuss some relevant works that are currently not mentioned.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "kPwqekFjh8x",
                "writer": "author",
                "reply_to": "CfXZ14Fazr",
                "title": "Thank you very much for your reply.",
                "comment": " Thank you very much for your positive reply and the score increase. We are very happy that you enjoy the revised paper and again want to thank you for the helpful feedback.\n\nIf you have any further hints on how to fix the write-up to make the paper more accessible, we would be glad to incorporate them for a possible final version. \n\nAgain, thank you so much for your time and the review, which greatly improved the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "bDvV09Rtq6",
                "writer": "author",
                "reply_to": "Omwh2wyqA-C",
                "title": "Thank you for your answer.",
                "comment": " Thank you very much for your positive reply and the score increase. We are very happy that you enjoy the revised paper and again want to thank you for the helpful feedback.\n\nWe will update the violins for a possible final version of the paper.\n\nWe use the section 4.3 weighted branch and reduce algorithm for the weighted case, and ReduMIS as described in section 4.1 of the user guide for unweighted graphs. Note that you could also input a \"weighted\" graphs with all weights 1 into the weighted branch and reduce algorithm, however, ReduMIS performs better for these cases, because it uses specialized reductions for the unweighted case.\n\nAgain, thank you so much for your review and time.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CfXZ14Fazr",
                "writer": "official_reviewer",
                "reply_to": "zTtRNWRJvYC",
                "title": "Good rebuttal",
                "comment": " I thank the authors for a thorough rebuttal.\n\nThe paper is now definitely above the acceptance threshold.\n\nTo be honest, I still think the paper undersells itself and is tricky to linearly navigate for anyone but most specialized readers. But the new version is a clear improvement and given the overall volume of the material and the overall complexity of the setup, one has to admit that a neat and crisp write-up is a Herculian task.\n\nMy updated score would be a clear 7 but this option is currently not available. Unless it opens up later in the review process, may the authors enjoy the rounded-up rating :).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Vskx9sgapOR",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_mk0HzdqY7i1",
                "title": "",
                "comment": "The paper conducts a thorough experimental evaluation of a line of work on \"deep-learning guided tree search in combinatorial optimization\". This line of work iterates on a relatively generic greedy-style algorithm for NP-Hard graph optimization problems; with a neural network -- trained on a training set of instances -- guiding the greedy decisions. The popularity of this approach is anchored in the premise of \"deep learning will help us solve NP-Hard problems.\n\nFindings of the experimental evaluation are the following:\n1) Some earlier work is not reproducible\n2) Traditional dedicated solvers are comparable or better; particularly on harder/larger problem instances\n3) Performance of data-driven methods barely changes when outputs of trained neural nets are replaced by random values (i.e. all the algorithmic power comes from explicit and traditional heuristical components) Strengths:\n\nThe empirical results of the paper have the potential to redirect (some) research in \"ML for combinatorial optimization\" to more fruitful directions in which it is indeed the learned components playing a vital role. I think this is fundamentally valuable.\n\nWeaknesses: \n\nI have multiple concerns regarding the presentation and methodology which I will list below. \n\n* Overall presentation\n\nFirst of all, Table 1 contains about 700 numbers none of which are legible on printed paper due to tiny font size (is it 3?). Given how strictly ML conferences force authors to respect font sizes, margins, and appropriate whitespace, I would almost think such a table merits a desk rejection. More importantly, even after zooming in, it is extremely hard for readers to navigate and draw meaningful conclusions from it. I suggest to a) decide on a subset of the most informative datasets (some are clearly too easy for all methods) and focus on that -- the rest can be in the appendix; b) similarly decide on the key subset of columns; surely the point isn't to compare all the heuristical components; c) This should win enough space to illustrate each of the main conclusions in a plot/table of its own (while displaying precisely the relevant information). d) Introducing a visual distinction between learning-based and classical method would be helpful.\n\nThe write-up assumes a lot of familiarity with tree-search methods and as a result, is uninviting even (!) to researchers from the wider MLxCombOpt community. I suggest the authors consider the following suggestions:\n\na) explaining tree-search basics in the main text\n\nb) explaining (some of) the heuristics in the main text -- it is important for the analysis of the results anyway.\n\nc) restructuring the related work (and introduction?) to categorize different ways ML is applied to NP-Hard combinatorial problems and explaining the place of \"tree-search\" in it (definitely add a discussion of learning to branch-and-bound as well as some details of (Nair 2021)). Categorizing by the degree of interaction between learned and algorithmic components is an option to consider. \n\nWith all of this in place, the paper would put itself in a position to frame its claims in much wider relevance (which I believe the claims deserve).\n\n* Confusion about the promise of DL for comb. opt.\n\nThe second paragraph of the introduction claims that learning-based approaches give a chance to learn to solve a *specific problem* (unlike Gurobi that doesn't make a distinction). I find this inaccurate. For one, there are obviously dedicated solvers to concrete problems; this feature isn't specific to DL. But mainly, the promise of DL, as I see it, is to learn solvers specialized to a *family of instances*. This view is well-motivated by industrial reality (e.g. Amazon's routing instances are almost the same every day) and it appears in the literature (see for example the introduction to (Khalil, 2017)).\n\nThe authors should be more explicit about evaluating the methods in a \"harder\" setup where the learned components are expected to generalize *across* families of instances (it seemed that training happens only on SATLIB). Evaluating the more favorable setting might also be interesting.\n\n* Treatment of Gurobi as a baseline\n\nThis paper has a unique chance to highlight a common issue with Gurobi comparisons. It has a lot of internal parameters that can be tuned to specific problems -- and in fact, they should, if other baselines are allowed to do it. Other than establishing the practice, it could further strengthen the points made by the authors.\n\n* Clarity about GPU utilization\n\nI assume that Gurobi and KaMIS do not utilize GPU whereas other methods do? Does rand use GPU? It seems it wouldn't need to. Given the massive computational advantage that 8 V100s give, the runtime comparisons should be very clear about it. Again, this clarity should go in favor of the main message: e.g. \"even with such computational advantage tree-search doesn't outperform traditional solvers\" or \"since tree search is comparable to rand; it is basically just a powerful GPU-friendly heuristical algorithm that is independent of any machine learning\". Despite the paper being purely experimental, its main point can have significant net-positive impact within the wider research area. Not only by casting doubt on the entire \"DL based tree-search\" but also by serving as a long-term warning -- claims about \"outperforming SOTA of classical combinatorial optimization\" can fall apart under proper experimental methodology.\n\n For this reason, I am comfortable disregarding the usual demands for technical novelty, beating benchmarks, or providing theory.\n\nHowever, in its current form, I do not believe the paper would realize its potential impact due to the issues listed above. I believe they require significant changes in the paper structure so I cannot, at this point, recommend acceptance.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "pQgoimM1NHL",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_mk0HzdqY7i1",
                "title": "",
                "comment": "This paper proposes a software package for generating data and training/evaluation some ML and non-ML approaches for the Maximum Independent Set (MIS) problem in its both its unweighted and weighted versions. It is shown that a highly-cited method that combines supervised learning using a graph neural network (GCN) with a (complete) tree search does not actually need the GCN if the MIS instance is preprocessed appropriately using existing non-ML based codes. On the other hand, a recent deep reinforcement learning (DRL) approach for the same problem is shown to actually use its GCN\u2019s predictions, obtaining better results overall. Last, non-ML solvers such as KaMIS and Gurobi are shown to find better solutions in a short amount of time for most instance datasets, putting into question the potential for ML-based approaches in general for MIS. **Strengths**\n\n- Clarity: The paper is generally very well-written, although the presentation of the results can be improved substantially.\n- Motivation: Li et al.\u2019s paper is being cited and compared against in tens of papers yearly, and so understanding its limitations and apparently fundamental issues is useful for the community.\n- Reproducibility: The authors are very systematic and transparent in how they generate datasets, implement the various methods, and evaluate them. I can see their code becoming widely used and built on in the development of ML-based methods for MIS.\n\n**Weaknesses**\n\n1. Presentation of experimental results: Tables 1 and 2 should be part of the paper\u2019s appendix for sure, but you really need to find better ways of presenting those thousands of statistics in the main text. As things currently stand, the reader needs to zoom-in to read the numbers; there are so many columns that it\u2019s hard to track which one corresponds to which method or what the trends are in terms of best method for a given dataset. Additionally, please label the paragraphs of section 3.1 so that the reader can immediately understand which aspect of the results you\u2019re discussing. As things stand, I\u2019ve had to decipher which columns I should query in the tables to see what you\u2019re saying in the paragraphs of 3.1.\n\n2. Statistical metrics: The average optimality ratios and running times in Tables 1 and 2 are certainly indicative of some trends. However, some box plots of the distributions of the optimality ratios/running times might shed more light into the robustness of the methods. For instance, the running time average may be biased by outliers whereas a box plot factors that in. Combining this suggestion with the one above, you could consider moving the tables to the appendix and replacing them in the main text with two box plots per datasets, one for optimality ratio and another for running time. This way, the reader can visually compare different methods without having to zoom-in and read hundreds of numbers. Since this paper\u2019s contributions are largely software/empirical results, you can also consider performing statistical testing for each pair of methods; see the two-sided Wilcoxon Signed Rank Test for example.\n\n3. Datasets: Please consider additional datasets which may be a bit more standard for MIS papers, e.g.:\n- http://vlsicad.eecs.umich.edu/BK/Slots/cache/www.nlsde.buaa.edu.cn/~kexu/benchmarks/graph-benchmarks.htm\n- http://lcs.ios.ac.cn/~caisw/graphs.html\n\nIn particular, the DIMACS implementation challenge graphs have been used in the KaMIS paper for example, among others. Also, this very recent dataset of large-scale instances may be of interest (even if only to evaluate and not train): https://arxiv.org/abs/2105.12623\n\n4. MIS heuristics and mathematical programming formulations: Please check Butenko\u2019s dissertation (Butenko, Sergiy. Maximum independent set and related problems, with applications. University of Florida, 2003.), particularly chapters 2-3 and the experimental results later on. The binary linear programming formulation you used with Gurobi is not the only one possible; there are quadratic formulations (see eq. (2.3) in Butenko) which may be easier to solve in practice than the linear one. Also please check the famous GRASP heuristic for MIS and consider implementing it: Feo, Thomas A., Mauricio GC Resende, and Stuart H. Smith. \"A greedy randomized adaptive search procedure for maximum independent set.\" Operations Research 42.5 (1994): 860-878.\n\n5. Where do your results leave us? I would\u2019ve expected you to identify datasets (existing or new) for which KaMIS and Gurobi underperform in some respect. Perhaps that is tricky; you\u2019ve tried many datasets (though there are more you could try as mentioned earlier) and the two solvers did very well. You might then want to consider other harder versions of MIS, for example the Generalized Independent Set Problem, see (Colombi, Marco, Renata Mansini, and Martin Savelsbergh. \"The generalized independent set problem: Polyhedral analysis and solution approaches.\" European Journal of Operational Research 260.1 (2017): 41-55.) and (Hosseinian, Seyedmohammadhossein, and Sergiy Butenko. \"Algorithms for the generalized independent set problem based on a quadratic optimization approach.\" Optimization Letters 13.6 (2019): 1211-1222.). In this variant, some edges may be \u201cpurchased\u201d and their endpoints may violate the independence requirement. This makes the problem much harder than MIS for integer programming solvers. Your paper should really push the community to advance the field.\n\n6. Solver parameter tuning as a baseline: Please consider adding a baseline in which KaMIS/Gurobi are \u201ctrained\u201d on the same datasets as the ML-based methods by tuning their parameters using off-the-shelf tools like SMAC (https://www.automl.org/automated-algorithm-design/algorithm-configuration/smac/). Such a baseline combines the best of both worlds in a sense: the stability and generality of these solvers with the potential benefits of leveraging the instance distribution.\n\nMinor comments:\n\n- page 8, \u201cOverall, we see that \u2026 cannot deal with some graphs\u201d, this is not true for Gurobi though, correct?  Overall, I like this paper and think it makes a solid contribution to the intersection between deep learning and combinatorial optimization. However, I think a paper that \u201cdebunks\u201d a highly-cited work should also establish convincing avenues for future research which are currently beyond the scope of (ML or non-ML) existing methods; I argue that this is missing at this stage. I would like to see the authors\u2019 responses to my questions and concerns before making a final decision, but am generally positive about this submission.",
                "rating": 8,
                "confidence": 5
            },
            {
                "review_id": "Omwh2wyqA-C",
                "writer": "official_reviewer",
                "reply_to": "LRf7KIxvtbq",
                "title": "Response to response",
                "comment": " Presentation and Metrics\n\nGreat! Consider adding median and lower/upper quartile lines to the violins, so that they provide strictly more information than their box plot counterparts. It would be good if the reader can immediately identify the median MIS size without having to visually estimate it by integrating the density.\n\nDatasets\n\nGreat!\n\nMIS heuristics\n\nUnderstood. Can you confirm that you use the branch and reduce algorithm in KaMIS as in section 4.3 of the user guide (http://algo2.iti.kit.edu/schulz/software_releases/kamis.pdf)? Seems like it from looking at your code, but it would be good to clarify to the reader.\n\nGeneralized Independent Set\n\nUnderstood.\n\nGurobi tuning\n\nWell done, thank you.\n\nOverall comment: this is an excellent rebuttal. I will increase my evaluation from 6 to 7, but no more. Your finding that LwD does better than the supervised methods is not necessarily new in that the LwD paper already makes that case. The \"debunking\" part is interesting in its own right, as is the contribution of a complete codebase for learning in MWIS with Tree Search, hence the increase.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4eI9-f919Ry",
                "writer": "author",
                "reply_to": "iclr_2022_mk0HzdqY7i1",
                "title": "Updated manuscript with revised structure and additional appendix sections",
                "comment": " We would like to thank all of the reviewers again for helping us improve the paper. We just uploaded a revised version of our paper, and answered your individual suggestions within your comments. In a nutshell, we\n\n- restructured the related work/background section, explaining the design space of deep learning for combinatorial optimization\n- improved the presentation of our data by just showing a smaller representative table in the main text and moving the large tables into the appendix\n- moved some explanation of the tree search algorithm into the main text\n- motivated DL for combinatorial optimization clearer (learning families of instances)\n- structured the evaluation texts better\n- added violin plots to analyze the robustness of the methods\n- added the DIMACS implementation challenge graphs and Amazon MWIS instances to our datasets\n- added a quadratic variant for formulating the MIS in Gurobi  to an additional evaluation section in the appendix\n- analyzed the impact of Gurobi parameter tuning using grbtune in an additional evaluation section in the appendix\n- fixed minor details\n\nThank you all again for your very valuable and insightful suggestions. Please let us know if you have additional questions or ideas for improvement.\n\nKind regards,\nAuthors\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zTtRNWRJvYC",
                "writer": "author",
                "reply_to": "Vskx9sgapOR",
                "title": "Response to Reviewer BEvM",
                "comment": " Thank you very much for your very thorough and insightful review. We would like to address your points individually. \n\n### Overall presentation\nWe received your criticism of the presentation and tried to improve it following your and the other reviewers\u2019 feedback. We\u2019ve moved the two tables into the appendix and included a small subset table for the main text. We\u2019ve further structured the evaluation text more clearly by labeling the paragraphs, and added some extra plots. \n\nRegarding the visual distinction, for the original table 1, this just were learning-based methods, and in the original table 2, we compared the full/default configurations of them to the classical solvers. Maybe this was a bit confusing, but as it just was 1-2 column(s) per solver in table 2, we hope it is sufficiently clear.\n\n> The write-up assumes a lot of familiarity with tree-search methods\n\nWe agree that the write-up assumes a lot of familiarity, and hence moved the textual description of the algorithm into the main text. Due to space constraints, the pseudo code has to stay within the Appendix. Unfortunately, the heuristics needed to stay in the appendix, to make some space for the plots that reviewer m13R suggested. We hope that using the text (and maybe a quick look into the pseudocode), the entry barrier for the paper is now a bit lower.\n\n### Related Work Restructuring\nThank you very much for that suggestion! Indeed, the related work discussion was not very strong until now. We\u2019ve integrated the related work section into section 2, and while doing that tried to discuss the different variants on how the algorithmic components and algorithms can interact and what the design space is (including branch-and-bound as well as Nair 2021). We hope that we now provide a much clearer introduction into the current state of the art. If you have any further suggestions for that, we are willing to include them for a final version.\n\n### Confusion about the promise of DL for comb. opt.\nWe agree we were confusing learning problem-specific solution structure and instance-specific solution structure. Thank you very much for pointing that out. We\u2019ve tried to make that more clear in the revised version, both in the introduction and the evaluation, where we discuss that the DGL/Intel-Treesearch neither learn the instance-specific structure (SATLIB) nor generalize beyond the training family.\n\n### Gurobi baseline tuning\nWhile we think it\u2019s not part of our main paper to evaluate parameter tuning for Gurobi, we agree it\u2019s an interesting experiment. Using Gurobi\u2019s internal optimization tool `grbtune`, we added another appendix section that discusses the impact of tuning Gurobi. For our use case, it shows that parameter tuning does not bring additional performance and Gurobi\u2019s defaults are sensible.\n\n### Clarity about GPU utilization\nIndeed, Gurobi and KaMIS are CPU-only, just as the randomized DGL-TreeSearch. Note that in the experiments we just used one GPU per experiment because even the multithreaded tree search was not able to even get close to maximum GPU utilization. Additionally, we ran a multithreaded tree search with 8 GPUs and see no impact on performance (which is explained by the non-GPU-boundedness of the tree search). We\u2019ve tried to make this more clear in the revised paper.\n\nThank you very much again for your feedback. We hope we addressed your points accordingly, and restructured the paper in a way that you now can comfortably make an acceptance decision. We are open for further feedback.\n\nKind regards,\nAuthors\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LRf7KIxvtbq",
                "writer": "author",
                "reply_to": "pQgoimM1NHL",
                "title": "Response to Reviewer m13R",
                "comment": " Thank you very much for your very thorough and insightful review. We would like to address your points individually. \n\n### Presentation and Metrics\nAs you\u2019ve suggested, we moved the long tables into the appendix and included a much smaller table for the main tree search results, focussing on two random graphs (ER, HRG), and a few real-world data sets. Furthermore, we\u2019ve followed your suggestion to label the paragraphs of the analysis. Thank you very much!\n\nWe\u2019ve added violin plots for some data sets comparing the different configurations, both for the tree searches and the other solvers. We think violin plots are more suitable than boxplots for our purposes here, and agree this is a valuable addition to the paper.\n\n### Datasets\nWe\u2019ve added the DIMACS implementation challenge to our evaluation. Thank you for making us aware. The other link and resources on the first link point to \"BHOSLIB\" which is equivalent to VC-BM in our paper. We added that name to the VC-BM subsection, to avoid future confusion.\nWe\u2019ve also included the Amazon MWIS dataset in our evaluation, at least on the smaller instances, as all solvers except Gurobi run into their limits on these graphs.\n\n### MIS heuristics\nAlthough we believe evaluating different variants for solving MIS with Gurobi is not the main focus of the paper, we added an additional appendix section that compares the linear formulation with the quadratic.\n\nRegarding the GRASP heuristic, we researched that algorithm and could not find any recent modern implementation (just a Fortran one), so we believe KaMIS to be the more widespread and modern heuristic solver. If you can point us to a modern (Python or C(++)) implementation, we would be willing to integrate it into the analysis, but we are not able to implement this algorithm within the timeframe for the discussion period in a way that would be fair, as we do not want to have some prototypical non-tested implementation that was built in a day. We hope you understand our decision here, and thank you very much for the pointer. In any case, we added GRASP to the related work section.\n\n### Generalized Independent Set\nWe agree that such a paper should push the community to advance the field and believe it would be a worthwhile addition to consider other versions of MIS (note that we already analyze the weighted variant, which makes the problem much harder, as vertices are not valued uniformly anymore). However, the related work we could find within the ML4CombOpt community does not target different MIS variants. With our paper, we wanted to make a first step towards a reproducible evaluation of the state of the art, and we are already at the limits of our conference submission. We agree that for a longer journal paper, it would be interesting to apply some solvers to other problems, but this would imply further engineering and we believe this to be out of scope for this paper. We added a discussion of this to the future work section and hope that our paper motivates the field to continue to push forward with a focus on rigorous empirical evaluation.\n\n### Gurobi tuning\nWhile we think it\u2019s not part of our main paper to evaluate parameter tuning for Gurobi, we agree it\u2019s an interesting experiment. Using `grbtune`, we added another appendix section that discusses the impact of tuning Gurobi. For our use case, it shows that parameter tuning does not bring additional performance and Gurobi\u2019s defaults are sensible.\n\nRegarding your minor comment, our formulation was a bit confusing as \u201calgorithmic solvers\u201d was referring to KaMIS only. We made it more clear in the revised version.\n\nAfter addressing the technical problems you mentioned as best as we could within the limited timeframe of the discussion period, the question still stands whether a \u201cdebunking\u201d paper should establish conving avenues for future research, as you write. With this paper, it is our goal to show how important rigorous empirical evaluation of newly proposed methods, especially in a rapidly evolving field such as machine learning, is. While we agree that for example other MIS variants would be interesting, we believe that taking the step back and check whether the assumption \u201cusing ML, we can efficiently solve MIS\u201d as Li et al. are often cited is correct, already advances the field a lot and lays the ground for future research on these methods. A formal analysis on why it actually is difficult to learn a solution structure here would be very insightful for future work. For such a \u201cdebunking\u201d benchmarking paper, we believe it\u2019s out of scope to additionally propose a novel method that then the next steps build up upon. But we understand your point and hope that our improvements, clarifications and additional evaluations will guide your tree search for an acceptance decision :-).\n\nThank you very much again, we really believe your feedback to have greatly improved our paper. Please let us know if this addresses your concerns for a final decision.\n\nKind regards,\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hJsdHeucxFv",
                "writer": "author",
                "reply_to": "EYQ5agHzK8r",
                "title": "Response to Reviewer oh3B",
                "comment": " Thank you very much for your thorough and insightful review. We would like to address your points individually.\n\n> All experiments are done on one problem type, MIS, while there is a lot of work on other graph-related problems such as TSP, VRP, etc.\n\nWe agree that for a final verdict on general combinatorial optimization, other graph problems like the ones that you mention should be evaluated. However, the weighted variant of MIS that we consider already makes the problem a lot harder, as vertices are not valued uniformly anymore. While Li et al. consider other problems, they are not vastly different at their core (e.g., Minimum Vertex Cover is just a flipped Maximum Independent Set), compared to TSP/VRP vs MIS. Due to time and space constraints we are not able to include detailed analyses of TSP/VRP architectures and we believe that it would be a bit out of scope for this paper; however, we made it more clear in the paper why we focus on MIS and added to the conclusion that future analyses on other problems, like TSP, should be made. Additionally, in Section 2, we additionally discus different architectures for ML-based combinatorial optimization, and discussed the TSP architecture by Kool et al. [2].\n\n> A study of a larger sample of deep learning solutions would be useful to support claims about the value of GNNs, neural-guided tree search, or reinforcement learning for combinatorial optimization.\n\nWe agree that adding more solvers to the benchmarking suite and analysis would provide more insights. However, the most important claims we make in the paper target specifically the Intel-TreeSearch/DGL-TreeSearch. It is not our goal to evaluate all approaches of DL4CompOpt that are out there; we believe this would be more fitting for a longer journal article, as space and scope of a conference submission are limited. Furthermore, we are not certain which solvers we would be missing. Notably, there is S2V-DQN [1] as a reinforcement learning solver, but Learning what to Defer has proven to be better than it in every aspect, so it is not included in our analysis due to space constraints and we have more space for the detailed tree search analysis.\n\n> The result that specialized solvers and even classical solvers are often better than deep learning solutions, especially on larger problems, has been reported for other computational problems\n\nThank you very much for making us aware of these papers. During the restructuring of the related work discussions that another reviewer suggested, we have added them to our related work and evaluation discussions and made it more clear that Ahn et al. in the LwD paper also analyze KaMIS.\n\n> While providing open-source benchmark suite, including implementations of several popular approaches, is important, as far as I understand this suite is primarily a collection of existing problems and randomly-generated graphs and does not introduce new benchmark datasets. I am not sure this is an important contribution of the work.\n\nOur suite provides a very comprehensive collection of standard real-world, random, and synthetically made hard graph instances on which the MIS problem is solved. We think the core contribution of a benchmarking suite is exactly that: Creating such a central collection of datasets, unifying the solvers -- which often are just available in a \u201cresearch-code\u201d status -- under a single interface, and providing the results. We do not think that introducing a new MIS benchmarking dataset is within the scope of our paper, especially because focusing on the existing ones enables comparison with other works. Often, there are papers published just for the release of a single new benchmark dataset. Based on another reviewer\u2019s feedback, we added the DIMACS and Amazon datasets to the paper.\n\nAgain, thank you for helping to improve the paper! We hoped we addressed the points that we believe to be within the scope of this conference submission, and made it clear why we think that targeting more problems like TSP does not fit into the scope of this submission. Furthermore, if there is any other solver you are missing that would provide additional insights, we would be grateful for a pointer. Please let us know if this addresses your concerns.\n\nKind regards,\nAuthors\n\n[1] Elias B. Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song.  Learning combinatorial optimization algorithms over graphs. In: Advances in Neural Information Processing Systems(NeurIPS), volume 30, 2017.\n\n[2] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems!  In: Proceedings of the 7th International Conference on Learning Representations (ICLR), 2019",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5IiqitgRpcT",
                "writer": "author",
                "reply_to": "7Iq3ynMr-7f",
                "title": "Response to Reviewer Qd1X",
                "comment": " Thank you very much for your very positive review and feedback. Regarding your point that one potential risk is that the guided tree search might actually be effective, we agree that there is no formal proof of correctness of the code, but as we observe similar behavior on the original implementation as well as our implementation from scratch, both with the provided weights as well as with newly trained weights, we believe that the probability for the error lying within a faulty implementation is rather small. \n\nOther than that, again, thank you very much for your feedback.\n\nKind regards,\nAuthors\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7Iq3ynMr-7f",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_mk0HzdqY7i1",
                "title": "",
                "comment": "The paper looks at an important problem: combinatorial optimization that arises in several real-world settings. Next, the paper focuses on a influential paper in the field (Li et al. 2018, 200+ citations so far) that presents a GNN approach and reports impressive performance numbers. Next, the paper presents investigates if these results are reproducible by (1) running the publicly available implementation after fixes; and (2) re-implementing the algorithm as described in the paper and documentation. None of these versions replicates the reported performance. The paper also presents a benchmark suite to make comparison of this task easier.    The primary strength of the paper is it is opening up an important direction towards scientific accuracy. Citation counts often work as a proxy for the perceived importance of a paper. A paper with 200+ citations in less than 3 years of publication is thus likely to be considered as highly influential in the field. Through re-looking at the claimed results, this paper makes a significant contribution towards a research philosophy that seeks to validate influential papers. \n\nThe benchmark data set is an important contribution and the detailed results presented in Table 2 can immensely benefit future performance comparisons. \n\nThe writing of the paper is excellent and the paper and the literature review is extensive.  \n\nThat said, one potential risk in this paper is what if the current implementation is wrong and indeed the guided tree search is effective?  Our field is producing papers at a fast rate. Reviewers can provide scientific checks and balances only up to a certain level. This type of rigorous effort can offer valuable information about highly influential papers to the scientific community. ",
                "rating": 8,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "an important contribution and a clear accept",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "HyxUIj09KX": {
        "paper_id": "iclr_2019_HyxUIj09KX",
        "paper_title": "S-System, Geometry, Learning, and Optimization: A Theory of Neural Networks",
        "paper_abstract": "We present a formal measure-theoretical theory of neural networks (NN) built on {\\it probability coupling theory}. Particularly, we present an algorithm framework, Hierarchical Measure Group and Approximate System (HMGAS), nicknamed S-System, of which NNs are special cases. In addition to many other results, the framework enables us to prove that 1) NNs implement {\\it renormalization group (RG)} using information geometry, which points out that the large scale property to renormalize is dual Bregman divergence and completes the analog between NNs and RG; 2) and under a set of {\\it realistic} boundedness and diversity conditions, for {\\it large size nonlinear deep} NNs with a class of losses, including the hinge loss, all local minima are global minima with zero loss errors, using random matrix theory.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "The paper is extremely difficult to read, even given that both reviewers have very strong math / theoretical background. Although it may potentially include interesting ideas, nothing in the work could not be understood by the ICLR audience. \n",
        "meta_review_title": "Paper unreadable ",
        "reviews": [
            {
                "review_id": "HJehf-1Lam",
                "reply_to": "iclr_2019_HyxUIj09KX",
                "title": "Interesting take on neural networks from a measure-theoretic viewpoint, however not easy to follow for a non-expert",
                "comment": "The paper provides a new framework \"S-System\" as a generalization of hierarchal models including neural networks. The paper shows an alternative way to derive the activation functions commonly used in practice in a principled way. It further shows that the landscape of the optimization problem of neural networks has nice properties in the setting where the number of input/hidden units tending to infinity and the neurons satisfy certain diversity conditions.\n\nOverall, the paper presents super interesting ideas that can potentially lead to a deeper understanding of the fundamentals of deep learning. However, for a general reader it is a hard-to-follow paper. Without a full understanding of the various domains this paper presents ideas from, it is hard to verify and fully understand the claims. I believe the paper would be better appreciated by an audience of a mathematical journal. As an alternative, I would encourage the readers to split the paper and possibly simplify the content by using a running example (more concrete than the one of MLP used) to explain the implications as well as assumptions.\n\nA clearer, more accessible presentation is necessary so that a non-expert can understand the paper's results. Thus, I vote to reject. \n",
                "rating": 4,
                "confidence": 2,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SygFw7xWpQ",
                "reply_to": "iclr_2019_HyxUIj09KX",
                "title": "has grand ideas but poorly written, cannot check for correctness",
                "comment": "The paper is extremely difficult to read. There are too many concepts introduced at once, casual comments mixed with semi-formal statements. The theorems sound interesting, the implications are grand and of interest to ICLR, but the proofs are impossible to follow. As such, I am not in a position to make a recommendation. \n\nI strongly recommend the authors to split the paper into multiple parts with clear-cut statements in each, with clear and detailed proofs, and submit to appropriate journals / conferences. \n",
                "rating": 4,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "is extremely difficult to read",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "nothing in the work",
                "Sentiment Expression": "could not be understood by the ICLR audience.",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "Ybx635VOYoM": {
        "paper_id": "iclr_2022_Ybx635VOYoM",
        "paper_title": "ContraQA: Question Answering under Contradicting Contexts",
        "paper_abstract": "With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over contradicting information to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the behavior of the QA model under contradicting contexts that are mixed with both real and fake information. We create the first large-scale dataset for this problem, namely ContraQA, which contains over 10K human-written and model-generated contradicting pairs of contexts. Experiments show that QA models are vulnerable under contradicting contexts brought by misinformation. To defend against such a threat, we build a misinformation-aware QA system as a counter-measure that integrates question answering and misinformation detection in a joint fashion. ",
        "paper_acceptance": "Reject",
        "meta_review": "This paper tackles a really interesting and realistic problem: how does contradictory (potentially) fake information affect QA systems? The authors try to approach this problem by building a new dataset, starting with the widely used SQuAD and adding contradictory information. This is quite interesting, but the rest of the paper does not follow through. Reviewers ask a critical question: how would you distinguish the information that is fake, as opposed to valid, truthful information? Without this distinction, how would you train a language model to detect the fakeness and answer the question using the valid information? Unfortunately, the authors did not reply to this critical question, so it is difficult to judge the validity and contributions of this paper. There are also serious ethical implications which are discussed in the ethics review.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "vC_KHAwCG8",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ybx635VOYoM",
                "title": "",
                "comment": "This paper addresses the problem of deriving the correct answer when contradicting examples are presented to the model. First, it introduces a dataset for the task. The dataset, ContraQA is built on SQuAD, and it contains contradicting contexts produced by humans and neural-models. Then, it presents a model for generating contradicting examples. The model, BART-FG, generates fake contexts by iteratively modifying and original input paragraph. The procedure starts by applying a constituency parsing to extract constituency spans from the input sentence. Then, it randomly masks some of these constituency spans, that are eventually fill by a BART model fine-tuned on Wikipedia dump. To study how QA models behave with contradicting examples, this work evaluates the performances in a scenario where the correct and the fake contexts are presented to the model. In order to make the QA system robust to fake contexts, it proposes a misinformation-aware framework that combines the score of the model with a trust score outputs by a fake detector, which is a transformer-based model trained to classify if a context is real or fake. The results show that under this setting, model performance decreases, and that the reduction can be mitigate by applying the fake detector model. Finally, it shows a comparison, between BART-FG and GPT-2, to identify which of the two can generate more impactful fake contexts for the QA model.  The purpose idea of the dataset is valuable, as well as the problem is proposing to address. However, there are some problems that do not allow me for acceptance. \n\nProblem statement. \n1)\tA considerable part of the paper is dedicated to the generation of the so-called contradicting examples. However, the way in which they are generated is in line with other works (cited in the paper as well) where the original text is perturbed by modifying entities. What makes the examples in ContraQA contradicting examples? A definition is indeed required. \n\n2)\tThe problem of misinformation has been explored in the literature, and many works treat the task as a fact checking problem. Because the literature regarding fact-checking is not considered at all in the paper, what makes the problem addressed in this work different from fact checking? If the ultimate goal is to design models that are robust to misinformation, fact-checking should be definitely considered, and models for fact-checking should be included in the evaluation. Refer to https://fever.ai/ for more details on the fact-checking literature and datasets.\n\n3)\tThe setup proposed in this paper is not simulating a real-word scenario, as instead claimed in the introduction. While fake contexts can be presented, when doing retrieval to retrieve relevant documents, not all of them will contain fake context. Thus, the distribution between real/fake presented in this work does not reflect a real scenario. \n\nContribution of the paper. \n1)\tFake detector. While in the introduction the paper claims of proposing a framework to detect against misinformation, in practice this solution is a simple combination of the score of the QA model with a fake/real classifier. This poses several limitations, including a little ability of the model to generalize when new contexts arrive. SQuAD suffers from a train test overlap problem [1]. These findings will apply to the classifier as well because it is trained on the same data. It is unclear how much the framework for contradictory QA is learning how to rely on an information, rather than memorizing because of text was already seen in train. How does the detector perform when evaluated on a set of question, paragraph and contradicting text not seen at all in train? Based on the paper, the fake contexts are perturbation of the original text, to which extend do you expect the detector working because of memoization, rather than reasoning over the text? \n2)\tBART-FG model. The main focus of the paper goes into this model. From the results, it remains unclear how the produced context can be contradictory, and thus what makes these contexts different from context generated for adversarial attack of QA models. This observation goes with my previous point regarding the definition of contradicting examples. \n\nEthical concerns.\nThe fact that the model is based on BART, and can be easily reproduced, is not a valid justification to release the trained model BERT-FG. Same goes with the point regarding the limited ability to generate disinformation. While every generative model can be used in theory to create fake contexts, this work describes a way of generative fake contexts, without providing (based on the results), any robust approach to mitigate the problem. \n\n[1] Lewis at al., Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets The purpose idea of the dataset is valuable, as well as the problem is proposing to address. However, there are some problems that do not allow me for acceptance. This includes problem statement unclarity, claims not well supported by results, and ethical concerns. ",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "EfSoh87ouv_",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ybx635VOYoM",
                "title": "",
                "comment": "The work investigates closed-domain Question Answering under contradicting contexts by introducing a new task ContraQA\u2014an extension of SQuAD1.1\u2014which includes contradicting contexts for the SQuAD articles, produced by both humans and neural models. The work also proposes a neural framework, BART-FG, to automatically generate these contradicting contexts by iteratively modifying constituency spans on the original context. Finally, the work gives a brief analysis on how SOTA QA systems perform on the new task, ContraQA, and proposes a misinformation detecting system which when unified with a Machine Reader performs significantly better than SOTA systems over ContraQA. **Strengths**:\n\n- To my belief, this is the first work that sets up contradicting contexts for closed-domain QA. It is very important for NLP and QA community to have more challenging evaluation benchmarks to understand how well models \"generalize\".\n- The work delineates an interesting study of how current QA systems perform when given contradicting contexts for reference, and also proposes a system that performs well at discriminating misinformation introduced by the ContraQA task.\n- The insight that human generated contradictions were \"stronger\" (were relatively more capable to fool QA systems) than Neural generations highlights the scope for developing better adversarial rewriting models.\n- The paper is fairly easy to follow\n\n**Weaknesses**:\n\nThe work leaves a few things to be desired:\n- The work only considers SQuAD which is demographically one of the most skewed datasets (Gor et. al. 2021). It would been interesting to see if this framework generalizes to NQ or newer and more challenging table-text based QA tasks like HybridQA (Chen et. al. 2020).\n- Though the error analysis is interesting, I find it a bit shallow. The work doesn't throw light on the discriminating features of human written and neurally generated contradicting contexts. Knowing this can help in corporate certain type of edits in neural models, or help humans write better contradicting contexts in some way.\n\n**Followup Questions:**\n\n1. Another interesting baseline to compare with ContraQA would be SQuAD + N Most similar (tf-idf) context passages (instead of random). Had that been tried ?\n2. The fourth guideline for fake context creation by humans: \"The modified paragraph should be fluent and look realistic, without commonsense errors.\", How was this objectively evaluated?\n3. From interpretability point of view, what helps the discriminator network filter off the contradicting contexts? Is it speculated to be just the Wikipedia pre-training? If so, how do you expect the miss information at source to play a role in confounding the QA systems?\n\n**Nits:**\n1. \u00a7 4. Contra-QA (w/ Detection): Do you mean $\\lambda$ instead of $\\mu$ ?\n Putting together all the strengths and weaknesses I believe the NLP and QA community will benefit from the insightful outcomes of this work. However at the same time, it does leave things to be desired. Nonetheless, I am inclining to accept this work.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "qhtcZVcC4RQ",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ybx635VOYoM",
                "title": "",
                "comment": "The authors studied how contradictory information affects the accuracy of QA systems.  They created a new dataset of ~10k from SQuAD with added contradictory paragraphs (context). The contradictory data was generated by two different ways. The authors employed Amazon Mechanical Turks raters to rewrite the original context. They also designed a BART-FG model to automatically produce such context by replacing spans with model generated value. \n\nThe authors also proposed a way to help QA systems avoid contradictory information. A RoBERTa-based model was used to classify a context into trustworthy or not, with an accuracy around 80%. The trustworthiness score is then used to weigh the final result, together with existing scores of quality/confidence.\n\nEvaluation was done with the new dataset. It showed that 1) adding contradictory information hurts QA performance badly, 2) the RoBERTa-based classifier can help regain some of the loss, but not all of them. The authors also measured the effectiveness of contradictory information creation, where human takes the top place by producing the least altered context with largest effect on final outcome.\n\nThe authors promised to share the dataset and the source code / weights of the proposed models. They further discussed the potential ethical impact of releasing the data, arguing that it's net beneficial.\n Strength. \n\n- It calls our attention to a very realistic problem, that misinformation could affect the QA systems, which a lot of people may blindly rely on.\n- Its pledge to share the data. It will help future studies in the same direction.\n- The description of data preparation and characteristics is very detailed and convincing.\n- The experiments are very detailed.\n- The paper itself is well-written and easy to follow.\n\nWeakness: Let me order them from most to least important.\n\n- It's not clear how we can tell the truth from the contradictory ones based on content only, and how the RoBERTa-based classifier did it (to 80%). For example, a common way to create the contradictory context is to replace one or more of the time/location or named entities (\"San Francisco\" -> \"Atlanta\"), or changing modifiers (\"all the time\" -> \"all the time except Sunday\"). It would be impossible for humans to tell, without strong background knowledge. Reviewer is very curious about what the classifier had learned, which is not discussed in the paper.\n\n- Leveraging the trustworthiness of the source, which seems like an apparent solution to Reviewer, is not discussed in the paper, at least not in the related work. For example, a quick search in Google Scholar led me to https://journals.sagepub.com/doi/abs/10.1177/0165551513478893.\n\n- How Gap Constituency Filling (GCF) Pre-Training produce a *contradictory* filler. The pre-training will result in the most likely filler. Avoiding the original text will make sure it's different, but not necessarily contradictory. How did the authors make sure it's contradictory? It's worth explaining.\n\n- In section 3.1 the requirement to human labelers, the authors insist that \"The worker should make at least M edits at different places, where M equals to one plus the number of sentences in the contexts\". Reviewer is not clear why we need this specific minimum\".\n  \n The paper proposed a novel study of QA system robustness under contradictory information. It's well written and provides a new dataset. However the discussion around how a classifier could tell truth from noise and why don't we leverage source authority are missing. Some smaller issues exist too. Reviewer would like to see the two bigger questions answered. \n",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "3H_24Hc0HmT",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ybx635VOYoM",
                "title": "",
                "comment": "This paper releases a new dataset with human and machine generated contradictory contexts for QA pairs from SQuAD 1.1. Amazon Mechanical Turk Workers are shown a paragraph and are asked to make edits such that it contradicts the original paragraph with respect to elements such as time, outcome, purpose, location, etc. In addition, the authors fine tune BART on a collection of masked constituent parses of Wikipedia sentences and it is then trained to fill the mask with an alternative phrase.  To automatically generate contradictory contexts, the authors use this fine tuned the BART model on the masked constituency parse of paragraph sentences. A dataset of 10,000 paragraphs from SQuAD  are transformed (once by mechanical Turk workers, and the rest by three different transformations by the BART Model). The paper presents experiments on this dataset for the task of QA -- specifically, machine reading comprehension. In one experiment, the QA system is first trained to predict which of the 5 (1 real + 4 contradictory) is correct. Then an off-the-shelf span based passage reader returns spans as answers. In the second experiment, the performance of QA models is compared on the unmodified SQuAD dataset as well as a version where a distracting passage is also added to context (by randomly choosing a different passage). The authors experiment using BERT, ROBERTA and SPAN-BERT and report a drop in performance in both experimental settings. In addition experiments reveal that models return worse performance on the subset of the data created by human workers ( perhaps unsurprising). Additional studies on the nature of edits have also been presented. \n\nOverall a well written and easy to read paper. However, I am not sure I am clear about the goals of the paper -- I elaborate further in the rest of the review. \n\n\n 1. What is the goal of the paper? If the goal is to present a dataset that is about misinformation -- it is not simply enough to create a contradictory passages. This is just another way of distracting a QA model but without having any reason for it to believe the information is \"fake\". In order for something to be \"fake\", there has to be some ground-truth known. The experiment called Contra-QA appears to be flawed given what it was supposed to check. How is a model expected to learn which passage is real? It has to be grounded in something that it can rely on for evidence isn't it? Would human beings know if something is fake unless there are also aware of what a \"trustworthy\" source says? Perhaps the authors can elaborate further (in case I have badly misunderstood the work). \n\n2. Similarly, if you add contradictory information to passages for a span-based QA model its no surprise it gets confused. Neither are those models trained to not respond in the presence of contradictory information nor are they being told which passage is real (the trust-score is truly not a trust-score -- it is just the output of a model that  frankly appears to be guessing because it has no way of knowing what is trustworthy!). \n\n3. What could perhaps have been interesting is to also see if a model could \"detect\" contradictions and says, that it should not answer. This is a model that you can easily train with this data and perhaps the only thing meaningful I can think of doing with this dataset without having any access to methods that tell the system what is \"real\". \n\nI found the methods for generating contradictory passages novel and interesting and could find more general use in other tasks related to dataset augmentation. That limited contribution, however is not enough to accept this paper in its current form.   The paper's experiments do not back up the claims of reasoning for misinformation. It is about reasoning with contradictory information and its no surprise QA models dont know what to answer when that happens. Why is this surprising? What is the goal of the work. \nI apologize if I have badly misunderstood the work and I'd encourage the authors to discuss these comments in the rebuttal. ",
                "rating": 3,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This paper",
                "Sentiment Expression": "tackles a really interesting and realistic problem",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The authors",
                "Sentiment Expression": "try to approach this problem by building a new dataset",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the rest of the paper",
                "Sentiment Expression": "does not follow through",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the validity and contributions of this paper",
                "Sentiment Expression": "it is difficult to judge",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "serious ethical implications",
                "Sentiment Expression": "are discussed",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "rkzjUoAcFX": {
        "paper_id": "iclr_2019_rkzjUoAcFX",
        "paper_title": "Sample Efficient Adaptive Text-to-Speech",
        "paper_abstract": "We present a meta-learning approach for adaptive text-to-speech (TTS) with few data. During training, we learn a multi-speaker model using a shared conditional WaveNet core and independent learned embeddings for each speaker. The aim of training is not to produce a neural network with fixed weights, which is then deployed as a TTS system. Instead, the aim is to produce a network that requires few data at deployment time to rapidly adapt to new speakers. We introduce and benchmark three strategies:\n      (i) learning the speaker embedding while keeping the WaveNet core fixed,\n      (ii) fine-tuning the entire architecture with stochastic gradient descent, and\n      (iii) predicting the speaker embedding with a trained neural network encoder.\n      The experiments show that these approaches are successful at adapting the multi-speaker neural network to new speakers, obtaining state-of-the-art results in both sample naturalness and voice similarity with merely a few minutes of audio data from new speakers.",
        "paper_acceptance": "accepted-poster-papers",
        "meta_review": "The paper benchmarks three strategies to adapt an existing TTS system (based on WaveNet) to new speakers.\n\nThe paper is clearly written. The models and adaptation strategies are not very novel, but still a scientific contribution. Overall, the experimental results are detailed and convincing. The rebuttals addressed some of the concerns.\n\nThis is a welcomed contribution to ICLR 2019.",
        "meta_review_title": "Limited novelty but sound experimental work",
        "reviews": [
            {
                "review_id": "B1gXKtGspX",
                "reply_to": "ByxoJb0w27",
                "title": "Thank you for your comments. A revision is submitted following the feedback from all reviewers.",
                "comment": "Thanks for your insightful comments. As discussed in the related work section, our proposed approaches are closely related to the methods in Arik et al. (2018) at a high level. However, this is a situation where the details seem to matter significantly. For instance, we find that the detail of applying few-shot adaptation to the WaveNet core results in better sample quality. \n\nWe have submitted a revision incorporating all reviewers\u2019 comments. Please see our response to your other questions below and let us know if any explanation is unclear.\n\n1) Details for acquiring linguistic features:\nWe did not provide a fine-grained explanation of the linguistic features originally because we used the same pipeline as van den Oord et al., (2016). We have now added a new section in the supplementary material of the revised paper (Section A) to explain how to extract the linguistic features and fundamental frequency at training and adaptation phases, and how to predict both at the inference phase.\n\n2) Speaker-dependent linguistic features\nThe linguistic features and F0 are provided as inputs during training, but are not available at inference time. Our experimental evaluation shows that even though the linguistic features are predicted by a standard speaker model, the voice similarity is high. This is because the speaker identity is closely dependent to the vocal tract properties, which are modelled by WaveNet. Nevertheless, we strongly agree that the linguistic features also contribute to the speaker identity in terms of prosody, accent, and so on. For this reason, we are considering future work on using our few-shot method for adapting F0 and linguistic features in the hope of improving performance.\n\n3) Superfluous naming\nYou raise a valid point. We however felt that those names captured the difference between the two approaches clearly, and fitted well with the overall presentation. If you think they reduce readability, please let us know and we\u2019ll consider alternatives.\n\n4) Method abbreviations without explanation\nSEA stands for our \u201csample efficient adaptive\u201d method as in the title. We have clarified this in the revision.\n\n5) Early stopping criterion\nThe early stopping criterion uses the validation loss on the hold-out data. \n\n6) Not comparable MOS in Table 1.\nYou\u2019re absolutely right. As discussed in Section 5.2, comparing all other methods in the exact same setting is challenging as it would require that we retrain all models on the same dataset with hyper-parameter sweeps. We did our best to make the setups as close as possible when reporting our MOS results and results from other references. Please note that the same five-point Likert Scale is shared by all evaluations, and the group of human raters is shared with Jia et al. (2018).\n\n7) Nachmani et al. (2018) and Arik et al. (2018) have also used speaker verification model as an objective evaluation.\nThis is true indeed. We even considered using this to compare approaches. However, any comparison with a speaker verification model is highly dependent on the model architecture, the training set, and the set of test speakers (both the identity and the size of the test set). For this reason, we decided to not include a naive comparison, even though we did find that we get the lowest equal error rate with SEA-ALL.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJeX8tzoT7",
                "reply_to": "Syxn5T5ch7",
                "title": "Thank you for your comments. A revision is submitted following the feedback from all reviewers.",
                "comment": "Thanks a lot for your terrific summary of our paper. We\u2019ve submitted a revision following the feedback from all reviewers. Please see our response to your questions below:\n\n- Comparison between SPSS based frontend and seq2seq models:\nThere are pros and cons to both approaches. On the one hand, SPSS+WaveNet obtains a natural decomposition of prosody (pace, intonation, etc) and vocal tract properties (more relevant to speaker identity) of a voice, something that is still difficult to do with seq2seq [1]. On the other hand, seq2seq models overcome the need of hand-crafted linguistic features and could be easily applied to different languages. \n\nWe compare the performance of our model with seq2seq models in terms of sample naturalness and voice similarity in Tables 1 and 2. However, as explained in our paper, we report the numbers of the closest experimental setup. Without access to the original code and all the dataset-specific hyper-parameters it is difficult to reproduce other works exactly.\n\nListening to the generated samples on our demo webpage is another way to qualitatively compare the approaches.\n\n- Linguistic features:\nPlease refer to our response to question 1 of Reviewer 3 for details. In short, we have added an additional appendix to the paper elaborating on the linguistic features.\n\n- Representations:\nCorrect, the output is an audio waveform.\n\nThanks for pointing out the typos.\n\nReferences:\n[1]: Skerry-Ryan, R. J., et al. \"Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron.\" arXiv preprint arXiv:1803.09047 (2018).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklAlKMipQ",
                "reply_to": "rJgr4cv03X",
                "title": "Thank you for your comments. A revision is submitted following the feedback from all reviewers.",
                "comment": "Thank you for your supportive comments. We\u2019ve submitted a revision following the feedback from all reviewers. Please see our response to your questions below:\n\na) Conditioning inputs: \nSpeaker identity is part of h. Because s is used to select the speaker-specific embedding parameter, we include it in the second equation as a subscript in e_s. We have added a note to clarify this in the revision.\n\nb) Citation to Fast speaker adaptation for speech recognition: \nThanks for bringing this paper to our attention. We\u2019ve included it in the revision.\n\nc) Synthesized speech outperforms real speech in speaker verification task: \nThis is a good point. Our experiments suggest that synthesized samples from SEA-ALL on LibriSpeech deviate less from the *centroid* of real utterances than the real samples. Ideally, when comparing samples of different generative models to real utterances we would like these to match in distribution and not only in terms of scalar point estimates. In our practical setting, we would like the generated and real samples to have overlapping tSNE projections as in Figure 3, and to have similar DET curves as in Figure 4. We expanded on this point in the revised version of the paper.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgr4cv03X",
                "reply_to": "iclr_2019_rkzjUoAcFX",
                "title": "nice paper, publish",
                "comment": "This paper presents an approach to customize or adapt a text-to-speech synthesis system to a new speaker, given relatively small amount of data from that speaker.  It is a very well written paper with rather strong results indicating high quality, naturalness, and similarity with real speech from a speaker can be achieved with the authors' proposed approach.  I think the paper should be accepted for presentation at the conference.\n\nFew comments:\na) In second equation in Section 2 authors state speaker identity \u201cs\u201d is part of conditioning inputs \u201ch\u201d but it is not shown in the Equation where \u201ch\u201d is replaced with \u201cl, f_0\u201d\nb) In related work, I think the speaker code work of Abdel-Hamid et al., e.g. Ossama Abdel-Hamid, Hui Jiang, \u201cFast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code,\u201d ICASSP 2013 is worth citing.\nc) The result that synthesized speech performs better than real speech in speaker verification task is interesting.  To me this points to a potential weakness in the verification methodology.  Please comment if this may be the case.",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Syxn5T5ch7",
                "reply_to": "iclr_2019_rkzjUoAcFX",
                "title": "Wavenet solution to speaker adaptation - accept ",
                "comment": "This paper proposes  an adaptation technique for TTS using wavenet as the speech backend, with the adaptation carried out on small data. The work is extremely significant in that speech data is hard to produce  (we need many hours of speaker data), and techniques to adapt (transfer learning?) data from large networks would be quite valuable. The main idea is that we train a network containing a large amount of data, and (assuming that we have a trained model), we adapt this network to the task of generating speech from text for a much smaller dataset. \n\nIn general (insofar as we can use that term), one trains such a network using <text/speech> pairs,  with speaker conditioning as added input so as to produce voice from a given speaker. The input text is converted to linguistic features in the \u2018front end\u2019, which is then injected with voice features to be synthesized into a voice output in the backend. More recent efforts in speech modeling have used RNN or wavenet based systems to carry out these transformations in the front/backends. The present work seems to use an SPSS technique (Zen et al 2016) to generate the linguistic features, while the task of converting to voice is carried out by a Wavenet. \n\nThe work is quite (conceptually) similar to \"Neural voice cloning with a few samples\" (Arik et al, https://arxiv.org/abs/1802.06006) in proposing techniques for few shot adaptation described below but with the significant difference that the latter used autoregressive DNNs (loosely speaking, seq2seq a la Tacotron) for the task in both the front and back ends, while in the current work, the linguistic features are computed with SPSS as in Zel et al (2016).\n\nThe paper proposes three quite related techniques for adaptation as shown clearly in Figure 2 of the paper. These techniques are again \u2018roughly\u2019 analogous to those described in the Baidu work \u201cNeural Voice Cloning with a few samples\u201d, with the difference in front and backend setups noted in the previous paragraph.\n\nWe take as text as input, and convert them to a representation for linguistic features as described in Zen et al (2016). To this, we now add the fundamental frequency F_0 for the sample voice. The key piece needed is the speaker embeddings (a vector), which is to be obtained by training. In addition to all this, we also have available the weights of a trained wavenet network (probably quite large) trained on many speakers, which we will modify (or not) using the strategies outlined for the few data dataset.\n\nSEA-EMB - Train embeddings, but not the network. We expect this to be \u2018fast\u2019, but not particularly accurate. \nSEA-ALL - Train embeddings, and network. This would be a much more accurate, if slower task. The authors note that since we train a very large network in this case, it could be prone to overfitting. They employ early stopping (as a practitioner, I would make note of the issue) with 10 % of the dataset being held out. Additional ideas such as initializing the emeddings - possibly with those that SEA-EMB calculates - are also stated to be useful.\nSEA-ENC - In this third version, they predict speaker embeddings from the trained larger network (the recipe is provided in the appendix). This task of predicting speaker embeddings is one of training a classifier.\n\n\nResults\nThe paper presents evaluations conducted with subjective, MOS based enrolment and with an evaluation metric from TI-SV d-vectors. Comparisons are made for all three models with human evaluated MOS scores, and it is seen that SEA-ALL outperforms the other two models, while performance in SEA-EMB depends on the amount of data used. Nevertheless, humans are still able to detect the difference between synthetic voices and real samples. \n\nThe TI-SV evaluations from Wan et al show t-SNE embeddings of \u2018clusters\u2019 of d-vectors for human and synthetic voices, where it is seen that inter-cluster distance (i.e. between different speakers) is high, showing that the model is able to discern speakers, and the intra-cluster distance (i.e. between real and synthetic voices) is low, showing that synthetic voices are \u2018similar\u2019 to real voices. In addition, three other measures - cosine similarity, and statistical measures for detection error trade off, ROC curves and cosine similarity measures are also presented, which show that that the adaptation models perform quite well. \n\n\nClarifications and comments:\n\nHave there been efforts to compare this model (with the SPSS based frontend) with seq2seq (Bahdanau/transformer) DNN based systems as in \u201cNeural Voice cloning with few samples\u201d?. How do they compare (is it even a valid comparison?)?\n\nI think the model for computing linguistic features could be elaborated upon further. \n\nRepresentations: I assume that the output audio representation is an audio waveform\n\nTypo 1 (minor): The reference  for \u201cBornschein et al\u201d in section 4 \u201cRelated work\u201d\n\u201cVariable inference for memory addressing\u201d. \nCorrection \u201cVariational memory addressing in generative models\u201d\n\nTypo 2 (minor): Figure 6: Lower curve indicate that the verification system is having a harder time distinguishing real from generated samples. \nCorrection (minor): Lower curve \u201cindicates\u201d ...\n\nSummary\n-------------\nIn summary, I am in favor of accepting this paper as it proposes a solution to adapt a trained network to one with has limited number of samples. A big issue in speech modeling is that datasets are tiny, and it is difficult to obtain good quality data at reasonable cost. It would be extremely useful to have a trained network that we can adapt for our own experiments. The related paper by Arik et al (Neural Voice cloning with a few samples) also operates with similar strategic aims, but uses a a different methodology using attention based DNNs. The paper under review should be a good addition to the toolbox of few shot adaptation/transfer learning for speech with much potential for practical use. ",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ByxoJb0w27",
                "reply_to": "iclr_2019_rkzjUoAcFX",
                "title": "A good work; limited novelty but solid results",
                "comment": "This paper investigates speaker adaption with a few samples based on an existing (pre-trained) multi-speaker TTS system. The three approaches in this paper are almost the same as the voice cloning work in Arik et al. (2018). However, it is still very beneficial to demonstrate these approaches for linguistic feature conditioned WaveNet.\n\nDetailed comments:\n\n1) This manuscript is not self-contained, as it omits the important details for acquiring linguistic features (e.g., phoneme duration model) and fundamental frequency (F0) at training and test time. The only information is that it uses existing model (Zen et al., 2016) to predict linguistic features and F0. What type of linguistic features are used in this work? Is the existing model (Zen et al., 2016) trained on the same training set as WaveNet model?\n\n2) It seems the only speaker-dependent part of the system is the embedding table for WaveNet. Actually, both linguistic features (e.g., phoneme duration) and fundamental frequency sequence are highly speaker-dependent. The authors normalize F0 to make it as speaker-independent as possible. What about the speaker-dependent linguistic features? Why not keep them as speaker dependent, and do speaker-adaption for the new speaker at inference?\n\n3) In my opinion, it\u2019s a bit superfluous to name fine tuning as non-parametric few-shot adaption, and auxiliary network (speaker encoding) as parametric few-short adaption. Both ideas are quite natural as in Arik et al. (2018).\n\n4) The abbreviations SEA-ALL, SEA-EMB and SEA-ENC are appeared without explanation. \n\n5) It would be better to provide more details about early termination criterion in Section 3.1. Is it simply the validation loss?\n\n6) In Table 1, the MOS from Arik et al. (2018) and Jia et al. (2018) are not comparable. The experimental settings are different. Perhaps more importantly, these MOS evaluations are done by different group of people.\n\n7) In Section 5.3, Nachmani et al. (2018) and Arik et al. (2018) have also used speaker verification model as an objective evaluation.\n\nOverall, this is a good work with limited novelty but solid results. However, it can be improved in many ways as detailed  in previous comments. I would like to raise my rating if these comments can be addressed properly.",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "is clearly written",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The models and adaptation strategies",
                "Sentiment Expression": "are not very novel, but still a scientific contribution",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the experimental results",
                "Sentiment Expression": "are detailed and convincing",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "This",
                "Sentiment Expression": "is a welcomed contribution",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "dgd4EJqsbW5": {
        "paper_id": "iclr_2021_dgd4EJqsbW5",
        "paper_title": "Control-Aware Representations for Model-based Reinforcement Learning",
        "paper_abstract": "A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations.   Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space.  Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control.  In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space.We call this model control-aware representation learning(CARL). We derive a loss function and three implementations for CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g., iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance.  Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.",
        "paper_acceptance": "poster-presentations",
        "meta_review": "This paper addresses the question of RL in high-dimensional spaces by learning lower-dimensional representations for control purposes. The work contains both theoretical and empirical results that shows the promise of the proposed approach.\n\nWhile the reviewers had initial concerns, including with a problem in a proof and questions around the contributions, after robust responses and discussions this paper is now in good shape.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "o67CDKifq89",
                "reply_to": "iclr_2021_dgd4EJqsbW5",
                "title": "The algorithm is promising but the theoretical foundation is inaccurate",
                "comment": "This paper aims to address an important question in reinforcement learning: policy learning from high-dimensional sensory observations. The authors propose an algorithm for Learning Controllable Embedding (LCE) based on policy iteration in the latent space. The authors provide a theorem to show how the policy performance in latent-space policy improvement depends on the learned representation and develop three algorithmic variations that attempt to maximize the theoretical lower bounds. In the experiments, the proposed algorithm CARL shows improved performance when compared with other LCE baseline algorithms. \n\nWhile I'm not particularly familiar with the field of LCE, I think the idea of learning a representation that is suitable for policy improvement is an interesting idea. The readability of this paper is also pretty good, which can be difficult to get right because the of the correspondence between the original space and the latent space. Overall the paper is easy to follow. \n\nWhile I do think Algorithm 1 is reasonable, I found its theoretical foundation, namely Theorem 1, is incorrect. In the proof of Theorem 7 on p15 in the appendix, I do not think the implication T^2 VE(x) < T VE(x) + \\gamma Delta(x) for all x, would hold. Because Bellman operator contracts in the L-inf norm, a basic inequality would rather take a form of  T^2 VE(x) < T VE(x) + \\gamma sup_y Delta(y). In addition to this, another minor error happens in the first equation on pg 16, where I believe the correct right hand side would be 1/(1-gamma) sup_y Delta(y), without the gamma dependency.\n\nHowever, a bound that depends on L-inf norm would be quite bad for Theorem 1, and current data collection process in Alg 1 is not sufficient for minimizing it. I think it might be possible not using an L-inf bound but using an expected error based on the policy's rollout distribution. However, this change would largely change the theoretical results, and perhaps the motivation or details of the algorithm design. Therefore, I do not think the paper is ready for acceptance at the current stage without a large revision. If the authors can address this question properly, I would raise my score.\n\nBeyond the flaw in the theory, there are some parts which can benefit from some clarification: \n1. In the offline CARL, how does the algorithm address the issue of out of distribution error due to using a batch dataset? \n2. The authors argue that the loss here is different from PCC many times in the paper, but they never explain whether the choice here is better (or in which way). \n3. In line 4 of Alg 1, how do we ensure such pi would exist?\n4. What is the definition of \"compatible reward function\" in the last paragraph on p4?\n5. For completeness of presentation, please include the definition of curvature loss.\n\n\n\n\n\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "NfITCzOkMzP",
                "reply_to": "QmfA7FZjVsT",
                "title": "Thanks for your feedback, more response to your questions ",
                "comment": "We would like to thank the reviewer for quick reaction to our responses, asking questions, providing feedback, and being open to conversation. We hope we have been able to address the issues raised by the reviewer and to clarify their questions. Here are a few points in response to the reviewer\u2019s latest post. Hope they further clarify different aspects of our work. \n\n\n\u201csimilarties between CARL and PCC\u2019s loss functions\u201d\n\nWe would like to emphasize there is similarity between the loss functions of PCC and offline CARL. The loss function of online CARL is significantly different because it depends on the current policy and not on state-action pairs. \n\n\n\u201cnumber of hyper-parameters\u201d\n\nCARL has 5 hyper-parameters, but we only fine-tune 2 of them (prediction and consistency), because our ablation studies in Appendix F suggest that these two play the most important roles in the performance of the algorithm. All other algorithms in this area (E2C, PCC, Dreamer) have the same number of hyper-parameters (the weights of the different terms in their loss function). They all have to tune 2-3 hyper-parameters. So, in that sense, we do not see much difference between CARL and its counterparts. \n\n\n\u201cprimary contribution\u201d\n\nWe see our primary contribution as both algorithmic and empirical. Our algorithmic contribution includes deriving a loss function from the principles of dynamic programming for learning representations that are suitable for a large class of control algorithms, namely approximate policy iteration algorithms, and three algorithms that use different forms of this loss function with SAC in offline and online settings. Our empirical contribution includes a number of ablation studies, comparing our algorithms with those (PCC, E2C, RCE) that have been derived for the setting considered in the paper and on the same domains used in these papers, and comparing them with Dreamer.\n\n\n\u201cexperiment with DM control suit\u201d\n\nThe number of images needed to be stacked for the tasks in the DM control suite is larger than those in our experiments because of the colored image, where each input observation for these tasks should be multiplied by 3. This is why we believe we need to combine CARL with more powerful encoders to handle these tasks. In any case, we used the black and white version of 4 tasks (Pendulum, Acrobot, Cart-Pole, and Cart-k-Pole) in the DM control suite in our experiments. We will try our best to convert Hopper and Walker to black-and-white and include the results of applying CARL to these tasks in the final version of the paper. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hyW1G_q-8aI",
                "reply_to": "iclr_2021_dgd4EJqsbW5",
                "title": "Official Blind Review #2",
                "comment": "This paper proposes a new representation learning + RL algorithm called CARL, with a specific objective for learning a latent representation and dynamics model coupled with SAC policy learning in the latent space. Experiments on a few domains show CARL outperforming previous algorithms such as DREAMER and PCC.\n\nPros:\n\n+ The key points of the paper are relatively well organized and motivated properly.\n+ The experimental results succinctly demonstrate the promise of the proposed approach.\n\nCons:\n\n- It is difficult to follow important details about the operation of this relatively complicated method.\n- The experimental results are not sufficient for this largely empirical work.\n\nWith these pros and cons in mind, I am recommending a weak reject. See below for additional detailed comments.\n\nEDIT: After discussion, I have increased my score and am recommending weak accept. See the discussion with the authors for details.\n\n\nQuality\n---\n\nThe paper studies an important problem, proposes a novel solution, and has promising experimental results. However, the main drawback in terms of the quality of the work is that the results are not complete enough. For work that is empirically driven, I do not view the current results as sufficient for publication.\n\nIn particular, DREAMER appears to be competitive with CARL on a few domains. But DREAMER was also evaluated much more broadly across many tasks from the DeepMind control suite, indicating a level of robustness and performance that is, at best, hinted at in this work for CARL. A wider suite of experiments, for example using the same tasks as DREAMER, would go a long way in better shaping the reader's understanding of the proposed method.\n\nClarity\n---\n\nAs mentioned, the main points of the paper are presented well. The problem is properly motivated, and a central theorem gives rise to the proposed representation learning method. I did not check the proof for this theorem, but it appears sensible.\n\nHowever, the finer points in the paper, which are also very important, are difficult to follow. For example, what is \"model-based SAC\"? There does not appear to be a proper explanation or citation for this. Is the learned dynamics model F used in some way to learn the Q-function? Is this novel, or is it from prior work?\n\nConsidering the proposition that replacing other control algorithms in the latent space with model-based SAC is important for the overall performance improvement, a description of model-based SAC is important. Furthermore, an ablation study would be helpful in terms of understanding the relative importance of this change vs the proposed representation learning approach, which seems to be the novel part.\n\nSome other minor concerns about the methodological sections: there are many hyperparameters and not much guidance as to how to pick these; more discussion of why there are different versions of CARL and what are their respective strengths and weaknesses would be useful, especially for VCARL; I personally found the last paragraph of the VCARL description almost impossible to follow.\n\nOriginality\n---\n\nTo the best of my knowledge, the representation learning algorithm itself is novel. Perhaps a related work that is overlooked is https://arxiv.org/abs/1907.00953, which apparently has been accepted to NeurIPS 2020 but has been out for some time. At a high level, this work also incorporates representation learning into SAC, though the underlying details are different. Still, this approach seems actually more closely related than some of the current citations and comparisons, e.g., SOLAR and DREAMER. At least a citation seems to be in order, and preferably a comparison. Indeed, this prior work also carries out a more comprehensive evaluation on more tasks than the current work.\n\nSignificance\n---\n\nThis work has the potential to be significant, as many researchers and practitioners are currently interested in how to make deep RL more efficient and performative, in particular in visual settings. However, without a more comprehensive evaluation, it is difficult to judge for sure.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "QmfA7FZjVsT",
                "reply_to": "NEcy-dLyxOr",
                "title": "Thanks, this slightly improves my overall assessment",
                "comment": "Thanks for providing additional comments, I believe this clarifies some of my confusion. I now better understand the significance of the theoretical bits, which result in a similar loss function as PCC but is motivated and derived from a different and more general perspective. I still believe that the link between the theorem and the actual loss function is weak, as evidenced by the number of hyperparameters that have to be searched over. But this is acceptable to me.\n\nThe way I (now) view it, the resulting loss function is primarily interesting and significant because it allows for seemingly easier integration of general model based policy iteration algorithms. But we are only interested in this because policy iteration algorithms seem like they should work better in practice on a larger variety of tasks. Therefore, the primary contribution of this paper is still empirical, and the empirical evaluation is still lacking.\n\nI still do not follow the authors' justification about not being able to run additional experiments. The authors clarify that they are indeed stacking images to produce Markovian observations. If this is the case, it should be even simpler to try more tasks such as DM control suite. Why would the number of images that need to be stacked be larger for other tasks? If the actions are torque control, then velocity is the missing information that is not present in one image, but can be inferred from a stack of two images. In practice, stacking just a few images should be enough for every task visualized in the [DREAMER paper](https://arxiv.org/pdf/1912.01603.pdf) Fig 2.\n\nAnyway, I'm happy to discuss further with the authors if they wish, but that is up to them, as I am increasing my overall score from weak reject to weak accept.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9g6jaCLJag0",
                "reply_to": "HzCWnu72cQJ",
                "title": "Thanks for the additional response",
                "comment": "We thank the reviewer for reading our response and providing more comments. Here we try to further clarify some of the questions raised by the reviewer. \n\n\u201cpaper\u2019s main contributions\u201d \n\nWe view the main contribution of the paper as deriving a loss function for representation learning from the principles of dynamic programming that is suitable to be used along with any (approximate) policy iteration algorithm. This is also reflected in the title of the paper, \u201ccontrol-aware representations ...\u201d. In the supplementary materials (Appendix) of the original submission, we support this with offline and online implementations of the algorithm; ablation studies that show the effect of each term in the loss function (Appendix F2), the effect of policy distillation (Appendix F1), the effect of the control algorithm (Appendix F3); and a series of experiments that compare our algorithms with those that have been derived for the setting considered in the paper (E2C, PCC, SOLAR) on the domains used in these papers. The main algorithm of our paper is online CARL that extends E2C and PCC to an interactive (RL) setting, and our main theoretical result studies the effect of data generation distribution in LCE-style representation learning and control, which is novel and interesting as pointed out by Reviewers 1 and 3. Offline CARL is only a special case to better compare our framework with the previous algorithms like PCC. We also derived the V-CARL to see how much more we can bring the control algorithm (particularly the value function) to the representation learning process. In comparisons, PCC's loss for representation learning is derived solely based on studying an offline stochastic control problem, without considering any effects of data generation.  \n\n\u201cCARL Loss Function\u201d\n\nThe CARL\u2019s loss function in the offline setting (offline CARL) has close connections to that of PCC, although they have been derived from different perspectives (one derived to be used with policy iteration style algorithms and one for locally linear control algorithms). The similarity between the loss functions of offline CARL and PCC is because they both have the prediction and consistency terms that according to our ablation studies (Appendix F2) are the most influential terms in CARL\u2019s loss function. However, it is important to note that the loss function of online CARL has a significant difference with those of offline CARL and PCC, because it depends on the current policy (see Theorem 1) and not on state-action pairs. As discussed in Section 3 and also in our response to Reviewer 3, PCC and offline CARL have been designed for an offline setting (i.e., one-shot representation learning and control), and thus, the terms in their loss function are independent of a particular policy and are defined for state-action pairs (see the description of offline CARL in Section 4). On the other hand, online CARL has been designed for an online setting (interleaving representation learning and control), and thus, all the terms in its loss function depend on the current policy (see Theorem 1 and the description of online CARL in Section 4). This is a significant difference between the loss function of CARL and those of the previous algorithms that should not be ignored. \n\n\n\u201cAblation Studies\u201d\n\nIn the original main paper (Page 7, in the last line of the paragraph starting with \"General Results\"), we already pointed to the readers that our ablation studies are in Appendix F. We do not exactly know what kind of ablation studies the reviewer would like to see, in any case we should have referred that to the reviewer again but forgot to do so in our original response. In Appendix F1, we studied the effect of policy distillation (Line 4 of the Algorithm). We explained this in details in response to a question by Reviewer 3 about Line 4 of the algorithm. In Appendix F2, we studied the effect of each term in the CARL\u2019s loss function and the results show the importance of the prediction and consistency terms, which is not that surprising. Finally in Appendix F3, we compare offline CARL and PCC. Because of the similarities between their loss functions, and given the fact that the prediction and consistency terms that they have in common happen to be the most influential terms in the loss function of offline CARL (ablation study of Appendix F2), we believe this comparison shows the effect of the control algorithms. The result suggests that SAC (an approximate policy iteration algorithm) performs better than iLQR (a locally linear controller) in the problems studied in the paper. Although other ablation studies can be done, ours focused on the loss function, control algorithm, and the effect of distillation, which we believe is a reasonable amount of ablation studies for a conference paper.   ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "NEcy-dLyxOr",
                "reply_to": "HzCWnu72cQJ",
                "title": "Thanks for the additional response (Cont'd)",
                "comment": "\u201cmore experiments\u201d\n\nWe would like to emphasize that this is not a pure empirical paper (see our discussion about the main contributions). Moreover, we would like to reiterate our argument that extending CARL to more involved problems (e.g., those in the DM suite) requires using more powerful encoders. We believe this extension is doable but requires a separate, dedicated work to extend our theoretical results and loss function to that case, which is an interesting future work. The reviewer refers to (i) Dreamer and SLAC and (ii) stacking frames to turn the problem Markovian. Dreamer and SLAC use sequence-to-sequence encoding, which is different than our formulation that maps observations to a single latent state. Our theoretical results currently do not support this type of advanced encoding mechanism. We already stack frames, but only a few of them, to turn our problems Markovian. When the number of frames required to turn the problem Markovian is large, we need more powerful encoders, such as the recurrent NN based encoder used by the above methods.\n\n\u201cPaper\u2019s Clarity\u201d\n\nWe hope Footnotes 5 and 6 that we added to the updated version of the paper have addressed the reviewer\u2019s concern regarding model-based SAC and the hyper-parameters of the algorithm. We will also revise the last paragraph of the VCARL description to make it more accessible. If there is something specific about this paragraph that the reviewer found difficult, please let us know to address it specifically, given the space limits of the paper.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HzCWnu72cQJ",
                "reply_to": "_aGxoqcmf4D",
                "title": "Main concerns still not resolved",
                "comment": "Thanks for your response, it is helpful and does clear up some of my original questions and inquiries. It does not, however, adequately address my primary concern about the lack of a comprehensive empirical evaluation.\n\nAs the authors point out, the empirical evaluation does not have to be viewed as the primary contribution. However, if the authors wish to present the theoretical result (and the loss function derived from it) as the primary contribution of the paper, the current state of the paper is still insufficient. The authors claim that comparing offline CARL with PCC is an ablation study. By my understanding of this work, I would disagree. Ablations are meant to isolate factors of variation in order to gain a deeper understanding of what changes and innovations contribute to observed differences. At least two things are varied between offline CARL and PCC: the loss function and the control algorithm. So, how important is newly proposed loss function vs the usage of model based SAC? I don't think that can be answered given the current experiments.\n\nThis question, however, is crucially important. As I view it, if the authors wish to highlight the theoretical result as an important contribution, then the authors should provide concrete evidence that the resulting loss function actually contributes significantly to better performance. Otherwise, it is just a weak relationship between a theoretical lower bound and a loss function with a lot of hyperparameters. And to provide this evidence, the authors must actually ablate prior methods such as PCC by swapping in the new loss function, while keeping as many other components constant as possible. I.e., don't just throw in model based SAC as another improvement.\n\nHowever, the authors seem to think that the comparison between offline CARL and PCC actually \"shows the importance of SAC as the control algorithm in place of iLQR\", which would place the contribution as more empirical, which then comes back to my original point about needing more experiments. If this is the case, then again, in my own view, the theoretical result and proposed loss function do not have supporting evidence that they are actually important.\n\nViewing this paper's contributions as a hybrid is also unsatisfying -- as I have discussed, currently neither the theoretical nor the empirical bits of the paper have enough evidence of their significance (though they are both promising).\n\nTo round out my point about additional experiments: while I do not pretend that this would not require substantial time and compute, I do not agree that this is out of scope for this paper. If the observations need to be Markovian, just pass in multiple images from the past few time steps. This pretty much suffices for any of the tasks that DREAMER and SLAC experimented with. Reward prediction can be done by the dynamics model, as MBPO does it.\n\n\nSome additional, more minor comments:\n\nIn a similar vein to what is discussed above, I do not view comparisons between online CARL and PCC/SOLAR as ablations.\n\nThanks for clarifying what is meant by \"model based SAC\". I'm not sure why the authors choose to cite the original SAC paper, which makes no mention whatsoever of models, rather than just directly citing the MBPO paper, since it sounds like the authors are basically doing exactly what MBPO prescribes? In my view, citing the original SAC paper does add confusion -- if a reader follows this citation, they will find a paper that never once mentions learning dynamics models, thus leading to ambiguity as to what the authors are actually doing.\n\nAs a final note, I believe it is disingenuous, in the author response, to imply that my quip about model based SAC was \"enough reason for [me] to question the clarity of the paper and to state that important details are difficult to follow\". This was one example I brought up of several, including other points such as \"there are many hyperparameters and not much guidance as to how to pick these\" (thanks for adding a bit more detail about this) and \"I personally found the last paragraph of the VCARL description almost impossible to follow\" (left unaddressed in the author response).\n\nMy score remains unchanged.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2FSGF7DWls4",
                "reply_to": "okBn6BHrzZ",
                "title": "Response to R1",
                "comment": "Response to main questions\n\n1)  Theorem 1 provides a high-level guideline for selecting the hyper-parameters of the loss function: \\lambda_{ed} = 2R_\\max / (1-\\gamma)^2, \\lambda_c = \\lambda_p = \\sqrt{2} \\gamma R_\\max / (1-\\gamma)^2, and \\lambda_{reg} = \\sqrt{2} \\gamma R_\\max / (1-\\gamma). However, in practice, to further optimize the performance of the CARL algorithms, we set (a subset of) these hyper-parameters via grid search. To address the reviewer\u2019s comment, we clarified this point by adding a discussion in Footnote 5.  \n\n2) It is important to note that we study the class of control problems in which the observations x have been selected such that the system is Markovian in the observation space X (see the beginning of Section 2). This is the same class of problems studied in E2C, RCE, SOLAR, and PCC, and this is why we selected them as our baselines. We did not include the results of E2C and RCE, because PCC has previously shown to be superior to them (see Levine et al. 2019). We compared our algorithms with Dreamer in X-Markovian problems considered in the paper, and Dreamer did not perform as well as CARL (with the same number of samples). This was expected as Dreamer has been designed for more general class of control problems (than X-Markovian), those that can be modeled as a POMDP. Extension of CARL to properly handle more involved environments (e.g., POMDP problems) requires using more powerful encoders (e.g., RNNs) and learning the latent reward function as a part of the representation learning process. As shown in Section 3, learning the reward function is in fact a part of the CARL\u2019s loss function and can be easily included in the algorithm. Using other encoders requires a bit more work but we believe it should be doable. We left this extension and experimenting with more involved environments as a part of our future work. \n\n3) We were not aware of this very new version of Dreamer and thank the reviewer for providing a reference to it. We added a citation to this work in the updated version of the paper. \n\n4) As described in the paper, CARL is a model-based RL (MBRL) algorithm that works in the latent space. Similar to most comparisons between MBRL and model-free RL algorithms (e.g., see the MBPO paper), when the dynamics is learned reasonably accurately, CARL can be much more data-efficient than any model-free algorithm. However, we can definitely find problems in which a model-free algorithm outperforms CARL.\n\n\n\u201cremoving F\u201d\n\nRemoving F and learning the mapping X -> Z -> X\u2019 is definitely a viable approach that definitely has the potential to be investigated as a future work. However, as described both in the paper and in Comment 4 about a model-free approach, these are all reasonable approaches that can work (or do not work) for some problems. However, the main idea of LCE (and as a result CARL) is to avoid direct prediction of the next observation, which could be challenging when the observation is high dimensional. Instead LCE suggests to learn a latent space and a latent dynamics F, and to control the systems there. \n\n\u201cCode\u201d\n\nThe code is in PyTorch and we plan to open-source it with the final version of the paper.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_aGxoqcmf4D",
                "reply_to": "hyW1G_q-8aI",
                "title": "Response to R2",
                "comment": "\u201cmore experiments\u201d\n\nIt is important to note that in addition to proposing algorithms to tackle the important problem of control from high-dimensional observations, we consider deriving a representation learning loss function from the control and dynamic programming principles as another contribution of our work. Returning to the sufficiency of our experiment, as explained in response to Reviewer 1, we study control problems in which the observations x have been selected such that the system is Markovian in the observation space X (see the beginning of Section 2). This is the same class of problems studied in E2C, RCE, SOLAR, and PCC, and this is why we selected them as our baselines, used the problems in their experiments, and conducted a comprehensive evaluation of these methods. We also experimented with Dreamer but as discussed in the paper, we did not expect it to perform well in our problems, because it has been designed for more general class of control problems (than X-Markovian), those that can be modeled as a POMDP. The goal of this paper is not to derive an algorithm that outperforms Dreamer (or similar algorithms) in problems that belong to the DeepMind suit. Our goal is to derive a representation learning loss function that is suitable for an important class of controllers (approximate policy iteration) and devise algorithms that properly interleave this representation learning and control. As explained in response to Reviewer 1, extending CARL to handle more involved problems requires using more powerful encoders (e.g., RNNs) and learning the latent reward function as a part of the representation learning process, which we believe both are doable. We left this extension and experimenting with more involved environments as a part of our future work. \n\n\n\u201cmodel-based SAC\u201d\n\nModel-based SAC is simply SAC when the data is generated from the model, instead of from the agent\u2019s interaction with the environment. We thought that the meaning is clear, but to further clarify, we added a footnote (Footnote 6) to the updated version of the paper. However, we do not believe that this is enough reason for the reviewer to question the clarity of the paper and to state that important details are difficult to follow. \n\n\n\u201cablation study\u201d\n\nWe have done several ablation studies in the paper. Comparing offline CARL with PCC, because of the close connections between their loss functions, shows the importance of SAC as the control algorithm in place of iLQR in PCC. Comparing online CARL with offline CARL and PCC shows the importance of interleaving representation learning and control. Comparing online CARL with SOLAR shows the advantage of using the CARL loss function. Comparing CARL with and without policy distillation shows the effect of this process in the performance of the algorithm. Although there are other combinations that can be investigated, we believe we have already done a fair amount of ablation studies in the paper. If the reviewer has a particular ablation study in mind, it would be good to clearly state it that we see if we can provide its results by the end of the rebuttal phase. \n\n\n\u201chyper-parameters\u201d\n\nPlease see our response to Reviewer 1. To summarize, the theory (Theorem 1) provides a high-level guideline for selecting the hyper-parameters of CARL\u2019s loss function. However, in practice, to further optimize the performance of the CARL algorithms, we set (a subset of) these hyper-parameters via grid search. To address the reviewer\u2019s comment, we clarified this point by adding a discussion in Footnote 5.  \n \n\n\u201cwhy there are different versions of CARL\u201d\n\nOffline CARL is for problems in which a large batch of exploratory data is available in advance, and thus, interleaving representation learning and control cannot add much value to the method. Moreover, it is used for an ablation study to compare CARL with PCC and see the effect of using SAC instead of iLQR. The main goal of V-CARL, as explained in the paper, is to establish a closer connection between representation learning and control by weighing the loss function using the TD-error of the current policy. \n\n\n\u201ca related work\u201d\n\nWe thank the reviewer for bringing the SLAC work into our attention. It definitely has some connections to our work. We added a reference to it in the updated version of the paper.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "c1vu1V6ciu",
                "reply_to": "o67CDKifq89",
                "title": "Response to R3",
                "comment": "\u201cProof of Theorem 7\u201d\n\nWe thank the reviewer for bringing into our attention this major typo in the proof of Theorem 7, and as a result in the statement of Theorem 1 (Eq. 4). You are right, when we apply the Bellman operator to TV - V < \\Delta, we do not obtain T^2V(x) - TV(x) < \\gamma \\Delta(x), for any x. However, we can show that for any x, we have T^2V(x) - TV(x) < \\gamma E_{x\\sim P_{\\pi o E}}][\\Delta(x)]). This will result in a change in the statement of Theorem 7 and all the subsequent results, and finally in the statement of Theorem 1, that x should come from the \\gamma-occupancy measure of the current policy (\\pi o E). So, we do not obtain a result in L-inf norm that as correctly mentioned by the reviewer is not desirable. We revised the appendix and the statement of Theorem 1 (Eq. 4) in the paper to reflect this change. However, this change has no effect on the CARL algorithms as the samples are collected by following the current policy (\\pi o E). \n\nResponse to other questions\n1) In Offline CARL, similar to other LCE methods, such as E2C and PCC, we assume that the data is collected by an exploratory policy that provides a good coverage of the parts of the state-action space that are relevant to the task at hand (see the discussions on Page 5). Violation of this assumption would add error to the process, similar to all offline RL settings, and require certain corrections to alleviate its effects. Studying this issue is outside the scope of this work and is an interesting future direction.\n\n2) As mentioned in the paper, the CARL\u2019s loss function has close connections to that in PCC (see Page 5), although they have been derived from completely different perspectives. While the loss function in CARL has been derived such that the learned representation is suitable for a policy iteration style algorithm, the one in PCC has been derived to learn a latent space that is amenable to locally linear control algorithms. As discussed in Section 3, since PCC has been designed for an offline setting (i.e., one-shot representation learning and control), its prediction and consistency terms are independent of a particular policy and are defined for state-action pairs. While CARL has been designed for an online setting (i.e., interleaving representation learning and control), and thus, all its loss terms depend on the current policy. Despite the similarities in loss functions, as we show in our experiments, offline CARL (which is a member of the CARL family) outperforms PCC. Moreover, the other members of the CARL family are more superior to PCC and offline CARL, as shown in our experiments, mainly because they better address the data collection issue discussed in Question 1, by interleaving representation learning and control. \n\n3) As discussed in the description of online CARL in Section 4, we approximate the operation in Line 4 of Algorithm 1 by a process we refer to as policy distillation. To compute \\pi, we project the observation policy \\mu onto the latent space by minimizing D_KL(\\mu || \\pi \\circ E). There might be other ways to implement this line of the algorithm (including removing it), which requires more investigation. We showed in our experiments (see Appendix F1) that the results when we remove this line (no distillation) are worse than those with distillation. We revised the paper to better explain this step of the algorithm.\n\n4) Similar to other LCE methods, such as E2C and PCC, we define the reward function in the latent space. We refer to a reward function as compatible, if when it is optimized (in the latent space), the resulting policy (projected back to the observation space) solves the problem. For example, in a goal-based task, a reward function that measures the negative distance from each latent state to the image of the goal (in the latent space) is compatible. For clarification, we added the above description to the paper (see Footnote 4). \n\n5) We added the definition of the curvature loss, which is identical to that in the PCC paper, at the end of Section 3 (see Page 5).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "75SPtsgzOsb",
                "reply_to": "iclr_2021_dgd4EJqsbW5",
                "title": "Rebuttal",
                "comment": "We thank the reviewers for their useful feedback. Please see response and the updated paper to the individual comments below.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "okBn6BHrzZ",
                "reply_to": "iclr_2021_dgd4EJqsbW5",
                "title": "Good theoretical intuition but lacking experimentation",
                "comment": "This paper examines the problem of learning controllable embedding (LCE), with the goal of learning good representations (usually achieved using variational inference algorithms) such that the maximum cumulative reward can be achieved. The main difference lies in the simultaneous learning of both the low-dimensional latent space as well as the action policy.\n\nOne of the main strengths of this paper is found in Theorem 1. The authors devise a simple policy iteration approach in the low dimensional learned space. Then, using mostly qualitative analysis, a bound on the policy improvement error is formulated. This error combines several intuitive and straightforward factors, which are then extracted to form more involved offline and online reinforcement learning algorithms. I have read through the proofs, and they seems correct.\n\nWhile I appreciate the quality of the theoretical work, the paper had some drawbacks that brought me to my current score :\n1. The loss function consists of many hyper-parameters. The authors should provide some guidelines for choosing these hyper-parameters due to the large number of possible combinations, and clearly state how the scalings affect performance.\n2. Experimentation is lacking. While the authors conducted experiments mostly on toy problems, I expect them to compare against more involved environments which are harder to model. Their comparison with state of the art algorithms (e.g. Dreamer) which were also tested on such environments is thus not fair.\n3. Minor comment: There is a newer version of Dreamer that the authors can compared against: https://arxiv.org/pdf/2010.02193.pdf \n4. Minor comment: How would CARL compare against model-free offline RL methods, or generally to algorithms that are not SAC?\n\nQuestion to authors:\nWould there be a benefit in removing F altogether and learning a mapping X -> Z -> X\u2019 without transitioning in the latent space? (i.e., errors III and IV in Theorem 1)\n\nFinally, it would be beneficial if the authors could include code for their work. If the authors can't supply the complete code base, even code snippets with clarifying explanations to demonstrate their main ideas would be beneficial. This would greatly improve the quality and credibility of their work as well as the reviews.\n\nTo conclude, the paper provides strong theoretical intuition, which is a significant value-add of the paper. Nevertheless, its lack of experimentation and large number of hyper-parameters limit its overall quality. If the authors provide substantial improvement in the experimentation I will increase my score.\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The work",
                "Sentiment Expression": "shows the promise",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "is now in good shape",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "8OH6t0YQGPJ": {
        "paper_id": "nips_2022_8OH6t0YQGPJ",
        "paper_title": "Modeling the Machine Learning Multiverse",
        "paper_abstract": "Amid mounting concern about the reliability and credibility of machine learning research, we present a principled framework for making robust and generalizable claims: the multiverse analysis. Our framework builds upon the multiverse analysis introduced in response to psychology's own reproducibility crisis. To efficiently explore high-dimensional and often continuous ML search spaces, we model the multiverse with a Gaussian Process surrogate and apply Bayesian experimental design. Our framework is designed to facilitate drawing robust scientific conclusions about model performance, and thus our approach focuses on exploration rather than conventional optimization. In the first of two case studies, we investigate disputed claims about the relative merit of adaptive optimizers.  Second, we synthesize conflicting research on the effect of learning rate on the large batch training generalization gap. For the machine learning community, the multiverse analysis is a simple and effective technique for identifying robust claims, for increasing transparency, and a step toward improved reproducibility.",
        "paper_acceptance": "Accept",
        "meta_review": "This paper suggests a novel way to conduct machine learning empirical research and report the results. While ablation studies became a common practice in analyzing the contributions of different components in the ML system, this paper takes it much further by suggesting a way to explore the entire hyper-parameter space. \nWhile the components used in the work (Gaussian Processes, Active Learning, \u2026) are not new, the novelty of this work is in combining them into addressing a timely question. This paper has the potential to contribute to the way ML research is conducted and reported.\n",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "V5hwPWFwvHj",
                "writer": "official_reviewer",
                "reply_to": "vH-hEo9wSg2",
                "title": "Understood",
                "comment": " Makes sense, though if your interpretation is correct, should we expect the reported gap between SGD and Adam to fit within the range of the surrogate model's posterior uncertainty? Not critical, but would be another sanity check on that claim. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "vH-hEo9wSg2",
                "writer": "author",
                "reply_to": "ev30gw6GN3",
                "title": "Response regarding question about epsilon ",
                "comment": " Thanks for your response. If figure 2.a. is still causing confusion, that\u2019s helpful feedback that it could be clearer. We\u2019ll do our best to tighten this up going forward.\n\nFor now, just to check we\u2019re on the same page, we interpret your question about epsilon as follows: How could epsilon be used to improve Adam\u2019s performance, given that fig 2.a. shows that one can move up and down the y axis (epsilon) almost freely with barely any change in relative performance?\n\nWe hope this is a fair and accurate representation of your question. If so, then our response is as follows:\n\nThe relative performance contour plot in fig. 2.a. is only an estimate of the true space. Our intent with the comment above is to say that if one continued to sample _at length_, and were to sample more densely around a specific region (i.e. via hyperparameter optimization), then perhaps the resulting _more detailed_ view of the LR x epsilon search space might reveal some _very modest_ room for improvement by tuning epsilon.\n\nCrucially, this is not in conflict with our claim that LR determines the relative merit of Adam/SGD, and that any effect of epsilon is dwarfed by that of learning rate. Indeed the sensitivity analysis in fig. 3. estimates a minuscule effect of epsilon. We are not suggesting that epsilon can \u201cbe used to close the Adam-SGD gap\u201d. Instead, we are simply not _ruling out_ that tuning epsilon might eke out a (tiny) drop more performance when using Adam. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "grYEV-dm4jU",
                "writer": "official_reviewer",
                "reply_to": "m-aruzgch22",
                "title": "Response to Rebuttal",
                "comment": " Thanks for the response. I think my concerns are well addressed. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ev30gw6GN3",
                "writer": "official_reviewer",
                "reply_to": "GOoPgD05sB",
                "title": "Response to rebuttal",
                "comment": " I buy the arguments regarding very large scale models and novelty. With respect to the case studies chosen, while it's true that the scale doesn't showcase the benefits of GP active learning, I agree that that's not important to the paper. What is important is that they don't show something new we didn't have evidence for before, which would make for a stronger paper -- I don't think the rebuttal disputes this. With respect to the Adam epsilon parameter, I agree that the paper does not make an explicit claim about performance, but I am still puzzled on how to interpret the figure in light of the claims in the text (see my original review). I hope the authors and / or the other reviewers will help me puzzle this out. \n\nRegardless, I think neither issue sinks the paper, and I will update my rating accordingly. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sOI9PS-aCQ8",
                "writer": "author",
                "reply_to": "nips_2022_8OH6t0YQGPJ",
                "title": "Thanks for your detailed and positive reviews",
                "comment": " Thank you to all four anonymous reviewers for your helpful and constructive comments.\n\nWe loved hearing that you found our work very novel (dGQ4), principled and practical (yyG7), are satisfied with our technical approach (wcgM) and particularly that you found using the Bayes factor to test for interaction effects a \u201cfantastic idea\u201d (dGQ4). We sincerely appreciate your kind feedback.\n\nWe are also pleased to see that you enjoyed reading (wcgM) the paper, that you found it well-written (wcgM, yyG7), and particularly that you found the core ideas well-presented (wcgM, fyXL) and well-demonstrated by our case studies (yyG7). We\u2014of course\u2014wholeheartedly agree that the problem is important (fyXL); indeed our core motivation is to facilitate more solid foundations for \u201ccontinued progress in the ML community\u201d (yyG7). \n\nWe\u2019ve added in-depth responses to each of your reviews below, and hope they serve as useful clarification and highlight how we think about the multiverse. We look forward to continuing the discussion with each of you.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "GOoPgD05sB",
                "writer": "author",
                "reply_to": "Gg0_hPI8yF0",
                "title": "Response to reviewer wcgM on contribution, large-scale training, case study choice and Adam's epsilon",
                "comment": " Thanks for the useful comments. Our specific responses are below.\n\n## 1. On contribution and novelty\n\nWe see our core contribution as introducing the multiverse analysis as a principled framework to the machine learning community, where it can add significant and long-term value by promoting the reporting of robust and generalizable conclusions. \n\nWhile we did not invent the multiverse, we believe that importing ideas from other disciplines can be of immense value to the machine learning community. In this specific case, we hope to demonstrate concretely how the multiverse analysis is relevant to machine learning and to make it practical with a GP surrogate and Bayesian experimental design. \n\nTo the reviewer\u2019s point about active learning, we are of course the beneficiaries of the long heritage of GP-enabled approaches across bayesian optimization, experimental design and active learning. One novel contribution here is to marry the multiverse up with a GP + Bayesian experimental design to enable efficient exploration. The mentioned neuroimaging pre-print [1] has only just (June 29th) been published in Nature Comms, and as such we consider this a concurrent work in an entirely separate field, though we will of course update our related work for the final version if accepted. \n\n## 2. On very large-scale models\n\nWe partially agree with this critique, and acknowledge that this kind of systematic exploration isn\u2019t suitable for all research.\nBeyond model training: we draw attention to the vast array of empirical evaluations on various topics throughout machine learning research. The multiverse as principled framework is equally applicable when one is evaluating a pre-trained, off-the-shelf model and its performance(s) on various datasets, under different conditions. \n\nSpecifically regarding model training: precisely because training very large-scale models is expensive, ML practitioners typically spend a lot of time and compute doing a hyperparameter search before training the full model. For example, a recent paper \"What Language Model to Train if You Have One Million GPU Hours?\" (https://openreview.net/pdf?id=rI7BL3fHIZq) is a great example of a hyperparameter search that would have benefitted from a multiverse analysis.\n\nIf, on the other hand, hyperparameter search is infeasible, then we would argue that this is a (possibly transient) limitation of massive scaling, rather than of the multiverse analysis. If models are to require significantly more investment in compute and carbon, and are to be deployed in increasingly important scenarios, we think it\u2019s all the more valuable to analyze their robustness in a principled way. \n\n## 3. On case study scale\n\nOur two principle case studies are designed to illustrate, separately, a \u201creproducibility\u201d example and a \u201csynthesis\u201d example.\nFor the former, case study 1, we chose our search space \u2014 admittedly small \u2014 to demonstrate the value of analyzing a space rather than a point estimate: it is not the aim of this paper to conclusively adjudicate between optimizers, but to highlight that the impact of learning rate changes the conclusions drawn by Wilson et al. [2]: the \u201cbest\u201d optimizer varies by learning rate.\nFor the latter, case study 2, we chose a slightly larger search space to demonstrate how our analysis can generalize to non-hyperparameter settings such as dataset and model. A goal of this case study is to show that the multiverse analysis is not just for reproducibility, but can also help synthesize a large and complex topic across many papers and even more experimental settings (although, as we note in the paper, a fuller analysis would be welcome). \nThese examples are exactly that: case studies designed explicitly to showcase the multiverse and to demonstrate our general framework, and we don\u2019t feel it is justified to spend more on compute for the sake of examples that would only differ in degree rather than in kind. We are, however, tremendously excited about the potential for subsequent work to apply the multiverse to larger domains and more complex questions, particularly in domains where robust conclusions have real\u2013world impacts, such as medical applications or scientific discovery.\n\n## 4. On Adam\u2019s epsilon\n\nIn lines 167-170, we are just acknowledging the possibility that when *optimizing* over both LR and epsilon, one could potentially improve Adam\u2019s performance: we don\u2019t at any point dispute that Adam\u2019s performance can\u2019t be improved by tuning epsilon. Instead, the principle claims investigated with the multiverse analysis are with respect to the relative performance of Adam and SGD, and we suggest that *LR is the driving factor* in determining relative merit. \n\n[1] Dafflon et al. A guided multiverse study of neuroimaging analyses. Nat Commun. 2022 . 13(1):3758.\n\n[2] Wilson et al. The marginal value of adaptive gradient methods in machine learning. NeurIPS, 2017.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "m-aruzgch22",
                "writer": "author",
                "reply_to": "9BqNInpAF7S",
                "title": "Response to reviewer fyXL on differences from BO, GP hyperparameter robustness and appropriate stopping criterion",
                "comment": " Thanks for the helpful review. We\u2019ve uploaded a new version of the paper with an additional figure in the appendix addressing your question about GP hyperparameters, and responded to your questions below.\n\n## 1. On differences from Bayesian Optimization\n\nThe primary contribution of our paper is to introduce the multiverse analysis to the machine learning community, in order to support drawing robust and generalizable scientific conclusions. To make the multiverse analysis tractable, we couple this approach with a GP surrogate and bayesian experimental design.\n\nWhile our approach builds on the wealth of literature in Bayesian Optimization, the goal of our approach is in fact not optimization, but rather exploration. This is a fairly fundamental difference, and we view it as a benefit that our approach has this connection, as future work building on the multiverse analysis can leverage the extensive machinery developed for BO. As the field of BO progresses, the multiverse analysis directly benefits. \n\nTo reiterate, a key takeaway from our paper is that while there is of course a role for optimization in machine learning, the role for exploration is often underestimated. Indeed, for those conducting research into new models, or exploring how and when models work, exploring the multiverse of choices systematically can provide much needed transparency that is of significant value to downstream researchers and practitioners.\n\nFinally, we note that our framework extends far beyond hyperparameters. Indeed, in case study 2 we explore (although at a small scale) the effect of both dataset and model choice on the generalization gap phenomenon. In lines 274-276, we suggest that an expanded search space could even include the evaluation metric and the termination criterion. While _optimizing_ over datasets and evaluation metrics would certainly be bad scientific practice, exploring them is exactly the converse: demonstrating robustness in the face of these choices would greatly reinforce the contributions of our community. Our case studies are just that; they are examples designed to illustrate the potential of the multiverse analysis as principled framework: the most exciting part of our research is not in what we have done so far, but in the possibilities that lie ahead.\n\n## 2. On robustness to GP hyperparameters\n\nWe think this is an excellent question: just like with search space, we fully appreciate that choosing how to do a multiverse analysis at a technical level is of course a choice.\n\nHowever, we have intentionally selected the Matern 5/2 kernel with ARD because it should be a sensible out-of-the-box choice for most use cases, and indeed is often used in Bayesian Optimization (which faces exactly this question also) for this reason.\n\nWe have also ran a few analyses on case study 1 and added a figure to the appendix (fig. S6) showing the minimal impact of the kernel on the contour plot: the plots change a little, but the broad conclusions remain consistent. We also analyzed a range of initial variance and lengthscale hyperparameters and find no change in the the Bayes factor outcome for case study 1. We think this is useful evidence supporting the general applicability of these choices. We will elaborate upon this idea and extend it to case study 2 for the camera-ready if accepted.\n\n## 3. On appropriate stopping criterion\n\nThanks for the great question. We describe the stopping criterion on lines  87-89: we continue sampling from the multiverse until we reach a conclusive Bayes factor. We acknowledge that \u201cappropriate stopping criterion\u201d is a little hand-wavy. We use this terminology because stopping criterion is application specific - for other researchers in different contexts, it could be a fixed number of experiments, or a certain amount of compute. We will endeavor to tighten up this idea in the final version.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "q2vjnz1w9m",
                "writer": "author",
                "reply_to": "XLjcdH_i-gy",
                "title": "Response to reviewer dGQ4 on interaction effects, initial design and search space choice, MC integration and Bayes factor",
                "comment": " Thanks for the thought-provoking questions. We\u2019ve responded to your questions below.\n\n## 1. On quantifying interaction effects & categorical parameters:\n\nThe use of Bayes factor does test for the conclusive existence of an interaction effect, though the reviewer is correct that it will not establish the magnitude of the interaction. Instead, we turn to Monte Carlo sensitivity analysis to estimate the relative importance of each dimension, and of their interaction by way of the total effect (e.g. fig. 3b and 6b).he \n\nWe also wish to clarify our choice of model when using categorical parameters. We explicitly choose an intrinsic coregionalization model (lines 197-200), rather than the standard product kernel for categorical parameters. This models each combination of categorical parameter value as its own output function, which enables two things. First, we are able to evaluate the precise effect of categorical parameters, e.g. in fig. 5a where we show that dataset choice can influence conclusions. Second, it allows us to identify shared variance between the different outputs. In this way, our interaction effect test (i.e. comparing additive/shared _base_ kernel models) tests for interaction effects across all model/dataset pairs. \n\n## 2. On initial design:\n\nThere are two \u201cinitial designs\u201d the reviewer might be referring to: how we draw the initial set of points, or how we determine which dimensions to include in the analysis and how to bound them.\n\n### 2.a. On sampling the initial points:\n\nFor our initial set of points, the best set of points will be one that optimally covers the space, i.e. one with low discrepancy. For this purpose, the Sobol sequence is close to optimal, and there are existing implementations that are available out-of-the-box in contemporary bayes opt / experimental design libraries.\n\nThat said, even sampling uniformly at random for the initial points is a reasonable strategy. The main motivation behind low discrepancy sequences is to avoid undersampling from large regions of the search space, but this is (a) relatively unlikely and (b) low-impact as any gaps would be immediately filled in by the points selected by IVR.\n\nWe are primarily concerned with whether conclusions about model performance are reproducible, and we believe sampling uniformly at random vs. using the Sobol sequence will not change the outcome here.\n\n### 2.b. On defining a search space:\n\nThe search space necessarily impacts conclusions drawn. Just as with any experiment, effort and consideration should go into the search space design - indeed, this is the trickiest part of the multiverse analysis.\n\nCandidate dimensions for inclusion are all those that the researcher suspects are \u201crelevant\u201d. In other words, the dimensions should explicitly codify auxiliary hypotheses about the conclusion(s) being drawn: assumptions about when results should hold, and when we expect them to break down. \n\nThere isn\u2019t an easy answer as to how to choose a space, and it requires judgment on the part of the researcher, and specialist knowledge about the domain under consideration. In short, defining a search space is indeed hard, but most attempts (if accompanied by the requisite transparency) are far better than nothing.\n\n## 3. On MC integration\n\nIn our work, we use MC to approximate the integral over the search space. This is the standard approach to integrated variance reduction and is commonly used. As we\u2019re only dealing with a handful of dimensions in our case studies, we think this is a sufficient approach to demonstrate the utility of an exploration-only acquisition function like IVR. In larger settings, an alternative sampling approach such as a Sobol sequence or other quasi-MC methods might be more appropriate, and this is indeed a design choice (though perhaps only for really large multiverses). We\u2019ll definitely add something to this effect to the discussion section..\n\n## 4. On Bayes factor model comparison\n\nIn this specific case, we use Bayes factor to test for interaction effects by comparing the model fit between a GP with an additive kernel and a GP with a product kernel. We note that this is specific to the question at hand, i.e. whether there exists an interaction effect between e.g. LR and epsilon, and is not a general property of all multiverse analyses: future studies may choose other tests that are appropriate to the claims at hand.\n\nIn interpreting the Bayes factor, a result close to 1 would indicate an equivocal result, i.e. that it is not conclusive whether there is or is not an interaction effect. In this case, a possible response would be to collect more data until a conclusive result is obtained. Indeed the applicability of optional stopping is a key motivation for using a Bayesian statistical hypothesis test as opposed to a frequentist. Collecting data until a hypothesis is (dis)confirmed up to a satisfactory degree of confidence is a useful secondary outcome of the multiverse framework.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uEtmDj430-m",
                "writer": "author",
                "reply_to": "jmncEMMr7LN",
                "title": "Response to reviewer yyG7 on compute cost and analyzing multiverse results",
                "comment": " Thanks once again for the considered and constructive review. We have included two comments below to respond to your comment about compute cost and your question about high-dimensional analysis.\n\n## 1. On compute cost and environmental impact:\n\nWe agree that compute cost and environmental impact needs to be carefully considered in all contemporary machine learning work. However, as we note on lines 284-5, we posit that our approach need not use substantially more compute that existing hyperparameter optimization schemes: instead, by switching from an optimization- to an exploration-based approach, researchers can spend a roughly equivalent amount of compute exploring in place of their traditional hyperparameter search. \n\nMoreover, on aggregate over a whole community, we argue that the added transparency around results and improved robustness of conclusions can indeed reduce net cost, by supporting researchers to spend their budget wisely on new experiments, rather than repeating past experiments performed by different researchers whose results may not have been reported.\n\nFinally, we are not advocating for the blanket adoption of the multiverse analysis in all scenarios. There may be scenarios where added compute may deliver little value, though in others \u2014 e.g. high-stakes settings such as clinical applications \u2014 the added benefit of more trustworthy and robust conclusions will likely outweigh the cost. \n\n## 2. On summarizing and visualizing multiverse results:\n\nWe think novel strategies to summarize main conclusions in a multiverse setting are an exciting area for future research. As we mention in lines (299-301), we see promise in interactive multiverse visualization, allowing the reader to slice up the multiverse into chunks most relevant for their interests.\n\nMoreover, in higher-dimensional settings, standard dimensionality reduction techniques might be able to highlight emergent trends. A concurrent and related work in neuroimaging [1] (its pre-print form was also mentioned by reviewer wcgM) applies multi-dimensional scaling (MDS) to visualize a high-dimensional search space in 2D.\n\nWe also highlight our application of Monte Carlo sensitivity analysis to the GP parameters which demonstrates how robust conclusions are in the face of variation along each dimension. Reporting the results of this sensitivity analysis can scale to larger and more complex search spaces. Furthermore, we expect that in the majority of cases, only a subset of dimensions will be identified as relevant by the sensitivity analysis, enabling more detailed visualization and analysis of just the dimensions that matter.\n\nFinally, we suggest that a key idea in the multiverse analysis as presented here is to move beyond summarization and towards increased, even radical, transparency. In our community, we often focus on a single conclusion or a top-line figure. Instead, we suggest it is important to transparently report results over the full (or at least, larger) space, such that future researchers and practitioners can navigate through and understand the different subspaces in which results hold and in which they differ.\n\n[1] Dafflon et al. A guided multiverse study of neuroimaging analyses. Nat Commun. 2022 Jun 29;13(1):3758.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Gg0_hPI8yF0",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_8OH6t0YQGPJ",
                "title": "",
                "comment": " The paper proposes to use active learning with Gaussian Processes (GPs) as a way to perform sample-efficient \"multiverse analysis\" of machine learning models, i.e. a way to understand the full space of hyperparameter settings and their influence on model performance rather than simply optimizing them. It then applies this analysis to three examples, one with a motivating (but artificial) goal related to inter-group variability, and two motivated by real recent debates in the literature.  ## Strengths\nI enjoyed reading the paper: it is well-written such that I have essentially no quibbles related to typos, confusing wording, or the like, and more importantly the motivation and core ideas are presented so clearly as to seem obvious post hoc. The idea of using a meta-model to improve efficiency of multiverse analysis is likewise useful (though I believe there's a similar idea in a biorxiv preprint from a few years ago, doi:10.1101/2020.10.29.359778, and in older work from psychology doi:10.1037/0033-295X.113.1.57). The technical approach is reasonable given the setup (flexible GP surrogates, global active learning with IVR by taking an expectation over the lookahead posterior variance), and the conclusions aren't overstated given the relatively modest studies undertaken. \n\n## Weaknesses \nThe essence of a good paper is an insightful idea or contribution taken to new payoffs, and both seem relatively modest in the paper as it stands. \n\nWith regards to the core idea, the paper imports a pre-existing conceptual framework (in the sense that multiverse analysis is not new, though it seems new to ML), and uses standard tools in only a slightly novel application (in the sense that GP active learning is not new in ML, though the typical application is to pure optimization). \n\nWith respect to the new payoffs, the analysis of adaptive optimizers comes to a similar high-level conclusion to what is in prior work (that both SGD and Adam can perform equally well depending on hyperparameter tuning), and the paper does not go beyond exploring the two parameters that have been explored previously. At the same time, when it comes to the details, the paper does not seem to replicate the effect of epsilon on the SGD-Adam gap. This is potentially interesting and while the paper claims this result is not incompatible with the potential to improve Adam performance by tuning epsilon, I'm not sure I see how both can be compatible (more on this in \"questions\" below). \n\nWith the large batch generalization gap study, the paper explores a space substantially smaller than the multiverse of reasonable choices previously explored in the literature, and the relationship it does find matches prior work regarding batch size and learning rate, \n\nWithout disputing that the present work undertakes both studies in an elegant and efficient way relative to prior work, I think the latter still serves as a \"warmup\" demonstration of shoring up existing results, and not a strong new thing that couldn't have been done without the proposed approach, while the former adds another datapoint to the space without providing a definitive understanding of the benefits of Adam over SGD. \n\nFurthermore, the small search spaces and large batches that the paper considers don't really showcase the benefit of active learning where, especially for a pure exploration setting, a quasi-random search baseline is likely to be quite strong.  I hope that the rebuttal can help clarify and justify the impact of the findings as presented, for example by more precisely discussing ways in which they drive new conclusions not present in the cited prior work. Alternatively, to the extent that the conclusions are similar in broad strokes, I hope the rebuttal can defend the ways in which the systematicity of the present experiments shores up some aspect of the two case studies that remained previously unclear or murky. A third way for the paper to have greater impact would be to demonstrate that additional conclusions can be drawn from the meta-models that extend beyond what has been hypothesized or shown previously. \n\nFinally, I also hope that the rebuttal will clarify the following puzzle regarding the adaptive optimizer study: on lines 167-170, the paper discusses how Adam's epsilon parameter seems to not matter for the purposes of determining whether it outperforms SGD, and grants that epsilon could be tuned to improve Adam's performance. If that is true, then one can take a vertical slice through figure 2a and move along it in some dimensions in which Adam performance improves but SGD performance does not (since SGD performance should be invariant to movement in that dimension of the space). To the extent that we do not see such a thing, the present contribution seems incompatible with the claim that epsilon can be used to improve Adam's performance, or be used to close the Adam-SGD gap. What am I missing here? \n Limitations are discussed, even alongside the claims / conclusions rather than being shunted to a separate section -- this is good. Benefits of using sample-efficient active learning are discussed. Negative societal implications are not discussed explicitly, though I don't think there's anything major to discuss here. One thing not mentioned that is possibly worth engaging with is that for very large-scale models that take thousands of hours to train on thousands of GPUs, even regular hyperparameter search may be out of reach, never mind a multiverse analysis, so the impact of this sort of work will likely be in smaller settings potentially farther from state of the art. ",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "9BqNInpAF7S",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_8OH6t0YQGPJ",
                "title": "",
                "comment": " The paper proposes the Multiverse analysis as a step toward more transparent, reliable and credible results when we are facing large number of choices in our research design even before running the actual experiments. Speficically the authors leverage Bayesian optimization idea to develop a framework to search over large search space, which is used to guide honest research designs.  Strengths: The problem itself is really important and the authors put a lot of effort in reproducibility, providing the code for running the experiments. Also, the presentation is very clear and the three examples chosen clearly convey the message the authors want to share. \n\nWeaknesses: My main concern is whether this is just a straightforward application of Bayesian Optimization in parameter tuning when the parameters are replaced by different research designs in this setup. Some other concerns are listed in the questions.  1. As the authors stated in the paper, someone would question the fact that search space is also a choice. I agree with the explanations given by the author, however, I think the choice of kernel (beyond the two in the paper) and the prior (e.g. $\\Sigma$) would also make a difference, especially when the budget is limited as we cannot run many steps. Do you have results showing robustness of your conclusion to these choices.\n2. I am not sure what the \"appropriate stopping criterion\" in step 4 is. Is it just a visualization as stated in line 82?  The authors addressed their limitations and negative societal impact. ",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "XLjcdH_i-gy",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_8OH6t0YQGPJ",
                "title": "",
                "comment": " The paper adopts a concept of psychology, called \u2018Multiverse Analysis\u2019, to systematically study credibility of claims and robustness of conclusions. Multiverse are universes each with a slightly different layout. Based on this concept, GP model is employed to quantify and emulate the relationship between interest of study and variance in layouts. The whole experiment is conduct in a Bayesian way: construct initial design; collect data; find next experiment point that reduce the variance. The existence of interaction effect is determined by comparison of probability of the model considering interaction effect and the model not given current data. Strength: \n(1) It is very novel to consider investigating the machine learning multiverse as the design of experiments problem with the steps of initial design, surrogate model, acquisition function, and sequential design.\n\n(2) The proposed framework is capable of comprehensively verify or study an interested claim of effect. Bayes factor is a fantastic idea to determine if interaction effect exists. \n\nWeakness: \n(1). The current method in the manuscript is not able to quantify the magnitude of interaction effects. When model the categorical factor, kernel function is a production. Therefore, the effects are kind of interaction effect among all categorical factors and continuous factors.\n\n(2). The current manuscript has not sufficiently described how to choose the initial design, and how the choice of the initial design affects the performance of the proposed method. \n How to verify the significance of model comparison (interaction effects)? If the Bayes factor is close to 1, say 0.998.  The authors have discussed the limitations. I have one more comment. \n\nFor the IVR acquisition function in Eq (6), the authors just simply use a Monte Carlo approximation to deal with multivariate integral. It cannot be accurate when for large-dimension problem.  ",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "jmncEMMr7LN",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_8OH6t0YQGPJ",
                "title": "",
                "comment": " This paper proposes an ML multiverse analysis framework to draw robust scientific conclusions about model performance by focusing on the model exploration using Gaussian process surrogate models and Bayesian experimental design. The paper shows two motivating case studies: \n1) presented a multiverse analysis demonstrating that different hyperparameter choices lead to different optimizer recommendations; \n2) used the framework to investigate the conflicting research on the effect of learning rate on the large batch training generalization gap. \n\nThe core idea of the ML multiverse analysis is to use integrated variance reduction (IVR) as the acquisition function and keep all available information to draw more bust conclusions. The analysis is carried out by visualizing the posterior predictive mean to the extent to which the surrogate's posterior variance has been adequately explored. Also, the analysis tests for the interaction effects of the different dimensions of the search space by calculating the Bayes factor of a GP with shared kernel against an additive kernel. Finally, a Monte Carlo sensitivity analysis is used to test how much a change in one of the parameters would influence the model outputs.  ## Strength:\n- The paper is well written and structured \n- The paper provides a principled and practical way to to achieve reproducible results and draw robust conclusions, which will help the continued progress in the ML community. \n- The effectiveness of the proposed framework is well-demonstrated by two comprehensive case studies. \n\n## Weakness: \n- As pointed out by the author, the computation time of the framework will scale with the multiverse size, given the recent critiques of environmental impacts on ML, needs to be carefully considered. One possible way to reduce the computation time is to make use of all the existing experiments results through efficient surrogate models.  -  As pointed out by the authors, the largest dimension of the multiverse in the two case studies is 4, therefore, faceted contour plots are sufficient for visualization purpose. In case of high-dimensional search space, besides visualization, are there any other quantitative techniques can be used to analyze the result?  The author did addressed the potential environmental concern of the framework due to computation scaling up with the search space. On the other, scalability of the proposed method with respect to the search space will probably limit the practicality of the proposed multiverse analysis framework. ",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "way to conduct machine learning empirical research and report the results",
                "Sentiment Expression": "novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "a way to explore the entire hyper-parameter space",
                "Sentiment Expression": "takes it much further",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "combining them into addressing a timely question",
                "Sentiment Expression": "the novelty of this work",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the way ML research is conducted and reported",
                "Sentiment Expression": "has the potential to contribute",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "xTYL1J6Xt-z": {
        "paper_id": "nips_2022_xTYL1J6Xt-z",
        "paper_title": "FasterRisk: Fast and Accurate Interpretable Risk Scores",
        "paper_abstract": "Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach  produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a \"star ray\" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications. ",
        "paper_acceptance": "Accept",
        "meta_review": "Thank you for submitting your paper to NeurIPS! This paper makes a valuable contribution to the scoring model literature, providing a fast and scalable algorithm to derive sparse risk scores. The reviewers uniformly appreciated the methodological approach (integrating beam search with logistic regression, diverse feature selection, and star search for choosing integer coefficients), and noted that the stand-alone Python implementation is also advantageous over competitors that rely on mathematical programming solvers. I am pleased to recommend acceptance of this practically relevant work.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "P7ujMZLqBH",
                "writer": "official_reviewer",
                "reply_to": "ZoARRwGhvpk",
                "title": "Thank you for addressing my concerns and questions",
                "comment": " I believe this work is a valuable addition to the literature of scoring systems.\n\nOne minor suggestion: In the response, the authors mentioned that \"FasterRisk solves the optimization automatically and engages the user only in selecting the best model.\" While it is important to automate the learning process, it is also crucial to note that some automatically selected variables may not make sense to clinicians. Ideally, domain knowledge should be integrated into the scoring system as early as possible. Moreover, the \"best model\" might not be the most suitable model/solution in clinical practice. While FasterRisk has good potential in clinical applications, the authors are suggested to expand their discussions a little bit to reflect the importance of domain knowledge and actual clinical needs in implementations.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QYkOr8haGm",
                "writer": "author",
                "reply_to": "1m7u2GpEG6b",
                "title": "Response to Reviewer ntN1; Newly Requested Time Plot Is in Appendix G.7",
                "comment": " We thank Reviewer ntN1 for the comments.\n\nYes, *all* comments from *all* reviewers will be incorporated into the main manuscript. We haven\u2019t done it yet for two reasons: (1) we didn\u2019t have enough time since we were asked to perform a huge number of experiments during the short rebuttal phase and we didn't have enough time to crush it all carefully into NeurIPS' 9 page limit, and (2) we thought it would be helpful for reviewers if we kept the original main paper and appendix while providing only a new Section G titled ''Reviewer-requested Extra Experiments and Discussion'' (Page 43-69). The reason for this arrangement is that we thought the reviewers might want to compare the originally submitted results and newly requested results in Section G. \n\nWith regards to the remaining concern on running time in the last reply, please see a new plot in Figure 40 under Appendix G.7. We think the replacement of Figure 4 with the new ones we propose in the appendix should do it. Just to recap: we have included new time results (time limit is 1h) in Figure 40 in Section G.7. Figure 4 and 7 in the original submission already shows that FasterRisk can finish running at most 5 minutes while RiskSLIM runs on most folds and datasets for 15 minutes (which was the original time limit) without finishing. In Figure 40 (page 70 in the appendix), we raised the time limit for RiskSLIM up to 1 hour. Most of the folds and datasets still do not finish. Therefore, using the 1-hour results of RiskSLIM show that FasterRisk has an even more impressive speed advantage. We also discussed in our earlier rebuttal about the necessity of faster run times - interactions with humans should be able to be done in real-time, not waiting over an hour between runs.\n\nWe believe all your technical comments were addressed in our rebuttal and in the new Appendix G for reviewers. Again, we will definitely incorporate all your revision comments in the final version if this submission gets accepted. We understand that you may want the revision writing and plotting to be done now, but this new comment is posted on August 8, and the deadline for author-reviewer discussion is August 9. It is very challenging to incorporate all these new writing and plotting into the main paper within a day without exceeding the 9-page limit required by NeurIPS. \n\nAgain thank you for your review, and for helping us to improve the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1m7u2GpEG6b",
                "writer": "official_reviewer",
                "reply_to": "tilqa8PSArB",
                "title": "Re: Response to Reviewer ntN1",
                "comment": " Thanks for taking the time to thoughtfully answer my questions and address some of my concerns!   \n\nI have some remaining concerns, and perhaps there have been some misunderstandings in terms of my original questions.  One of my main questions was about Figure 4, and how it was cut off at 15 minutes, so it's difficult to interpret how the runtime of RiskSLIM compares with FasterRisk. While it's nice to see a newer version of Figure 3 in Appendix G1.1 and G1.2 with a longer runtime, my concern about the interpretability of Figure 4 remains, and in the final version, it would be great to see a version of Figure 4 with a much higher y-axis limit. \n\nWhile I appreciate you answering the questions below, I'd like to see that some of these answers are actually incorporated into a published version of your paper. In particular, for the final version, I'd love to see revisions based on your comments:\n- 1: why speed is important\n- 4: # rows/columns  in datasets incorporated into the **MAIN** text, e.g. as part of figure 4, since readers shouldn't need to dig around in your appendix for this basic information\n- 6: a comment that interpretability is a general feature of risk scores and not just your method\n- 8: why it's useful to have a pool of solutions, and how you would expect them to be used in practice\n- 9: ethical considerations (possibly in the discussion section, this could perhaps be combined with 6 and 8)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "15PuFVq0qSm",
                "writer": "author",
                "reply_to": "i-i0eHnRLJ",
                "title": "Response to Reviewer DdPk; All Requested New Experiments Are in Appendix G",
                "comment": " 1. $\\textbf{Are real scores considered in the current contribution?}$ By real scores, do you mean continuous coefficients? We want integer coefficients. Creating sparse continuous solutions as in Section 3.1 and 3.2 are intermediate milestones towards this goal. So we do this too.\n    \n2. $\\textbf{Are there any possible large error accumulation?}$ Not according to our experiments. We don't lose error at any step so error doesn't accumulate.\n    \n3. $\\textbf{How to understand the multiplier $m$?}$ When we do StarRaySearch in Algorithm 3, we shrink the feature matrix by $m$ and $\\textit{multiply}$ the coefficients by $m$ (therefore the name ''multiplier''). When we calculate the risk probability, because the scores are based on the original features instead of shrunk features, we have to divide the total score by this multiplier $m$ to get the right probability.\n    \n4. $\\textbf{Can you comment on swapping one feature at a time is computationally efficient?}$ Actually, swapping one feature for another is efficient. If you consider swapping 2 features, there are ${k \\choose 2}$ choices, where $k$ is the number of nonzero features. 3 features requires you to choose among $\\binom{k}{3}$ features. It is $\\textit{much}$ computationally easier just to enumerate $k$ options for swapping.\n    \n5. $\\textbf{What is the range of multiplier?}$ This has already been specified in Line 2 in Algorithm 3. We re-state here in words: if all coefficients have magnitude all less than 5, we want to stretch the coefficients with a multiplier. The largest value for the multiplier is $m_{max} = 5/max |w|$ because we want the coefficients to stay within the box constraints. The smallest multiplier is $m_{min}=1$. If some coefficients are already on the boundary of the box constraints (either +5 or -5), we explore shrinking the coefficients by a multiplier. We choose the smallest multiplier to be $m_{min}=0.5$ and the largest multiplier to be $m_{max}=1$. We pick $N_m$ (default value is 20) equally spaced multiplier values from the interval $[m_{min}, m_{max}]$. We have done perturbation study on this hyperparameter $N_m$ during rebuttal. If you are interested, please go to Appendix G4.4 to see the results.\n    \n6. $\\textbf{How many intermediate pool models did you generate?}$ At most 50. See Appendix D4: Hyperparameter Specification.\n\nThank you once again for your review!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZoARRwGhvpk",
                "writer": "author",
                "reply_to": "x2z82euYSrP",
                "title": "Response to Reviewer aCoP; All Requested New Experiments Are in Appendix G",
                "comment": " 1. $\\textbf{Is the competitor RiskSLIM being slow concerning in practical applications?}$ Yes! Speed is very important in these settings: (1) When we cannot compute the answer at all using the slow method because it does not scale to reasonably-sized datasets. It could take over a week to compute the solution for even reasonably small datasets. (2) Interactive machine learning. Machine learning in the wild is essentially never a single run of an algorithm. Often times, the users want to explore the data and adjust various constraints along the way as they get more familiar with possible models. Fast speed allows users to go through this iteration process many times without interruptions (of several days perhaps) for the algorithm to run. This is where FasterRisk will be very useful in high stakes offline settings. This is because after the pool of models is generated within 5 minutes, interacting with the pool is essentially instantaneous, allowing users to interact with it. Please see  Appendix G1 (in the new appendix for reviewers, Appendix G).\n    \n2. $\\textbf{Are there datasets where our method FasterRisk has significant performance (solution quality) advantage?}$ Yes! Please see Figure 5 (Appendix E1) and Figure 6 (Appendix E2). These three datasets have high dimensional features and feature correlation is also very high (See Table 2 in Appendix D1 for data set information), which make them challenging. Our method FasterRisk significantly outperforms other baselines in AUC. Additionally, please look at Figure 11 in Appendix E5 and Figures 18-21 in Appendix G1 for the logistic loss curves on the training set. FasterRisk achieves much lower logistic loss than RiskSLIM.\n    \n3. $\\textbf{Can you clarify some mathematical notations?}$\n\n    3.1. $\\epsilon_{w^*}$ in Eq 4 is a user-defined value for the gap tolerance level corresponding to the optimal solution $w^*$. If we set $\\epsilon_{w^*}$ too close to 0, we may not find any solutions (since it is NP-hard to find the optimal solution); if we set $\\epsilon_{w^*}$ too large, we could find too many solutions to evaluate and some solutions are not as good quality.\n\n    3.2. $\\epsilon_{t}$ in Eq 5 is any arbitrary value that makes Eq 5 hold. We can think of $\\epsilon_{t}$ as the loss difference between the rounded integer solution $\\{w^{+t}, w_0^{+t}\\}$ and original continuous solution $\\{w^{t}, w_0^{t}\\}$.\n\n    3.3. In Eq 6, $d$ is the coefficient on coordinate $j$. $\\textbf{e}$${}_j$ is a unit vector with 1 on coordinate $j$ and 0 on other coordinates.\n\n    3.4. In Eq 7, $c_{d_j}$ is already defined in Line 182. It is the coefficient on coordiante $j$ after one step of coordinate descent.\n\n    3.5. In Eq 8, you can think of $a$ and $b$ as coefficients and intercept. We try to avoid using $w, w_0$ here because they appear on the left.\n\n    3.6. In Eq 9, you can think of $a$ as the coefficient on coordinate $j$.\n    \n4. $\\textbf{Can we compare with AutoScore?}$ Thank you for providing this paper. We have cited this in Line 394. Please see Figure 25-27 in Appendix G3 for the comparison between AutoScore, RiskSLIM, and FasterRisk. FasterRisk outperforms AutoScore in all cases.\n    \n    We are solving a more challenging optimization problem than AutoScore because AutoScore does not impose the box constraints on the coefficients during feature selection. We want scores to be small (in one digit) so that ordinary people can add/subtract numbers in their heads.\n    \n    The philosophy of these two approaches is different. AutoScore is designed to involve the user in the optimization process. In contrast, FasterRisk solves the optimization automatically and engages the user only in selecting the best model. Our method FasterRisk is complementary to AutoScore's random forest-based feature selection approach; one can apply FasterRisk for feature selection by using our beam search method. Again, please see Figure 25-27 in Appendix G3 for the comparison between AutoScore, RiskSLIM, and FasterRisk.\n    \n5. $\\textbf{Can we show results for other $\\epsilon$ values for the diverse pool?}$ Please see Figure 31-33 in Appendix G4.2.\n    \n6. $\\textbf{Can we show other empirical results regarding change in other hyperparameters?}$ In Figure 28-30 in Appendix G4.1, we changed the beam size hyperparameter; in Figure 34-36 in Appendix G4.3, we changed the number of attempts hyperparameter; in Figure 37-39 in Appendix G4.4, we changed the number of multipliers hyperparameter.\n    \n7. $\\textbf{Can we show examples from the pool of solutions?}$ Yes! Please see Tables 30-41 in Appendix G5 for 12 models from the pool.\n    \n8. $\\textbf{Add to the Checklist. Is there insufficient discussion of fairness?}$ Good point. We added a discussion in the checklist to say that even if a model is interpretable, it could still have negative societal bias, and looking at a variety of models from the pool could help find models that are more fair.\n\nThank you so much for your review!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fnmXG5_uBu",
                "writer": "author",
                "reply_to": "X3fZeJvDzA7",
                "title": "Response to Reviewer ifPK; All Requested New Experiments Are in Appendix G",
                "comment": " 1. $\\textbf{Examples of scoring systems in the Introduction are out of date.}$ Indeed, we used a lot of historical examples including many established and popular medical scoring systems. We added a few more recent ones. See Line 42 for new citations, including several scoring systems for COVID-19 patients.\n    \n2. $\\textbf{Can we discuss differences with traditional framework of the scoring system?}$ We have discussed this in Section 2 Related Work. \n    \n3. $\\textbf{Can we provide more discussion/experiments on efficiency?}$ Yes! Please see the experiments and discussions in the new Appendix G1.\n    \n4. $\\textbf{Is performance improvement significant?}$ The timing improvement is orders of magnitude smaller, which is the most important. RiskSLIM is guaranteed to produce optimal solutions (with respect to its search space) eventually, so we don't expect to always achieve a performance improvement with respect to it. However, since our search space is larger, we often see a performance improvement that is significant, especially on the three extra datasets in the Appendix. Please see Figure 5 (Appendix E1) and Figure 6 (Appendix E2). Additionally, please look at Figure 11 in Appendix E5 and Figures 18-21 in Appendix G1 for the logistic loss curves on the training set. FasterRisk achieves better logistic losses than RiskSLIM.\n    \n5. $\\textbf{Can we provide calibration curves?}$ Yes! Please see Figure 22-24 in Appendix G2.\n    \n6. $\\textbf{Can we prove the feasibility of the generated scoring system?}$ All scoring systems satisfy the constraints and are thus feasible. Sorry, perhaps we don't understand this question. Do you mind elaborating or paraphrasing the question?\n\nThank you so much for your review!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tilqa8PSArB",
                "writer": "author",
                "reply_to": "hDYcngHdlgq",
                "title": "Response to Reviewer ntN1; All Requested New Experiments Are in Appendix G",
                "comment": " 1. $\\textbf{Why is speed improvement important?}$ Speed is very important in these settings: (1) when we cannot compute the answer at all using the slow method because it does not scale to reasonably-sized datasets. It could take over a week to compute the solution for even reasonably small datasets. (2) Interactive design of models. Machine learning in the wild is generally never a single run of an algorithm. Often times, the users want to explore the data and adjust various constraints along the way as they get more familiar with possible models. Fast speed allows users to go through this iteration process many times without interruptions (of several days perhaps) for the algorithm to run. This is where FasterRisk will be very useful in high stakes offline settings. This is because after the pool of models is generated within 5 minutes, interacting with the pool is essentially instantaneous, allowing users to interact with it.\n    \n2. $\\textbf{Are there datasets where there are some significant differences (beyond 2 percent) in AUC between FasterRisk and alternative methods?}$ Please see Figure 5 (Appendix E1) and Figure 6 (Appendix E2). These three datasets have high dimensional features and feature correlation is also very high (See Table 2 in Appendix D1 for data set information), which makes them challenging. Our method FasterRisk significantly outperforms other baselines in AUC. Additionally, please look at Figure 11 in Appendix E5 and Figures 18-21 in Appendix G1 for the logistic loss curves on the training set. FasterRisk is doing a much better optimization job than RiskSLIM because it can use a larger search space.\n    \n3. $\\textbf{What happens when we run the baseline RiskSlim longer?}$ We added an experiment in Figures 18-21 in Appendix G1.1 and G1.2. When we run RiskSLIM for 1 hour, FasterRisk still outperforms RiskSLIM; when we run RiskSLIM longer for 4 days, FasterRisk outperforms RiskSLIM on 7 out of 9 datasets except on Mushroom and Spambase. (In the two remaining cases, the results are essentially tied.) \n    \n4. $\\textbf{Can we share the number of rows and columns in each data set?}$ Yes, the dataset information is already shown in Table 2 of Appendix D1.\n    \n5. $\\textbf{How were scoring system examples selected in section 4.3?}$ They were selected from the diverse pool based on the smallest logistic loss on the training set. It is in exactly the same manner as detailed in Line 253-254 for the pooled-PLR baselines.\n    \n6. $\\textbf{Is interpretability unique to our approach?}$ No, all methods that produce risk scores are interpretable. This includes our approach and the baselines we compared with. \n    \n7. $\\textbf{Can I see pool of solutions?}$ Sure! We have included several large tables in Appendix G5 in Table 30-41 of 12 models from the pool.\n    \n8. $\\textbf{Do we expect users to look through all solutions in a pool or only the best one with the lowest error?}$ Ideally a user-interface would help guide the user from the lowest-error model to one that would best suit their needs. Error is generally not the only criteria users would consider when deciding to implement a model. Our contribution here is just to design the algorithm for finding these models, but we plan to develop this interface next. The user could also simply rank the models from the smallest to the largest error and choose one.\n    \n9. $\\textbf{Why do we answer ``N/A'' for the checklist question about any negative societal impacts?}$ The reviewer makes an excellent point. We have edited this in the checklist to say that even if a model is interpretable, it can still have negative societal bias (though it is easier to check for such biases with scoring systems), and looking at a variety of models from the pool could help find models that are more fair. We placed this information into Appendix G, which is the new appendix for reviewers.\n\nThank you so much for your review!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hDYcngHdlgq",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_xTYL1J6Xt-z",
                "title": "",
                "comment": " The authors propose a method for efficiently automatically generating a pool of \u201crisk scores\u201d (sparse linear models with integer coefficients), involving (1) a beam search algorithm to identify a sparse set of features, (2) given the original set of features, identify a pool of sparse solutions with similar performance (but \u201cdiverse\u201d set of features), and (3) \u201cstar ray\u201d search to choose integer coefficients. They evaluate both the speed and accuracy of their approach on multiple benchmark datasets.  The authors describe an interesting method for quickly identifying risk scores in a diverse range of settings. The main improvement seems to be speed (which wasn\u2019t very well quantified with respect to baselines), since performance-wise  it was similar to a previous approach \u2013 and I wonder how useful this speed would actually be in this high-stakes offline setting. \n\nOriginality/clarity: this seems to be a creative combination of past algorithms, and they described their algorithms in detail\n\nQuality/significance: the authors evaluated their methods and baselines along both accuracy and speed metrics, and also shared extensive additional experiments in their supplement. One aspect that seemed problematic was that for speed plots, they cut off algorithms after 15 minutes, and it\u2019s unclear exactly how their baselines scale because they tend to be censored after just one or two points along the x-axis. It would also be helpful to see if quantitatively, there are any significant differences in AUC between FasterRisk and alternative methods. Based on these two results, it would be easier to assess whether their method provides a meaningful contribution to real-world use cases of risk scores.\n\nThe authors also do not describe any considerations of societal impact which is an important factor given their suggested use cases. \n - You mention  (line 54)  \u201cWe need an approach that exhibits the best of both worlds: speed fast enough to operate in a few  minutes on a laptop and optimization and search capability as powerful as that of the mathematical programming tools. Our method, FasterRisk, lies at this intersection.\u201d \u2014 Of course, it makes sense to have a goal of developing an accurate model, but I\u2019m wondering why it is important that risk scores must operate in a few minutes on a laptop. If these are risk scores for a high-impact situation, why can\u2019t we run an analysis for an hour? Or even a week? \n\n- Figure 4:   I find this figure a bit frustrating, because it\u2019s not really allowing me to see how the baseline method scales compared to yours.  A 15 minute time-out seems kind of silly/arbitrary (in the real world, I\u2019d expect people to be willing to train their methods for quite a while if it\u2019s for a high stakes setting), and I would highly recommend allowing your baselines to run for longer (at least several hours) to show how the times actually scale (and then possibly display with a log scale as needed). It would also be helpful to share the number of rows and columns in each of the datasets to give a sense of scale. \n\n- Section 4.3 Example scoring systems: how were these examples selected?   Also, you mention that risk scores offer interpretability \u2013 is this something that you argue is unique to your approach and not your baselines?\n\n- Your algorithm is often described to return a \u201cpool\u201d of solutions, but I didn\u2019t see much discussion of what  that pool actually looks like. For example, I would want to see how many solutions were produced, how different they are from each other, etc.   In a use case, would you expect the user to look through all of them and then choose one, or just defer to the lowest-error one?\n The authors describe some limitations related to their algorithms, which is appreciated. However, they describe the potential negative societal impacts as \u201c[N/A]\u201d which seems like a huge oversight to me. As they say, \u201c[risk scores] are possibly the most popular form of predictive model for high stakes decisions through the last century and are the standard form of model used in criminal justice  and medicine,\u201d it seems obvious that any contribution they make to this field could have serious societal impacts, for better or worse.\n\nHere\u2019s an example of how this approach could be used problematically:  Let\u2019s say we have a criminal justice scenario in which we have access to race and some other features that are essentially proxies for race (e.g., zip code). \tNow let\u2019s assume a user is given a pool of \u201cdiverse\u201d solutions by the FasterRisk approach, and they know that they don\u2019t want a model that\u2019s \u201cracist\u201d. They may notice one risk score has a highest accuracy but relies on race, so they decide they shouldn\u2019t use that model. They then notice an almost identical model that has all the same features except race has been replaced by zip code, and they choose this model instead and deploy it in some real world scenario (e.g., recommending whether someone should be placed on parole).\n",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "X3fZeJvDzA7",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_xTYL1J6Xt-z",
                "title": "",
                "comment": " This study proposed a novel method to accurately and efficiently generate a collection of high-quality risk scores based on the integration of the beam-search-based algorithm for LR, the generation of diverse high-quality solutions with different support sets, and the star search for integer solutions. It achieved SOTA performance with less time cost in some datasets.  Strengths:\n1.\tThe proposed three-step framework includes a beam-search-based algorithm for logistic regression with box constraints and L0 regularization, the search algorithm to collect the sparse diverse pool with different support set, and the star search technique using multipliers, and a theorem guaranteeing the quality of the star search results. The whole methodology was solid and efficient. \n2.\tThe introductions of the research context and related work were well-organized and clear. \n3.\tThe proposed method achieved the SOTA performance with significantly less time (as shown in figure 4), showing its reliability and efficiency. \n4.\tTheir theoretical discussion and supplement material were abundant, Moreover, they also conducted extensive experiments on performance, including performance comparison, efficiency, and ablation experiments. \n\nWeaknesses:\n1.\tThe examples of scoring systems in the Introduction seem out of date, there are many newer and recognized clinical scoring systems. It also should briefly introduce the traditional framework of the scoring system and its difference in methodology and performance with the proposed method. \n2.\tAs shown in figure 3, the performance improvement of proposed methods seems not so significant, the biggest improvement in the bank dataset was ~0.02. Additionally, using some tables to directly show the key improvements may be more intuitive and detailed. \n3.\tAlthough extensive experiments and discussion on performance, in my opinion, its most significant improvement would be efficiency, and there are few discussions or ablation experiments on efficiency. \n4.\tThe model AUC can assess the model discriminant ability, i.e., the probability of a positive case is bigger than that of a negative case, but may be hard to show its consistency between predicted score and actual risk. However, this consistency may be more crucial to the clinical scoring system (differentiated with classification task). Therefore, the related studies are encouraged to conduct calibration curves to show the agreement. It would be better to prove the feasibility of the generated scoring system?  The difference between the traditional method and our method can also be discussed in this paper.  \nNone None",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "x2z82euYSrP",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_xTYL1J6Xt-z",
                "title": "",
                "comment": " This paper aims to provide a fast algorithm to derive sparse risk scores that scales to high-dimensional datasets. The authors identified a few major limitations in current methods, and described how these were addressed by the three components in their proposed algorithm. In several experiments with low and high dimension data, the authors showed that their method outperformed the current state-of-the-art and several other baseline methods. The algorithm is implemented in stand-alone Python code, which is advantageous over competitors that rely on mathematical programming solvers. This paper describes the authors\u2019 original work to resolve several methodological difficulties in current development of risk scores. The writing is clear in defining several major challenges the authors aimed to address, and the proposed algorithm consists of separate components to address them. In addition to evaluating the algorithm with respect to baselines, the authors also showed the importance of each component by assessing the reduction in performance without them.\n\nMy major concern is the scope of this study, which affects its quality and significance. The authors aim to develop a fast and well-performing algorithm, FasterRisk, to develop sparse risk scores, which, if successful, would be very useful to healthcare applications. But when discussing related work, the authors did not include some recent works that have partially addressed some of the limitations the authors proposed to address (elaborated in Question 1 below). When evaluating FasterRisk, there is practically only one competitor algorithm, and FasterRisk only had marginal advantage in performance (in most experiments). FasterRisk is indeed much faster than the competitor, but by timing out the run time at 15 minutes, I am not convinced that the competitor is slow enough to be concerning in practice.\n\nThe clarity in mathematics notations can be improved. The equations became difficult to follow when the author used some notations without introducing them. For example, $\\{epsilon}_w$ in equation (4) and $\\{epsilon}_t$ in equation (5) lack bounds, and it is difficult to understand what the arbitrary a, b, c, d, e in equations (6)-(9) stand for. These affect my trust in the work.\n 1. The authors focused on developing scores by finding integer sparse solutions, and showed some advantages of FasterRisk over the state-of-the-art. But I find the discussion of alternative approaches inadequate, therefore I could not fully appreciate the contribution of this work. For example, the authors pointed out two major limitations of building scores by rounding logistic regression coefficients: (i) $l_1$ and $l_0$ regularizations not able to get sparse solutions, and (ii) rounding of coefficients worsens performance by making the scores too coarse. But (i) may be resolved by using alternative variable selection methods and (ii) by using smallest non-zero coefficient to scale all coefficients and then rounding to larger integer values (e.g., total score ranging from 0 to 100). For example, a 2020 paper (https://doi.org/10.2196/21798) describes such an alternative approach that worked reasonably well in several clinical applications, and by separating variable selection from score development, domain experts are more easily engaged in the development process to ensure clinical meaningfulness and fairness. I find this paper lacking in discussion on this general approach. Could the authors include such more recent works in their discussion and method evaluation?\n\n2. The authors stated in appendix that the choice of hyperparameters does not have much impact on performance, but did not provide empirical evidence. I am particularly concerned with the choice of tolerance gap level $\\{epsilon}=0.3$ (equation (9)), meaning we are willing to tolerate up to 30% increase in loss when expanding to \u201cequally good\u201d scores. Without detailed explanation, 30% seems too large to me. Can the authors justify their choice empirically or by citing related literature? I would also like to see some empirical results regarding change in other hyperparameters.\n\n3. Although the authors generated a pool of \u201cequally good\u201d scores, they did not seem to make use of them other than selecting the best-performing one to report. This pool of scores could be useful for users to select well-performing AND fair scores. This is related to Limitations below. Fairness of scores developed from the proposed algorithm is not adequately discussed. Related to Question 3 above, a na\u00efve application of the proposed method may lead to unfair risk scores. For example, in Table 3 of Appendix F, the method generated a 3-variable risk score to predict salary>30K using education level and marital status, and this direct link of being married with salary level is highly debatable. Marital status might represent a mixed effect of age and socio-economic status, and it may be better to use the latter in the score for more meaningful interpretation. Since the authors have generated a pool of \u201cequally good\u201d scores that make use of alternative predictors in a step of the algorithm, I suggest the authors make use of this pool to help users balance performance and fairness.",
                "rating": 7,
                "confidence": 5
            },
            {
                "review_id": "i-i0eHnRLJ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_xTYL1J6Xt-z",
                "title": "",
                "comment": " The paper is focused on risk scores learning which are simple but efficient (in terms of performance) models. The main idea is to produce a pool of almost-optimal sparse continuous solutions with different support sets using a beam-search algorithm. Each of these solutions is explored: the real-values models are transformed into feasible integer-valued solutions along multipliers (what allows for a large space of possible solutions). The method is computationally efficient.  Strengths. The paper clearly describes a novel three step framework to learn simple interpretable models. The numerical results are convincing. \n\nWeaknesses. The method has three separate steps what can lead to some inconveniences (some kind of error cumulation is possible; coordinate descent can be long as well as the line search).  \nAs also mentioned by the authors, real scores are not considered in the current contribution. It seems that m is defined quite late in text, and it is not clear from the beginning of Section 3 that it is the multiplier. Why did you decide to divide by m and not to multiply (if it is a multiplier)?\n\nIn Section 3.2. it is mentioned that \"swapping one feature at a time is computationally efficient\". I would say rather not efficient, if there are a lot of features. \n\nI am not sure whether Section 4.3. is necessary. It underlines that there are not any results on real scores in the current submission. I guess the example provided in the Introduction (and Appendix) is enough. \n\nI am curious to know what the range of m (multiplier) values was in your experiments?\nHow many intermediate (pool) models did you generate? And what is the percentage of reasonable final (integer) models?\n The authors provide the limitations on page 3. ",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This paper",
                "Sentiment Expression": "makes a valuable contribution to the scoring model literature, providing a fast and scalable algorithm to derive sparse risk scores.",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "_idcJrecij": {
        "paper_id": "nips_2021__idcJrecij",
        "paper_title": "Arbitrary Conditional Distributions with Energy",
        "paper_abstract": "Ryan Strauss, Junier B. Oliva",
        "paper_acceptance": "accept",
        "meta_review": "This paper proposes an approach for performing conditional density estimation of the form $p(x_u|x_o)$ for any arbitrary choice of the observed ($x_o$) and unobserved ($x_u$) variables by using an energy function parametrized by a neural network to estimate the 1D distributions.\n\nThree reviewers recommend acceptance, and one reviewer believes the paper is below the acceptance threshold. After reading the reviews and the rebuttal, I recommend this paper for acceptance. I also encourage the authors to improve the paper by considering the reviewers' comments. In particular:\n+ Add an analysis of the number of samples needed for the importance sampling approximations.\n+ As noted by reviewer AEG1, the imputation method may not be appropriate for certain applications because the imputation is performed using the marginal means, e.g., the means of $p(X_2|X_1=x_1)$ and $p(X_3|X_1=x_1)$, therefore, ignoring the dependences between $X_2$ and $X_3$. This should be mentioned/discussed in Section 4.4.2 or 5.1.2. \n+ Add a sentence motivating importance sampling over the grid.\n+ Add some discussion on the skip-connection and multiple latent vectors.\n+ Include a derivation or discussion of the stochastic approximation of the autoregressive score in the Appendix.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "eV1vys059f",
                "writer": "official_reviewer",
                "reply_to": "9jeknTbpmjB",
                "title": "Thanks!",
                "comment": " Thanks for your helpful comments.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KzCzQ77yMcP",
                "writer": "author",
                "reply_to": "9i7V0ffZnQW",
                "title": "Reply to reviewer's response",
                "comment": " The non-negativity requirement is placed on $\\mathcal{E}(x)$, the energy. The unnormalized likelihood, $e^{-\\mathcal{E}(x)}$, is therefore a value between 0 and 1. You are correct that we have no loss of generality though, due to the normalizing constant. This is a typical setup when using energy-based models to define probability distributions (see LeCun et al., A Tutorial on Energy-Based Learning, Equation 2). Thank you for clarifying your question about the normalizing flows. In theory, there is no inherent restriction on the type of distribution that can be represented with an appropriate change of variables and base distribution. In practice, however, flow models are limited to the invertible and efficient (for computing Jacobian determinants) transformations that are possible with neural networks (e.g. affine coupling). These limited transformations may not always be able to express the (potentially complicated) change of variables required given a base distribution (e.g. a standard Gaussian).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uYFyi5hBuUl",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__idcJrecij",
                "title": "",
                "comment": "The authors propose a method for performing conditional density estimation through estimating an energy function and using importance weighting to normalize the density estimate. They focus on estimating single-dimensional distribution and make use of the product rule to estimate multivariate distributions. The authors show the method is empirically promising on data imputation tasks.  ## Summary of review\n\nThe authors approach seems reasonably well-motivated, and appears to empirically perform well. I am somewhat concerned by the heavy reliance on importance sampling to estimate intermediate quantities, and then taking products of/dividing by the results. I think it would be useful if the authors provided some numerical verification (perhaps in a simple case) that the estimates used are in fact reliable. I think the authors perhaps slightly over-emphasize the approach being order-agnostic, as this is only true during training, and as applied the model may depend heavily on ordering. I didn't see an experiment showing the degree to which estimated densities depend on order in the appendix, and this might be worth illustrating when the authors discuss strategies to mitigate this dependence. However, the authors are upfront in acknowledging that order dpendence at test time is a limitation at several points in the paper, and acknowledge there is more work to be done in this regard. Overall, I thought the paper proposed an interesting and reasonably simple method, and was enjoyable to read. \n\n## Questions for authors\n- In the related work section, the authors claim they construct an 'order-agnostic weight-sharing technique'. However, the final conditional density estimates depend on an arbitrary ordering, as later acknowledged. What is the advantage of the training process not depending on choosing an ordering if applying the model still has this constraint?\n- In line 114 the authors say the *energy* is non-negative. Later the energy is taken to be $\\mathcal{E}$, which is the output of a neural netrowk if I understand correctly. Is it actually intended for $\\mathcal{E}$ to be non-negative, is this a typo, or have I misunderstood?\n- The authors justify the importance sampling estimator by the claim that importance sampling is effective in 1D. However, it seems the authors need to take products of the estimated normalizing constants in order to estimate the overall density. Does this not lead to an accumulation of errors as the dimension of the distribution you are trying to estimate increases? \n- The authors claim that normalizing flows place many restrictions on the types of distrbutions that can be modelled. Is this true even when one-dimensional distributions are estimated with the flow? Can you give concrete examples of what cannot be modelled/is difficult to model with flows?\n- What is the effect of ignoring derivatives with respect to the samples from the proposal distribution and density in 8? Is the estimate of the graident still consistent (as the number of proposal samples goes to infinity)? Is there an effect on the variance of the estimator?\n- In Equation 9 how many samples must be used in practice to obtain an accurare estimate of the mean? ## Limitations\nThe authors point to several limitations of the model considered. For example:\n- missing information is modelled as uninformative, which is not a good assumption in some cases\n- Estimation of the density of a joint distribution may depend on the order in which the product rule is applied (i.e. the network used to model conditional densities may not be consistent).\n\n## Societal implications\nThe authors do not point to societal implications of their work. As the proposed method might be applicable to a wide range of tasks, this seems reasonable. Any societal implications would depend signficantly on which tasks the proposed method is applied to, and it is difficult to assess what these might be.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "9i7V0ffZnQW",
                "writer": "official_reviewer",
                "reply_to": "6lnVtwCbozy",
                "title": "Reply to author's response",
                "comment": " Thank you for your reply; it has addressed my several of my concerns and I will raise my score from a 6 to a 7 accordingly. A few other notes regarding your reply:\n- Why must the energy function be non-negative? The non-negativity requirement seems to be on $e^{-\\mathcal{E}(x)}$; restricting $\\mathcal{E}(x)$ to be non-negative seems to ensure that the unnormalized pdf has values less than $1$. However, perhaps this results in no loss of generality because of the normalizing constant? This point doesn't seem particularly important, but if the authors can clarify, it would be helpful.\n- Even if the errors were in fact independent and zero mean, we would the sum of the errors to grow (just at a slower rate). In particular, the sum of $n$ iid variables with standard deviation $1$ is $\\sqrt{n}$ (it is the mean of these random variables which has a standard deviation tending to $0$). However, your argument that the errors at least may not grow quickly seems reasonable, if perhaps speculative in terms of strong assumptions about the nature of the errors.\n- Regarding normalizing flows: I understand that there are limitations on the parameterization of the network that accompany the use of normalizing flows. I intended to ask about whether this implies limitations on the distributions implied by base distribution and network. I am not aware of what the existing results in this area are, but perhaps it would be worth checking the literature to see if the claim that normalizing flows limit the flexibility of the implied distribution can be substantiated more precisely.\n- Regarding importance sampling: Thank you for addressing my point, as at least one other reviewer (point 2a of review ENb9) asked about the efficacy of IS in this application and if sufficient samples were used, please consider including experimental evidence of how many samples are needed in a subsequent version of the paper. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zjy3VUm1ItI",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__idcJrecij",
                "title": "",
                "comment": "This paper introduces Arbitrary Conditioning with Energy (ACE) to estimate arbitrary conditional densities, using a Boltzmann distribution with an energy function parametrized by a neural network to estimate 1-dimensional conditional densities. An intractable normalizing constant is estimated with importance sampling and a proposal network. The method is then demonstrated on a few examples for density estimation under missingness and for imputation.   Strengths:\n\nThe paper is clearly written, and I find the setup interesting and useful especially under missing data. Only requiring the estimation of 1-dimensional densities is also easier to understand and computationally appealing. The work is an interesting combination of the setup of ACFlow [Li et al., 2020] and the method of Autoregressive Energy Machines (AEM) [Nash \\& Durkan, 2019]. The experimental results also improve on ACFlow for density estimation, whilst being simpler.\n\n#####################################################################\n\nWeaknesses/Questions:\n\nI summarize the main weaknesses/questions below:\n\n1.) I am not completely familiar with the energy function literature, but are there any guarantees that the integral $Z = \\int \\exp\\left(-\\mathcal{E} (\\mathbf{x}) \\right) d\\mathbf{x}$ is finite? I do not see how a randomly-initialized network necessarily satisfies this property. \n\n2.) Although the idea is simple, I find the actual implementation of ACE to be potentially over-complicated:\n\na.) As we are only in 1 dimension, is there any issue with using numerical quadrature instead of importance sampling? The proposal adds a lot of complexity to training, and I'm not sure if 20 importance samples (as used in the paper) is more accurate than just estimating $Z$ on a small grid.\n\nb.) I could be missing something, but why does the energy network need the inputs $(\\boldsymbol{\\phi}(\\mathbf{x_o}; \\mathbf{b}),\\mathbf{b})$ in addition to the latent vector $\\boldsymbol{\\gamma}(u_i; \\mathbf{x_o})$? Would the latent vector not suffice, as is done for AEMs?\n\nc.) Why does the proposal network output multiple latent vectors, that is one for each $u_i$? If it contains the information in $\\mathbf{x_{o}}$, then it seems better motivated to just use the same context vector for each $u_i$. \n\n\n3.) What is the motivation for training non-autoregressively, that is with \n$ \\sum_{i=1}^{|u|} \\log p(x_{u_i}\\mid \\mathbf{x_o}) $ instead of $\\sum_{i=1}^{|u|}\\log p(x_{u_i} \\mid \\mathbf{x_{o \\cup u_{<i}}})$?  It seems that $\\log p(\\mathbf{x}_{\\mathbf{u}} \\mid \\mathbf{x}_\\mathbf{o})$ is better motivated, since that is being evaluated in the experiments. Also, is $\\mathbf{o}$ selected at random for each gradient step? It wasn't clear to me from the paper. \n\n#####################################################################\n\nOverall:\n\nI think the idea is good and has potential. However, some of the design choices seem over-complicated given the simple motivation, and either need to be simplified or justified better.  \n\n#####################################################################\n\nReferences:\n\nLi, Y., Akbar, S., \\& Oliva, J. (2020, November). ACFlow: Flow models for arbitrary conditional likelihoods. In International Conference on Machine Learning (pp. 5831-5841). PMLR.\n\nNash, C., \\& Durkan, C. (2019, May). Autoregressive energy machines. In International Conference on Machine Learning (pp. 1735-1744). PMLR. The authors have described some of the limitations of their method, such as the computational cost of averaging over multiple orderings of the dimensions. ",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "Cd_yC6LI0JY",
                "writer": "official_reviewer",
                "reply_to": "exrOCywaz7c",
                "title": "Reply to rebuttal",
                "comment": " Thank you for the clear and detailed reply. My concerns have been addressed, so I will be raising my score by 1 point. \n\nI just have a few recommendations for the revision below:\n\n- I would recommend adding a sentence motivating importance sampling over the grid. In particular I liked the point about how simulating from the proposal is very cheap, so we save on the number of neural network passes overall.\n\n- Some discussion on the skip-connection and multiple latent vectors would be useful motivation for readers who are familiar with AEMs (1 or 2 sentences should suffice).\n\n- The stochastic approximation to the autoregressive score is an interesting point - maybe a comment in the paper that points to a derivation/discussion in the Appendix would be really helpful. I would also recommend discussing the random sampling of the vector $o$ earlier on if possible, e.g. in the paragraph starting Line 195. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "exrOCywaz7c",
                "writer": "author",
                "reply_to": "zjy3VUm1ItI",
                "title": "Author's Response",
                "comment": " Thank you for your thoughtful questions. We clarify these points below, and shall add clarifications to the manuscript accordingly. We hope that you recommend the article for publication as we also agree that this work has great potential and would be of interest to the ML community.\n\nAs with Nash, C., & Durkan, C. (2019), we did not observe any practical limitations based on the random-initialization of the energy network. One could guarantee a finite integral by utilizing the empirical support of the data to restrict the energy function.\n\nLet us motivate the use of a proposal distribution below. We would like to obtain a good estimate of $Z$ using few (e.g. 20) samples since each sample incurs the overhead for evaluating a neural network to compute $\\mathcal{E}(x_{u_i} | x_{o})$. A denser grid may yield a usable estimate of $Z$. However, since distributions may span over a large domain and may be non-smooth, importance sampling with a good proposal distribution will be much more sample efficient. This allows one to obtain as good an estimate as a grid with much fewer samples, which in turn will be much more computationally efficient since one will compute $\\mathcal{E}(x_{u_i} | x_{o})$ on fewer points. (One would have to compute $\\mathcal{E}(x_{u_i}=g_j | x_{o})$ on each grid point, $g_j$.) Note that our proposal distribution is a good approximator as indicated by the likelihoods we obtained empirically. Furthermore, sampling from $q(x_{u_i} | x_{o})$ is actually very efficient as it only requires one neural network evaluation to obtain the parameters of a 1d-GMM, which is efficient to sample from. Thus our importance sampling scheme nets much fewer neural network evaluations since a single pass to obtain the proposal distribution\u2019s parameters saves the many evaluations of the energy network that would be required on a grid.\n\nThe energy network could operate over latent vectors or on observed features. We chose to include both because including inputs acts as a skip-connection (which often helps with a deeper network) and early experiments indicated that this led to better performance. Our motivation for using multiple latent vectors is that this allows them to represent the fact that two different unobserved features may depend on $x_o$ in different ways, and the model can start making such distinctions sooner (i.e. at the end of the proposal net instead of inside the energy net).\n\nWe have two primary reasons for training non-autoregressively. The first is a matter of speed per batch of instances. Training autoregressively would require evaluating every 1D conditional in the chain-rule decomposition of $p(x_u | x_o)$. This is certainly possible, however it would require effectively expanding the batch size during training to include all of those conditionals. For high-dimensional data, this can quickly become infeasible, or at best make training very slow. Our second motivation is that the non-autoregresive scheme used in the paper is effectively equivalent to stochastically approximating the autoregressive procedure. Randomly sampling one of the conditionals from the autoregressive chain of each example yields our training scheme. We do indeed randomly sample $o$ at each training step, so over the course of training, the model will see the same distribution of inputs under either approach.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6lnVtwCbozy",
                "writer": "author",
                "reply_to": "uYFyi5hBuUl",
                "title": "Author's Response",
                "comment": " Thank you for your thoughtful comments and questions.\n\nFirst, let us clarify the terms in which we will talk about a model\u2019s sensitivity to ordering. A model can be *order-agnostic* (as ACE is), which is to say that autoregressive likelihoods can be evaluated in any order. As you point out, ACE is not what we will call *order-consistent*, meaning that evaluating the same likelihood with different orderings can produce different values. The advantage of being order-agnostic, even when not order-consistent, is tied to the advantages of arbitrary conditioning. By definition, since ACE can evaluate any arbitrary 1D conditional, it must be order-agnostic. That is, ACE can evaluate the 1d conditional of an $x_j$ given any subset of features $x_o$, thus it is able to evaluate likelihoods using any order. Hence, our motivation for this type of training is intertwined with our larger motivations for doing arbitrary conditioning.\n\nIt is correct that the energy, the function $\\mathcal{E}$, is non-negative, and we parameterize it as the output of a neural network. In order to enforce the nonnegativity constraint, the network has a softplus activation on its final layer.\n\nThe accumulation of errors is not impactful due to the following. First, from a practical perspective, the curse of dimensionality makes it computationally infeasible to estimate the normalizer for the full distribution $p(x_u | x_o)$ over all dimensions in $u$ all at once. Thus an alternative scheme is needed. Also, while it is true that we are summing the log-normalizers across dimensions, the error present in the normalizer for a given dimension does not affect the estimate of the normalizer for any other dimension. That is, each dimension\u2019s normalizer is computed independently of the others. Thus, we don\u2019t expect to see a compounding effect in which errors accumulate and consequently exacerbate future errors. Finally, if the errors are symmetrically distributed around zero (which Figure 5 in Nash and Durkan 2019 would suggest), we can expect them to cancel out on average, rather than grow.\n\nEven when estimating 1D conditionals, normalizing flows fall under certain restrictions. Namely, all transformations must be invertible and must have Jacobians that are efficiently computed. These restrictions inevitably place limitations on the network's architecture and capacity, which is why we advocate for an approach that allows the use of arbitrary networks.\n\nIgnoring the derivatives of the proposal samples effectively means the proposal distribution is treated independently to the energy distribution (i.e. is not tuned to the normalizer) which leads to a proposal that attempts to fit the distribution (which in itself is desirable for importance sampling).\n\nWhen estimating the mean of the energy distribution (Equation 9) , we have found that relatively few samples are needed in order to produce reliable estimates, likely due to the high quality of the proposal distribution. For example, on the Gas dataset, when using only 100 importance samples, the average variance of the estimates across 10 seeds is small (~0.003). In our reported results, we use from 3000 to 20000 samples, depending on the dataset.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9jeknTbpmjB",
                "writer": "author",
                "reply_to": "NSo0JnycLtb",
                "title": "Author's Response",
                "comment": " Thank you for your thoughtful comments and questions. We believe that the gain in performance stems primarily from decomposing the arbitrary conditioning problem into 1d conditional estimation, and parameterizing these 1d conditionals flexibly with an energy network. In terms of training cost, ACE performs similarly to ACFlow in most cases. Both methods can be trained in anywhere from a few hours to 1-2 days (on a single GPU), depending on the dataset. When evaluating likelihoods, ACE (in contrast to general ACFlow models) can compute the autoregressive components in parallel, but may be more memory intensive.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "42Azf7SSsjM",
                "writer": "author",
                "reply_to": "-bLP8Yr1UqZ",
                "title": "Author's Response",
                "comment": " Thank you, we are glad that you found the work to be clever, and we hope you recommend our paper for publication. Let us clarify your question. When evaluating $p(x_2, x_3 | X_1=x_1)$, one must evaluate the likelihood at some particular values for features; i.e., one would evaluate $p(X_2=x_2, X_3=x_3 | X_1=x_1) = p(X_2=x_2|X_1=x_1) p(X_3=x_3|X_1=x_1,X_2=x_2)$. This is akin to the case with a Gaussian where one may ask the likelihood of observing $X_2=x_2, X_3=x_3$ given that $X_1=x_1$. Thus, when computing the normalizer $Z_{{x_3}|X_1=x_1, X_2}$, we must normalize for the case when $X_2=x_2$ (i.e., $Z_{{x_3}|X_1=x_1, X_2=x_2}$), since we are seeking to normalize the likelihood $p(X_3|X_1=x_1,X_2=x_2)$ (where we are given that $X_1=x_1, X_2=x_2$). Thus, the proposal distribution samples are only used for estimating the respective normalizers for 1d conditionals and are not percolated up to condition; only the values that one is evaluating the likelihood on ($X_1=x_1, X_2=x_2, X_3=x_3$) are ever used in conditioning. When imputing one is no longer evaluating a likelihood, but is instead drawing a sample of $X_u$  given $X_o=x_o$, or computing $E[X_u | X_o=x_o]$. We shall add some of these details to further clarify such subtleties. With regards to the number of missing variables in the proposal (or energy) network, it is not the case that there is always the same number. During training, we randomly sample the missing features for each batch (see lines 244-246 for additional details), so the networks are exposed to observed sets of different sizes.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-bLP8Yr1UqZ",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__idcJrecij",
                "title": "",
                "comment": "In this paper, the authors propose to compute p(x_u|x_o) for any combination of observed variables (x_o) and unobserved varaibles (x_u) with a single NN. The authors actually propose to compute p(x_u|x_o) = prod p(x_{u_i}|x_o,x_{u_1} \u2026 x_{u_{i-1}}). Basically, they train a 1D conditional estimator that that can depend on any of the inputs. \n\nThe proposed mechanism is based on two networks. A Proposal network that takes as input a mask for the observed inputs and a vector with the inputs for the observed variables at the right positions. The output of the proposal network is the components for a mixture of Gaussians ($\\omega$) and a latent vector for each missing component of x. The second network takes the latent vector and estimates the probability.   I must say that the algorithm is very clever, and it seems to perform well compared to other methods. I would be in favor of having the paper presenting at the conference. But I have a few questions that I do not understand how the algorithm works at inference time, and I would like the authors to be able to explain how their solution performs. Let me start with a very simple example, this would probably be a good simple example for the readers too. Let us assume that we have 4 variables x_1, x_2, and x_3 coming from a multivariate Gaussian. Let\u2019s assume that we train the proposal network with two missing variables and one observed. Side question: Do we always have to have the same number of missing variables in the proposal network, I would say so, but please let me know if I am wrong about this? \n\nAt test time, we are given x_1 = X1 and we compute:\nP(x_2, x_3|X_1=X1) = p(x_2|x_1=X1) p(x_3|x_1=X1,x_2)\n\nThe first thing that, we need to do is to get N samples from q(x_2|x_1=X1) and Z_{{x_2}|x_1=X1}, using equation 5.\n\nNow, how do I compute, Z_{{x_3}|x_1=X1, x_2}. This partition function is no longer a number but a function of x_2. Do I compute this for a specific value of x_2, such as a sample from p(x_2|x_1=X1) or the maximum or the true value that x_2 takes? I guess it is the last one, using this procedure would only work if we are computing the likelihood for the full sample. So Z_{{x_3}|x_1=X1, x_2} is not a function of x_2 but a constant with x_2=X2.\n\nI think this is more evident when looking at the imputation in which there is no access to X2 and the authors use the following approximation:\nP(x_2, x_3|X_1=X1) = p(x_2|x_1=X1) p(x_3|x_1=X1)\nAnd use the mean p(x_2|x_1=X1) and p(x_3|x_1=X1) to impute the variables. For example, using this to generate the top half of an MNIST image would be horrible and would only provide a blurred image. In other image processing would even be worst. \n Not needed",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "NSo0JnycLtb",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__idcJrecij",
                "title": "",
                "comment": "The authors propose a strategy to estimate any arbitrary conditional distribution by using an the chain rule:  they estimate all one-dimensional distributions.  While this would require learning exponentially many distributions, a weight sharing scheme is used to limit the number of parameters that need to be learned from data.  The resulting approach represents each univariate distribution in the form exp(neural network) and the normalizing constant is estimated via importance sampling (a proposal distribution can also be estimated if desired).  They show that this simple approach is flexible and outperforms more sophisticated approaches on real-world data sets.  Overall, I quite like the approach. It's a simple and clever combination of modern techniques that should be easy to replicate.  My only real criticisms are related to the experimental evaluation (see below) and these are minor.\n\nOriginality:  The work appears to be a novel approach for density estimation tasks.\n\nQuality:  1) I don't personally love using likelihoods to evaluate continuous models (despite the prevalence of these types of evaluations).  While the average conditional likelihood is definitely better, it still isn't great.  I've seen plenty of examples with sum-product networks and similarly expressive models where high likelihoods does not correspond to good model performance on real tasks.  Thankfully, the imputation/predictive performance is also better for ACE.  Honestly, I'd suggest moving the likelihood stuff to the appendix and replacing it with larger figures (the bar charts in particular are difficult to read) and more experiments.\n\n2)  I actually find Figure 2 somewhat perplexing.  Why does ACE seem to do so much better in the missingness .1 regime than the other competitors? The performance seems more comparable in the other two settings.  Thoughts?\n\n3) While the current experiments are sufficient, in my opinion, for publication, I think it would make an even stronger case if the authors could include a few more interesting data sets (or even one practical domain) for which this approach succeeds where other approaches fail significantly.\n\n4) No mention is really made of wall-clock comparison between the methods (or I missed it).  Could the authors comment a bit?\n\nClarity:  The paper is well-written and easy to follow with only minor typos.\n\nSignificance: The simplicity of the approach lends itself to a variety of different applications. n/a + see above",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "below the acceptance threshold",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "recommend for acceptance",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "encourage to improve",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "Yn4CPz_LRKO": {
        "paper_id": "iclr_2022_Yn4CPz_LRKO",
        "paper_title": "Conditional GANs with Auxiliary Discriminative Classifier",
        "paper_abstract": "Conditional generative models aim to learn the underlying joint distribution of data and labels, and thus realize conditional generation. Among them, auxiliary classifier generative adversarial networks (AC-GAN) have been widely used, but suffer from the problem of low intra-class diversity on generated samples. In this paper, we point out that the fundamental reason is that the classifier of AC-GAN is generator-agnostic, and therefore cannot provide informative guidance to the generator to approximate the target distribution, resulting in minimization of conditional entropy that decreases the intra-class diversity. Motivated by this observation, we propose a novel conditional GAN with auxiliary \\textit{discriminative} classifier (ADC-GAN) to resolve the problem of AC-GAN. Specifically, the proposed auxiliary \\textit{discriminative} classifier becomes generator-aware by recognizing the labels of the real data and the generated data \\textit{discriminatively}. Our theoretical analysis reveals that the generator can faithfully replicate the target distribution even without the original discriminator, making the proposed ADC-GAN robust to the hyper-parameter and stable during the training process. Extensive experimental results on synthetic and real-world datasets demonstrate the superiority of ADC-GAN on conditional generative modeling compared to competing methods.",
        "paper_acceptance": "Reject",
        "meta_review": "The paper proposes a conditional generative adversarial network with an auxiliary discriminative classifier for conditional generative modeling. The auxiliary discriminative classifier can provide the discrepancy between the joint distribution of the real data and labels and that of the generated data and labels to the generator by discriminatively predicting the label of the real and generated data. Experiment results are provided to demonstrate the effectiveness of the proposed idea.  The current paper receives mixed ratings after rebuttal (5, 6, 5, 8). Except that one reviewer (the Reviewer uPwH) will champion the paper with a score of 8, the concerns of the other three reviewers remain. To be specific, even though Reviewer ebJs assigns a score of 6, he/she doesn\u2019t champion the paper because additional experiments requested are not provided by the authors, including (i) training on more datasets or higher resolutions, (ii) visualizing feature norm and grad norm as done in ReACGAN, (iii) experiments on ADC-GAN without unconditional GAN loss. The Reviewer DPgR pointed out that the paper might have a novelty issue because it bears some similarities with other works but it lacks a discussion in the revision. Additionally, Reviewer mZT7 pointed out that the authors didn\u2019t provide a revised paper during the rebuttal, thus leading to a difficulty to assess the quality of the final paper. As a result, AC thinks that the paper is not ready to publish at the current stage and recommends a rejection.  The AC urges the authors to revise their paper according to the comments provided by the reviewers, and resubmit their work in a future venue.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "bzkKdkKBDlF",
                "writer": "official_reviewer",
                "reply_to": "TGwuFQXPpr",
                "title": "Reply",
                "comment": " ADC-GAN and SSGAN-LA are devised to solve problems in different fields (conditional GAN and self-supervised GANs), but these two fields can be grouped under the theme of GAN. Also, both models address the same problem, specifically the generator-agnostic optimization process of GAN. So, I stick with my previous score as the technical contributions of this paper are not sufficient.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "TGwuFQXPpr",
                "writer": "author",
                "reply_to": "xVvICWi03tc",
                "title": "Authors' Response",
                "comment": " ADC-GAN and SSGAN-LA solve problems in different fields respectively, although their solutions are similar in mathematical formulation. SSGAN-LA belongs to self-supervised GANs that improve the training stability of unsupervised GANs by introducing self-supervised tasks. ADC-GAN focuses on solving the low intra-class diversity issue of AC-GAN, which is an important issue in the class-conditional GAN literature. \n\nIn addition to proposing ADC-GAN, the contribution of this paper also includes analyzing the drawbacks of previous conditional GANs (e.g., AC-GAN, TAC-GAN, PD-GAN) as well as the advantages of ADC-GAN. Besides, the analysis results and proof are different from that in SSGAN-LA.\n\nWe will clarify the difference between ADC-GAN and SSGAN-LA clearly in the updated version. In general, ADC-GAN provides a theoretically grounded and superior (v.s prior work) solution to implement conditional GANs, and we argue that ADC-GAN will have a broad and significant impact on this field.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "F9V6r9VE3YK",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Yn4CPz_LRKO",
                "title": "",
                "comment": "This paper proposes the Auxiliary Discriminative Classifier GAN (ADC-GAN) to eliminate a contractionary objective and conditional entropy in ACGAN generator training. Specifically, the authors mathematically demonstrate that training ACGAN without a discriminative label classifier causes minimizing an undesirable divergence (KL(q(x)||p(x))) which conflicts with the joint distribution matching (KL(q(x,y)||p(x,y))). Also, they insist that the lack of intra-class diversity of ACGAN results from the absence of generator guidance for training the discriminator. To resolve all these issues, they devise a new classifier, the auxiliary discriminative classifier and deploy the new classifier directly on the ACGAN framework. Experiments demonstrate that ADC-GAN can successfully learn the joint distribution whose conditional marginals have non-negligible support overlap using MoG dataset. In addition, they show the effectiveness of ADCGAN compared to ACGAN, projection discriminator, and TAC-GAN on four benchmark datasets (CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet) using IS, FID, iFID metrics.  Strengths:\n\n(+) The paper exactly points out the primitive problem of ACGAN from the optimization perspective. Since ACGAN is widely adopted in the machine learning area, analyzing problems of ACGAN is necessary and valuable.\n\n(+) The proposed auxiliary discriminative classifier is reasonable, and easy to implement. Also, ADC-GAN does not require much computational burden.\n\n(+) Section 4.2 is very interesting and the explanation of why projection discriminator fails to approximate the joint distribution in Figure 2 is reasonable.\n\nWeaknesses:\n\n(-) It seems that Theorem 1 has already been covered in TAC-GAN paper (paragraphs below eq.4 of the TAC-GAN paper [R1]). Although mathematical formulations are different from each other, the arguments of Theorem 1 and the paragraphs seem to be very similar. I think it is essential to clarify differences between two arguments.\n\n(-) It seems that all experiments were conducted once. It would be better to conduct experiments several times since GANs have been known to have a large performance variance. \n\n(-) The contribution that ADC-GAN can generate diverse images compared to ACGAN and TAC-GAN is not fully demonstrated. Although FID has been a widely used metric to measure fidelity and diversity of generated images, I think It is not enough. I recommend the authors to utilize the improved precision and recall [R2], classification accuracy score [R3], or density and coverage [R4] to quantify the ability of generating diverse images of ADC-GAN.\n\n(-) In section 5.1, the authors conducted the distribution learning experiment using one-dimensional conditional gaussians whose supports are overlapped. I accept that ADC-GAN can learn the joint distribution which consists of the one-dimensional conditional gaussians better than PD-GAN, AC-GAN, and TAC-GAN. However, what about a joint distribution which consists of conditional gaussians with disjoint supports? Can ADC-GAN learn the joint distribution better than other cGANs? The authors propose a new type of ACGAN named ADC-GAN to address an improper optimization process of ACGAN. They apply adversarial training not only for the discriminator but also for the auxiliary classifier to eliminate a contradictory divergence and conditional entropy in ACGAN training. In the experimental results, they prove the effectiveness of ADC-GAN using synthetic datasets and various benchmark datasets. However, I think Theorem 1 has already been addressed in TAC-GAN paper and experimental results do not fully demonstrate the effectiveness of the proposed method. ",
                "rating": 5,
                "confidence": 5
            },
            {
                "review_id": "xVvICWi03tc",
                "writer": "official_reviewer",
                "reply_to": "i5QfqaFehlT",
                "title": "I have a major concern about this paper",
                "comment": " I found that Eq. (5) in this paper is very similar to Eq. (8) in the [paper](https://arxiv.org/abs/2106.08601v4) \"self-supervised gans with label augmentation\", which was published on arXiv on 2021.06.16. Since the archive publication date is before the ICLR submission deadline, I think the authors should have clarified the differences between these two papers. \nBoth studies (ADCGAN and SSGAN-LA) tackle the generator-agnostic optimization process of GANs and insist that the process make the generator's implicit distribution converge to a degenerate distribution. To remove the problem, both works propose discriminative classifiers, which are trained using Eq. (5) and Eq. (8).\n\nFor these reasons, I think the technical contributions of this paper are not significant, and the authors do not adequately describe the closely related work. I rearrange my score from 6 to 5.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UZFrokFokuy",
                "writer": "author",
                "reply_to": "g4yYrWSi6zb",
                "title": "Thank you and further response",
                "comment": " Thank you for your reply. First, we apologize for ignoring the fact that ICLR allows us to update drafts directly. We will revise our paper according to our responses in the updated version. As for experiments on CIFAR-10/100, we follow the experimental settings of BigGAN-PyTorch, which is also a standard protocol in the GAN literature. As for experiment on ImageNet, we adopted the settings of TAC-GAN to make the comparison as fair as possible and due to our limited and temporarily available computational resources (TAC-GAN modifies $ch=96\\rightarrow 64$). TAC-GAN is an appropriate competitor to our ADC-GAN because it also belongs to the classifier-based cGANs. And ADC-GAN surpasses TAC-GAN, as well as PD-GAN, under the same setting.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "g4yYrWSi6zb",
                "writer": "official_reviewer",
                "reply_to": "20CALrcGm7Y",
                "title": "Thanks for the response. I decide to keep my score.",
                "comment": " Thanks for your response.  Some of my concerns are addressed. However, as ICLR allows authors to update the draft, and I don't see any updates in the draft has been made. Also, for the experiment, as GAN are sensitive to the hyperparameters, it's not convincing to me that you use your own setting instead of the standard protocols given you are able to train on ImageNet. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "MJcu9y0lVnm",
                "writer": "official_reviewer",
                "reply_to": "xkN1-XjXWlY",
                "title": "Reply",
                "comment": " Thanks for your response. ADCGAN is a stable version of ACGANs (only change the loss function). However, the current paper lacks complete ImageNet experiments. Therefore, I am not sure whether ADCGAN still performs well on high-resolution images of the ImageNet dataset. Anyway, ADCGAN provides a theoretical perspective for us to understand ACGANs, which is the author's core contribution. So I raise my score to 'accept'.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Glk94RfuJ5",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Yn4CPz_LRKO",
                "title": "",
                "comment": "- This paper aims to solve the low intra-class diversity on generated images of AC-GAN, a classifier-based cGAN. \n- As far as I know, this is an important issue that limits classifier-based cGANs (the counterpart is the projection-based cGAN, i.e, PD-GAN).\n- The authors point out that the reason is that the classifier of AC-GAN is generator-agnostic and minimization of conditional entropy decreases the intra-class diversity. \n- The authors propose ADC-GAN (auxiliary discriminative classifier) to solve this problem, and theoretical analysis is also presented.\n Strengths:\n\n- The problem of low intra-class diversity of classifier-based cGANs is important. \n    - Reasons: The projection-based cGAN, PD-GAN, does not suffer from the low intra-class diversity problem, but it converges more slowly than classifier-based cGANs (see Omni-GAN, arXiv:2011.13074).  The classifier-based cGANs converge faster but suffer from low intra-class diversity. Therefore, it is of great value to improve classifier-based cGANs so that we can completely abandon the PD-GAN converging slowly in practice. \n- This paper provides a theoretical perspective for analyzing the loss function of different cGANs. \n- The proposed ADC-GAN is very simple to implement without additional computational overhead.\n\nWeaknesses:\n\n- Please detail in the paper how the FID in Table 2 is calculated (for example, how many generated images are used, whether the training set or the validation set is used, and whether the inception model is from PyTorch or tensorflow). In addition, I also recommend including IS in Table 2.\n- In Equ. 5, the notations of $C_d(y,1|x)$ and $C_d(y,0|x)$ are a bit confusing. After checking the code in the supplementary material, I understood the meaning of the equation. In fact, $C_d(y,1|x)$ and $C_d(y,0|x)$ are implemented using a fully connected layer with output dimensions of num_classes * 2. I suggest that the author use much clearer notation to make it easier to understand. The author can refer to the notation of equation 8, which is clear. \n- In Figure 4e, I am surprised that as a classifier-based cGAN, ADC-GAN did not suffer from mode collapse. The author also does not seem to apply weight decay for the discriminator, as Omni-GAN does. I am not sure if the author has used other regularization techniques to stabilize the training. As far as I know, if do not add regularizations such as weight decay, other classifier-based cGANs will collapse earlier, such as AC-GAN, Multi-hinge GAN, and Omni-GAN. It would be better if the author could explain this phenomenon. \n- I know that there is an improved version of AC-GAN (ImAC-GAN), as discussed in section 3.3 of the Omni-GAN paper. ImAC-GAN has a clear performance gain compared to AC-GAN. It would be better if the author could discuss the relationship between ImAC-GAN and ADC-GAN. \n- In Figure 4 (c) and (f), the results of T-SNE visualization are not very convincing. In my opinion, the author should not use the discriminator to extract feature representations, because the PD-GAN discriminator is less supervised than ADC-GAN. I suggest that the author use a pre-trained classification model to extract features for a fair comparison.\n- In the caption of Figure 4, the author says that the T-SNE uses training data, but in the last paragraph of section 5.3, the author says that the T-SNE uses the validation data. \n The author proposes a simple but seemingly promising classifier-based cGAN. I hope the author can answer my questions above in detail. I will improve the score based on the author's answer. ",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "tyY7GLw994z",
                "writer": "author",
                "reply_to": "LbdeIQnr2Df",
                "title": "Response to Reviewer mZT7 [2/2]",
                "comment": " **Q6**: I have the concern of the analysis. Most of the results rely on assuming sth components are optimal. However, in reality, they are not hold in reality, and there is no convergence analysis provided. Could the authors comment on it?\n\n**R6**: We follow the common practice in the GAN literature (e.g., the original GAN paper) to analyze the intrinsic learning objective for the generator based on the optimal discriminator/classifier. We leave the convergence analysis as the future work as this is not a fully theoretical paper.\n\n**Q7**: The biased issue of ACGAN is known. For example,\n  - AC-GAN Learns a Biased Distribution, 2017\n  - Unbiased Auxiliary Classifier GANs with MINE, 2020\n\n**R7**: In this paper, we give a new explanation of the biased issue of AC-GAN that the classifier of AC-GAN is generator-agnostic (and thus cannot provide only the discrepancy between q(x,y) and p(x,y) for the generator). And we also show in Theorem 1 that AC-GAN optimizes contradictory divergence, which is not stated in previous work. However, we would like to thank the reviewer for constructive comments and will review the related work in more detail and systematically in the updated version.\n\n**Q8**: Why the proposed ADC-GAN (KL(P||Q)) is better than PD-GAN (KL(P||Q))?\n\n**R8**: As we discussed in Section 4.2 and shown in Figure 4(c,f), PD-GAN is incapable of modeling the data-to-label relationships since it ignores the partition term (a). We argue that modeling the data-to-label relationships is beneficial for conditional generative modeling. And ADC-GAN can therefore achieve better results due to modeling such relationships. In addition, ADC-GAN also minimizes JS(p(x)||q(x)), which is also symmetric, in addition to the KL(p(x,y)||q(x,y)).\n\n**Q9**: There are some very recent works in NeurIPS and also highly relayed,\n\n  - [1] Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training, NeurIPS 2021\n  - [2] A Unified View of cGANs with and without Classifiers, NeurIPS 2021 \n\n**R9**: \n\nReACGAN [1] essentially does not solve the problem of low intra-class diversity in AC-GAN because it still inherits the non-discriminative classifier of AC-GAN. The ReACGAN authors also acknowledged in their paper (page 19 and 20) that ReACGAN fails to estimate the 1-D MoG dataset, which our ADC-GAN can faithfully replicate.\n\nECGAN [2] proposed a framework that unifies existing projection-based cGANs (PD-GAN) and classifier-based cGANs (AC-GAN). However, our proposed ADC-GAN does not follow this framework due to the fact that our discriminative classifier is different from the classifier in existing cGANs.\n\nWe would like to thank the reviewer for the constructive suggestion and agree that discussing the similarities and differences between the proposed method and more related work is valuable for this paper and the community. We will add the discussion with the recent related work in the updated version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DHiFAqzr0zH",
                "writer": "author",
                "reply_to": "sPLQC5gW_2w",
                "title": "Response to Reviewer ebJs",
                "comment": " Thank you for spending time reviewing our paper and your insightful comments. We sincerely hope that our clarification will resolve your concerns.\n\n**Q1**: It might be true that term (a) is \"ignored\" or set to zero, but it is not sufficient to say this inductive bias is a mistake.\n\n**R1**: We will refine our claim into that the term (a) is ignored in PD-GAN's implementation. This inductive bias may not be a mistake but it will adversely affect the results as shown in P2GAN[A] that optimizing term (a) additionally would improve the performance.\n\n**Q2**: Equation 8 is consistent with the actual implementation of TAC-GAN. It is good to state this clearly in the paper.\n\n**R2**: We would like to thank you for your valuable suggestion. Indeed, our Equation 8 corresponds to the actual implementation of TAC-GAN. We will state it clearly in the updated version.\n\n**Q3**: **Implementations of ADC-GAN and TAC-GAN are the same?** Can we construct C_d of ADC-GAN by stacking C and C_mi of TAC-GAN?\n\n**R3**: ADC-GAN and TAC-GAN are indeed different. Recall that the two classifiers of TAC-GAN at their optimum can be written in the form of \n\n$$C^*(y|x)=\\frac{p(x,y)}{p(x)}=\\frac{\\exp(v_y^p\\cdot \\phi(x))}{\\sum_{k} \\exp(v_{k}^p\\cdot \\phi(x))},C_{mi}^*(y|x)=\\frac{p(x,y)}{p(x)}=\\frac{\\exp(v_y^q\\cdot \\phi(x))}{\\sum_{k} \\exp(v_{k}^q\\cdot \\phi(x))},$$\n\nwhich conclude \n\n$$\\exp(v_y^p\\cdot\\phi(x))=k_1 p(x,y),\\sum_k\\exp(v_k^p\\cdot\\phi(x))=k_1p(x),$$\n\n$$\\exp(v_y^q\\cdot\\phi(x))=k_2 q(x,y),\\sum_k\\exp(v_k^q\\cdot\\phi(x))=k_2 q(x),$$\n\nfor $k_1,k_2\\in\\mathbb{R}^+$. If we stack $C$ and $C_{mi}$ together, then the stacked classifier will be\n\n$$C_s^*(y,1|x)=\\frac{\\exp(v_y^p\\cdot \\phi(x))}{\\sum_{k} \\exp(v_{k}^p\\cdot \\phi(x))+\\sum_{k} \\exp(v_{k}^q\\cdot \\phi(x))}=\\frac{k_1 p(x,y)}{k_1 p(x)+k_2 q(x)},$$\n\n$$C_s^*(y,0|x)=\\frac{\\exp(v_y^q\\cdot \\phi(x))}{\\sum_{k} \\exp(v_{k}^p\\cdot \\phi(x))+\\sum_{k} \\exp(v_{k}^q\\cdot \\phi(x))}=\\frac{k_2 q(x,y)}{k_1 p(x)+k_2 q(x)},$$\n\nwhich is different from the optimal discriminative classifier of ADC-GAN $C_d^*(y,1|x)=\\frac{p(x,y)}{p(x)+q(x)},C_d^*(y,0|x)=\\frac{q(x,y)}{p(x)+q(x)}$ if $k_1\\neq k_2$, which is quite possible (the probability is almost 100%). Therefore, it is almost impossible to stack C and Cmi to construct Cd. \n\n**Q4**: It would be helpful if the author could provide results of ADC-GAN on ImageNet at 256 resolution.\n\n**R4**: Due to our limited computational resources, we are sorry that we cannot provide results of ADC-GAN on ImageNet at 256 resolution during the short rebuttal period.\n\n**Q5**: Table 2, why not use the reported numbers in TAC-GAN paper?\n\n**R5**: We implement all methods in Table 2 based on the official BigGAN-PyTorch repository. The experimental settings are different from TAC-GAN. For example, the batch size is 50 instead of 100 used in TAC-GAN, and the discriminator update steps are 4 per generator step while it is 2 in TAC-GAN. And we use a single GPU to conduct these experiments due to our limited computational resources while TAC-GAN uses two GPUs. Besides, we cannot reproduce the results of TAC-GAN on one GPU using their code. In general, our comparison is fair as we implement all methods using the same experimental settings.\n\n**Q6**: I am curious how the model performs (w/o GAN loss) on challenging datasets such as ImageNet? It is also surprising that in Supplementary Table 4, Hinge loss does worse than no GAN loss. Can the author explain this?\n\n**R6**: Indeed, Figure 4(a,d) shows that ADC-GAN w/o GAN loss on CIFAR-10 and CIFAR-100 can achieve comparable performance with the full ADC-GAN model. However, due to our limited computational resources, conducting experiments on ImageNet during the short rebuttal period is difficult for us. As for the results in Table 4, we suspect that the reason is that the MoG dataset is too simple for hinge loss to have an advantage over others. Nevertheless, our ADC-GAN can outperform others when all methods use hinge loss.\n\n**Q7**: Comparing Theorem 2 with a plain cGAN which minimizes JS(QX,Y||PX,Y), does the reverse KL tend to cause mode collapse?\n\n**R7**: In our experiments, our method is more stable than PD-GAN during training and hardly encounters mode collapse (shown in Figure 4). Our Theorem 2 aims to show that ADC-GAN has the unique global optimal solution ($Q_{X,Y}=P_{X,Y}$). Note that reverse KL tends to cause mode collapse only when the model capability is not enough, which can be easily resolved by enlarging the model size. In addition, our ADC-GAN contains the original discriminator that helps to minimize $\\text{JS}(P_X | |Q_X)$. which is symmetric that can prevent the generator from mode collapse.\n\n[A] Han, Ligong, et al. \"Dual Projection Generative Adversarial Networks for Conditional Image Generation.\" *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2021.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sPLQC5gW_2w",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Yn4CPz_LRKO",
                "title": "",
                "comment": "This paper proposes a new conditional GAN model that employs a discriminative classifier that predicts in the joint space of label and real/fake domain. The theoretical analysis shows the proposed ADC-GAN can minimize the reverse KL between joint $Q_{X,Y}$ and $P_{X,Y}$. ## Strengths\n1. The paper presents interesting analysis of AC-GAN, TAC-GAN, PD-GAN. Especially the Theorem 3 reveals potential drawbacks of TAC-GAN.\n2. The experimental results on synthetic and real datasets demonstrate the superiority of ADC-GAN on conditional generative modeling tasks.\n\n## Weaknesses\n1. I don't fully agree with some claims made by the authors:\n   1. Page 5, footnote 2, it might be true that term (a) is \"ignored\" or set to zero, but it is not sufficient to say this inductive bias is a mistake.\n2. Equation 8, this is not the original form of TAC-GAN. The original TAC-GAN is built upon AC-GAN, so term (c) in Equation 3 in the TAC-GAN paper is missing. I notice that Equation 8 is consistent with the actual implementation of TAC-GAN, but I guess it is good to state this clearly in the paper.\n3. Implementations of ADC-GAN and TAC-GAN are the same? As I checked the provided code in supplementary, I think the proposed ADC-GAN is very similar to TAC-GAN (as defined in Equation 8): In fact, if spectral norm (SN) and bias are not used in the linear classification layer, they are exactly equivalent. This is because the weight of $C_d$ is just $C$ and $C_{mi}$ stacked together. I would consider this as an implementation difference. Note that Theorem 2 and 3 are different, I doubt the superior performance of ADC-GAN might come from the difference in SN or a different choice of hyperparameters. In such case, it would be helpful if the author could provide code for MoG experiment, which is cleaner, simpler, and no SN applied (if the code is borrowed from TAC-GAN). Please correct me if I am wrong, and I'm happy to amend my score accordingly.\n4. It would be helpful if the author could provide results of ADC-GAN on ImageNet at 256 resolution.\n5. Table 2, why not use the reported numbers in TAC-GAN paper? I checked Table 1 in TAC-GAN paper, and their FID on CIFAR100 is 7.22 which is lower than the reported 7.98, any explanation?\n6. It is nice to see ADC-GAN worked even without GAN loss, I am curious how the model performs (w/o GAN loss) on challenging datasets such as ImageNet? It is also surprising that in Supplementary Table 4, Hinge loss does worse than no GAN loss: if Theorem 2 holds, the solution set of ADC-GAN is a subset of (unconditional) GAN (say with hinge loss), so adding GAN loss wouldn't affect ADC-GAN training. Can the author explain this?\n7. Comparing Theorem 2 with a plain cGAN which minimizes $JS(Q_{X,Y}||P_{X,Y})$, does the reverse KL tend to cause mode collapse? (in theory it seems that JS is better than reverse KL? a typical yet imprecise point to be made is that KL causes mode averaging and reverse KL causes mode collapse. [1])\n\n[1] Zhao, Miaoyun, et al. \"Bridging Maximum Likelihood and Adversarial Learning via \u03b1-Divergence.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020. I think the paper presents an interesting analysis of TAC-GAN, AC-GAN, and other cGAN methods. My concern is that the proposed method has the same actual implementation as existing method (TAC-GAN). The paper also lacks results on high-resolution generation. I am willing to raise my score if my concerns are resolved.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "mtl34WDRDzH",
                "writer": "official_reviewer",
                "reply_to": "DHiFAqzr0zH",
                "title": "Further comments",
                "comment": " I thank the authors for their clarification and explanations. I understand that given the period of time and limited resources it can be hard to run additional experiments. Since my main concern about ADC-GAN and TAC-GAN being equivalent is resolved, I'm raising my score to 6, but I'm not willing to champion it.\n\nIn my previous comment, I ignored the fact that in a \"stacked\" classifier, logits of the fake-classifier are also being normalized in the softmax when computing the real-classificaiton loss (the same goes for the fake-classification loss). I guess the performance gain comes from this different normalization in the denominator. I think this might be a valid trick, although I am still not convinced by its theoretical analysis. I highly suggest the authors do some more analysis in this direction (without training on more datasets or higher resolutions). An example would be visualizing feature norm and grad norm as done in ReACGAN (Figure 2). Also, it would be good if the authors provided more results/analysis on ADC-GAN without unconditional GAN loss.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i5QfqaFehlT",
                "writer": "official_reviewer",
                "reply_to": "N4Jdjox-2m",
                "title": "Reply",
                "comment": " Thanks for your response which addresses my concerns. I will be grateful if you add the experiments and discussions done during the rebuttal period in your final paper. However, there are still remaining concerns regarding inconsistent numbers from the results of PyTorch-StudioGAN library. Also, I have understated that the auxiliary discriminative classifier can be applied to any type of ACGAN variants, such as ContraGAN [1], OmniGAN [2], and ReACGAN [3]. I think conducting more experiments using those GANs will make ADCGAN more convincing. Anyway, I'm going to raise my score by one point.\n\n[1] Kang, M., & Park, J. (2020). ContraGAN: Contrastive Learning for Conditional Image Generation. Neurips.\n\n[2] Zhou, P., Xie, L., Ni, B., Geng, C., & Tian, Q. (2021). Omni-gan: On the secrets of cgans and beyond. CVPR.\n\n[3] Kang, M., Shim, W., Cho, M., & Park, J. (2021). Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training. Neurips.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xkN1-XjXWlY",
                "writer": "author",
                "reply_to": "Glk94RfuJ5",
                "title": "Response to Reviewer uPwH",
                "comment": " Thank you very much for recognizing our contribution and thoughtful review.\n\n**Q**1: Please detail in the paper how the FID in Table 2 is calculated (for example, how many generated images are used, whether the training set or the validation set is used, and whether the inception model is from PyTorch or tensorflow). In addition, I also recommend including IS in Table 2.\n\n**R1**: We follow the practice of the BigGAN-PyTorch repository to calculate FID in Table 2 with 50k generated images. The reference is the training set, and the inception model is from PyTorch. In this work, our goal is to resolve the low intra-class diversity of AC-GAN. As IS is not capable of measuring the intra-class diversity, we do not report them in Table 2. Instead, we use intra-FID which can measure the intra-class diversity to compare methods.\n\n**Q2**: In Equ. 5, the notations of Cd(y,1|x) and Cd(y,0|x) are a bit confusing. I suggest that the author use much clearer notation to make it easier to understand.\n\n**R2**: We would like to thank you for the valuable suggestion. We will give a detailed description in the updated version.\n\n**Q3**: In Figure 4e, I am surprised that as a classifier-based cGAN, ADC-GAN did not suffer from mode collapse. I am not sure if the author has used other regularization techniques to stabilize the training. It would be better if the author could explain this phenomenon.\n\n**R3**: We implement ADC-GAN based on the BigGAN-PyTorch repository with minor modifications. We do not add extra regularization techniques to stabilize the training. Indeed, ADC-GAN performs more stable during training compared to previous classifier-based cGANs, we argue that the reason is that the classifier of ADC-GAN provides harmonious objective for the generator to learn the data distribution which AC-GAN and TAC-GAN provide contradictory objective (please refer to Table 1).\n\n**Q4**: It would be better if the author could discuss the relationship between ImAC-GAN and ADC-GAN.\n\n**R4**: The output dimension of the ImAC-GAN classifier is C+1 while ours is C*2. ImAC-GAN improves AC-GAN by adding a fake class (say 0) to the classifier Cim: X --> Y U {0}, and formulates the objective functions with the improved classifier as follows:\n\n\n\nmax_Cim E_x,y\\~p(x,y) [log Cim (y|x)] + E_x\\~q(x) [log Cim (0|x)],\n\nmax_G E_x,y\\~q(x) [log Cim (y|x)].\n\n\n\nDefine pm(x,y) = 1/2 p(x,y), for y \\in {1,2,...,C} and pm(x,0) = 1/2 q(x), then we can obtain the new optimal classifier\n\nmax_Cim E_x,y\\~p(x,y) [log Cim (y|x)] + E_x\\~q(x) [log Cim (0|x)]\n\n=> max_Cim E_x,y\\~pm(x,y) [log Cim (y|x)]\n\n=> min_Cim E_x\\~pm(x) E_y\\~pm(y|x) [- log Cim (y|x)]\n\n=> min_Cim E_x\\~pm(x) [H(pm(y|x)) + KL(pm(y|x) || Cim (y|x))]\n\n=> Cim* (y|x) = pm(y|x) => Cim* (y|x) = p(x,y) / (p(x) + q(x)), for y in {1,2,...,C}\n\n\n\nNow, optimizing the generator with the optimal classifier is equivalent to\n\nmax_G E_x,y\\~q(x,y) [log Cim* (y|x)]\n\n=> max_G E_x,y\\~q(x,y) [log p(x,y) / (p(x) + q(x))]\n\n=> min_G E_x,y\\~q(x,y) [log q(x,y) / p(x,y) * (p(x) + q(x)) / q(x,y)] > E_x,y\\~q(x,y) [log q(x,y) / p(x,y) * sqrt(p(x) + q(x)) / q(x,y)] + log 2\n\n=> min_G E_x,y\\~q(x,y) [log q(x,y) / p(x,y) * sqrt(p(x) / q(x)) * q(x) / q(x,y)]\n\n=> min_G E_x,y\\~q(x,y) [log q(x,y) / p(x,y)] + 1/2 E_x\\~q(x) [log p(x) / q(x)] + E_x,y\\~q(x,y) [log q(x) / q(x,y)]\n\n=> min_G KL(q(x,y)||p(x,y)) - 1/2 KL(q(x)||p(x)) + H(q(y|x))\n\n\n\nIn general, the classifier of ImAC-GAN enforces the generator to minimize an upper-bound of KL(q(x,y)||p(x,y)) - 1/2 KL(q(x)||p(x)) + H(q(y|x)) which still contains the drawbacks (i.e., contradictory divergences and unwanted conditional entropy) of AC-GAN as we discussed in Section 2.2.\n\n\n\n**Q5**: In Figure 4 (c) and (f), the results of T-SNE visualization are not very convincing. In my opinion, the author should not use the discriminator to extract feature representations, because the PD-GAN discriminator is less supervised than ADC-GAN. I suggest that the author use a pre-trained classification model to extract features for a fair comparison.\n\n**R5**: We provide T-SNE visualization to investigate whether the discriminator/classifier (they are shared with each other except the output layer) captures the data-to-label relationships. Therefore, we need to use the discriminator/classifier to extract feature representations. Indeed, the PD-GAN discriminator is less supervised than ADC-GAN, and thus can hardly capture the data-to-label relationships, which is a weakness in conditional generative modeling compared with our ADC-GAN.\n\n**Q6**: In the caption of Figure 4, the author says that the T-SNE uses training data, but in the last paragraph of section 5.3, the author says that the T-SNE uses the validation data.\n\n**R6**: The T-SNE visualizes the training set in Figure 4 and we will fix the typo in the updated version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "N4Jdjox-2m",
                "writer": "author",
                "reply_to": "F9V6r9VE3YK",
                "title": "Response to Reviewer DPgR",
                "comment": " Thank you for your efforts in reviewing this paper. We respond to your concerns to clarify the theoretical contribution made in this work and highlight the effectiveness of the proposed ADC-GAN.\n\n**Q1**: It seems that Theorem 1 has already been covered in TAC-GAN paper (paragraphs below eq.4 of the TAC-GAN paper [R1]). Although mathematical formulations are different from each other, the arguments of Theorem 1 and the paragraphs seem to be very similar. I think it is essential to clarify differences between two arguments.\n\n**R1**: Our Theorem 1 reveals more than Theorem 1 in TAC-GAN. TAC-GAN's theorem just shows that AC-GAN will induce degenerate conditional distribution $Q_{Y|X}$ even when $Q_X=P_X$. In addition to revealing this issue, our theorem also shows that AC-GAN attempts to maximize $\\text{KL}(Q_X\\|P_X)$, which is contrary to conditional generative modeling and is not stated in the TAC-GAN paper. The contradictory divergence leads to the training stability and non-robustness with respect to $\\lambda$ of TAC-GAN (see Figure 4).\n\n**Q2**: It seems that all experiments were conducted once. It would be better to conduct experiments several times since GANs have been known to have a large performance variance.\n\n**R2**: We ran two new experiments on CIFAR-10 and CIFAR-100 during the rebuttal period. With the previous results, there are now three trails of all methods on CIFAR-10 and CIFAR-100. We here report the means and standard deviationtions over the three training runs. Experiments on Tiny-ImageNet are ongoing and the results will be reported in the updated version.\n\n| FID       | PD-GAN         | AC-GAN          | TAC-GAN         | ADC-GAN                 |\n| --------- | -------------- | --------------- | --------------- | ----------------------- |\n| CIFAR-10  | $6.30\\pm 0.19$ | $6.81\\pm 0.11$  | $5.88\\pm 0.14$  | $\\mathbf{5.75}\\pm 0.14$ |\n| CIFAR-100 | $8.64\\pm 0.20$ | $11.58\\pm 0.03$ | $10.99\\pm 0.29$ | $\\mathbf{7.94}\\pm 0.03$ |\n\n**Q3**: The contribution that ADC-GAN can generate diverse images compared to ACGAN and TAC-GAN is not fully demonstrated. Although FID has been a widely used metric to measure fidelity and diversity of generated images, I think It is not enough. I recommend the authors to utilize the improved precision and recall [R2], classification accuracy score [R3], or density and coverage [R4] to quantify the ability of generating diverse images of ADC-GAN.\n\n**R3**: We report the improved precision (P), recall (R), density (D), and coverage (C) results on CIFRA-10 and CIFAR-100 in the following table.\n\n| P, R, D, C | PD-GAN                     | AC-GAN                     | TAC-GAN                    | ADC-GAN                                    |\n| ---------- | -------------------------- | -------------------------- | -------------------------- | ------------------------------------------ |\n| CIFAR-10   | 0.772, 0.646, 0.999, 0.887 | 0.767, 0.618, 0.981, 0.884 | 0.756, 0.647, 0.994, 0.883 | 0.754, **0.686**, 0.958, **0.891**         |\n| CIFAR-100  | 0.743, 0.649, 0.868, 0.826 | 0.730, 0.538, 0.766, 0.754 | 0.740, 0.545, 0.812, 0.775 | **0.772**, **0.652**, **0.949**, **0.845** |\n\nThe proposed ADC-GAN achieves the best recall (R) and coverage (C) results, verifying that it improves the diversity of generated samples. ADC-GAN also obtains the best precision (P) and density (D) scores on CIFAR-100, showing better image fidelity on the more fine-grained dataset.\n\n**Q4**: In section 5.1, the authors conducted the distribution learning experiment using one-dimensional conditional gaussians whose supports are overlapped. I accept that ADC-GAN can learn the joint distribution which consists of the one-dimensional conditional gaussians better than PD-GAN, AC-GAN, and TAC-GAN. However, what about a joint distribution which consists of conditional gaussians with disjoint supports? Can ADC-GAN learn the joint distribution better than other cGANs?\n\n**R4**: To evaluate the performance on MoG with disjoint supports, we set the MoG dataset with $\\mu_0=0, \\mu_1=6, \\mu_2=12$ and $\\sigma_0=1,\\sigma_1=1,\\sigma_2=1$. The following table reports the MMD results of different methods with and without the GAN loss.\n\n| Log Loss | PD-GAN | AC-GAN | TAC-GAN | ADC-GAN |\n| -------- | ------ | ------ | ------- | ------- |\n| Class_0  | 0.0048 | 0.0432 | 0.0142  | 0.0255  |\n| Class_1  | 0.9581 | 0.4196 | 0.1940  | 0.0020  |\n| Class_2  | 3.5979 | 0.3317 | 0.6559  | 0.7495  |\n| Marginal | 0.7329 | 0.0848 | 1.3020  | 0.0621  |\n\nIn general, our ADC-GAN can faithfully learn the real joint distribution regardless of its shape.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "20CALrcGm7Y",
                "writer": "author",
                "reply_to": "LbdeIQnr2Df",
                "title": "Response to Reviewer mZT7 [1/2]",
                "comment": " We would like to thank the reviewer for spending time reviewing our paper and providing constructive feedback. We here respond to each weakness raised by the reviewer.\n\n**Q1**: The key results seems to be Thm 2, which is based on Prop. 2. However, the proof shows pm(x, y, l) = pm(x, y, 1) + pm(x, y, 0) = 1/2 p(x, y) + 1/2q(x, y). at the very beginning. How do you get the second equation?\n\n**R1**: The second equation comes from our definition of pm. We define pm(x,y,1) = 1/2 p(x,y) and pm(x,y,0) = 1/2 q(x,y). We will clarify it more clearly in the updated version.\n\n**Q2**: The main criticism of AC-GAN is being generator-agnostic, which I think it's not fully appropriate. A common practical implementation of AC-GAN is also using generated data to train classifier. Under this, whether it can simply resolve the generator-agnostic issue? Could you comment on it?\n\n**R2**: As we discussed in Appendix B that the original AC-GAN could also suffer from the generator-agnostic issue of our used \"stable\" AC-GAN. As the authors of TAC-GAN said in https://github.com/batmanlab/twin-auxiliary-classifiers-gan/issues/1, training the auxiliary classifier with fake data may influence the classification accuracy of the auxiliary classifier due to that the fake data is poor quality at the beginning, and thus there is no advantage to training the auxiliary classifier on the fake data. In addition, the authors of ReACGAN also suggested in https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/issues/74 that removing conditional loss on fake images when training the discriminator (classifier) will enable ACGAN to generate more realistic images. Many other papers also use the \"stable\" version of AC-GAN (e.g., Eq. 6,7,8,9 in https://arxiv.org/pdf/1703.02000.pdf, Eq. 2,3,4 in https://arxiv.org/pdf/1811.11163.pdf, Eq. 3,4,5,6 in https://arxiv.org/pdf/2105.05501.pdf). In summary, training the classifier of AC-GAN without the generated data is also a common practice and can yield better performance.\n\n**Q3**: The reported numbers looks not quite consistent with other works to me.\n\n**R3**: \n\n- For CIFAR-10, we implement methods and conduct experiments on CIFAR-10 based on the official BigGAN-PyTorch repository, which is different from the PyTorch-StudioGAN repository in aspects of training settings and evaluation protocols.\n\n- For CIFAR-100, we implement methods and conduct experiments on CIFAR-100 based on the official BigGAN-PyTorch repository, which is slightly different from the TAC-GAN codebase. For example, the batch size is 50 instead of 100 used in TAC-GAN, and the discriminator update steps are 4 per generator step while it is 2 in TAC-GAN. And we use a single GPU to conduct these experiments due to our limited computational resources while TAC-GAN uses two GPUs. Besides, we cannot reproduce the results of TAC-GAN on one GPU using their code.\n\n- For ImageNet, the results of BigGAN/PD-GAN and TAC-GAN on ImageNet are from the TAC-GAN paper, where the experimental settings such as the number of channels and training iterations are different from the original BigGAN. We follow the experimental settings of TAC-GAN (due to our limited computational resources) to run ADC-GAN on ImageNet.\n\nIn summary, our comparison is fair as we implement all methods using the same experimental settings.\n\n\n\n**Q4**: Some descriptions are not accurate. For example, there are multiple sentences about \"GANs are notoriously unstable to train\", which is true back to 2014. However, there are already \"tons\" of papers working on it to resolve the issues. The authors should at least cite those and make a fair description. Secondly, the below Eq (6), the authors mention they proposed method can be \"unbiasedly optimize\", which is not true under alternative and minibatch setting. Again, there are lots of works (check all the works on GAN optimization) discussing this issue. The authors should remove this incorrect claim.\n\n**R4**: We would like to thank the reviewer for valuable feedback and will refine these claims in the updated version.\n\n**Q5** Proposition 1 seems trivial. Also, Theorem 1 is quite similar to the analysis in TAC-GAN.\n\n**R5**: Our Theorem 1 reveals more than Theorem 1 in TAC-GAN. TAC-GAN's theorem just shows that AC-GAN will induce degenerate conditional distribution q(y|x) even when q(x)=p(x). In addition to revealing this issue, our theorem also shows that AC-GAN attempts to minimize -KL(q(x)||p(x)), which is contrary to conditional generative modeling and is not stated in the TAC-GAN paper. The contradictory divergence leads to the training stability and non-robustness with respect to $\\lambda$ of TAC-GAN (see Figure 4).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LbdeIQnr2Df",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Yn4CPz_LRKO",
                "title": "",
                "comment": "The paper is about improving conditional GANs. To be specifically, it also aims to resolve the bias issue of ACGAN by proposing a discriminative classifier. The discriminative classifier is a hybrid model of discriminator and classifier, where it has to not tell real or fake, but also the class.  Preliminary analysis of the proposed method are provided. Experiments are conducted on the standard benchmarks.  Strength:\n\n* The proposed discriminative classifiers seems interesting to resolve the biased issue of ACGAN. \n\nWeakness:\n\n* The key results seems to be Thm 2, which is based on Prop. 2. However, the proof shows pm(x, y, l) = pm(x, y, 1) + pm(x, y, 0) = 1/2 p(x, y) + 1/2q(x, y). at the very beginning. How do you get the second equation?  I guess it may not be a fatal error, but I can't tell the correctness at the moment. \n\n* The main criticism of AC-GAN is being generator-agnostic, which I think it's not fully appropriate. A common practical implementation of AC-GAN is also using generated data to train classifier, which I think it's a straightforward idea. Under this, whether it can simply resolve the generator-agnostic issue? However, by just doing so, the performance seems not as competitive as the reported numbers of the proposed method.  Could you comment on it? \n\n* The reported numbers looks not quite consistent with other works to me.  \n\n  - For CIFAR10 results, the reported FID are all below 7, however, it's not the case for most of the existing works. For example, all the models reported here https://github.com/POSTECH-CVLab/PyTorch-StudioGAN#cifar10-3x32x32 are all with FID > 7. \n\n - For CIFAR100, the numbers reported in TAC-GAN seems better than the 11.37 reported in the paper for the TAC-GAN and 7.98 for the proposed method. \n\n- For ImageNet, again the reported BigGAN is worse than it should be.  For example, see Table 2 in https://arxiv.org/pdf/2111.01118v1.pdf which provides a nice comparison. \n\n* Some descriptions are not accurate. For example, there are multiple sentences about \"GANs are notoriously unstable to train\", which is true back to 2014. However, there are already \"tons\" of papers working on it to resolve the issues. The authors should at least cite those and make a fair description.  Secondly, the below Eq (6), the authors mention they proposed method can be \"unbiasedly optimize\", which is not true under alternative and minibatch setting. Again, there are lots of works (check all the works on GAN optimization) discussing this issue. The authors should remove this incorrect claim. \n\n* Proposition 1 seems trivial. Also, Theorem 1 is quite similar to the analysis in TAC-GAN. I would highly suggest removing Proposition 1,  which you don't have to pretend to be a theoretical paper. Also, cite TAC-GAN for the analysis. We should not copy or redo the analysis from the predecessor. \n\n* I have the concern of the analysis. Most of the results rely on assuming sth components are optimal. However, in reality, they are not hold in reality, and there is no convergence analysis provided.  Could the authors comment on it?  \n\n* The biased issue of ACGAN is known.  For example, \n\n  - AC-GAN Learns a Biased Distribution, 2017\n - Unbiased Auxiliary Classifier GANs with MINE, 2020\n\n There are many more. The authors should provide a better overview for the progress of this direction.\n\n\nQuestion:\n\n* In Table 1, PD-GAN is optimizing JS(P||Q) while the proposed ADC-GAN is optimizing KL(P||Q).  To me, there should not much difference. Any insights why the proposed ADC-GAN, which optimizes an asymmeric loss, should be better? \n\nSuggestion:\n\n* There are some very recent works in NeurIPS and also highly relayed, \n  - Rebooting ACGAN: Auxiliary Classifier GANs with Stable Training, NeurIPS 2021\n  - A Unified View of cGANs with and without Classifiers, NeurIPS 2021\n  Although they are posted online after ICLR deadline, I would strongly encourage the authors comment on the similarities and differences between the proposed work and these two, because they will be on public for a when the paper decision of ICLR is out anyway. Note that I won't judge the paper decision based on this, but I think it's great to have for the community. It's fine even though the ideas are overlapping.  The main concerns are first on the analysis, which I couldn't tell the correctness at the moment. Second issue is the empirical results, which seems not consistent with other works. ",
                "rating": 5,
                "confidence": 5
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "might have a novelty issue because it bears some similarities with other works but it lacks a discussion in the revision",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the authors didn\u2019t provide a revised paper during the rebuttal",
                "Sentiment Expression": "leading to a difficulty to assess the quality of the final paper",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "is not ready to publish at the current stage and recommends a rejection",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper because additional experiments requested are not provided by the authors",
                "Sentiment Expression": "doesn\u2019t champion",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "yxafu6ZtUux": {
        "paper_id": "iclr_2021_yxafu6ZtUux",
        "paper_title": "AN ONLINE SEQUENTIAL TEST FOR QUALITATIVE TREATMENT EFFECTS",
        "paper_abstract": "Tech companies (e.g., Google or Facebook) often use randomized online experiments and/or A/B testing primarily based on the average treatment effects to compare their new product with an old one. However, it is also critically important to detect qualitative treatment effects such that the new one may significantly outperform the existing one only under some specific circumstances. The aim of this paper is to develop a powerful testing procedure to efficiently detect such qualitative treatment effects. We propose a scalable online updating algorithm to implement our test procedure. It has three novelties including adaptive randomization, sequential monitoring, and online updating with guaranteed type-I error control. We also thoroughly examine the theoretical proper- ties of our testing procedure including the limiting distribution of test statistics and the justification of an efficient bootstrap method. Extensive empirical studies are conducted to examine the finite sample performance of our test procedure. ",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The paper proposes a new framework for online hypothesis testing aimed at detecting causal effects (of treatments on outcomes) within subgroups in online settings where treatments are randomized.  Such settings occur in online advertising where different versions of the same website may be presented to a set of otherwise exchangeable users via A/B testing.\n\nUnder the standard causal assumptions of SUTVA, and sequential ignorability, in addition to a set of regularity conditions, the authors derive a result (Theorem 1) leading to an online test (Theorem 2).  Since the resulting test's limiting distribution does not have an exact analytic form, the authors instead propose a bootstrap approach to determine a set of parameters to properly control the error rate.\n\nThe author validate their approach by a simulation study, as well as via a user click log data from Yahoo!\n\nThe reviewer opinion was somewhat split on this paper, in particular some reviewers raised concern about some (conceptually significant) typos, interpretability of assumptions, and the need for parametric assumptions (the dichotomy between linear models and neural networks is surely a false one -- the semi-parametric literature obtains nice parametric style results, although perhaps not always for tests, without assuming parametric likelihoods all the time).\n",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "xL55g60uoiw",
                "reply_to": "iclr_2021_yxafu6ZtUux",
                "title": "Well written paper with an interesting approach for sequential A/B testing",
                "comment": "=== Contributions ===\n\nThis paper proposes a new framework for A/B testing in the frame of randomized online experiments. This new framework enables testing whether qualitative treatment effects for some specific segment(s) of the tested population can be detected or not. \n\nThe approach relies on a scalable algorithm with:\n- adaptive randomization: in this setting, observations are assumed to be dependent on each other, since treatment can be adjusted by looking at previous rewards;\n- a nonasymptotic upper bound on the type-I error for the online updating,\n- a maximum number of data peeking times that is growing with the number of observations.\n\nMoreover, a bootstrap method is provided in order to determine the stopping boundary. This  circumvents the absence of any tractable analytical form for the limiting distribution of this new test statistic.\n\nFinally, the method is accompanied with experiments on the finite sample performance of the test procedure with simulated and real data from Yahoo!.\n\n=== Strong points ===\n\nThe paper is well written and all results are well introduced with interpretation in words which makes it easy to follow.\nDirect application in practice of the approach can be personalization which is currently a challenge for tech companies. Hence this paper is of great interest for the ML community.\n\n=== Weak points ===\n\nMinor:\nAuthors claim that Figure 3 reports experiment results regarding QTE. However, on this figure, we see only the results for ATE and HTE. Would it be possible to add them? I assume QTE results are better than ATE and HTE ones...\n\n=== Recommendation ===\n\nOverall, I vote for accepting this submission. My acceptance is supported by the strong points stated above. My grade can be further strengthened if the authors can address the points which for me need to be clarified, the biggest one being the correction of Figure 3 to fully support the efficiency of the approach.\n\n=== Additional feedback ===\n\nFor Figure 2, what do S1 and S2 mean? I guess \u201crandom\u201d refers to probability 0.5 to be assigned to one or the other treatment and Adaptive to the epsilon-greedy approach.\n\nHow long does it take on average to compute each test for Yahoo data? Do you recommend applying the test for low dimensional data (number of features is 5 for Yahoo)?\n\nMinor details:\n-For readability, I would add in the title of Figure 1 that A is the treatment applied and Y the associated reward, since they have not been yet introduced at the time of the figure\u2019s reference.\n\nFor reproducibility of the results, are the Yahoo data used for the experiment freely available? If yes, would it be possible to add a link to the repository?\n\n=== Questions to help to clarify ===\n\nHow does it relate with Bandits tests that are also online? Would it make sense to add some experiments to show when it is better to use Bandits tests over BAT method for A/B testing?\n\n=== After authors' feedback period ===\n\nI read carefully authors' responses to all reviews. The author's addressed my concerns and I guess the ones of the other reviewers too. One limitation that can be raised now is that the method is better suited for low-dimensional data.\nHence, I keep my accept score.\n",
                "rating": 7,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "8j7dgYR4Mqt",
                "reply_to": "Z3lhs-56g-t",
                "title": "Response to Reviewer 4",
                "comment": "We greatly thank your valuable comments, many of which will lead to a much improved paper. In particular, we have added discussions on the assumption. In our numerical experiments, we also have a setting where the treatments are adaptively generated according to the $\\epsilon$-greedy policy in order to balance the tradeoff between exploration and exploitation (see the results in Figure 2 under adaptive designs). We attempt to address all the points one by one in the following. The revised manuscript taking into accounts all your suggestions has been uploaded. Please refer to the most updated revision for details.\n\n**Equation (1)** We apologize for the confusion. It should be $\\beta_0^*$ and $\\beta_1^*$. They are defined as the true parameters in the Q-function (page 3, line 5). We have corrected the notations in the paper (Equation (1), page 3). \n\n**Assumption (A2)** (A2) is different from the assumption that $A_i$ is measurable with respect to $\\mathcal{F_{i-1}}\\cup \\sigma(X_i)$. Take a completely randomized study as an example where ${A_1,A_2,\\cdots,}$ is independent of ${(X_1,Y_1), (X_2,Y_2),\\cdots}$. In this case, (A2) is satisfied. However, $A_i$ is not a deterministic function of $\\mathcal{F_{i-1}}$ and $\\sigma(X_i)$. Consequently, the assumption that $A_i$ is measurable with respect to $\\mathcal{F_{i-1}}\\cup \\sigma(X_i)$ is violated. \n\nIn the literature, Assumption (A2) is referred to as the sequential randomization assumption (Zhang et al., 2013). It essentially assumes there is no unmeasured confounders and is automatically satisfied in a randomized study where the treatments are independently generated of the observed data. It is also satisfied when treatments are adaptively generated according to $\\epsilon$-greedy, upper confidence bound or Thompson sampling algorithms. It guarantees that the causal estimand (defined through the potential outcomes) is estimable from the observed dataset. We have added the related discussions in the paper (page 3, line 12).\n\n**Theorem 1**. Thanks for your comment! We first clarify the meaning of Equation (5). It requires the strategy to choose the arms aggregated over different decision points to converge to a fixed strategy under certain rate. \n\nWe next discuss on the choice of $\\alpha_0$. In Appendix C, we show the parameter $\\alpha_0=1/2$ when an $\\epsilon$-greedy strategy is used for randomization to balance the trade-off between exploration and exploitation. Please see Page 12 for details. \n\nFinally, we discuss the relation to contextual bandit. As we have commented, this condition holds with $\\alpha_0=1/2$ when an $\\epsilon$-greedy strategy is used. More generally, one might use other commonly-employed randomizations strategies in the linear bandits (e.g., UCB or Thompson sampling). This condition holds as long as the randomization strategy converges at certain rate. We have added the related discussions in the paper (page 5, line 4).\n\n**Equations (6) and (7)** In Appendix C, we show the parameter $\\alpha_0=1/2$ when an $\\epsilon$-greedy strategy is used for randomization to balance the trade-off between exploration and exploitation. (6) is thus equivalent to require the number of basis function $q$ to grow at a rate slower than $N^{1/6}(t_1)$. It is automatically satisfied when $q$ is bounded. (7) is satisfied when $K$ grows polynominally fast with respect to $n$. \n\nWe have added these interpretations in the page (page 5, line 1). \n\n**Online FDR control** Thanks again for this comment! Our problem is different from multi-armed bandit testing with online FDR control and is thus not comparable.\n\nFirst, we consider a single null hypothesis in our paper whereas multi-armed bandit testing with online FDR control consider settings with multiple testing hypotheses.\n\nSecond, for the multi-armed bandit testing problem, the goal is to control FDR whereas in our setup, we aim to control the type-I error of the test procedure. \n\nFinally, we remark that it is also interesting to extend our current proposal to settings with multiple treatments. This yields a multiple testing problem with online FDR control. We leave this for future research. We have added the related discussions in the paper (Section 5, page 8).\n\nWe once again appreciate your effort in reviewing our paper. We hope that the above discussion can address your concern.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xO8ODHjDHjl",
                "reply_to": "fJZDfhxUN2",
                "title": "Response to Reviewer 1",
                "comment": "We greatly thank your valuable comments, many of which will lead to a much improved paper. We attempt to address all your questions one by one in the following. The revised manuscript taking into accounts all your suggestions has been uploaded. Please refer to the most updated revision for details.\n\n1. Thanks very much for your suggestion. In addition to ATE, sometimes we are interested to locate the subgroup (if exists) that the new product performs significantly better than the existing one, as early as possible. This amounts to QTE. Take a ride-hailing company as an example. Suppose some passengers are in the recession state (at a high risk of stopping using the company\u2019s app) and the company comes up with certain strategy to intervene the recession process. We would like to if there are some subgroups that are sensitive to our strategy and pin-point these subgroups if exists. In this case, $X$ includes age, gender and other related features, $A$ is a binary strategy indicator (whether such a strategy is applied to the passenger or not) and $Y$ is the passenger\u2019s number of rides in the following two weeks. \n\n    We have added the related discussions in the paper (see page 1, the first paragraph in the introduction and page 2, the caption of Figure 1).\n\n2. Thanks for your comment. We do have a dataset from a ride-hailing company comparing the performance of two subsidy strategies. However, due to privacy concerns, we cannot use that data in our paper. Therefore, we use the Yahoo! Today Module data as it is publicly available. We understand it is not very interesting to compare the effects of two articles, the dataset in our paper is used as an illustration.  \n\nWe once again appreciate your effort in reviewing our paper. We hope that the above discussion can address your concern.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "u1-7ZVS8csh",
                "reply_to": "xL55g60uoiw",
                "title": "Response to Reviewer 2",
                "comment": "We greatly thank your valuable comments, many of which will lead to a much improved paper. We attempt to address all your comments one by one in the following. The revised manuscript taking into accounts all your suggestions has been uploaded. Please refer to the most updated revision for details.\n\n**Weak points on Figure 3** Many thanks for pointing this out. We apologize for the typo. We did calculate QTE instead of HTE in the real data example and the y-axis label should be QTE instead of HTE. We have corrected this in the paper (page 8, Figure 3). \n  \n**S1 and S2 in Figure 2** Thanks for the comment. We apologize for the confusion. We consider two scenarios in our simulation (page 7, line 23). S1 refers to the first scenario and S2 refers to the second scenario. As you commented, \u201crandom\u201d refers to the first design where the treatment assignment is completely random (page 7, line 17). \u201cAdaptive\u201d refers to the second design where the treatment is adaptively generated (page 7, line 19). We have added these clarifications the caption of Figure 2 (page 8). \n\n**Notation in Figure 1** Thanks again for this suggestion. These notations have been introduced in the caption of Figure 1 (page 2).\n \n**Reproducibility and computation time of the real data**  The dataset is available online. Following your suggestion, we have added the link in the paper (page 7, line-1). \n\nAs for the computation time, although it depends on the number of specified interim looks, on average the computation time is around several seconds. We have added the discussion in the paper (page 8, line 13)\n\n**Performance with low dimensional data** We do recommend applying the test for low dimensional data. The proposed test achieves good performance in both the synthetic and real datasets. Specifically, in our real dataset where $d=5$, we find the proposed test is consistent using A/A and A/B experiments. In our synthetic dataset where $d=3$, we find the proposed test is more powerful than other competing baselines. \n\n**Relation to the bandit tests** Thanks for this comment. In bandit tests, one typically conducts a multi-armed bandit experiment to identify the arm that receives the maximum reward. We discuss the difference and similarity between bandit tests and the proposed tests below.\n\nDifference: Similar to most of the existing A/B testing methods, it focuses on comparing the ATE between different arms whereas the proposed test mainly considers identifying the QTE. Bandit tests and the proposed tests target different problems. The two tests might not be comparable. \n\nSimilarly: Both the bandit test and the proposed test apply to online experiments and allows for adaptive treatment allocation. More specifically, in bandit test, one adaptively allocates the treatment based on the observed data stream to maximize the cumulative reward. The proposed test is consistent under adaptive design as well (see Figure 2, page 8).  \n\nWe once again appreciate your effort in reviewing our paper. We hope that the above discussion can address your concern.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0cJ1oihF2t1",
                "reply_to": "01t8mv2r25n",
                "title": "Response to Reviewer 3",
                "comment": "We greatly thank your valuable comments, many of which will lead to a much improved paper. We attempt to address all the points one by one in the following. The revised manuscript taking into accounts all your suggestions has been uploaded. Please refer to the most updated revision for details.\n\n1. We respectfully disagree with this comment. This comment is incorrect. We indeed addressed this issue but you may overlook this. \nSpecifically, we coupled the $\\alpha$-spending approach (Jennison & Turnbull, 1999) with bootstrap to adaptively adjust the p-values. The $\\alpha$-spending approach allocates the total allowable type I error at each interim stage according to an error-spending function. This guarantees our test controls the type-I error. As a result, we show both theoretically (Theorem 4, page 6) and numerically (Figure 2, page 8) that our test is valid. \n\n    Please refer to page 2, line 12 and page 6, line 14 for details. \n\n2. First, we did not require the approximation error to be zero. The proposed test is valid as long as the approximation error converges at certain rates. In Appendix B, we discuss the approximation space that satisfies these conditions.\n\n    Second, the linear approximation space is used to facilitate the computation. In online experiments, the decision is made every few minutes to determine whether to stop the experiment or continue collecting more data. Nonlinear models such as neural networks, on one hand, are much more computationally expensive. On the other hand, it is difficult to quantify the uncertainty of neural network estimates and derive the corresponding p-value. \n\n    Third, unlike complicated nonlinear models, the linear approximation space makes the estimated Q-function more stable and interpretable. This is important in industrial applications. \n\n3. Following your suggestion, we have moved the two theorems in the main text (page 6).  \n\nWe once again appreciate your effort in reviewing our paper. We hope that the above discussion can address your concern.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fJZDfhxUN2",
                "reply_to": "iclr_2021_yxafu6ZtUux",
                "title": "Official Blind Review #1",
                "comment": "This paper proposed a powerful online sequential test which can efficiently detect qualitative treatment effects (QTE). The test algorithm involves adaptive randomization, sequential monitoring and online updating.  Theoretical guarantee on the Type-I error is presented.\n\nOverall, the paper is well-written, with a clear mathematical definition of the problem, solid theoretical result and experiment results. However, there are some questions that I did not fully understand.\n\n1. In the introduction, the difference of ATE (which is widely used in AB test in tech companies) and QTE is discussed only in words. Is there a more specific case on how QTE can be applied in tech companies where ATE fails? Could you present what are X, Y and A in your specific case, and why this case is specifically important for tech companies? I think this paper will be more interesting to readers in the industry if a detailed case is presented to bridge the introduction and the content.\n\n2.  The author may argue that the experiment on Yahoo! Today Module is an example, but I did not find the point on this experiment. If I understand it correctly, A=0/1 represent two specific article IDs. However, in tech companies, A=0/1 usually represent two algorithms, two sets of hyperparameters or two strategies. The set of article IDs is very huge so it is not very interesting to compare the effect on two article IDs only.\n\n",
                "rating": 6,
                "confidence": 2,
                "writer": "official_reviewer"
            },
            {
                "review_id": "01t8mv2r25n",
                "reply_to": "iclr_2021_yxafu6ZtUux",
                "title": "Reviewer 3's Report",
                "comment": "This paper studies online test for qualitative treatment tests. The authors propose a scalable online algorithm for Type 1 error control. I find the paper under-developed that the writing has to be substantially improved, and the presentation, especially in Section 3.2, is not friendly.\n\n1. The authors claim to have an \"Online\" algorithm. However, I don't see that the algorithm addresses any real online challenge. The challenge of online testing is that we are doing testing at each iteration, and it is difficult to adjust all p-values. The authors didn't address this issue at all, which is a fatal flaw.\n\n2. The linear space approximation is very artificial to me. Can the authors give some real motivating applications?\n\n3. I highly suggest the authors to move the theorems 3 and 4 to Section 3.2, and move the derivation (which are standard) to the appendix.\n\nOverall, I don't see much novelty, and the authors are overclaming the contribution. This is a clear rejection.",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Z3lhs-56g-t",
                "reply_to": "iclr_2021_yxafu6ZtUux",
                "title": "online sequential test",
                "comment": "I am a statistician but I am not an expert in sequential test. My questions and remarks can therefore only be those of a rather naive reader. I understand the motivation behind the paper, but I would clearly have liked to be able to clearly understand the assumptions used. I would also have to work on a  \"real\" algorithm for which one is able to control \"everything\" (in particular, the strategy to choose the . I do not have the impression that this is completely the case here and some aspects need to be clarified.\n\n- I do not understand Eq. (1). The null and the alternative in this case depends on $\\beta_0,\\beta_1$ this is very strange. \n- I do not understand (A2): since $\\{X_k, Y_k^*(0), Y_k^*(1)\\}_{k \\geq 1}$ are independent, it suffices to say that $A_i$ is measurable with respect to $ \\mathcal{F}_i \\vee \\sigma(X_i)$ \n- Theorem~1: the key is to check Eq. (5) which means that you have a strategy to choose the arms with a non trivial regret bound ?  What are the typical values of $\\alpha_0$ ? How this is related to contextual bandits (since there is notion regret), to best arm identification problems in linear bandits ? \n- The meaning of Eqs. (6) and (7) are very difficult to grasp \n- How your results compare to multi-armed bandit testing with online FDR control ? ",
                "rating": 4,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "(conceptually significant) typos, interpretability of assumptions, and the need for parametric assumptions",
                "Sentiment Expression": "raised concern",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "47lpv23LDPr": {
        "paper_id": "nips_2022_47lpv23LDPr",
        "paper_title": "Unsupervised Learning of Group Invariant and Equivariant Representations",
        "paper_abstract": "Equivariant neural networks, whose hidden features transform according to representations of a group $G$ acting on the data, exhibit training efficiency and an improved generalisation performance. In this work, we extend group invariant and equivariant representation learning to the field of unsupervised deep learning.  We propose a general learning strategy based on an encoder-decoder framework in which the latent representation is separated in an invariant term and an equivariant group action component. The key idea is that the network learns to encode and decode data to and from a group-invariant representation by additionally learning to predict the appropriate group action to align input and output pose to solve the reconstruction task.  We derive the necessary conditions on the equivariant encoder, and we present a construction valid for any $G$, both discrete and continuous. We describe explicitly our construction for rotations, translations and permutations. We test the validity and the robustness of our approach in a variety of experiments with diverse data types employing different network architectures.",
        "paper_acceptance": "Accept",
        "meta_review": "The paper proposes an auto-encoder that maps a point to a canonical point and  to a group element such that the composition of the group and the canonical point reconstructs the point (the invariance / equivariance is in this sense).   The method is promising since it learn in an unsupervised way the group actions. Experiments were on simple tasks and group actions. Reviewers were overall positive and the paper improved a lot during rebuttal/ revision thanks to their recommendations.  \n\nI have one recommendation to the authors to include the use of the representation learned in a classification task to see how the learned representation alleviates the need of large training samples. \n\nAccept \n ",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "rHMXfEdHxt6",
                "writer": "author",
                "reply_to": "nips_2022_47lpv23LDPr",
                "title": "General Response after Rebuttal ",
                "comment": " We take again the opportunity to thank all the reviewers for the insightful reviews, comments and discussion. \n\nWe have uploaded a new updated version of the paper with an expended related work section. This can be currently found in Appendix G to respect the current page limit, but will be moved in the main section of the final version. We point out that this section is still in a preliminary form, as we are working on including further relevant literature, but since the deadline for\nupdating the draft is today, we decided to share it anyway in its current form.\n\nAdditionally, we performed an additional experiment (MNIST with $G=\\text{SO}(2)$), where we compared our model to a fully equivariant AE (that is, its embedding is equivariant), where we extracted **post-training** an invariant embedding through an invariant pooling of the equivariant one. While the reconstruction loss of the fully equivariant embedding is comparable to ours, the corresponding invariant embeddings are, as claimed, less informative, as we show in the updated draft (Appendix B) through TSNE plots as well as the accuracy of a KNN classifier, which performs at best on par with a classical model.\n\nWe believe that the additional experiments and discussions substantially improved our paper (and we thank again the reviewers for the insightful suggestions!), and we would appreciate if the reviewers were willing to reconsider their evaluation to reflect such improvements.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "lrMpuRzrZP",
                "writer": "author",
                "reply_to": "vc_MwH93TNK",
                "title": "Response to Revised Review 2: Additional Related Work Section",
                "comment": " We wish again to thank the reviewer for encouraging us to expand the related work section. \n\nWe updated our manuscript with a preliminary additional related work section (currently in Appendix G due to the current page limit). Please note that this is still a work-in-progress and we are actually working on it and include further relevant literature, but since the deadline for updating the draft is today, we decided to upload it in its current form, to show our commitment to actually follow the reviewer's advice.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "frspvHIBmL2",
                "writer": "official_reviewer",
                "reply_to": "7XU-A0-bDJ",
                "title": "Response",
                "comment": " Thank you! I've updated my recommendation.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RMc1R3Uk-A",
                "writer": "official_reviewer",
                "reply_to": "ipu2AjqQHQP",
                "title": "Theoretical Motivation",
                "comment": " Thanks for addressing my concerns regarding the motivation for your approach so thoroughly. Although my concerns regarding a number of statements in the manuscript remain, the authors have addressed most of my concerns. I've reflected this in my recommendation.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4tTd4DaNvqm",
                "writer": "official_reviewer",
                "reply_to": "nmijBZ-0hZ",
                "title": "The need for equivariant and invariant parts",
                "comment": " Thank you for looking into this. I, too, think that this experiment adds to the motivation for your approach and strengthens your submission.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Uz-IB04RXoI",
                "writer": "author",
                "reply_to": "pZOP7eIYdfJ",
                "title": "General Response to the Revised Review",
                "comment": " We thank the reviewer again for responding to our rebuttal and clarifying some of the concerns not properly addressed by us. Below we posted detailed answers to different comments made by the reviewer.\n\nMost notably, we added another theoretical and empirical justification for our motivation to separate the invariant and equivariant part in our framework in order to learn to extract expressive invariant representations of data. We think that these additional responses and experiments further clarify and underline the contribution and impact of our work and hope that the reviewer reconsiders their evaluation of our work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7XU-A0-bDJ",
                "writer": "author",
                "reply_to": "4qamzVm0D2",
                "title": "Response",
                "comment": " > Before publication, I would recommend the authors weaken these statements about the learning of the group action, and be transparent about the limits/requirements of their pipeline.\n\nAgreed. We will replace the mentioned sentence from the abstract with the following sentence:\n\n*The key idea is that the network learns to encode and decode data to and from a group-invariant representation by additionally learning to predict the appropriate group action to align input and output pose to solve the reconstruction task.*\n\nMoreover, we will be more clear about what we mean with \"learning the group action\" in the final manuscript, i.e. adding another sentence clarify that we don't learn the group itself but the transformation of an (a priori) known group to align the input and (learned) canonical pose. \n\n> Your answer to question 4: Thanks for clarifying, I would recommend you make this clear in your manuscript as well, to avoid confusion with future readers.\n\nAgreed. We will clarify this in the revision. \n\nWe thank the the reviewer for raising these concerns helping us to avoid confusion.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "AOGuo00rI5H",
                "writer": "author",
                "reply_to": "-YUe6QSI1RG",
                "title": "Response",
                "comment": " We totally agree. We will clarify this point in the final manuscript. We thank the reviewer for making us aware of this.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nmijBZ-0hZ",
                "writer": "author",
                "reply_to": "UF9qo5oz5g",
                "title": "Response 2: Empirical Motivation",
                "comment": " Following the very meaningful suggestion of the reviewer, we performed an additional experiment, where we compare \nthe approaches listed above in the theoretical motivation. Namely, we trained a fully equivariant AE (that is, the embedding itself is fully equivariant, i.e. multiple 2-dimensional vectors) on MNIST with $G=\\text{SO}(2)$, and we perform an invariant pooling afterwards to extract the invariant part. \nSpecifically, following the discussion above, we have trained KNN classifiers on (a) the invariant embedding corresponding to the norm of the 2-dimensional vectors forming the bottleneck representation, (b) the angles  between the first and all other vectors and on (c) the full invariant embedding we obtained by combining the the norms and angles. We choose the number of vectors in the bottleneck in such a way that the dimensionality of the full invariant representation coincides with the one of our model. We visualized the resulting TSNE embeddings in Appendix B (Figure 6) of the newly revised manuscript. We also updated the corresponding table in the revised manuscript (Appendix B Table 1) as follows \n\n| Model      | Rec. Loss | KNN Acc. \n| ----------- | ----------- | ----------- | \n| classical      | 0.0170       | 0.68\n| QAE   | 0.0227        | 0.82\n| ours   | 0.0162       | 0.90\n| equiv AE (norm)      | 0.0189       | 0.56\n| equiv AE (angle   | 0.0189        | 0.53\n| equiv AE (complete)   | 0.0189       | 0.67\n\nFrom the results we can see that, in comparison to the approximate invariant (QAE) and our invariant trained model, the invariant projected equivariant representations perform inferior. Although, as discussed above, we extract a complete invariant representation (which performs better than a subset of this representation like the norm or angle part), the resulting representation is apparently not as expressive and e.g. useful in a downstream classification task. This aligns well with our hypothesis, that our proposed framework poses a sensible supervisory signal to extract expressive and higher-level (e.g. well-structured according to the signal such as the digit class) invariant representations that are superior to invariant projections of equivariant features. We belief that this experiment gives strong empirical evidence for the motivation of this work and thank the reviewer very much for suggesting such a comparison, strengthening our manuscript.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ipu2AjqQHQP",
                "writer": "author",
                "reply_to": "UF9qo5oz5g",
                "title": "Response 1: Theoretical Motivation",
                "comment": " We thank the reviewer for this response. We want to provide additional evidence that the two following approaches are very different regarding the expressiveness of the invariant representation $z$:\n\n- Equivariant AE with pooling **after** training: In this approach the full AE is equivariant, and can be trained with no necessity of extra strategies since the loss is automatically invariant. The bottleneck representation $z'$ is equivariant with respect to $G$, and in order to extract an invariant representation $z$ we can apply an invariant pooling $z = p(z')$.\n\n- Equivariant AE with pooling **during** training: In this approach the AE is equivariant, but an invariant pooling $z = p(z')$ is applied in the network during training. This means, all the information flow (up to a group transformation) passes through the invariant representation $z$ during training.\n\nIf we understand correctly, the reviewer is concerned that Approach 1 might result in similarly expressive representations as (more sophisticated) Approach 2, which we follow in our work. That is, we want to leverage the expressiveness of equivariant models (encoder and decoder functions), but at the same time force all the information to flow through an invariant bottleneck, to force the model to store all the relevant information necessary to reconstruct the input in an invariant fashion. \n\nWe can show mathematically that, given one function $p$, it is often not enough to capture the full information stored in an equivariant representation. \nFor example, given two vectors $\\mathbf{r}_1 = (x_1, y_1), \\mathbf{r}_2=(x_2, y_2)$ in 2D and for group $G = \\text{SO}(2)$, taking as our invariant pooling the norm of the two vectors \n\n$z = (||\\mathbf{r}_1||, ||\\mathbf{r}_2||) = (\\sqrt{x_1^2+y_1^2}, \\sqrt{x_2^2 +y_2^2})$~,\n\ndoes not allow the full system reconstruction. To do that, we would need to know the angles between the two vectors as well.\n\nTheoretically, it would be possible to extract a complete invariant representation from an equivariant one. This would imply extracting all possible independent pooling functions. In the above example, these corresponds to $||\\mathbf{r}_1||$, $||\\mathbf{r}_2||$, $\\mathbf{r}_1 \\cdot \\mathbf{r}_2$. However, for higher dimensional and more complex data and general groups it is unfeasible to extract all possible invariant pooling. On the other hand, when we choose a specific subset of pooling, we will inevitably loose some amount of information stored in the equivariant representations.\n\nOur methods avoids this problem since the model learns to solve the reconstruction task with the specific subset of pooling we implement in the architecture. Thus, even if our invariant embedding will loose dimensionality with respect to the equivariant representation, we are guaranteed that it is enough for the reconstruction if the model is trained successfully. \n\nWe can extend the above to any number of vectors in 2D with $\\text{SO}(2)$ symmetry. Let us suppose we have $M$ vectors\n$\\mathbf{r}_i=(x_i, y_i)$ for $i=1,\\dots,M$. We can completely determine the system up to a global rotation by considering, as before, the $M$ norms $||\\mathbf{r}_i||$ as well as all the angles (determined by the scalar products) between the vectors $\\mathbf{r}\\_{2, \\dots,M}$ and $\\mathbf{r}_1$, which we represent as $\\mathbf{r}\\_{2,\\dots,M}\\cdot \\mathbf{r}_1$. These correspond in total to $2M-1$ parameters, namely, $M$ norms and $M-1$ angles. This is the complete invariant representation, as the only parameter missing to recover the full $2M$ Cartesian system is one parameter corresponding to the angle for a global\n$\\text{SO}(2)$ rotation. \n\nAs we have seen, while this is indeed possible for simple and smaller groups, it is still tedious and rather quickly impractical for larger groups (or product of groups). ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "My031lmnnQC",
                "writer": "author",
                "reply_to": "vc_MwH93TNK",
                "title": "Response to Revised Review",
                "comment": " First of all, we thank again the reviewer for suggesting relevant related work and for suggesting the additional experiments. Here we briefly address the additional comments from the revised review. \n\nRegarding the additional references, we do think this is related work to ours and it will be included in the revised version of the paper. In our previous answer we mostly focused on the elements where these work differ from ours to highlight the novelty of theirs and our approach. Nonetheless, it is definitely related to our work, and we thank again the reviewer for point these out to us. \n\nWe wish, however, to emphasise once more one very crucial aspect of our work that differs from the above cited work (b). In their approach, the authors define two functions that during training learn separate aspects of the images. While these two functions are approximately related to shape and deformation of an image, they do not satisfy the strict (group-theoretic) definition of invariant and equivariant, respectively, embeddings for the local group of deformations. In other words, their AE is only approximately equivariant with respect to deformations, that is, a deformation of the input image will only be approximately equivariant. In our framework, the AE is *exactly* equivariant with respect to the group transformation of the input. \n\nFinally, our framework can be extended to local (infinite) groups as well. An example would be the case of the group of dilations, that is, an element $g\\in G$ act as a (point dependent) scaling factor. That is, if $f$ is a $V$-valued function, where $V$ is a vector space (for example $\\mathbb{R}^2$ for 2D images or $\\mathbb{R}^3$ for 3D point clouds), we have\n\n$ \\rho_X(g)f(v) = \\lambda(v)f(v), \\text{ for all } v\\in V~.$\n\nIn fact, this group has infinite dimension since it is defined by the parameters $\\lambda(v)$, where $v\\in V$, which is\noften infinite dimensional. (More precisely, $\\lambda(v)\\in G$ is a section of a principal $G$-bundle over $V$.)\nTo apply our framework, we would need to apply the same procedure of parameterizing a group action $\\psi$, but in \nthis case this will also depend on $v$, that is, $\\psi = \\psi(v)$. \nPractically, we would need to discretize the space (like a grid for image data or temporal intervals for time series), and predict a group action for all the points in the discretized space. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-YUe6QSI1RG",
                "writer": "official_reviewer",
                "reply_to": "j836-tmDV9F",
                "title": "Response to answers",
                "comment": " Thanks for your patient answers to my questions. Regarding my 7th question, I see I must have misunderstood this line in your manuscript; I interpreted your comment in line 335 to mean that any SE(3) invariant models would be unable to distinguish these chiral shapes, but you only state that these specific instances of SE(3) invariant models are unable to do so. I would recommend making this point clear; SE(3) invariance does not necessarily prevent distinguishing chiral shapes.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4qamzVm0D2",
                "writer": "official_reviewer",
                "reply_to": "kjG1Rp4uCN",
                "title": "Response to answers",
                "comment": " Thanks for your patient answers to my questions. Regarding questions 2, 3, and 5, my concerns have been addressed.\n\nYour answer to question 1: I still believe the current version of your manuscript contains misleading / incorrect statements which need to be addressed prior to publication. In multiple passages you make it seem like your model learns the group action itself. Instead, your model learns a pose estimation, right? What's more, to learn this pose estimation you need the ad-hoc group-specific implementations you claim to do away with. E.g. your abstract contains the following line:\n\n> The key idea is that the network learns the group action on the data space and thus is able to solve the reconstruction task from an invariant data representation hence avoiding the necessity of ad-hoc group-specific implementations.\n\nI don't see how you can square this line with section 3, in which you list multiple such group-specific implementations. Before publication, I would recommend the authors weaken these statements about the learning of the group action, and be transparent about the limits/requirements of their pipeline.\n\nYour answer to question 4: Thanks for clarifying, I would recommend you make this clear in your manuscript as well, to avoid confusion with future readers.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UF9qo5oz5g",
                "writer": "official_reviewer",
                "reply_to": "kblwrCGgEuh",
                "title": "The need for equivariant and invariant parts",
                "comment": " Thank you for clarifying, I believe I now understand your motivation for separate equivariant and invariant pipelines in the self-supervised setting. You would like to learn transformation-invariant representations, but doing this in a self-supervised setting with a reconstruction task requires an autoencoder equivariant to this transformation. To have supervision for the invariant representation, you argue for an invariant reconstruction followed by a transformation to obtain an equivariant reconstruction. Indeed, this seems sensible.\n\n> However, if the autoencoder is able to perfectly reconstruct the inputs from the equivariant embedding vectors, it is easy to see that the norm of the vectors will not necessarily store the information needed to reconstruct the input up to a rotation nor do we have a decoding function that is trained to do so.  \n\nIt took me a bit to understand your argument against simply using an equivariant autoencoder and invariantly aggregating the equivariant latent representation, but I believe you argue that this does not lead to expressive invariant representations. This does not necessarily seem trivial to me, and I would like to see some more extensive (either empirical or theoretical) motivation here. I believe in the above example the norm of the vector would in fact store all information needed to reconstruct the input invariantly; information on the pose of the input is only stored implicitly, in the direction of the embedding vector.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "en-mspcCH-",
                "writer": "author",
                "reply_to": "nips_2022_47lpv23LDPr",
                "title": "General Response",
                "comment": " We wish to thank all reviewers for their insightful comments. We addressed the specific concerns in detailed responses to each of the reviews below. Here, we wish to summarize the main points that might be relevant for all the reviewers and the overall discussion, highlighting the main differences from the original draft and the revised one.\n\nMotivated by the reviewers to apply our method to further complex dataset and to perform further analysis, we conducted the additional experiments, which can be found in the Appendix of the revised draft:\n \n- **ShapeNet with group $G=\\text{SO}(3)$**. We showed that our AE can successfully reconstruct rotated ShapeNet samples. We show that it clusters different shapes in the embedding space much better than a classical AE. We show the superiority of our embeddings by training a KNN classifier on them, achieving a $\\sim30$% performance gain in comparison to the classical version.\n\n- **QM9 regression with** $G=${$S_n, \\text{SO}(3)$} **as pretrained embeddings**. We finetuned the (newly pretrained on GEOM-QM9) AE on a reduced set of QM9 geometries, showing that the pretrained network outperforms the one trained from scratch on property prediction tasks.\n\n- **Comparison with Mehr et al. (2018)**. We compared our approach with the one of Mehr et al. (2018)\n    on the rotated MNIST dataset. Our approach is superior both in term of reconstruction loss as well as in the accuracy of a KNN\n    classifier trained on the bottleneck embeddings. \n\n\nIn summary, we think that by\n\n- clarifying the main motivation and contribution of our work,\n\n- addressing unclarities in the notation,\n\n- adding a discussion of missing related work and their relation to our work,\n\n- adding additional experiments to compare our work to such related work,\n\n- adding quantitative measures of the (superior) performance of our method (compared to other work) and\n\n- adding additional experiments on more complex datasets,\n\nwe address the main concerns of the reviewers and hope that the reviewers reconsider their evaluation of our work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dJBwneNXCf1y",
                "writer": "author",
                "reply_to": "vc_MwH93TNK",
                "title": "Response 4: New Experiment on ShapeNet",
                "comment": " The reviewer shares their concern about the use of toy data sets like MNIST and Tetris. We believe that our contribution is mainly in proposing a novel framework for unsupervised invariant representation learning which we think is best demonstrated on easy to interpret datasets. Moreover we demonstrate the performance of our proposed framework on the complex, real world dataset QM9 of molecular conformation. Still, we understand this concern, and followed the reviewers advise, running **additional experiments on the ShapeNet dataset**. As the dataset comes in an aligned form (e.g. cars are always aligned in the same orientation), we additionally applied random 90 degree rotations to remove this bias. \n\nWe utilized 3D Steerable CNNs proposed by Weiler et al. (2018) as equivariant encoder for the 3d voxel input space. We utilized the scalar outputs as rotation-invariant embedding ($z$) and predict (analogously to our experiments on 3d point clouds) 2 rotation-equivariant vectors to construct a rotation matrix ($g$). Similar to our MNIST experiment, we compared the resulting embedding space to the embeddings produced by a non-invariant autoencoder model. We again uploaded a revised manuscript where we visualize in Appendix F a TSNE projection of the embeddings of both models. We can see a well structured embedding space for our model with distinct clusters for the different shape classes. On the other hand, the embeddings produced by the non-invariant autoencoder is less structured and one can make out different clusters for the same shape label but in different orientations. \n\nMoreover, we compared the downstream performance and generalizability of a KNN classifier on shape classification, trained on 1000 embeddings and tested on the rest. The classifier based on our rotation-invariant embeddings achieved an **accuracy of 0.81** while the classifier based on the non-invariant embeddings achieved an accuracy of only 0.63.\n\nWe hope that this additional experiment on another complex dataset demonstrates the effectiveness and broad applicability of our method, as suggested by the reviewer. We will add this experiment to main part of the paper in final manuscript. We thank the reviewer for suggesting this additional experiment, as we think it is a strong demonstration of the performance of our proposed method.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZvcWs6WyhyJ",
                "writer": "author",
                "reply_to": "ZdQeyp-BVm4",
                "title": "Response",
                "comment": " We thank the reviewer for their valuable comments. Below you will find our responses to your comments.\n\nRegarding the comment that the reviewer makes, stating that our solution is \"too natural to be non-trivial\", we wish to re-iterate how our methods works, and why it is actually highly non-trivial.\n\nWe wish to learn a (decoder) map from an invariant bottleck to a non-invariant representation of the data. Now, since the bottleneck is invariant, different poses of the same shape will be decoded to the same (from encoder unknown) shape. Let us call this the decoder-canonical-shape.  In order to enable the model to complete the reconstruction task, we learn an additional function $\\psi$ (the group function), whose task is to predict the group element corresponding to the transformation between the input shape and the decoder-canonical-shape.\nThis is indeed non-trivial since the point of $\\psi$ is not learning an absolute pose (which would be indeed trivial because of equivariance) but a relative pose with respect to the reconstructed sample. The equivariance property merely ensures that this needs to be learned once per orbit (and not separately for each elements of the orbit).\n\nSpecifically:\n- The decoder is unaware of the input pose, since it only has access to the invariant bottleneck embedding.\n- The (full) encoder (including the group function $\\psi$) is unaware of the decoder-canonical pose, since this is obtained later in the network.\n - The only connection happens during training through the loss $d(\\rho_X(\\psi(x))\\delta(\\eta(x)), x)~,$ where $\\psi$ must learn how the decoder canonically reconstructs orbits.\n\nThus, it is only through the learning process that $\\psi$ can learn how to properly transform the input to match the decoder canonical choice.\n \nThe other non-trivial contribution of our work is concerns how to design such a group function $\\psi$ given *any* group $G$, both discrete and continuous. \n\n- First, we derive sufficient conditions that such a map needs to satisfy in order for the learning process to be possible. Again, we do not make any assumptions on the group $G$ or on the data space $X$. This is summarized in Propositions 2.3, 2.4 and Lemma 2.5, whose proofs can be found in the appendix. These statements are highly non-trivial, for instance, the fact that $\\psi$ needs to be equivariant up to the stabilizers of all the points of $X$.\n\n- Second, we propose an explicit construction which makes use of the two maps $\\mu$ and $\\xi$ and of the intermediate space $Y$. This construction satisfy all the requirement and it is group-agnostic. This is described in Proposition 2.6. Again, the design through an intermediate homogeneous space is very non-trivial, and its applicability is completely general, as we show in Propositions 2.8, 2.7, 2.9, as well as in the examples in Section 3.  \n\n#### Additional Experiments\n\nAnother non-trivial aspect of our work is that it is applicable on real-world dataset, as the QM9 dataset in section 5.3, where the model is able to reconstruct to a very high degree of accuracy conformations (3d coordinates) of small molecules. \n\nIn this context, the learned group invariant latent space can be used for robust (supervised) learning tasks of invariant properties (e.g. ground state energy of a molecular conformation), where in a low-data regime, training a classifier from scratch bares the risk of poor generalizability due to data scarcity of training labels (e.g., see our new experiment on MNIST downstream classification in Appendix B of the revised manuscript).\n\nWe performed additional experiments on the pretrained group-invariant AE on the extended GEOM-QM9 dataset which, as opposed to the standard QM9 dataset ($\\sim 130k$ samples), contains multiple conformations of small molecules. We trained the autoencoder on a reduced set of GEOM-QM9 ($\\sim641k$), containing up to $10$ conformations per molecule and utilized this pretrained encoder network to regress (invariant) energy targets, such as internal energy $U$ or enthalpy $H$ on the original QM9 dataset.\n\nWe observed that the pretrained encoder network learns faster and achieves better generalization performance than the architectural identical network trained from scratch. In Appendix D of the revised manuscript we  illustrates the learning curves for the two networks on different fraction on $5\\%$ and $25\\%$ labelled samples from original QM9 dataset to analyze the benefit of finetuning a pretrained encoder network on a low-data regime, when regressing on $H$. On a held-out test dataset of $1000$ samples, the pretrained encoder network achieves superior generalization performance in terms of $R^2$ with $0.7529$ vs. $0.0970$ in the $5\\%$ data regime, and $0.9908$ vs. $0.9093$ in the $25\\%$ data regime compared to the encoder that was trained from scratch.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UmF2iXsd5c_",
                "writer": "author",
                "reply_to": "gZFXNuV-DlI",
                "title": "Response",
                "comment": " ### General Response\n\nWe thank the reviewer for their valuable comments and the encouraging assessment of our work. Below you will find our responses to your comments.\n\n### Notation\n\nWe thank the reviewer for spotting some typos/missing definition in the text. We have corrected those in the revised version. \n\nWe just point our here that the maps $f_X$ and $f$ are different maps. The first represent a group action $f_X: X, G \\rightarrow X$ or, equivalently,\n$f_X: G \\rightarrow \\text{GL}(X;n)$, where $\\text{GL}(X;n)$ is the general linear group, defined as the group of $n\\times n$ invertible matrices with the matrix multiplication operation, and $n$ is the dimension of the vector space $X$.\nThe second, which we use in eq. (1), is a map between vector spaces $f:V\\rightarrow W$. Thus, for the notation, for $g\\in G$, we write $f_X(g)x$, since now $f_X(g)$ is a matrix acting on a vector space, thus we do not need additional parenthesis. \n\nWe noticed however, that the notation is prone to confusion given the similarity of the symbols. Thus, we replaced in the revised text $f_X$ with $\\rho_X$, in accordance with the notation used in the remainder of the text.\n\n### Group action vs. Representations\n\nHere we try to motivate the formalism behind the notation used to represent representations acting on vector space. \n\nFirst, we noticed a serious typo in the main text, which may have caused difficulties in understanding our notation. In the text, we erroneously wrote\n$\\rho_X: X \\rightarrow \\text{GL}(X)$ instead of the correct $\\rho_X: G \\rightarrow \\text{GL}(X)$. That is, a representation is a map that maps a group element $g\\in G$\nto an invertible $n\\times n$ matrix (assuming $X$ is a $n$-dimensional vector space). This notation is consistent with our usage of it in the main text, as we represent data (or features) as points in the corresponding vector spaces, and we use the corresponding representations in order to apply group transformations.\n\nEquivalently, a representation of the form $\\rho_X: G \\rightarrow \\text{GL}(X)$ can be seen as a group action of the form $\\rho : G, X \\rightarrow X$, as follows:\n\n$\n\\rho(g,x) = \\rho_X(g)x~.\n$\n\nThus, for vector spaces, the two formalism are equivalent.\n\nIn the main text, we mentioned with the group action formalism before restricting to representation on vector spaces, as they can be formally also applied to more general spaces (not necessary with a linear structure like vector spaces). Additionally, we made clear in the main text that we restrict to vector spaces, so the representation formalism is justified. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KuhdvDPvdIY",
                "writer": "author",
                "reply_to": "vc_MwH93TNK",
                "title": "Response 3: Questions",
                "comment": " The reviewer writes:\n>The authors should discuss what, if any, are the downstream applications of such a method. Is it data compression? Or do they envision the group invariant features learned by their method to be useful for high-level tasks like robust classification?\n\nIndeed, the learned group invariant latent space can be used for robust (supervised) learning tasks of invariant properties (e.g. ground state energy of a molecular conformation), where in a low-data regime, training a classifier from scratch bares the risk of poor generalizability due to data scarcity of training labels (e.g., see our new experiment on MNIST downstream classification).\n\nAnother interesting use case for our proposed unsupervised framework is the pretraining of powerful feature extractors that can later be fine-tuned on classification/regression tasks. We performed additional experiments on the pretrained group-invariant AE on the extended GEOM-QM9 dataset which, as opposed to the standard QM9 dataset ($\\sim 130k$ samples), contains multiple conformations of small molecules. We trained the autoencoder on a reduced set of GEOM-QM9 ($\\sim 641k$), containing up to $10$ conformations per molecule and utilized this pretrained encoder network to regress (invariant) energy targets, such as internal energy $U$ or enthalpy $H$ on the original QM9 dataset.\n\nWe observed that the pretrained encoder network learns faster and achieves better generalization performance than the architectural identical network trained from scratch. In Appendix D of the revised manuscript we  illustrates the learning curves for the two networks on different fraction on $5\\%$ and $25\\%$ labelled samples from original QM9 dataset to analyze the benefit of finetuning a pretrained encoder network on a low-data regime, when regressing on $H$. On a held-out test dataset of $1000$ samples, the pretrained encoder network achieves superior generalization performance in terms of $R^2$ with $0.7529$ vs. $0.0970$ in the $5\\%$ data regime, and $0.9908$ vs. $0.9093$ in the $25\\%$ data regime compared to the encoder that was trained from scratch.\n\nAnother useful application of our framework is generative modelling in the group invariant latent space. The proposed method can easily be transformed to a Variational Autoencoder which can be used to generate samples conditioned on an invariant representation. \n\nWe will include a more in-depth discussion of potential downstream applications of our proposed method to the manuscript. We thank the reviewer for suggesting this. \n\nThe reviewer also writes:\n>Related to reference (a) above, if the eventual goal is to use these group-invariant features for tasks like classification, I wonder why we need to find the group action at all. If we have a group-invariant latent feature, we can use a decoder and a group invariant reconstruction loss for training the autoencoder.\n\nAs discussed above, in general it is not trivial to define a group-invariant loss. While this might be possible, following (a), for discrete groups, for continuous groups one can define only an approximately invariant loss. This will lead to worse group-invariant representations (compare our new experiment where we compare our method against the method proposed in Mehr et al. (a)).\n\nAnother option for defining an invariant loss is to align/match the encoded and decoded elements by an additional algorithm before calculating and back-propagating the reconstruction loss. We argue that this can get computationally infeasible quickly for large discrete groups (e.g. optimal assignment to match two graphs) or continuous groups. To tackle this issue, we propose to learn to predict the group action to solve the co-alignment problem in the reconstruction loss as a non-approximated and efficient way.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eJNRktEfCFv",
                "writer": "author",
                "reply_to": "vc_MwH93TNK",
                "title": "Response 2: New experimental evaluations",
                "comment": " As mentioned above, we implemented and trained a Quotient autoencoder (QAE) proposed by Mehr et al. (ref (a) mentioned by reviewer) on the MNIST dataset for the group $\\text{SO}(2)$, discretized in 36 rotations with the loss\n\n$\n    \\text{min}_{\\theta \\in \\{10i, i=0,\\dots, 35\\}} \\text{MSE}(x - \\rho_X(g(\\theta))y)~,\n$\n\nwhere $x$ is a MNIST sample and $y$ is the reconstructed sample.\n\nWe evaluated the resulting embeddings on the rotated MNIST test set (in such a way that the evaluation is the same as for our model). Note, that this test set comprises more than just 36 rotations. We computed TSNE embeddings for this approach (compare Appendix B in revised manuscript), and we can observe that the embedding space shows a clearer structure, in comparison with the classical model. However, in comparison, our approach results in a better clustering of the different digits classes. That shows that the discretization step, while it helps in structuring the embedding space in ``signal clusters'', still does not capture the full continuous nature of the group.\n\nTo further quantitatively compare the three methods (ours, QAE (a) and classical AE), we evaluated the reconstruction loss as well as the (digit class) classification accuracy of a KNN classifier trained on 1000 embeddings of each method. We present in the table below the results for the reconstruction loss and for the classification accuracy of a KNN classifier trained on the AE embeddings. To obtain a fair comparison, we kept the architecture and the training hyperparameters exactly identical for all the strategies. We note that our strategy outperforms both the classical AE as well as the strategy of (a) in both tasks.\n\n| Model      | Rec. Loss | KNN Acc. \n| ----------- | ----------- | ----------- | \n| classical      | 0.0170       | 0.68\n| QAE   | 0.0227        | 0.82\n| ours   | 0.0162       | 0.90\n\nWe will include this comparison and discussion in the updated manuscript. Moreover,  as suggested by the reviewer, we will include reconstruction accuracies of the other experiments to the manuscript as well. We thank the reviewer for suggesting to discuss and compare our work to this related work, strengthening our manuscript.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "o_-MCW02RMH",
                "writer": "author",
                "reply_to": "vc_MwH93TNK",
                "title": "Response 1: Missing Related Work",
                "comment": " We thank the reviewer for their valuable comments. Below you will find our responses to your comments.\n\nWe thank the reviewer for making us aware of missing related work. We will rework the related work section and also include the works mentioned by the reviewer.\nEspecially we will additionally discuss our work in the context of related work that either utilizes approximated group invariant representation learning or defines a group invariant loss function by iterating/optimizing over all group elements at every back propagation step. We take here the opportunity to clarify the relation of the mentioned works (a-c) to our submission:\n\n- a) Manifold Learning in Quotient Spaces: Indeed, this referenced work is closely related to our work. They propose an autoencoder that maps orbits of elements (quotient space) to an invariant representation and decodes them back to the input space - similar to our work. As discussed in both our and their manuscript, such an autoencoder can only efficiently be trained if the encoded and decoded elements are co-aligned. They approach this issue by minimizing an adapted reconstruction loss $\\bar{d}(x, y) = \\text{inf}_{h\\in G}d(h.x,y)$. Specifically, during training they have to iterate over all group elements $h\\in G$ to find the one that minimizes the reconstruction loss (bringing x and y in the same pose). While this is feasible for (small) finite groups, for continuous groups they either have to approximately discretize them or perform a separate optimization of $h$ at every back propagation step to find the best match. For example, they descritize the continuous group SO$(2)$ by only considering a subset of 36 rotations. This is of course only an approximate solution to the alignment problem and hence the resulting loss is only approximately group-invariant.\nIn contrast, we propose to learn to predict the group element directly from the input, effectively parameterizing $h$ and optimizing it along the encoder and decoder model, minimizing the reconstruction objective (compare line 176 in our updated manuscript). By utilizing proper group-equivariant functions, the resulting loss is perfectly group-invariant and not only approximately. We included this discussion in the related work section.\nMoreover we compared our model against the approach proposed in (a) and demonstrate how our proposed model results in more expressive representations (see below).\n\n- b) Deforming Autoencoders: This work is indeed \nrelated to ours regarding the idea to disentangle the latent space in an unsupervised learning framework. The fundamental difference consists in the fact that we consider *global* transformations\non the input space (rotations, translations, etc), and the corresponding embedding is split into \na shape part and a global transformation action part. Reference (b) instead considers *local*\ndeformations, so that the embedding is split into an appearance and a deformation part. Another main difference with our method is that, moreover, their approach is not manifestly equivariant with respect to such deformations. Their networks do not learn representations of the group of local deformations, but rather learns an approximate deformation in such a way the reconstruction task is successful.\nThat is, such approach needs to see various deformation of the same sample \nin order to learn to correctly reconstruct (hence, it still needs augmentation), while our explicit equivarint approach needs just one sample per orbit in order to learn the reconstruction. \nOne potential connection between theirs and our work would be applying our formalism to local or gauge groups. Loosely speaking, a gauge group $G$ is a group that acts locally, that is, its elements $g\\in G$ depends on the space coordinates, that is, $g = g(x)$. These groups are very relevant in mathematics and especially in physics, where the Lagrangian of a system always admits invariance with respect to gauge groups (potentially trivial). One example is the Schr\u00f6dinger equation, which is invariant with respect to a U$(1)$ gauge symmetry. Physically, this mean that in quantum mechanics the theory is invariant to the phase of the wave function. Even if it would be out of scope for the current work, it would be compelling to explore this further, and extend our formalism to local/gauge groups. Such models could be potentially useful to learn, for example, quantum mechanical or physically-relevant embeddings. \n\n- c) Rate-Invariant Autoencoding of Time-Series: This work is similar in spirit to the reference (b), \nas the authors' goal, in the context of time series, is to obtain an embedding which is rate-invariant. Their AE strategy is then to learn a splitting of the embedding in a rate-invariant and a rate-variant part, in such a way that the reconstruction task can be achieved. Again, this approach focuses on local transformation of the data, as opposite to ours, which focuses on global transformations. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "j836-tmDV9F",
                "writer": "author",
                "reply_to": "pZOP7eIYdfJ",
                "title": "Response 4: Questions 6-7 and missing description of Xi, Mu and Y",
                "comment": " 6. In the two Experiments of 5.3, we train an $S_N$ and SE$(3)$ equivariant autoencoder but the bottleneck $z$ is $G-$invariant as it should capture the intrinsic information of the data. For the reconstruction, we in fact, restore the set-elements with the correct orientation (i.e. considering translation and rotation) as well as correct indexing (i.e. with the right order). Hence, we predict translation vector $t\\in\\mathbb{R}^3$, rotation matrix $R\\in$ SO$(3)$ as well as the permutation matrix $P_{\\pi} = (p_{ij}) \\in \\mathbb{R}^{N \\times N}$ which are applied on the decoder outputs, to perform the `alignment' and subsequently the reconstruction. In Figure 5, we only illustrate reconstructions after applying the rotation to emphasize the visual effect of the alignment. We thank the reviewer for raising this question and we will clarify this point in the updated manuscript.\n7. The reviewer is correct is stating that a general SE-$(3)$-invariant model is able to distinguish such chiral shapes. We just wanted to point out that a class of such invariant models, namely the ones that restrict themselves to only considering distances between points, cannot distinguish the two shapes. In fact, the relative distance matrix for the two shape is identical, as there is no difference whether the last point is on the left or on the right. Obviously, an invariant model which also takes angles into consideration would be able to distinguish such shapes. The reason why we made such a comment in the text is to point out that the correct application of our approach avoids the risk to encounter such problems, since the autoencoder must be able to reconstruct the whole data space, while invariant ad-hoc models, even if very successful like in the case of SchNet, which were designed for ad-hoc task, suffer from lack of generalization when applied to different datasets.\n\n### Description of $Y, \\mu, \\xi$  for $\\text{SO}(n)$ and $T_n$\nWe thank the reviewer for pointing out that that we are not perhaps always explicit enough in describing the various maps determining the function $\\psi$. We will complement this here, and we will make sure to add these additional details in a separate appendix section.\n\n#### $\\text{SO}(n)$\n\nWe describe here the general procedure for $\\text{SO}(n)$, which automatically includes the cases $\\text{SO}(2)$ and $\\text{SO}(3)$.\n\nThe space $Y$ is $S^{n+1}$, the $n+1$-dimensional sphere. We obtain a point y in $Y$ through the map $\\mu$, which predicts $n$ $n$-dimensional vectors and take the various ortho-normal combinations, so to obtain $n$ orthonormal vectors $\\widehat{y}_i$ in $S^{n+1}$. The collection of such vectors . By choosing $y_0=I_n$, the $n\\times n$ unit matrix, the map $\\xi$ assigns to a point\n\n$ y = \\begin{pmatrix}\n        \\widehat{y}_1 & \\widehat{y}_2 & \\cdots & \\widehat{y}_n\n    \\end{pmatrix} $\n\nthe group element $g\\in  \\text{SO}(n)$, such that \n\n$\n    \\rho_Y(g)y_0 = y~.\n$\n\nThis indeed, by construction, corresponds to rotation element whose rotation matrix is given exactly by $y$ defined in the equation above.\n\n#### $T_n$\n\nHere $Y=\\mathbb{R}^n$, and $\\mu$ predicts simply a $n$-dimensional vector in $\\mathbb{R}^n$. In all our experiments, we take $y_0=\\mathbf{0}$, the origin of $\\mathbb{R}^n$. Thus, $\\xi$ is the map that assign to each element $y\\in Y$ the group element $g\\in T_n$ such that $y=y+ \\mathbf{0} = y + y_0$. In this sense, since $T_n=\\mathbb{R}^n$, $\\xi$ is the identity function, $\\xi(y)=y$.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kjG1Rp4uCN",
                "writer": "author",
                "reply_to": "pZOP7eIYdfJ",
                "title": "Response 3: Questions 1-5",
                "comment": " 1. We thank the reviewer for giving up the opportunity to clarify what we mean with \"learning the group action\". The map $\\psi: X\\rightarrow G$ is a map from the data space $X$ to the group $G$. At the beginning of the training, that is, for a randomly initialized network, $\\psi$ will randomly map data samples to group elements. Given our requirement that $\\psi$ needs to be equivariant, the group relationship will be maintained by $\\psi$, that is, if $x'=\\rho_X(g')(x)$, then $\\psi(x') = g\\cdot \\psi(x)$. Now, during training, $\\psi$ needs to learn to map a sample point $x$ to the group element $g_x=\\psi(x)\\in G$ such that $\\rho_X(g_x)\\delta(\\eta(x)) = x$, that is, the group element that maps the (invariant) decoded output to the (non-invariant) input.  Now, in order to achieve that we are faced with the questions: What are the conditions that such a map $\\psi$ needs to satisfy and more specifically, how do we implement it in a \"systematic\" fashion?\n In Section 2.3 we derived such sufficient conditions. Further we propose an explicit construction which makes use of the two maps $\\mu$ and $\\xi$ and of the intermediate space $Y$. This construction satisfy all the requirement and it is group-agnostic, but it is for sure not the only one. This is where our claim that our construction works for any group comes from, which perhaps might leads to a misunderstanding. Specifically, what we mean is as follows: The mathematical property of the map $\\mu$ and $\\xi$, defined in Property 2.6, are general and valid for any group $G$. \n    We agree with the reviwer that the specific form, of the space $Y$ will need to depend on the group $G$, since homogeneity is a property that is group-dependent. Moreover, note that our framework is in general flexible enough to allow the use of already existing group-equivariant neural network implementations for the encoder ($\\eta$ and $\\mu$).\n\n2. The reviewer is completely correct. In fact, this is what we often do in practice, that is, we have one single equivariant encoder, and the invariant bottleneck is obtained through a pooling of the equivariant output. We merely depicted the architecture in such a way in the methods section because in our opinion it makes the general learning framework clearer. However, note that we still have to separate the invariant from the equivariant part after encoding, as discussed above, to train a powerful invariant feature extractor. Also please note, that we only pass on as few equivariant features as needed to construct the group action (e.g. only one 2-dimensional vector for group SO(2)), while the invariant part might be higher dimensional (e.g. 32 dimensions in our MNIST experiment). No additional information other than a group action is passed on by $\\psi$.\n\n3. As discussed above, our main motivation in this work is to train an unsupervised model on extracting expressive representations of data that are invariant under transformation of a defined group (We do not \"want a latent space structured according to a group action\" as mentioned by the reviewer). In essence, the decoder $\\delta$ and group function $\\psi$ are just a means to this end. (a) If we are just interest in reconstruction/generation we could use a completely equivariant model (with an equivariant embedding). (b) If we would have access to labeled data, we could use a supervised model with an equivariant encoder followed by a group invariant projection. However, in our unsupervised setting, the learning signal is a non-invariant reconstruction objective. In order to extract an invariant representation, we have to separate the input representation into a invariant part (our desired output) and a equivariant part that is merely used to transform the decoded canonical element in order to evaluate the non-invariant reconstruction objective.\n\n4. Yes, the reviewer is correct in that the encoder is invariant by design (as a consequence of using a group-specific invariant model) and the deviations discussed in 5.2 are results of interpolation artifacts. However, we want to emphasize that the expressiveness of the resulting invariant representations are consequence of our framework and not a trivial outcome of an invariant encoder.\n\n5. The task described in Section 5.2 (set of digits) is permutation equivariant, that is, the order of the digits is relevant in the reconstruction. It is true that for reconstruction we need in addition to the invariant embedding (which effectively stores the composition as visualized in Figure 4c) the permutation matrix (visualized in Figure 4b). As our goal is the invariant representation of a set and the equivariant part (permutation matrix) is only used during training we do not count this part in Figure 4a. However, we acknowledge that Figure 4a might indeed be misleading and we will add a comment regarding this in the manuscript. Thank you for pointing this out.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kblwrCGgEuh",
                "writer": "author",
                "reply_to": "pZOP7eIYdfJ",
                "title": "Response 2: Why separate invariant and equivariant parts?",
                "comment": " The reviewer raises multiple times their concern about the separation of the invariant and equivariant encoders and representations. We are not completely sure if the reviewer is concerned about either (a) the necessity of invariant part during training, i.e., whether we can train the autoencoder in a purely equivariant way and apply an invariant projection on the equivariant representation after the training to extract invariant embeddings or (b) the utilization of two different networks and weights for the equivariant ($\\psi$) and invariant ($\\eta$) part of of our framework.\n\nFor (a) we would like to emphasize that training purely in an equivariant way and symmetrizing after training is in general not straight-forward to do and will not lead to expressive representations. Let's discuss the example of the rotation group (e.g. $\\text{SO}(2)$). If we train an autoencoder with an equivariant bottleneck (e.g. vectors) we can make the representation after the training rotation invariant by e.g. only consider the norm of the embedding vectors. However, if the autoencoder is able to perfectly reconstruct the inputs from the equivariant embedding vectors, it is easy to see that the norm of the vectors will not necessarily store the information needed to reconstruct the input up to a rotation nor do we have a decoding function that is trained to do so. By separating the embedding into a invariant and equivariant part during training, we force the model to store all invariant information (shape) in the invariant embedding and train a decoder that can reconstruct the input (up to a rotation) from these invariant features. The equivariant part is only used to align input and decoded elements to enable optimization by a reconstruction objective.\n\nFor (b), we are in fact sharing the same network for the invariant and equivariant part and extract the invariant information by applying a invariant projection (e.g. taking the norm of rotation equivariant vectors) as proposed by the reviewer (also compare answer to question 2). However, crucially we are doing this during the training. Hence the network has to learn to store meaningful information in the invariant projections (e.g. norm of a vector) to minimize the reconstruction loss.\n\nThe reviewer also notes that \n\n>...this separation between equivariant and invariant representations hardly seems like a contribution; as you yourself mention, most applications of equivariant neural networks learn equivariant representations which are then projected to an invariant representation.\n\nPlease note that in order to extract meaningful invariant representations by projecting invariantly from an equivariant representation one needs some kind of supervision. Most works do this by utilizing labeled data in a supervised learning paradigm (i.e. input -> equivariant model -> invariant projection -> invariant label). In unsupervised learning we dont have access to such labels. Here we utilize the reconstruction loss of an autoencoder as supervision signal, however, as discussed and motivated in our manuscript this loss is non-invariant (unlike in supervised learning case), which we tackle by our proposed method. Hence we believe our work poses a major contribution, as it allows to train invariant feature extractor in an unsupervised way.\n\nThe reviewer also writes:\n\n>The whole point of this encoder $[\\psi]$ is to extract pose, but since you presuppose the encoder to be equivariant to the group action, is this not trivial?\n\nThis is not quite correct. As addressed below in the questions, the point of $\\psi$ is not learning a absolute pose (which in some sense would be indeed trivial because of equivariance) but the pose with respect to the (unknown to the encoder) canonical (invariant) pose of the reconstructed sample (which is not trivial and subject to the joined learning process of $\\eta$, $\\delta$ and $\\psi$). The equivariance property merely ensures that this needs to be learned once per orbit (and not separately for each elements of the orbit).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Y9jRrcROe02",
                "writer": "author",
                "reply_to": "pZOP7eIYdfJ",
                "title": "Response 1: General Motivation",
                "comment": " We thank the reviewer for their valuable comments. Below you will find our responses to your comments.\n\nWe take the opportunity here and perhaps clarify some doubts the reviewer raised regarding our motivation. Our main goal is to design an effective method to **learn the most powerful invariant embeddings in an unsupervised manner utilizing equivariant neural networks**. To this end, the decoder $\\delta$ and group function $\\psi$ are mainly used as auxiliary tool to enable the training of such an invariant encoder.\n\n1. **Why invariant?** Many interesting properties are invariant to certain group actions (e.g. ground state energy of a molecular conformation is invariant to rotations/translations of the molecule). The advantage of invariant representations for e.g. a downstream model is the necessity of less data, no need for augmentation, and the fact that the model can focus on the actual signal/shape and not its pose.\n2. **Why equivariant?** We still want to extract as most information as possible out of the data, and therefore we do not wish to restrict ourselves to (a) invariant input features or (b) models that are just a compositions of invariant functions.\n3. **Why Autoencoders?** We do not want to rely on labeled data as supervisory signal to extract expressive invariant representations but use unsupervised learning. Autoencoders (AE) are a straight-forward approach to extract expressive high-level features from data. However, as AEs are trained on a reconstruction loss in the non-invariant data domain, we propose our framework to still enable encoding of invariant features by separating the invariant (shape) from the equivariant (pose) part. \n\nHence, as the reviewer suggest to \"compare with a fully equivariant encoder-decoder\" would be out of scope of this work, since a trained equivariant encoder-decoder cannot afterwards be used to extract expressive invariant embeddings (as discussed below).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZdQeyp-BVm4",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_47lpv23LDPr",
                "title": "",
                "comment": " This paper proposes an invariant and equivariant unsupervised learning method, i.e., an invariant and equivariant autoencoder.\nWhen considering such a problem, the naive problem is the decoder construction.\nThis is because when the invariant model maps to an intermediate space, the group action is already obvious in that space, and the information about the group has disappeared.\nThe method used in this paper avoids this problem by adding group labels to the intermediate space, and from their experimental results with rotated MNIST and others, it appears that this method indeed achieves a better representation than the usual auto-encoder. Strengths\nThis paper is based on a natural conception and solves the problem of invariant auto encoders.\n\nWeaknesses\nThe proposed method and solution are too natural to be non-trivial.\nIt would be better if situations in which expressions created in this way would be useful could be shown. See Weaknesses. Not applicable.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "gZFXNuV-DlI",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_47lpv23LDPr",
                "title": "",
                "comment": " Designs an autoencoder that projects each input element into a pair of vectors: one that is group-invariant, and one that represents the group action necessary to recover the original vector from the decoded group-invariant representation.  The group action encoder is designed as a learnable map followed by a deterministic map; the consequent of proposition 2.7 is a sufficient condition to guarantee that this construction results in a suitable group function.  Learnable maps are proposed (and proven suitable) for 2d and 3d rotation groups, permutation groups, and translation groups. Experimental examples show that the proposed encoder results in group-invariant representations with very high classification accuracy and very high cluster purity for rotation, permutation, and rotation+permutation+translation groups.\n\nObviously, many other papers have proposed group-equivariant and group-invariant autoencoders.  This paper seems, to me, to go significantly beyond those previous proposals, because the consequent of Proposition 2.7 provides a sufficient condition for the design of an autoencoder with separable group-invariant and group-action representations.\n\nIt's interesting that Prop. 2.7 says that if mu is G-equivariant, then G_x=G_\\mu, but only the consequent is necessary in Prop. 2.9 to prove that \\xi\\circ\\mu is a suitable group function.  As a result of affirming the consequent in this way, we never get a proof that a suitable group function is also G-equivariant.  It seems intuitively to be the case, but it is not proven, nor, perhaps, necessary.\n The notation is challenging, and its challenges are exacerbated by typos and undefined terms.  For example:\n\nGL(X) is never defined.\n\nOn line 82 of p. 3, the application of f_X to g and x is written as f_X(g)x, but in equation (1), it is written as f(\\rho_V(g^{-1})x) -- I think that in both cases, the x should be inside the parentheses.\n\nA map \\phi is said to be G-equivariant if \\psi(...) = ...\\psi -- either the \\phi should be a \\psi or vice-versa\n\nwe wish to learn the invariant map \\phi_{inv}, thus... --- but \\phi_{inv} is never mentioned in the definition on the next line\n\nJust in general, I didn't understand the purpose of the \\rho_X(g) notation.  If I had to guess, I would say that it takes a general representation, g, of a member of the set G, and converts it into the instantiation of that member that is applicable to external dataset X?  The definition of this notation in the text is self-contradictory and unhelpful: a group action is defined as f:G,X -> X, then a representation is defined as (1) a group action, but also (2) as a function f:X -> GL(X), which seems to be a contradiction.  Neither (1) nor (2) seems to fit with the way \\rho_X(g) is used in the rest of the article.  \n\n Limitations and ethical implications are not discussed.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "vc_MwH93TNK",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_47lpv23LDPr",
                "title": "",
                "comment": " An autoencoder architecture that is based on equivariant and invariant neural layers is proposed. The most important aspect of the architecture is that the encoder stage can map the input to a guaranteed group invariant feature and a group action such that the decoder operates on the group invariant feature and produces the reconstruction of the input in a canonical form. The predicted group action is then applied on this canonical decoder output to reconstruct the input exactly (assuming the autoencoder is trained well). Experiments on images and point clouds are performed for a variety of groups. Strengths:\n\n1. The method provides guaranteed invariance to the transformation group.\n2. The method is general enough to apply to both discrete (such as permutations) and continuous (such as rotations and translations) groups, and to a diverse set of domains -- images, point clouds. \n3. Experiments show that the network is indeed able to learn a group invariant and the corresponding group action when trained with the reconstruction loss alone.\n\n\nWeaknesses:\n\n1. Missing related work: I think the main weakness in the paper is that the authors have missed plenty of related work in literature. I give some examples here of works which consider unsupervised learning of group invariant representations:\n\n(a) Manifold Learning in Quotient Spaces: https://openaccess.thecvf.com/content_cvpr_2018/papers/Mehr_Manifold_Learning_in_CVPR_2018_paper.pdf\n\n(b) Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance: https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Zhixin_Shu_Deforming_Autoencoders_Unsupervised_ECCV_2018_paper.pdf\n\n(c) Rate-Invariant Autoencoding of Time-Series: https://ieeexplore.ieee.org/abstract/document/9053983\n\nIn (a), group invariant features are learned in an unsupervised fashion, and a group-invariant loss function is used. Similar ideas have been applied for rotation-invariant autoencoders for spherical images as well. In (b) and (c), the invariant feature and the group transformation are disentangled in an autoencoder which is closely related to this submission. In (b) and (c), the ideas are applicable to much larger groups than considered in the present submission as well -- diffeomorphisms -- which I believe cannot be handled by the method presented submission, at least for now. \n\nGiven these works, I think the authors should significantly improve the related work section. They should put this submission in context of such works and better explain the main advantages their method provides.\n\n2. Missing comparison with all these related work: I do think that the papers I have cited and the ideas therein are closely related to the spirit of this work. And thus, the authors should provide some experimental comparisons to one or more of these works.\n\n3. MNIST and Tetris are toy datasets in my opinion. At least some experiments with more complex datasets like CIFAR-100, Patch Camelyon or ShapeNet are needed to better validate the effectiveness of the method. Authors do not provide quantitative measures of reconstruction quality for all experiments, which makes reproducibility hard. Also, non-equivariant baselines and corresponding quantitative evaluation are useful to compare the proposed method and to find out whether indeed the group invariant and equivariant network can be significantly smaller for the same reconstruction quality. \n\nOverall, I think there are too many weaknesses in this submission and am recommending a rejection. \n\n\n## Post author response\n\nI think the authors have addressed all my questions well. They have also addressed the weaknesses pointed out by the other reviewers well. The additional experiments and comparison with the QAE baseline make the contributions of the paper stronger. \n\nI hope the authors will expand their related work section and put the work in context of several other closely related papers (I only mentioned three, but there are others). I don't fully agree that because some of the papers can also model local transformations and this submission can handle only global transformations, that these papers are not closely related. In fact, they point to some limitations of the current work. For example, how can this submission be extended to handle infinite-dimensional groups which are important in image and time series registration? I am now recommending a weak accept. The authors should discuss what, if any, are the downstream applications of such a method. Is it data compression? Or do they envision the group invariant features learned by their method to be useful for high-level tasks like robust classification?\n\nRelated to reference (a) above, if the eventual goal is to use these group-invariant features for tasks like classification, I wonder why we need to find the group action at all. If we have a group-invariant latent feature, we can use a decoder and a group invariant reconstruction loss for training the autoencoder. \n\nI do hope that the authors address the weaknesses in the paper. Particularly, I would like them to clarify the contributions of the paper better, especially in light of other closely related work already in literature. Experiments need to be improved as well, as I have indicated. The authors have not discussed the limitations of this work.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "pZOP7eIYdfJ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_47lpv23LDPr",
                "title": "",
                "comment": " Equivariant neural networks have been shown to improve data efficiency and model performance, but require knowledge about the group acting on the data space to be implemented. This work promises to learn the group action from data instead, foregoing the need for ad hoc implementations of the group action on the feature maps / data space. To this end, authors propose learning two separate mappings: (1) from the input sample to a group-invariant canonical representation and (2) from the input sample to a group element / group action. Together, these allow for reconstructing the input sample under its observed pose. The authors show, in a range of experiments, that their framework is able to learn to successfully reconstruct input samples under their respective poses, from an invariant embedding and a learned group action. **Originality and Significance** The authors propose a sound method for implementing group equivariant auto-encoders. I have some concerns regarding claims the authors make throughout regarding learning the group action. If my understanding is correct, this method only \"learns\" the group action on the data space in the sense that the $\\mu$ operator learns a map from any point in the data space $x \\in X$ into an embedding space $Y$, where this map is required to be equivariant and $Y$ is required to be a homogeneous space of the group. Next, an operator $\\xi$ essentially maps this homogeneous space into the left-regular group action. The whole point of this encoder is to extract pose, but since you presuppose the encoder to be equivariant to the group action, is this not trivial?\n\nSince $\\mu$ is equivariant to the group action it inherently preserves pose, i.e. prop 2.7 is a property of any operator equivariant to the group action, and therefore the group action isn't actually \"learned\"; its structure in the embedding space is given by the equivariant operator. Furthermore, this separation between equivariant and invariant representations hardly seems like a contribution; as you yourself mention, most applications of equivariant neural networks learn equivariant representations which are then projected to an invariant representation. I don't fully understand the need for learning separate encoders $\\eta$ and $\\mu$. Can you not reuse the same embedding in both cases and simply apply a symmetric operation to $\\mu(x)$ obtain to obtain $z$? In fact, for experiment 5.1 you mention that you use the same architecture for both pipelines, why not share the representation?\n\nI would like to see the authors note on how this work relates to Keller & Welling (2021). The methods seem related in that both authors propose learning group equivariant latent representations from data without explicitly encoding the group action.\n\n**Quality and Clarity** The authors explain their derivations in thorough and clear manner. In section 3 I'm left with some questions. [line 186, 187] \"*we turn to describing our construction of $\\xi, \\mu$ and $Y$ for a number of groups*\". For ${\\rm SO(2)}$ you only describe how to construct $Y$ and $\\xi$, what about $\\mu$? For ${\\rm SO(3)}$ you only describe $Y$, what about $\\xi, \\mu$? Same for the rest of the examples. Could you address this?\n\n$\\xi$ needs to be constructed by hand, i.e. a new group requires a new implementation of $\\xi$ AND a new implementation of $\\eta$ and $\\mu$ because of new equivariance constraints. There seems to be no actual learning of the group action and although you claim your method \"avoids the necessity of ad-hoc group-specific implementations\" it does in fact require such group-specific implementations to be implemented itself?\n\nFrom my view, the experiments used by the authors to investigate and evaluate their approach do not adequately show its merits. For details on my concerns, see below questions 4-7. The experiments do not convince me of the need for separate invariant-equivariant terms. To this end, could the authors, in their experimental setups, compare with a fully equivariant encoder-decoder?\n\n*References*\n\nKeller, T. A., & Welling, M. (2021). Topographic vaes learn equivariant capsules. Advances in Neural Information Processing Systems, 34, 28585-28597. 1. I would like to see the authors address my concern regarding their claim on avoiding the necessity of ad-hoc group-specific implementations. Is it not correct that you need such group-specific implementations of $\\eta, \\mu$ and $\\xi$ in your framework? If so, how does your framework reflect the claim that you are \"learning the group action\"?\n\n2. I don't fully understand the need for learning separate encoders $\\eta$ and $\\mu$. Can you not reuse the same embedding in both cases and to obtain $z$ simply group invariantly project over the equivariant embedding?\n\n3. I'm not completely clear on the need for separating equivariant and invariant representations in the first place, could the authors expand on this a bit? Is it simply because we want to place no constraints on the decoder $\\delta$ that we discard information on group action over $X$ in the $\\eta\\rightarrow\\delta$ pathway? But if we want a latent space structured according to a group action, can we not simply use an equivariant encoder and decoder?\n\n4. Regarding the experiments, in 5.1 you mention that, upon inspection, the encoded latent representation is indeed rotation invariant up to machine error for 90 deg rotations, and shows interpolation artefacts otherwise. But this is just a consequence of the method of interpolation you are using to rotate the images and the encoder (SO(2)-CNN) you are using, and isn't necessarily a result of your framework correct? You use an encoder which is invariant, hence the learned representation is invariant. I also believe the invariance property of your encoder completely explains the rotation invariant structure of your latent space. To me this doesn't seem a result of your framework, but of the (group-specific) encoder you are using?\n\n5. For the set of digits experiment in 5.2, the experimental setup is not clear to me. Is the task permutation invariant or equivariant? Do you only evaluate the presence of digits in the reconstruction or their order as well? If you are only evaluating the set contents, is the equivariant pipeline discarded for this experiment? Or, if you are evaluating their order as well, I think Fig 4a is misleading, as in addition to the embedding your framework needs the permutation matrix for reconstruction (which is of larger dimensionality than the embedding).\n\n6. In experiment 5.3 you investigate a SE(3) and S_N autoencoder. From line 322, it seems you are only evaluating your reconstruction up to to a translation and permutation (you are only predicting a rotation with $\\psi$)? So the autoencoder as a whole is invariant to permutations and translations and only equivariant to rotations, correct? In which case, why didn't you make it invariant to translations and permutations as well? As your autoencoder is invariant to translation, why do you apply translation augmentations to the input?\n\n7. In line 330 you note that your SE(3)-equivariant model (which it is not, it is only SO(3)-equivariant) can distinguish between mirrored chiral shapes, and that SE(3)-invariant models are unable to do so. I believe this is incorrect; reflections are part of the E(3) group, and not of the SE(3) group, hence SE(3)-invariant models should be able to distinguish between these shapes. Could you comment on this?\n\n\n**General post-rebuttal response**\nI would like to thank the authors for their very thorough rebuttal, addressing most of my concerns, and the concerns raised by the other reviewers. I think the work significantly improved in experimental evaluation and comparison. As indicated in my last comments, parts of the motivation and framing of the model proposed by the authors remains misleading / incorrect to me. I would recommend that, were this work to be accepted, the authors review some of the claims they make in the paper regarding the \"learning of a group action\". I feel such claims should be addressed before publication.\n\nThe authors did address many of my questions regarding the broad motivation for their invariant-equivariant approach, but from my point of view, the authors skip a step by discarding fully invariant aggregation of equivariant self-supervised representations. I would recommend the authors empirically or theoretically show that such representations don't work in practice, as motivation for their (arguably more complex) approach. \n\nOn the condition of inclusion of their responses in an updated version of the manuscript, I raise my recommendation to a borderline accept.\n\n I did not see the authors address any limitations of their work.",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "auto-encoder that maps a point to a canonical point and to a group element",
                "Sentiment Expression": "proposes",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The method",
                "Sentiment Expression": "is promising",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "Experiments",
                "Sentiment Expression": "were on simple tasks and group actions",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "improved a lot during rebuttal/ revision",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the representation learned in a classification task",
                "Sentiment Expression": "to see how [it] alleviates the need of large training samples",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "BZ92dxDS3tO": {
        "paper_id": "nips_2022_BZ92dxDS3tO",
        "paper_title": "OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models",
        "paper_abstract": "We propose a new method for object pose estimation without CAD models. The previous feature-matching-based method OnePose has shown promising results under a one-shot setting which eliminates the need for CAD models or object-specific training. However, OnePose relies on detecting repeatable image keypoints and is thus prone to failure on low-textured objects. We propose a keypoint-free pose estimation pipeline to remove the need for repeatable keypoint detection. Built upon the detector-free feature matching method LoFTR, we devise a new keypoint-free SfM method to reconstruct a semi-dense point-cloud model for the object. Given a query image for object pose estimation, a 2D-3D matching network directly establishes 2D-3D correspondences between the query image and the reconstructed point-cloud model without first detecting keypoints in the image. Experiments show that the proposed pipeline outperforms existing one-shot CAD-model-free methods by a large margin and is comparable to CAD-model-based methods on LINEMOD even for low-textured objects. We also collect a new dataset composed of 80 sequences of 40 low-textured objects to facilitate future research on one-shot object pose estimation. The supplementary material, code and dataset are available on the project page: https://zju3dv.github.io/onepose_plus_plus.",
        "paper_acceptance": "Accept",
        "meta_review": "This paper originally received slightly positive reviews overall, except for one review, which was plenty of requests of specific clarifications and comments. Main issues regarded just the need of clarifying some parts of the method and put better in context of the state of the art and former evaluations. Unclear novelty was another raised problem, as well as the need of off-the-shelf 2D object detector and real images for training, which might affect the general applicability of the method while weakening the \"one/shot\" claim of the work. A lot of concerns were also raised about missing baselines and prior work discussion. \nAuthors provided detailed answers to the comments, also engaging in long discussions, especially with the most critical reviewer.\nIn the end, the most positive reviewers seem to be satisfied of the answers to their comments, maintaining the original positive ratings, and also the critical reviewer resulted convinced of the discussion with authors, raising his/her score to weak accept.\nOverall, assuming that the comments and discussions could be included in the final version, this paper can be considered acceptable for NeurIPS 2022 publication.\n",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "tsi6AjpEVys",
                "writer": "official_reviewer",
                "reply_to": "2EYVgYc0E64",
                "title": "Re: The response to the follow-up questions -4 ",
                "comment": " Thank you very much for the detailed description of the relation to prior work.\n\n> Our idea is to directly disambiguate and augment features by encoding their spatial information and relations with others into features with the help of the attention module.\n\nThere is actually prior work on using 3D point relations (in terms of which points are covisible with each other) to disambiguate matches, e.g., see Sec. 4 in [11]. In essence, image retrieval / determining which matching points can be seen together encodes relations between 3D points for disambiguation.\n\n> Moreover, we eliminate the keypoint detection on the query image, which helps pose estimation of low-textured objects. \n\nThere is prior work on eliminating keypoint detection in the query image for the purpose of better handling challenging conditions where features cannot be reliably re-detected. For example, [Germain et al., Sparse-to-Dense Hypercolumn Matching for Long-Term Visual Localization, 3DV 2019] and [Germain et al., S2DNet : Learning Image Features for Accurate Sparse-to-Dense Matching, ECCV 2020] match sparse features extracted from database images against dense features extracted from a query image.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iYiTEpz2Bde",
                "writer": "official_reviewer",
                "reply_to": "x4p0nN_K--z",
                "title": "Re: The response to the follow-up questions -3 ",
                "comment": " > Thank you very much for your comments. We clarify that our comment \"renovating the pipeline with a learning-based approach\" refers to the comparison with the previous SfM-based methods[1,2,3,4,5] in the area of 6D object pose estimation. \n\nThanks for the clarification. I misunderstand the previous statement as something more general pertaining to SfM-based pose estimation in general, which would also include the visual localization literature (where learning by now is a central part of the pipeline).\n\nStill, it might be better to not make this statement, as the approaches from the localization literature are also applicable to the object pose estimation case (see your experiments shown above). As such, claiming to renovate the pipeline with learning-based approaches seems still to strong to me.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mIZZrjxryt",
                "writer": "official_reviewer",
                "reply_to": "t9b298mKdGs",
                "title": "Re: The response to the follow-up questions -2 ",
                "comment": " >  The keypoint relocalization phase in Widya et al. doesn't leverage the two-view constraint.\n\nYou are right. I seem to have confused things with the two-view-based refinement from the InLoc paper [Taira et al., CVPR 2018], which implements the refinement strategy based on local matching discussed in the beginning of Sec. 3.3 of Widya et al.\n\n> SuperGlue+Patch2Pix is not a keypoint-free matcher\n\nThank you very much for the additional results. I think this rounds out the experiment.\n\nI would however argue that SuperGlue+Patch2Pix is a keypoint-free matcher as the Patch2Pix stage refines the SuperGlue matches in terms of their spatial positions. As a result, the matching pixel positions can differ from the original feature detections. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "U3mMiBBhh3D",
                "writer": "official_reviewer",
                "reply_to": "0UrqmCnxjU",
                "title": "Re: The response to the follow-up questions -1 ",
                "comment": " Thank you very much for the detailed answers. Please find my comments below.\n\n> Thank you very much for your comments. We will change 'generalizability' to 'no object-specific network training' in the revised version to avoid misleading. \n\nThis is indeed a better description of OnePose, OnePose++, hloc, etc. \n\nNote that not requiring \"object-specific network training\" is not a virtue in itself. For example, the baseline using NeuS in the discussion above does not require object-specific network training. But given the long reconstruction times, it would certainly be feasible to do object-specific network training in the same time. I don't think it matters whether an instance-level method requires network training or not. The important question to me seems to be the trade-off between pose accuracy and the time required to adapt an approach to a given object instance (e.g., training network parameters or building a 3D model).\n\n> The differences between PatchFlow are as follows.\n\nBased on the description, I would still argue that the proposed approach is a special case of PatchFlow:\n\n1. Refining matches between pairs of matches is a special case of computing a flow field (as only the flow for a single pixel in the patch around one match is computed) and is conceptually identical to the two-view case of PatchFlow (with the main technical difference that a transformer is used, but I would not consider this too novel).\n2. \"We keep the selected reference node fixed and search around each query node for the fine-level match.\": In essence, this corresponds to the chaining of refined matches discussed in the PatchFlow paper, with the special structure that the resulting graph has a star-like structure. Since potential constraints between query nodes are not taken into account, the resulting graph is a sub-graph of the one used by PatchFlow. Hence, the proposed approach is a special case of the more general PatchFlow framework.\n\nThus, I see limited novelty in this part of the proposed approach.\n\n> It can be seen that the feature distance maps of PixSfM contain large or multiple minimal regions(blue region) incurred by the ambiguous CNN features in low-textured regions, which are not discriminative enough to find real optimal locations in feature-metric optimization. We attribute the accuracy improvement of our method to the discriminative features in fine-level matching. \n\nMy point was that based on the numbers provided above (point cloud and object pose accuracy), the difference between the proposed approach and PixSfM seems to be very small. The new visualizations indeed show a difference, but these do not seem to influence the quantitative results too much. Hence, the statement that PixSfM struggles with low-texture regions seems to be too strong given the similar numbers.\n\n**Q4**: Thank you very much, this answers my question.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KHKY75jO7vu",
                "writer": "official_reviewer",
                "reply_to": "gvsHS3HJQdW",
                "title": "Re: The response to the follow-up question ",
                "comment": " Thank you very much. That answers my question.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "k6hTyESR8f4",
                "writer": "official_reviewer",
                "reply_to": "WQc0ZgE92MW",
                "title": "Re: The response to the follow-up questions ",
                "comment": " Thank you very much for the answer and the additional experiment.\n\nI would not count this baseline as concurrent work. The idea of getting 3D point coordinates from a dense model instead of a sparse SfM point cloud predates the MeshLoc paper (I think the MeshLoc paper provides multiple references to such prior work). One example are methods evaluated on the InLoc dataset, where database images are very sparse and building a SfM model is thus hard. Since the dataset provides a depth map per database image, these depth maps are used to obtain the 3D points corresponding to 2D positions in the database images. The methods that I am aware of that evaluate on InLoc rely on image retrieval and run pose estimation separately for the 2D-3D matches obtained from a retrieved database image. Running a single pose estimation step over all 2D-3D matches seems like a minor modification to me.\n\nI am not convinced that the results show that the baseline is not feasible in practice. As mentioned in my previous comment, there are multi-view stereo approaches that are very efficient, e.g., Capturing Reality. NeuS does not seem to fall into this category.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gvsHS3HJQdW",
                "writer": "author",
                "reply_to": "ZphKQxqijC",
                "title": "The response to the follow-up question",
                "comment": " > **Q1:** I have a follow-up question on the OnePose++ dataset: How many training and testing images are there? I can't seem to find this information in the supp. material.\n\nThank you very much for your comments. As described in Line 247-249, the OnePose-HARD is an evaluation set to supplement the original OnePose dataset. We use all of the objects in the OnePose-HARD dataset for testing. The total number of images in the reference sequences is 35521, and the total number of images in the query sequences is 32477. We will add the missing information in the final version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "x4p0nN_K--z",
                "writer": "author",
                "reply_to": "-HwdPtmJMvw",
                "title": "The response to the follow-up questions -3",
                "comment": " > **Q7:** This is rather vague to me. What does \"renovating the pipeline with a learning-based approach\" mean? Prior work, e.g., SuperGlue, LoFTR, Patch2Pix, NC-Net, Sparse NC-Net, Dual RC-Net, DSAC (++, *), InLoc, D2-Net, has already integrated learning into SfM-based pose estimation. Some of these approaches also claim to better handle weakly textured regions (e.g., InLoc motivates dense matching to better handle such regions).\n\nThank you very much for your comments. We clarify that our comment \"renovating the pipeline with a learning-based approach\" refers to the comparison with the previous SfM-based methods[1,2,3,4,5] in the area of 6D object pose estimation. We further discuss the differences with these methods as follows.\n\nSome previous methods[2,3,5] extract keypoints on the query image firstly and perform matching with reference images or SfM model to obtain 2D-3D matches for pose estimation. Unlike them, which reject ambiguous matches by ratio test in matching, [4] proposes preserving ambiguous matches at the matching stage by vector quantizing and solving ambiguation by hypothesis testing at the outlier filter stage. [1] proposes the spatial feature clustering and multi-prioritized RANSAC to cope with repeated patterns for multiple instances detection.\n\nDifferent from these previous methods, our framework eliminates the keypoint detection for the query image by directly performing matching between the 2D feature map and the 3D model, which benefits pose estimation for low-textured objects. Moreover, we leverage the attention mechanism to disambigute 2D and 3D features for matching, while the direct feature disambiguation is not explored by these methods. The keypoint-free design and the attention mechansim in our 2D-3D matching network bring improvement on low-textured objects, which are challenging for these previous methods.\n\n**References:**\n\n[1] Fenzi, Michele, Ralf Dragon, Laura Leal-Taix\u00e9, Bodo Rosenhahn and J\u00f6rn Ostermann. \u201c3D Object Recognition and Pose Estimation for Multiple Objects Using Multi-Prioritized RANSAC and Model Updating.\u201d DAGM/OAGM Symposium (2012).\n\n[2] Martinez, Manuel, Alvaro Collet and Siddhartha S. Srinivasa. \u201cMOPED: A scalable and low latency object recognition and pose estimation system.\u201d 2010 IEEE International Conference on Robotics and Automation (2010): 2043-2049.\n\n[3] Gordon, Iryna and David G. Lowe. \u201cWhat and Where: 3D Object Recognition with Accurate Pose.\u201d Toward Category-Level Object Recognition (2006).\n\n[4] Hsiao, Edward, Alvaro Collet and Martial Hebert. \u201cMaking specific features less discriminative to improve point-based 3D object recognition.\u201d 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2010): 2653-2660.\n\n[5] Skrypnyk, Iryna and David G. Lowe. \u201cScene modelling, recognition and tracking with invariant image features.\u201d Third IEEE and ACM International Symposium on Mixed and Augmented Reality (2004): 110-119.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2EYVgYc0E64",
                "writer": "author",
                "reply_to": "-HwdPtmJMvw",
                "title": "The response to the follow-up questions -4",
                "comment": " > **Q8:** The challenge at large scale is that feature descriptors become ambiguous as more and more locally similar structures need to be considered (see the work by Li et al., Svarm et al., and Zeisl et al.). The result is that some form of disambiguation is needed, as is the case for weakly texture regions, which also produce ambiguous matches. These works thus need to deal with a very similar problem. In my opinion, the differences need to be discussed in more detail.\n\nThank you very much for your comments. Previous visual localization methods based on direct 2D-3D matching improve efficiency, accuracy and cope with ambiguous matches in the 2D-3D matching and outlier filtering.\n\nMany previous methods [1,2,8] leverage priors for 2D-3D matching. They define the prioritization criteria, such as co-visibility for the 3D points, and matching is performed by order of descending priorities. This strategy improves efficiency but helps little in disambiguation. Some methods [1,4,11] compress the 3D model by quantizing features to improve matching efficiency. However, the quantization can further lead to ambiguous matches and they rely on outlier filtering for disambiguation. [9] regards 2D-3D matching as a classification problem, but it assumes the known pose prior. Our method also works on the 2D-3D matching phase but focuses on disambiguating features. Our idea is to directly disambiguate and augment features by encoding their spatial information and relations with others into features with the help of the attention module. In this way, both 2D and 3D features are provided the global receptive field and become discriminative for 2D-3D matching. Moreover, we eliminate the keypoint detection on the query image, which helps pose estimation of low-textured objects. Since our module operates on the features, we believe it can be combined with the previous prior-based methods to perform 2D-3D matching.\n\nA number of solutions [1,3,4,5,6,7,10,11] work on the outlier filter stage. Since the ambiguous matches may contain correct matches, some methods relax the matching threshold [3,5,6,7] or quantize features [1,4,11] to preserve ambiguous matches and reject wrong matches at the outlier filter stage. Many approaches[1,4,7,11] use co-visibility priors to filter outliers. The co-visibility encoded in the 3D model is used to select a subset of matches that is more likely to be correct from all putative matches. Some other methods[3,5,6,10] propose efficient geometric verification to filter large amounts of outliers. Since our work focus on the 2D-3D matching phase, these outlier filtering methods are orthogonal to our method, which can be further explored to integrate into our pipeline.\n\n**References**\n\n[1] Sattler, Torsten, B. Leibe and Leif P. Kobbelt. \u201cEfficient & Effective Prioritized Matching for Large-Scale Image-Based Localization.\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (2017): 1744-1756.\n\n[2] Li, Yunpeng, Noah Snavely and Daniel P. Huttenlocher. \u201cLocation Recognition Using Prioritized Feature Matching.\u201d ECCV (2010).\n\n[3] Sv\u00e4rm, Linus, Olof Enqvist, Fredrik Kahl and Magnus Oskarsson. \u201cCity-Scale Localization for Cameras with Known Vertical Direction.\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (2017): 1455-1461.\n\n[4] Liu, Liu, Hongdong Li and Yuchao Dai. \u201cEfficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map.\u201d 2017 IEEE International Conference on Computer Vision (ICCV) (2017): 2391-2400.\n\n[5] Camposeco, Federico, Torsten Sattler, Andrea Cohen, Andreas Geiger and Marc Pollefeys. \u201cToroidal Constraints for Two-Point Localization Under High Outlier Ratios.\u201d 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 6700-6708.\n\n[6] Zeisl, Bernhard, Torsten Sattler and Marc Pollefeys. \u201cCamera Pose Voting for Large-Scale Image-Based Localization.\u201d 2015 IEEE International Conference on Computer Vision (ICCV) (2015): 2704-2712.\n\n[7] Li, Yunpeng, Noah Snavely, Daniel P. Huttenlocher and Pascal V. Fua. \u201cWorldwide Pose Estimation Using 3D Point Clouds.\u201d ECCV (2012).\n\n[8] Choudhary, Siddharth and P. J. Narayanan. \u201cVisibility Probability Structure from SfM Datasets and Applications.\u201d ECCV (2012).\n\n[9] Donoser, Michael and Dieter Schmalstieg. \u201cDiscriminative Feature-to-Point Matching in Image-Based Localization.\u201d 2014 IEEE Conference on Computer Vision and Pattern Recognition (2014): 516-523.\n\n[10] Sv\u00e4rm, Linus, Olof Enqvist, Magnus Oskarsson and Fredrik Kahl. \u201cAccurate Localization and Pose Estimation for Large 3D Models.\u201d 2014 IEEE Conference on Computer Vision and Pattern Recognition (2014): 532-539.\n\n[11] Sattler, Torsten, Michal Havlena, Filip Radenovi\u0107, Konrad Schindler and Marc Pollefeys. \u201cHyperpoints and Fine Vocabularies for Large-Scale Location Recognition.\u201d 2015 IEEE International Conference on Computer Vision (ICCV) (2015): 2102-2110.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "t9b298mKdGs",
                "writer": "author",
                "reply_to": "8HujlfSeJqc",
                "title": "The response to the follow-up questions -2",
                "comment": " > **Q5:** Widya et al. start with coarse matches that are then refined locally: given a match established using features extracted at one layer in the network, the refinement aims at finding more accurate coordinates locally in regions around the initial match (where the region size depends on the receptive field of the features). Isn't this a similar two-view constraint used by the proposed approach?\n\nThank you very much for your comments. The keypoint relocalization phase in Widya et al. doesn't leverage the two-view constraint. In its refinement phase, the keypoint relocalization still operates on a single view by the local feature patch instead of considering relations with other views' patches. It can be regarded as performing keypoint detection within the coarse match local region. In contrast, our refinement leverage two-view patches and transformer to find more accurate matches in the query view relative to the reference view.\n\n> **Q6:** Using Patch2Pix to refine matches found by SuperGlue (denoted as SuperGlue + Patch2Pix in the Patch2Pix paper) leads to state of the art results for the visual localization task. Unfortunately, this stronger baseline is missing.\n\nThank you very much for your comments. Since the SuperGlue+Patch2Pix is not a keypoint-free matcher, we did not include this baseline in the answer of the original Q3. We apologize for misunderstanding the question, and we add the evaluation of SuperGlue+Patch2Pix on the OnePose-HARD dataset as follows.\n\n||1cm1deg|3cm3deg|5cm5deg|\n|:-|:-|:-|:-|\n|Ours|**16.3**|**55.4**|**70.3**|\n|LoFTR(round)|15.4|43.7|53.4|\n|SPP+SPG+Patch2Pix|10.1|37.2|47.6|\n|SPP+SPG|13.8|36.1|42.2|\n|DRC-Net|11.3|37.0|47.8|\n|Patch2Pix|2.42|19.0|30.4|",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0UrqmCnxjU",
                "writer": "author",
                "reply_to": "_tyeP4Sljo1y",
                "title": "The response to the follow-up questions -1",
                "comment": " > **Q1:** If generalizability is defined as \"the property of eliminating object/category-specific training\", then I don't think that OnePose, HLoc, and OnePose++ are qualify as generalizable. They all need to build an object/category-specific scene representation, in the form of a 3D model, from the input images and their known poses. I don't why building these 3D models would not qualify as object/category-specific training as it involves optimizing an objective function and since these 3D models are fundamental parts of the object pose estimation stage.\n\nThank you very much for your comments. We will change 'generalizability' to 'no object-specific network training' in the revised version to avoid misleading. \n\n> **Q2:** Relation to PatchFlow[Dusmanu et al., Multi-View Optimization of Local Feature Geometry, ECCV 2020]\n\nThank you very much for your comments. The differences between PatchFlow are as follows.\n\n- We leverage the fine-level matching module with the transformer to refine matches instead of estimating the dense flow field between patches like PatchFlow. The advantages are accuracy and storage efficiency. Since the multiview refinement of PatchFlow requires flow field interpolation, the fine matching module is not adaptable for its framework.\n- We thus propose a simple yet effective strategy to achieve consistent matches for later 3D model refinement. We keep the selected reference node fixed and search around each query node for the fine-level match. The advantages are that our graph structure is significantly simpler than PatchFlow, which is efficient for matching. And we do not need to store and interpolate flow fields for optimization.\n\nNotably, our graph structure is not a sub-graph of the coarse feature track since it contains connections which not exist in the coarse feature track(e.g., the reference and the query node may not be directly matched in tentative matches).\n\n> **Q3:** Where can I see that PixSfM struggles in \"low-texture regions\"? The reported performance seems rather very similar to me.\n\nThe feature cost maps of the local patch around the coarse matches are visualized [here](https://sites.google.com/view/oneposeplusplus/%E9%A6%96%E9%A1%B5). The feature cost maps are calculated by the distance between the corresponding reference feature of another view and each element within the local patch, and the values are normalized to 0~1 for visualization.\n\nIt can be seen that the feature distance maps of PixSfM contain large or multiple minimal regions(blue region) incurred by the ambiguous CNN features in low-textured regions, which are not discriminative enough to find real optimal locations in feature-metric optimization.  We attribute the accuracy improvement of our method to the discriminative features in fine-level matching.\n\n> **Q4:** Looking at Sec. 4.4 of the PixSfM paper (and its supplementary material), the high memory costs can be avoided by pre-computing and storing cost maps rather than the descriptors. This comes at a small loss in accuracy. In the provided table, are the 7.35GB required for storing descriptors or the cost maps?\n\nThe reported memory requirement is for storing descriptors. We also conduct the experiment for PixSfM with cost maps on the OnePose-HARD dataset. The results show that although the feature storage cost decreases significantly, the accuracy decreases accordingly.\n||1mm|3mm|5mm|Feature Storage Cost|\n|:-|:-|:-|:-|:-|\n|LoFTR coarse + Our refinement|**29.5**|**73.6**|**85.8**|-|\n|LoFTR coarse + PixSfM|27.6|71.2|84.4|7.35GB|\n|LoFTR coarse + PixSfM(cost map)|25.8|67.4|80.8|0.17GB|",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WQc0ZgE92MW",
                "writer": "author",
                "reply_to": "kQVgUrUfsuR",
                "title": "The response to the follow-up questions",
                "comment": " > **Q1:** Wouldn't the following be a suitable (and rather simple) baseline?...\n\nThank you very much for your comments. We observe the mentioned pipeline is similar to MeshLoc[1], which is a recent concurrent work. We follow the mentioned pipeline to conduct the evaluation on the OnePose-HARD dataset.\n\nWe use the current state-of-the-art object dense reconstruction method NeuS[2] to reconstruct object mesh for each object in the dataset, which takes ~10 hours per object. Then we render depth maps for reference images and estimate the object pose of the query image following the mentioned pipeline.\nResults are shown as follows.\n||1cm1deg|3cm3deg|5cm5deg| Reconstruction Time (per object) | Pose Estimation Time (per frame)|\n|:-|:-|:-|:-|:-|:-|\n|Ours|**16.3**|**55.4**|**70.3**| **347s**|**87ms**|\n|Neus + LoFTR|15.5|49.9|61.8| ~10 hours| 897ms|\n|Neus + Patch2Pix(SPG)|12.5|43.7|55.0|~10 hours| 936ms|\n\nThe results demonstrate that our method achieves higher accuracy, and both the reconstruction and pose estimation are significantly faster.\n\n**Reference:**\n\n[1] P\u00e1nek, Vojt\u011bch, Zuzana Kukelova and Torsten Sattler. \u201cMeshLoc: Mesh-Based Visual Localization.\u201d ArXiv abs/2207.10762 (2022): n. pag.\n\n[2] Wang, Peng, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura and Wenping Wang. \u201cNeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction.\u201d NeurIPS (2021).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kQVgUrUfsuR",
                "writer": "official_reviewer",
                "reply_to": "QEJXgCgfevt",
                "title": "Re: The response for the major concerns ",
                "comment": " Thank you very much for the detailed answer. I have some follow-up questions and comments regarding **R2**:\n\n> The main reason is that there is no existing baseline that performs dense reconstruction on the given video and estimates object poses without object-specific training, i.e., identical to our setting.\n\nWouldn't the following be a suitable (and rather simple) baseline?\n* At training time, create a dense 3D model of the object, e.g., using MVS.\n* At test time:\n  * Match features between the query image and the training images (as is done, e.g., by hloc) to obtain 2D-2D matches.\n  * Rather than obtaining 2D-3D matches using 3D points (from SfM) associated with the features in the training images, corresponding 3D points can be obtained by rendering depth maps of the dense model (the same is done by localization methods evaluating on the InLoc dataset).\n  * Do pose estimation with all 2D-3D matches.\n\n> Since our setting aims for efficient pose estimation with the given video, we believe the SfM-based sparse reconstruction is more suitable for the setting because it is more computationally efficient than dense reconstruction.\n\nI am not sure I understand why this argument holds. After all, the proposed approach is based on dense matching between images, as is the case for dense MVS. I don't see why dense MVS would thus be necessarily faster. E.g., according to the supp. mat., the proposed SfM method takes 347 seconds for 193 images at a resolution of 512x512 pixels. For comparison, starting with known extrinsics and intrinsics, Reality Capture, a state-of-the-art commercial 3D reconstruction system, takes 30 seconds to build a sparse point cloud from 392 images at size 800x600 for scan 65 of the DTU dataset (including feature extraction and matching). Dense reconstruction, including generating a mesh, then takes 394 seconds and computing per-vertex colors for the mesh takes about another 30 seconds. This shows that dense reconstruction is feasible in a comparable time. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZphKQxqijC",
                "writer": "official_reviewer",
                "reply_to": "i1x3fLn9X2O",
                "title": "Re: The response for the major concerns ",
                "comment": " I have a follow-up question on the OnePose++ dataset: How many training and testing images are there? I can't seem to find this information in the supp. material.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-HwdPtmJMvw",
                "writer": "official_reviewer",
                "reply_to": "Mt2SG4bBbry",
                "title": "Re: The response for the major concerns (2/2) ",
                "comment": " Thank you very much for the answers. Please find my comments and concerns below.\n \n> Compared with previous works on SfM-based pose estimation, our pipeline can be regarded as \"renovating the pipeline with a learning-based approach\"[30]. The main contribution of our method is the keypoint-free framework to eliminate the pipeline's reliance on detected keypoints. Thus our method achieves improvements on low-textured scenarios, which are challenging for previous methods.\n\nThis is rather vague to me. What does \"renovating the pipeline with a learning-based approach\" mean? Prior work, e.g., SuperGlue, LoFTR, Patch2Pix, NC-Net, Sparse NC-Net, Dual RC-Net, DSAC (++, *), InLoc, D2-Net, has already integrated learning into SfM-based pose estimation. Some of these approaches also claim to better handle weakly textured regions (e.g., InLoc motivates dense matching to better handle such regions).\n\n> The previous visual localization methods based on the direct 2D-3D matching focus on handling the large-scale problem, while the main challenge in our task is how to match the query image with the low-textured 3D model for object pose estimation. In our 2D-3D matching network, we eliminate the keypoint detection on the query image and leverage the attention module to provide the global receptive field and yield the discriminative features for 2D-3D matching.\n\nThe challenge at large scale is that feature descriptors become ambiguous as more and more locally similar structures need to be considered (see the work by Li et al., Svarm et al., and Zeisl et al.). The result is that some form of disambiguation is needed, as is the case for weakly texture regions, which also produce ambiguous matches. These works thus need to deal with a very similar problem. In my opinion, the differences need to be discussed in more detail.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8HujlfSeJqc",
                "writer": "official_reviewer",
                "reply_to": "_tyeP4Sljo1y",
                "title": "Re: The Response for the major concerns (1/2) - 2",
                "comment": " > The main difference in refinement is that [Widya et al.] only leverages the local information from each matched point to relocalize points. Since the lack of two-view or multiview constraints, the keypoint detection noise exists in its relocalization phase.\n\nWidya et al. start with coarse matches that are then refined locally: given a match established using features extracted at one layer in the network, the refinement aims at finding more accurate coordinates locally in regions around the initial match (where the region size depends on the receptive field of the features). Isn't this a similar two-view constraint used by the proposed approach?\n\n> **R3**: The main reason is that these methods are not state-of-the-art regarding their performance on both two-view matching and visual localization. \n\nUsing Patch2Pix to refine matches found by SuperGlue (denoted as SuperGlue + Patch2Pix in the Patch2Pix paper) leads to state of the art results for the visual localization task (results from visuallocalization.net) (higher is better):\n\n| Method | Aachen Day-Night v1.1 | InLoc |\n|----------|----------------------------|--------|\n| LoFTR  | day: 88.7 / 95.6 / 99.0, night: 78.5 / 90.6 / 99.0 | duc1: 47.5 / 72.2 / 84.8, duc2: 54.2 / 74.8 / 85.5 |\n| SuperGlue + Patch2Pix | day: 89.3 / 95.8 / 99.2, night: 78.0 / 90.6 / 99.0 | duc1: 50.0 / 68.2 / 81.8, duc2: 57.3 / 77.9 / 80.2 |\n| Patch2Pix | day: 86.4 / 93.0 / 97.5, night: 72.3 / 88.5 / 97.9 | duc1: 44.4 / 66.7 / 78.3, duc2: 49.6 / 64.9 / 72.5 |\n\nUnfortunately, this stronger baseline is missing.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_tyeP4Sljo1y",
                "writer": "official_reviewer",
                "reply_to": "L-BU2a6v9k9",
                "title": "Re: The Response for the major concerns (1/2) - 1",
                "comment": " Thank you very much for the detailed feedback. Please find my comments and follow-up questions below.\n\n> **R1**: Thank you very much for your comments. We clarify that the 'generalizable' baselines in our paper include OnePose, HLoc, Gen6D, which are given the same input and share exactly the same setting as ours. Therefore, the comparison in our experiments is fair and substantial. We follow the naming of previous methods OnePose[30] and Gen6D[19], which denote the property of eliminating object/category-specific training as \u2018generalizability\u2019.\n\nIf generalizability is defined as \"the property of eliminating object/category-specific training\", then I don't think that OnePose, HLoc, and OnePose++ are qualify as generalizable. They all need to build an object/category-specific scene representation, in the form of a 3D model, from the input images and their known poses. I don't why building these 3D models would not qualify as object/category-specific training as it involves optimizing an objective function and since these 3D models are fundamental parts of the object pose estimation stage. \n\n> **R2**: We clarify the difference with previous works as follows and promise to add the discussion and reference of these previous methods in the final version.\n\n\n**Relation to [Dusmanu et al., Multi-View Optimization of Local Feature Geometry, ECCV 2020]**\n\nThe closer I look at the proposed refinement, the more it seems a special case of the approach of Dusmanu et al. In their work, Dusmanu et al. deal with refining keypoint positions for feature matches. For the two-view case, they estimate the flow from one keypoint to a position in a patch around the other matching keypoint by matching features and regressing the flow. This seems conceptually the same as the fine matching stage (with probably the main difference being that Dusmanu et al. did not use a transformer). For the multi-view case, Dusmanu et al. state that \"Firstly, since corresponding features are generally observed from different viewpoints and looking at non-planar scene structures, the computed displacement vector is only valid for the central pixel and not constant within the patch [...]. Thus, when refining keypoint locations u, v, w, . . . over multiple views, consistent results can only be produced by forming displacement chains (e.g., du\u2192v + d(v+du\u2192v)\u2192w + . . .) without loops. However, such an approach does not consider all possible edges in the graph and quickly accumulate errors along the chain.\" Rather than computing the offset / flow for a single pixel (the original feature position) to a patch, Dusmanu et al. thus compute flow fields between patches and use these fields to jointly refine all keypoint positions (after fixing one keypoint position in one of the images), using as many pairwise matches as possible. The proposed approach is a special case in the sense that (1) it uses a sub-graph of the pairwise matching graph that connects the reference node with the other nodes, but does not include any connections between the other nodes, and (2) only computes a single offset per pair and not a full flow field.\n\nUnless I am overlooking something, I believe that the claim that a novel keypoint-less SfM approach is proposed needs to be adjusted. \n\n**Compare with PixSfM[18]:**\n\nThank you very much for the detailed answer. I have two comments / questions:\n\n> Accuracy. The capability of the two-view transformer module in fine-level matching can be leveraged by our refinement to find accurate matches at low-texture regions, where the CNN feature map used by PixSfM struggles.\n\nWhere can I see that PixSfM struggles in \"low-texture regions\"? The reported performance seems rather very similar to me.\n\n> Storage Efficiency. We do not need to extract and store dense local features around each 2D point and keep them in memory to perform feature-metric optimization like PixSfM. Therefore the storage and memory peak during refinement is low.\n\nLooking at Sec. 4.4 of the PixSfM paper (and its supplementary material), the high memory costs can be avoided by pre-computing and storing cost maps rather than the descriptors. This comes at a small loss in accuracy. In the provided table, are the 7.35GB required for storing descriptors or the cost maps?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i1x3fLn9X2O",
                "writer": "author",
                "reply_to": "XUE5-0wKa9D",
                "title": "The response for the major concerns",
                "comment": " We thank the reviewers for the insightful suggestions. We address the major concerns below:\n\n>**Q1:** The description of the proposed OnePose-HARD dataset is quite abstract. Example images of the new dataset are not included in the paper. Please consider including example images of the OnePose-HARD dataset in the paper.\n\n**R1:** Thank you very much for the suggestions. In fact, we describe the details of the OnePose-HARD dataset, and the example images are included in the supplementary L62-78. We will provide more detailed information and example images in the revised paper.\n\n>**Q2:** In the evaluation, it is not always clear where the results of the other methods come from. Please clarify the source of the results of other methods that you compare your method to.\n\n**R2:** We thank the reviewer for pointing out this missing information. We clarify as follows and promise to add to the final paper.\n\nFor the evaluation of the OnePose dataset, because of the limited space for writing, we report the overall metric by average over the whole OnePose evaluation set. The overall metric results come from OnePose's supplementary material(https://zju3dv.github.io/onepose/files/onepose_supp.pdf, the first row of results in Tab.2). We believe the overall metric won't affect the comparison.\n\nAdditionally, we use underlines to denote the second place results while using bold to denote first place results in Tab.1. We will add the illustrations of these symbols in the caption.\n\nThe results of PVNet on the OnePose-HARD dataset are obtained by running their open-source code. Details are located at L267-268.\nFor the experiments on the LINEMOD dataset, since OnePose contains no such evaluation, we evaluate the OnePose by running their open-source code and using their pre-trained model. As described in L274-275, the results of other baselines, including PVNet, CDPN, Gen6D, are from their original paper.\n\n>**Q3:** The authors claim that their method is \"~10\u00d7 faster\" [line 290] than other methods, but the actual runtimes of the other methods are not listed in the paper. Please consider including the runtimes of your method and the methods you compare to, in order to support the claim that your method is 10\u00d7 faster.\n\n**R3:** We thank the reviewer for pointing out this missing information. We provide the runtimes of generalizable pose estimators as follows and will add them to Tab.1\n|  Ours|  OnePose|  HLoc(LoFTR)| HLoc(SPP+SPG)|\n|:-    |:-       |:-           |:-            |\n| 87ms |   66ms  |    909ms    |     835ms    |\n\nThe runtimes are evaluated on the same server described in L236. As described in L288-291, our method runs ~10\u00d7 faster than HLoc-based methods. Since we use more 3D points to perform matching with query feature maps in a coarse-to-fine manner, our method is a little slower than OnePose.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "L-BU2a6v9k9",
                "writer": "author",
                "reply_to": "OSrNdlttdmZ",
                "title": "The response for the major concerns (1/2)",
                "comment": " We thank the reviewers for the insightful suggestions. We address the major concerns below:\n>**Q1:** Both OnePose and OnePose++ are misclassified as being generalizable object pose estimation approaches. It is not too surprising that it outperforms the generalizable baselines as it is able to train (in the form of building a SfM model) per object.\n\n**R1:** Thank you very much for your comments. We clarify that the 'generalizable' baselines in our paper include OnePose, HLoc, Gen6D, which are given the same input and share exactly the same setting as ours. Therefore, the comparison in our experiments is fair and substantial. We follow the naming of previous methods OnePose[30] and Gen6D[19], which denote the property of eliminating object/category-specific training as \u2018generalizability\u2019.\n\n>**Q2:** Please describe in detail how the proposed keypoint-free SfM approach differs from prior work in this area.\n\n**R2:** We clarify the difference with previous works as follows and promise to add the discussion and reference of these previous methods in the final version.\n\n**Compare with PixSfM[18]:**\nThe main difference in refinement is that we leverage fine-level matching with Transformer to refine the 2D locations of coarse feature tracks and then optimize the 3D model with geometric error, while PixSfM uses pre-stored dense feature maps and feature-metric BA to refine the 3D model and 2D keypoints globally.\nThe advantages of our refinement are\n\n- Accuracy. The capability of the two-view transformer module in fine-level matching can be leveraged by our refinement to find accurate matches at low-texture regions, where the CNN feature map used by PixSfM struggles.\n- Storage Efficiency. We do not need to extract and store dense local features around each 2D point and keep them in memory to perform feature-metric optimization like PixSfM. Therefore the storage and memory peak during refinement is low.\n\nWe report the point cloud accuracy evaluated on OnePose-HARD scanned objects as follows. The results demonstrate that the 3D models reconstructed by our refinement achieve higher accuracy. Our refinement is also more storage efficient in terms of dense features storage cost. Notably, the image resolution in the dataset is 512\u00d7512. With image resolution increase, the storage cost of PixSfM will rise significantly since keypoint-free matchers will yield much more matches.\n||1mm|3mm|5mm|Feature Storage Cost|\n|:-|:-|:-|:-|:-|\n|LoFTR coarse + Our refinement|**29.5**|**73.6**|**85.8**|-|\n|LoFTR coarse + PixSfM|27.6|71.2|84.4|7.35GB|\n|LoFTR coarse (no refinement)|25.6|68.9|83.6|-|\n\nThe following results evaluated on the OnePose dataset illustrate that our refinement also brings improvement for the object pose estimation.\n||1cm1deg|3cm3deg|5cm5deg|\n|:-|:-|:-|:-|\n|LoFTR coarse + Our refinement|**50.7**|**80.0**|**87.0**|\n|LoFTR coarse + PixSfM|48.9|79.3|86.4|\n|LoFTR coarse (no refinement)|45.5|78.6|86.0|\n\n**Compare with SfM approaches provided by keypoint-free descriptors:**\n\nThe main difference is that all these approaches face the trade-off between point accuracy and repeatability. I.e., they scarface the sub-pixel match accuracy by rounding matches to grid level or merging matches within a grid to obtain repeatable 'keypoints' for SfM. On the contrary, our SfM obtains repeatable features while preserving the sub-pixel matching accuracy by the refinement phase.\n\n**Compare with [Widya et al., Structure from motion using dense CNN features with keypoint relocalization]:**\n\n[Widya et al.] and OnePose++ share a similar pipeline in terms of SfM, which firstly strikes for repeatable matches with low-res dense feature grids, then refines matching positions for higher accuracy.\n\nThe main difference in refinement is that [Widya et al.] only leverages the local information from each matched point to relocalize points. Since the lack of two-view or multiview constraints, the keypoint detection noise exists in its relocalization phase.\nIn contrast, our refinement phase performs multiple two-view dense matching in the local regions based on the coarse feature tracks to refine 2D point locations. Therefore, the detection error is avoided, and the capability of keypoint-free matchers' fine level matching is leveraged to boost the performance on low-textured objects.\n\n>**Q3:** Why were Patch2Pix, Dual RCNet, etc., not considered as baselines?\n\n**R3:** The main reason is that these methods are not state-of-the-art regarding their performance on both two-view matching and visual localization. We also conduct experiments on the OnePose-HARD dataset to compare our pipeline with these methods based on their SfM and localization methods. The results show that our method outperforms them by a large margin. We will add the results and references in the final version.\n\n||1cm1deg|3cm3deg|5cm5deg|\n|:-|:-|:-|:-|\n|Ours|**16.3**|**55.4**|**70.3**|\n|LoFTR(round)|15.4|43.7|53.4|\n|DRC-Net|11.3|37.0|47.8|\n|Patch2Pix|2.42|19.0|30.4|",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Mt2SG4bBbry",
                "writer": "author",
                "reply_to": "OSrNdlttdmZ",
                "title": "The response for the major concerns (2/2)",
                "comment": " >**Q4:** The relation of OnePose++ to prior work on SfM-based object pose estimation and visual localization.\n\n**R4:** We thank the reviewer for pointing out the missing discussions and the references of the prior works. We describe the relations as follows, and we promise to add them to the final version.\n\n**Discussion with prior works on SfM-based pose estimation**\n\nCompared with previous works on SfM-based pose estimation, our pipeline can be regarded as \"renovating the pipeline with a learning-based approach\"[30]. The main contribution of our method is the keypoint-free framework to eliminate the pipeline's reliance on detected keypoints. Thus our method achieves improvements on low-textured scenarios, which are challenging for previous methods.\n\n**Discussion with prior works on visual localization**\n\nThe previous visual localization methods based on the direct 2D-3D matching focus on handling the large-scale problem, while the main challenge in our task is how to match the query image with the low-textured 3D model for object pose estimation. In our 2D-3D matching network, we eliminate the keypoint detection on the query image and leverage the attention module to provide the global receptive field and yield the discriminative features for 2D-3D matching.\n\nThe visualizations in Fig. 5 and discussions in Sec. 4.5 demonstrate the attention module plays a critical role in the 2D-3D matching and pose estimation of low-textured objects.\n\n> **Q5:** References for RANSAC and the PnP solver used are missing.\n\n**R5:** We thank the reviewer for pointing out the missing references. We promise to add them in the final version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QEJXgCgfevt",
                "writer": "author",
                "reply_to": "dmZVK7RrOZ",
                "title": "The response for the major concerns",
                "comment": " We thank the reviewers for the insightful suggestions. We address the major concerns below:\n>**Q1:** How is the current method's dense reconstruction different from Multi-View Stereo paradigms? The major advantage or the difference between the proposed network and the previous baseline (onepose [30]) is the dense object reconstruction of the objects from videos. The contribution in the dense reconstruction from videos is well studied in the Multi-View Stereo frameworks. So I feel that the method doesn't have a lot of novelty in terms of reconstruction.\n\n**R1:** The comment 'the current method's dense reconstruction' may be a misunderstanding. As described in the paper, our reconstruction part is still an SfM-based method instead of Multi-View Stereo based, and we denote our SfM point cloud as semi-dense since we adapt the keypoint-free image matcher LoFTR, which performs semi-dense matching, to the SfM framework. Therefore our SfM-based pipeline should still be categorized into sparse reconstruction methods, and the reconstructed point cloud is significantly sparser than the dense reconstruction since we do not perform pixel-wise depth estimation such as PatchMatch or PlaneSweep.\n\nCompared with OnePose, our contribution in the reconstruction part is our SfM design to adapt keypoint-free feature matching methods to the SfM. As discussed in L38-42, the keypoint-free matcher LoFTR cannot be directly used for SfM since the inconsistent matches. Our keypoint-free SfM framework solves this problem and yields more complete 3D point clouds compared with the previous keypoint-based SfM framework, which benefits pose estimation.\n\n>**Q2:** Why are evaluations not compared to these paradigms to show the accuracy improvement if the dense reconstruction is very good?\n\n**R2:** The main reason is that there is no existing baseline that performs dense reconstruction on the given video and estimates object poses without object-specific training, i.e., identical to our setting. \n\nSince our setting aims for efficient pose estimation with the given video, we believe the SfM-based sparse reconstruction is more suitable for the setting because it is more computationally efficient than dense reconstruction.\n\nMoreover, the reconstructed SfM point clouds are more compact than the dense MVS point clouds because they are sparser and contain mainly informative 3D points, thus more suitable for storing 3D point features and efficient for performing direct 2D-3D matching in our pipeline.\n\nWe believe incorporating the dense reconstruction methods for object pose estimation in our setting can be explored as a direction for future works.\n\n>**Q3:** The paper assumes that the object videos are given aprior, so one-shot object pose estimation might be a misleading term as the method will fail if the objects videos are not provided beforehand.\n\n**R3:** Thank you very much for your comments. We clarify that the 'one-shot' naming indicates the setting that given one video shot of the object with annotated poses, our method can estimate its poses in arbitrary environments without additional pose estimator training.\n\nThis is similar to the \"one-shot\" setting in 2D detection and segmentation [1,2,3], which assumes \"given an example image of a novel, previously unknown object category (the reference), find and segment all objects of this category within a complex scene (the query image)\"[2]\n\n>**Q4:** Literature review of Multi-view stereo needs to be well studied and the difference between these methods and the proposed keypoint-free methods need to be well established. \n\n**R4:** Thank you very much for your comments. We will add the review of Multi-View Stereo methods and the discussion with our keypoint-free SfM framework in the final version.\n\n>**Q5:** The evaluations are not substantial as comparisons to other methods like CAD-model based pose estimation have not been well studied.\n\n**R5:** The comment \"comparisons to other methods like CAD-model based pose estimation have not been well studied\" may be a misunderstanding. In fact, we compare the proposed method with CAD-model-based baselines PVNet and CDPN on multiple datasets, as pointed out in Line 16-17, shown in Tab 2, 3 and discussed in Sec 4.3, 4.4.\n\nThe results demonstrate that our method achieves comparable results with CAD-model-based pose estimation methods, which are trained for each object with the given CAD model.\n\n**References**\n\n[1] Li, Xiang, Lin Zhang, Yau Pun Chen, Yu-Wing Tai and Chi-Keung Tang. \u201cOne-Shot Object Detection without Fine-Tuning.\u201d ArXiv abs/2005.03819 (2020): n. pag.\n\n[2] Michaelis, Claudio, Ivan Ustyuzhaninov, Matthias Bethge and Alexander S. Ecker. \u201cOne-Shot Instance Segmentation.\u201d ArXiv abs/1811.11507 (2018): n. pag.\n\n[3] Caelles, Sergi, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix\u00e9, Daniel Cremers and Luc Van Gool. \u201cOne-Shot Video Object Segmentation.\u201d 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 5320-5329.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "q-Xb_zHOvBJ",
                "writer": "author",
                "reply_to": "w-rI0KbKqIW",
                "title": "The response for the major concerns",
                "comment": " We thank the reviewers for the insightful suggestions. We address the major concerns below:\n>**Q1:** The proposed method needs off-the-shelf 2D object detector.\n\n**R1:** In practice, the need for an off-the-shelf 2D object detector can be eliminated by leveraging 2D-2D feature matching. This issue has been addressed in Section 2 in the supplementary material of OnePose[30] (https://zju3dv.github.io/onepose/files/onepose_supp.pdf).\n\nFollowing OnePose, We first perform multiple 2D-2D feature matching between reference-query image pairs, and then select the image pair with the most inliers to estimate 2D affine transformation. The region of interest(RoI) in the query image is then detected by transforming the corner of RoI in the reference image with the estimated transformation.\n\nTo validate the effectiveness of this method, we present the evaluation results on the OnePose dataset with the feature-matching-based 2D detector below. The results demonstrate that the performance of the proposed method does not degrade significantly.\n\n|   |  1cm1deg   |  3cm3deg   |   5cm5deg    |\n|:- |:-   |:-   |:-     |\n| Ours use GT bounding box (reported in the paper)  |  **50.4**   |  80.0   |  87.0     |\n| Ours use bounding box from feature matching 2D detector | 49.6 | **80.4** | **87.2**|\n\n>**Q2:** The proposed method needs real images for training.\n\n**R2:** Our 2D-3D matching network is trained on real images but can be generalized to novel objects. Besides, our method needs an object video for building an SfM model, but we don't think this is a disadvantage, as in most scenarios capturing a video of an object is much easier than acquiring its CAD model or doing object-specific training.\n\n>**Q3:** CAD model or its equivalent can be reconstructed from the movie of target object with surrounded AR markers. This might decrease the advantage of the proposed method.\n\n**R3:** Thank you very much for your comments. Leveraging AR markers can only help solve the camera poses, while dense reconstruction itself requires other modules, and the quality of the dense reconstruction is not guaranteed, especially for low-textured objects. Moreover, dense reconstruction also requires more computation than our SfM-based pipeline. Therefore we believe it is not ideal for our one-shot scenario.\n\n>**Q4:** How about processing time for training and testing?\n\n**R4:** As detailed in L236-238, our 2D-3D matching network is trained on the OnePose training set. The training takes about 20 hours with a batch size of 32 on 8 NVIDIA-V100 GPUs. At test time, our matching module runs at 87ms for a 512 \u00d7 512 query image on a single V100 GPU.\n\n>**Q5:** Is the pose estimation network trained for each object?\n\n**R5:** No. As described in L28-29, our method eliminates the need for per-object training and CAD models. Therefore, it is more applicable for AR scenarios.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "w-rI0KbKqIW",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_BZ92dxDS3tO",
                "title": "",
                "comment": " The authors proposed 6-DoF object pose estimation algorithms which does not require CAD models of target objects.  They require only video sequence of the target object with camera pose.  The keypoint-free SfM build 3D models in training and the keypoint-free 2D-3D matching network can estimate the correspondences between the models and image query. The proposed method handles low-textured objects and achieved state-of-the-art accuracy on public dataset. Strengths:\n- The proposed algorithm can handle low-textured objects and does not require CAD model for training, those are useful for real applications.\n- The ablation study shows the effectiveness of each component.\n\nWeaknesses:\n- The proposed method needs off-the-shelf 2D object detector and real images for training. - How about processing time for training and testing?\n- Is the pose estimation network trained for each object? - CAD model or its equivalent can be reconstructed from the movie of target object with surrounded AR markers.  This might decrease the advantage of the proposed method.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "dmZVK7RrOZ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_BZ92dxDS3tO",
                "title": "",
                "comment": " The paper looks at the problem of one-shot object pose estimation on textureless objects where previous keypoint-based methods fail to perform well. The main contribution is using the keypoint-free SFM pipeline to create a repeatable semi-dense point cloud, which automatically helps improve 3D-2D correspondence to estimate the object pose. Comparisons show that the method performs better than a previous method which uses sparse point cloud reconstructions. Strengths:\n- Moving away from keypoint-based methods helps in automatic object pose estimation of objects in the wild.\n- The semi-dense reconstruction of the objects seems to be very helpful in the pose estimation due to better 2D-3D correpondences.\n- The results show that the method is more robust to occlusions than previous methods and a live demo helps show the method's accuracy.\n- Ablation study shows the advantages of the refinement step and the attention module in the pose estimation network.\n- Openpose-Hard dataset is useful for research in pose estimation of textureless objects.\n\nWeaknesses:\n- The major advantage or the difference between the proposed network and the previous baseline (onepose [30]) is the dense object reconstruction of the objects from videos. The contribution in the dense reconstruction from videos is well studied in the Multi-View Stereo frameworks. So I feel that the method doesn't have a lot of novelty in terms of reconstruction.\n- The paper assumes that the object videos are given aprior, so one-shot object pose estimation might be a misleading term as the method will fail if the objects videos are not provided beforehand.\n-  Literature review of Multi-view stereo needs to be well studied and the difference between these methods and the proposed keypoint-free methods need to be well established.\n- The evaluations are not substantial as comparisons to other methods like CAD-model based pose estimation have not been well studied. How is the current method's dense reconstruction different from Multi-View Stereo paradigms. Why are evaluations not compared to these paradigms to show the accuracy improvement if the dense reconstruction is very good?\n\n The limitations and potential negative impact are well studied in the paper. ",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "OSrNdlttdmZ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_BZ92dxDS3tO",
                "title": "",
                "comment": " The paper considers the problem of object pose estimation in scenarios where CAD model of the objects are not available. The paper describes OnePose++, a variant of the recently proposed OnePose approach that uses densely extracted descriptors (via LoFTR) rather than the SuperPoint keypoints used by OnePose. LoFTR provides matches between pairs of images, where the 2D positions of matching points vary depending on the image pair. As the resulting 2D positions are not repeatable, the paper uses a SfM approach designed to handle this scenario in order to build the 3D model of the object used for pose estimation. 2D-3D matches between a query image and the SfM model are established by directly matching descriptors against the 3D model in a coarse-to-fine manner. In addition to the OnePose++ method, the paper introduces a harder variant of the OnePose dataset, named OnePose-HARD. Experimental results show that OnePose++ outperforms most baselines by a wide margin (in particular, OnePose++ consistently outperforms OnePose). The paper has multiple strengths:\nS1) The proposed OnePose++ approach is a natural extension of OnePose that shows how to swap keypoint-based features with keypoint-free features. Since the latter have shown promise in challenging scenes, e.g., for weakly textured objects or under strong illumination conditions, this is interesting.\n\nS2) OnePose++ clearly outperforms OnePose and also most of the baseline methods. The strong results are a strength of the paper.\n\nS3) The paper provides a detailed ablation study that analyzes the impact of the individual components of OnePose++.\n\nS4) The proposed dataset, OnePose-HARD, seems very challenging and thus has the potential to drive research in the field. It will be of interest to the community.\n\nOn the negative side, there are also multiple weaknesses:\nW1) Both OnePose and OnePose++ are misclassified as being generalizable object pose estimation approaches. It is true that the underlying LoFTR features and the coarse and fine matching stages generalize beyond the data they were trained on. Yet, OnePose++ requires that \"a video sequence with annotated poses is available for each object\" is available and builds a SfM model for this particular type of object. I don't see how this is not an instance-level method. The SfM model for one object might give reasonable results for another object if both objects have very similar shapes (and textures). But a single model for one particular type of an object, e.g., a particular chair, will not generalize over the full class (e.g., all potential chairs).\nGiven that OnePose++ is an instance-level method, it should be more closely compared to other instance-level methods. It is not too surprising that it outperforms the generalizable baselines as it is able to train (in the form of building a SfM model) per object.\n\nW2) The paper claims proposing a \"keypoint-free SfM framework for accurate and complete semi-dense reconstruction\" as one of its main contributions. It is not clear to my how the described framework contributes novelty to the literature:\na) Besides LoFTR, there are other keypoint-free descriptors, e.g., Patch2Pix [Zhou et al., Patch2Pix: Epipolar-Guided Pixel-Level Correspondences, CVPR'21], Sparse NCNet [Rocco et al., Efficient \u00b4neighbourhood consensus networks via submanifold sparse\nconvolutions, ECCV'20], and Dual RCNet [Li et al., Dual-Resolution Correspondence Networks, NeurIPS'20], that are evaluated in a visual localization setting that requires an underlying SfM model. They thus also provide approaches for keypoint-free SfM. All of them are inherently applicable to object pose estimation (since they do not make any assumption on the type of scenes). Similarly, LoFTR also uses a keypoint-free SfM approach (based on Dual RCNet) (see https://github.com/zju3dv/LoFTR/issues/9). Another approach to keypoint-free SfM based on dense feature matching between images is [Widya et al., Structure from motion using dense CNN features with keypoint relocalization, IPSJ Transactions on Computer Vision and Applications 2018]. Yet, this prior work is not discussed. The differences between this prior work and the proposed approach should be clearly described. Furthermore, comparisons with other keypoint-free approaches, e.g., Patch2Pix or Dual RCNet, are missing.\nb) As far as I can see, the coarse reconstruction stage for keypoint-free SfM is the same as for LoFTR (based on the description provided here: https://github.com/zju3dv/LoFTR/issues/9). The refinement stage seems identical to [18]. The paper states that \"Note that our keypoint-free SfM framework is also related to PixSfM [18] but comes with different motivations. PixSfM improves keypoint-based SfM for more accurate 3D reconstructions by refining inaccurately-detected sparse local features with dense feature maps. Different from PixSfM, we aim to adapt the keypoint-free method LoFTR [29] to SfM for object pose estimation.\" However, I disagree with this statement. As stated in the paper, \"Every pixel in the downsampled image can be regarded as a \u201ckeypoint\u201d in the original image.\" The goal of the refinement stage is to \"refine the object point cloud with sub-pixel correspondences.\" In other words, the motivation for the refinement stage is to obtain a more accurate 3D model by refining initially inaccurate keypoint positions. This is achieved using dense feature maps to detect more accurate keypoint positions.\n\nW3) As in the case of keypoint-free SfM, there are other directions of highly related work that omitted:\na) Work on object pose estimation using SfM rather than CAD models certainly predates OnePose. Examples include [Gordon & Lowe, What and Where: 3D Object Recognition with Accurate Pose, Toward Category-Level Object Recognition, 2006], [Rothganger et al., 3D Object Modeling and Recognition Using Local Affine-Invariant Image Descriptors and Multi-View Spatial Constraints, 3D Object Modeling and Recognition Using Local Affine-Invariant Image Descriptors and Multi-View Spatial Constraints, IJCV 2006], [Hsiao et al., Making specific features less discriminative to improve point-based 3D object recognition, CVPR 2010[, [Bhat et al., Visual words for 3D reconstruction and pose computation, 3DIM/3DPVT 2011], and [Fenzi et al., 3D Object Recognition and Pose Estimation for Multiple Objects using Multi-Prioritized RANSAC and Model Updating, DAGM/OAGM 2012]. This prior work should be properly acknowledged.\nb) The paper states that \"HLoc is slow during pose estimation because it depends on multiple 2D-2D image matchings as the proxy for building 2D-3D correspondences.\" Yet, there is quite some literature on visual localization algorithms that do not use image retrieval but directly match 2D features against 3D points via associated feature descriptors. Examples include: [Arth et al., Wide Area Localization on\nMobile Phones. ISMAR 2009], [Li et al., Location Recognition using Prioritized Feature Matching, ECCV 2010], [Li et al., Worldwide Pose Estimation Using 3D Point Clouds, ECCV 2012], [Choudhary & Narayanan, Visibility probability structure from sfm datasets and applications, ECCV 2012], [Donoser & Schmalstieg, Discriminative featureto-point matching in image-based localization, CVPR 2014], [Cao & Snavely, Minimal scene descriptions from structure from motion models, CVPR 2014], [Lim et al., Real-time monocular image-based 6-dof localization, IJRR 2015], [Lynen et al., Get out of my lab: Largescale, real-time visual-inertial localization, RSS 2015], [Zeisl et al., Camera Pose Voting for Large-Scale Image-Based Localization, ICCV 2015], [Camposeco et al., Toroidal Constraints for TwoPoint Localization under High Outlier Ratios, CVPR 2017], [DuToit et al., Consistent map-based 3d localization on mobile devices, ICRA 2017], [Liu et al., Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map, ICCV 2017], [Sattler et al., Efficient & effective prioritized matching for large-scale image-based localization, PAMI 2017], [Svarm et al., City-scale localization for cameras with known vertical direction, PAMI 2017], and [Lynen et al., Large-scale, real-time visual-inertial localization revisited, IJRR 2019]. Many of these approaches are directly applicable to the object pose estimation setting based on SfM models and should thus be discussed.\n\nThe following additional comments did not affect my recommendation:\n* References for RANSAC and the PnP solver used are missing. In order to consider raising my score, I would like to see the following points addressed in a rebuttal:\nQ1) Please describe in detail how the proposed keypoint-free SfM approach differs from prior work in this area.\nQ2) Why were Patch2Pix, Dual RCNet, etc. not considered as baselines?\nQ3) Please describe the relation of OnePose++ to prior work on SfM-based object pose estimation and visual localization (see W3 above). The paper adequately discusses limitations and potential negative social impact.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "XUE5-0wKa9D",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_BZ92dxDS3tO",
                "title": "",
                "comment": " The paper proposes an improvement of the one-shot pose estimation system OnePose to better estimate the pose of low-texture objects. In particular, the keypoint-based matching component of OnePose is replaced with the key-point free matching method LoFTR. For the evaluation of the improved functionality, the authors propose the new dataset OnePose-HARD, which contains low-texture objects along with their pose annotations.\n Originality:\n+ The idea of replacing keypoint-based matching with keypoint-free matching for improved low-texture performance is straightforward and a logical extension of the prior work.\n\nQuality:\n+ The paper is well written and structured.\n+ The paper precisely adheres to the formatting requirements and the page limit.\n+ Related work is described sufficiently and it is made clear how the proposed method is positioned in the existing landscape.\n+ The proposed method is evaluated on appropriate datasets and compared to relevant state-of-the-art methods.\n\nClarity:\n+ The language is clear and easy to follow.\n+ Methods used in the paper, e.g., LoFTR, are explained briefly, but well understandable.\n- The description of the proposed OnePose-HARD dataset is quite abstract. Example images of the new dataset are not included in the paper.\n- In the evaluation, it is not always clear where the results of the other methods come from. E.g., in Table 1, the results of the OnePose method on the OnePose dataset is given as an overall single value for the whole dataset, while the original OnePose paper lists the results of the three categories \"large\", \"medium\", and \"small\" separately, but no overall single value. Similarly, in Table 3, the paper gives results for OnePose on the LINEMOD dataset, but the original OnePose paper contains no such evaluation.\n- The authors claim that their method is \"~10\u00d7 faster\" [line 290] than other methods, but the actual runtimes of the other methods are not listed in the paper.\n\nSignificance:\n+ The addressed issue with low-texture objects is relevant in practice.\n+ The results of the proposed method seem to be significantly better than comparable methods, particularly in low-texture scenarios. \n- Please consider including example images of the OnePose-HARD dataset in the paper.\n- Please clarify the source of the results of other methods that you compare your method to: Where are these numbers taken from or did you evaluate the methods yourself? In particular, the results of OnePose on the OnePose dataset and on LINEMOD, as described above. Please also describe why some results are underlined in Table 1.\n- Please consider including the runtimes of your method and the methods you compare to, in order to support the claim that your method is 10\u00d7 faster.\n\n\nAdditional remarks (typos, suggestions etc.), no need to address in the rebuttal:\n- line 49: \"establish\" -> \"establishes\"\n- line 70: \"OnePose-HARD ,\" -> \"OnePose-HARD,\"\n- line 104: \"keypoints .\" -> \"keypoints.\"\n- Figure 2, caption: \"a reference image sequences\" -> \"a reference image sequence\"\n- Figure 2, caption: \"point cloud which are\" -> \"point cloud which is\"\n- line 131: \"build\" -> \"builds\"\n- line 295: \"number(~ 5000)\" -> \"number (~ 5000)\"\n- On the naming of the proposed dataset: The suffix \"HARD\" is very generic and does not tell what the difficulties actually are. Please consider a more informative suffix, such as \"LowTexture\".\n Yes, limitations and impact were discussed where applicable.",
                "rating": 6,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "clarifying some parts of the method and put better in context of the state of the art and former evaluations",
                "Sentiment Expression": "Main issues",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "Unclear novelty",
                "Sentiment Expression": "another raised problem",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "missing baselines and prior work discussion",
                "Sentiment Expression": "A lot of concerns were also raised",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the answers to their comments",
                "Sentiment Expression": "the most positive reviewers seem to be satisfied",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "can be considered acceptable for NeurIPS 2022 publication",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "_VjQlMeSB_J": {
        "paper_id": "nips_2022__VjQlMeSB_J",
        "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "paper_abstract": "We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
        "paper_acceptance": "Accept",
        "meta_review": "All reviewers have voted to accept the paper, and this is a solid work.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "s3gR3QY1AzU",
                "writer": "official_reviewer",
                "reply_to": "btdmTr7gCmG",
                "title": "Thank you for answering questions! I'm happy to increase my raiting. ",
                "comment": " Thank you for addressing my concersn and answering my quesitons! The experimental results and discussions are intriguing. I'm happy to increase my score. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "btdmTr7gCmG",
                "writer": "author",
                "reply_to": "UigApuo4wz0",
                "title": "Thanks for the discussion!",
                "comment": " Thanks for the further discussion!\n\n> Do you have any insights into why the performance improvement by CoT with smaller models is somewheat limited or smaller than the ones with larger LMs?\n\nOur thoughts on this are detailed in the FAQ on page 16, Question 1 (Appendix A.1, and the corresponding Figures 9 and 10), and we summarize them below:\n- Why model scale improves chain of thought prompting is a very interesting open question. Chain of thought prompting is just one of many emergent abilities that have been observed in recent years that substantially improves as a result of scaling. \n- The answer to this question will likely be multi-faceted, and we consider it mostly out of the scope of this current paper. Nonetheless, we did a small preliminary error analysis. \n    - We manually examined 45 problems that a 62B model got wrong and looked at what percentage of them were fixed by scaling to 540B.\n    - These errors were broadly grouped into semantic understanding, one-step missing, and other (hallucinations, repetitive outputs, symbol mapping errors).\n    - For all three of these categories, the 540B model got a substantial portion of these questions correct. This suggests that some skills that the 62B model may lack are achieved by scaling up to 540B, which is consistent with a hypothesis that language models acquire a range of semantic understanding and logical reasoning skills as a function of scale.\n- This observation that language models acquire many abilities at scale is consistent with a lot of prior literature (see the GPT-3 paper\u2014Brown et al., 2020; the BIG-Bench paper\u2014Srivastava et al., 2022, and many more).\n- In summary, one hypothesis is that successful multi-step reasoning requires a combination of many skills (semantic understanding, symbol mapping, coherent text generation, staying on topic, etc) that likely improve as a function of model scale.\n\n> Do you have any thoughts on why the model performance deteriorates on StrategyQA with CoT?\n\nThanks for this question. One reason why small models have worse CoT performance on StrategyQA is that StrategyQA requires substantial world knowledge (e.g., an example question from StrategyQA is *\u201cIs growing seedless cucumber good for a gardener with entomophobia?\u201d*). Smaller models have fewer parameters to memorize such world knowledge (e.g., Roberts et al., 2020), and so they are less likely to be able to produce correct reasoning steps, thus deteriorating performance. \n\n> on AQuA, adding the calculator doesn't help\n\nYes, thanks for pointing this out. Unlike all the other datasets which are free response, AQuA is a multiple choice dataset (see Table 21 for an example). Our external calculation function only applied for free response questions since it did not have functionality to change the mappings to final multiple choice answers. We will clarify this in the next version.\n\nIf you have more questions we will be happy to discuss further! We hope that these new experimental results addressed the concerns in your review feedback.\n\nReferences in this response:\n- Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? EMNLP 2020. https://arxiv.org/abs/2002.08910\n- Srivastava et al., and 300+ authors. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. https://arxiv.org/abs/2206.04615",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UigApuo4wz0",
                "writer": "official_reviewer",
                "reply_to": "wa180-OY605",
                "title": "Thank you for all of the new results! I have a few follow up questions. ",
                "comment": " Thank you for adding all of the new results and adding new discussions. The results using other models also look promising. \n\n> We added these results to the paper (Appendix B, Tables 1-4), and also show them in the below tables. For nine out of ten arithmetic/commonsense tasks, CoT prompting improves performance compared to standard prompting. \n\nThe results are interesting. On Codex or your LM2 CoT generally gives large performance improvements across different datasets while with UL2, the results are mixed. \n\n> With an external calculator (implementation described in Appendix B), the average gain on arithmetic reasoning jumps to (13.7%), which suggests that many of UL2\u2019s arithmetic errors are from calculations instead of reasoning (potentially because C4 is a noisy pre-training dataset). \n\nI agree with the point that the model may make more calculation errors, but on AQuA, adding the calculator doesn't help, which mght indicate that there's another issue besides the caldulation error. Do you have any inights into why the performance improvement by CoT with smaller models is somewheat limted or smaller than the ones with larger LMs? \n\nAlso on non arithmatic tasks particularily in StrategyQA, adding CoT actually hurts the performance. Do you have any thoughts on why the model performance deteriorates on StrategyQA with CoT? \n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "F1NW7CQl-Ic",
                "writer": "author",
                "reply_to": "nips_2022__VjQlMeSB_J",
                "title": "Overall comment",
                "comment": " Thank you all for the detailed reviews! Our paper explored a simple approach for multi-step reasoning via language models. In an empirical evaluation on twelve reasoning benchmarks, the approach improved performance over standard prompting by a large margin, achieving SOTA results on several benchmarks. We are encouraged that the reviewers agreed on the strengths of the work, including that the method is \u201csimple and well-motivated\u201d (TQF2, UpwE), the analysis is \u201ccomprehensive/thorough\u201d (nMs2, TQF2), and the empirical gains are \u201clarge/substantial/effective\u201d (all three reviewers).\n\nIn the individual replies, we describe additional improvements that we have made based on reviewer suggestions, which we believe make the paper stronger. Notably, we did new evaluations on UL2 and Codex, for which chain of thought prompting substantially outperformed standard prompting (consistent with the three models already evaluated in the paper). We hope this addresses the comment raised by Reviewer TQF2, as UL2 is a completely open-source model and Codex is currently free (and the strongest overall model we evaluated), which facilitates reproducibility.\n\nWe enjoyed discussing the feedback. Should the reviewers decide to engage further, we are happy to continue!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wa180-OY605",
                "writer": "author",
                "reply_to": "MnisCno20Ha",
                "title": "Response continued",
                "comment": " > Recent work (Ye and Durret, 2022) shows that explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations.\n\nThank you for this pointer. Yes, one of the most important open problems in language model and natural language generation research broadly is generating factual outputs that are grounded on the input context (Maynez et al., 2021; Rashkin et al., 2021; Ye and Durret, 2022; Marasovic et al., 2022; Weigreffe et al., 2022; and more). The fact that language models (even with CoT prompting) do not solve elementary-school level math problems with perfect accuracy (100%) is consistent with the observation that language model generations can have factual hallucinations that lead to incorrect reasoning. We acknowledge that there is further room for factuality improvements, which could in turn improve performance. \n\nIn the revised paper, we added additional discussion of this, both as a limitation in Section 6 Paragraph 3 and a new paragraph in Appendix D.2. We leave performance and factuality improvements for future work (and indeed, Ye and Durret 2022 actually show that such explanations, even if unreliable, can be used calibrate model predictions and hence improve performance). \n\n> The evaluations are done in mostly relatively limited domains (e.g., mathword problem)\n\nThanks for this comment. The paper evaluated chain of thought prompting on five math word problem benchmarks. However, it also includes experiments on two symbolic reasoning datasets (last letter concatenation, coin flip), as well as five commonsense reasoning benchmarks (CommonsenseQA, StrategyQA, Date Understanding, and Sports Understanding, and SayCan). This covers a broad range of reasoning tasks. We leave evaluation on other types of tasks (e.g., traditional NLP tasks such as machine translation) for future work. We have further clarified this in Appendix A.3. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "MnisCno20Ha",
                "writer": "author",
                "reply_to": "w8sX2wPYe0",
                "title": "Response to review TQF2",
                "comment": " Thank you for the detailed review! Our paper evaluated a simple method for improving the reasoning abilities of language models, and we are glad you found it well-motivated and thorough. Based on your suggestions, we have conducted new experiments and added them to the revised paper.\n\n> Although the performance improvements by CoT is impressive when the LMs are big, with models smaller than 175B, CoT prompting does not provide significant improvements from standard prompting. Many groups cannot afford 175B or larger models, which may make the applicability and usefulness of CoT limited. \n\nThank you for noting that CoT prompting is an \u201cemergent ability\u201d of scaling. Doing experiments on the GPT-3 API, though publicly available, costs money. For this reason, we further added evaluation on Codex (https://openai.com/blog/openai-codex/), which is currently free to use via the OpenAI API. \n\nSimilar to the three models already in the paper, chain of thought prompting substantially outperforms the baseline when using Codex. We added these results to the revised paper (Appendix B, Tables 1-4) and summarize them here:\n\n| Codex Results                     | GSM8K | SVAMP | ASDiv | AQuA | MAWPS | Avg.         |\n|-----------------------------------|-------|-------|-------|------|-------|--------------|\n| Standard prompting                | 19.7  | 69.9  | 74.0  | 29.5 | 78.7  | 54.4         |\n| Chain of thought prompting        | 63.1  | 76.4  | 80.4  | 45.3 | 92.6  | 71.6 **(+17.2)** |\n| Chain of thought prompting + calc | 65.4  | 77.0  | 80.0  | 45.3 | 93.3  | 72.2 **(+17.9)** |\n\n| Codex Results              | CSQA | StrategyQA | Date | Sports | SayCan | Avg.        |\n|----------------------------|------|------------|------|--------|--------|-------------|\n| Standard prompting         | 82.3 | 67.1       | 49.0 | 71.7   | 85.8   | 71.2        |\n| Chain of thought prompting | 77.9 | 73.2       | 64.8 | 98.5   | 88.3   | 80.5 **(+9.3)** |\n\nWe really appreciate this suggestion, because the Codex results are comparable to or better than the 540B model we evaluate. Notably, Codex does well on both arithmetic and commonsense reasoning tasks. As Codex is free, anyone can reproduce this result.\n\n> All of the models \u2026 are only partially publically available (GPT-3 APIs) or completely publically unavailable (LM1 and LM2), which makes it difficult to verify the experimental results. Is it possible to test CoT with the publically available models with the same scale (e.g., Meta OPT)?\n\nIn addition to the three large language models we tested in the paper and the Codex result above, we further did an additional evaluation using UL2 20B (https://arxiv.org/abs/2205.05131). \n\nUL2 is a new, completely open-source model that combines various language learning objectives. We chose UL2 because it has similar or better performance than GPT-3 and Meta OPT on zero-shot SuperGLUE, and it is only 20B parameters (compared to 175B parameters in OPT). Thus, the publicly-available UL2 is more compute-friendly than OPT, in addition to being higher-performance. \n\nWe added these results to the paper (Appendix B, Tables 1-4), and also show them in the below tables. For nine out of ten arithmetic/commonsense tasks, CoT prompting improves performance compared to standard prompting. The UL2 results are weaker in raw performance than the 540B and InstructGPT models (see Tables 1\u20135), but they are comparable to LM-1 68B, which is good considering that UL2 is only 20B. With an external calculator (implementation described in Appendix B), the average gain on arithmetic reasoning jumps to (13.7%), which suggests that many of UL2\u2019s arithmetic errors are from calculations instead of reasoning (potentially because C4 is a noisy pre-training dataset). Note that calculator is not applicable for standard prompting since there are no intermediate reasoning paths.\n\n\n| UL2 Results                       | GSM8K | SVAMP | ASDiv | AQuA | MAWPS | Avg           |\n|-----------------------------------|-------|-------|-------|------|-------|---------------|\n| Standard prompting                | 4.1   | 10.1  | 16.0  | 20.5 | 16.6  | 13.5          |\n| Chain of thought prompting        | 4.4   | 12.5  | 16.9  | 23.6 | 19.1  | 15.3 **(+1.8%)**  |\n| Chain of thought prompting + calc | 6.9   | 28.3  | 34.3  | 23.6 | 42.7  | 27.2 **(+13.7%)** |\n\n| UL2 Results                | CSQA | StrategyQA | Date | Sports | SayCan | Avg          |\n|----------------------------|------|------------|------|--------|--------|--------------|\n| Standard prompting         | 34.2 | 59.0       | 13.5 | 57.9   | 20.0   | 36.9         |\n| Chain of thought prompting | 51.4 | 53.3       | 14.0 | 65.3   | 41.7   | 45.1 **(+8.2%)** |\n\nOne final note is while CoT currently works best with large models, there is growing research making language models more computationally efficient (Chinchilla), and more accessible (BLOOM). Hence, such methods may gain even further traction and are forward-looking.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0G7K0T-FrqL",
                "writer": "author",
                "reply_to": "EOkXV8TeeZ",
                "title": "Response to review nMs2",
                "comment": " Thank you for the insightful review! Our paper explored a simple approach for improving reasoning in language models, and we are happy that your review found the improvements substantial and the analyses comprehensive (and that you liked the FAQ section). Below we answer the questions and describe how we have revised the paper. \n\n> When annotator A, B, and C wrote chain of thought, what\u2019re their strategies?\n\nThanks for asking about this detail. The annotators were not given specific instructions other than to simply write the step-by-step reasoning that led to the final answer. Thus, the annotations had each annotator\u2019s own linguistic \u201cchain of thought\u201d writing style. This chain of thought annotation strategy is similar to the one in Cobbe et al., 2021. To make this more clear, in the revised paper we added in this detail in Appendix A.2.\n\n> The paper mentioned that one contribution of the proposed model is facilitating interpretation and debugging of the few-shot learning models. Can the authors add some discussion on how to perform the debug?\n\nThank you for this question. A simple example of analyzing incorrect chains of thought could be to see what percent of errors involved calculation errors, where the language model correctly reasoned about the problem but made a simple calculator error (e.g., 6 * 13 = 68). This is described in detail in Appendix D.2. Based on manual analysis of reading chain of thought generations, we found that 8% of a subset of 50 errors from LM-1 137B on GSM8K involved such calculator errors. Accordingly with this observation, we implemented an ad-hoc calculator that uses python to parse and correct such calculation errors. This improved the accuracy of LM-1 by several points from 14.3% to 17.3%. \n\n> For example, the paper mentioned that spurious paths (wrong reasoning steps leading to correct answers) seem to be less of a concern in some of the datasets used in this paper, but that problem could be more serious in other complicated reasoning tasks. One may think that the reasoning problems that need more expressive/powerful model for commonsense may put challenges to the proposed approach.\n\nThank you for this insightful observation. Indeed, not all model-generated reasoning paths are 100% perfect, and wrong reasoning can lead to both wrong answers and accidentally correct answers (which are both problematic). This is a general problem not just for the proposed method, but also more broadly in language model generation and neuro-symbolic reasoning (Maynez et al., 2021; Rashkin et al., 2021; Chen et al., 2019, and others). In the revised paper, we added this discussion into limitations (Section 6 paragraph 3), as well as in a new paragraph in Appendix D.2 (page 27). Overall, improving the factuality of generated reasoning paths is an important open direction for future research. For instance, training a re-ranker to score many randomly-sampled generations is a potential technique for improving the quality of language model generations (Shen et al., 2021; Thoppilan et al., 2021; inter alia).\n\n> In terms of generalizability, the paper claims that the proposed approach is applicable in principle to any tasks that humans can solve via language. This is not well supported in the paper and may need further justification. (The contributions of the paper, though, are not dependent on that claim.)\n\nThank you for pointing this out (and noting that the paper\u2019s contributions are not dependent on this claim). While the paper focused on 12 reasoning benchmarks, it is hard to empirically show that chain of thought prompting works for all language tasks (hence why it is caveated with \u201cin principle\u201d). We have softened the language in the revised paper (added the word \u201cpotentially\u201d in addition to \u201cin principle\u201d in Sections 1, 2, and 7). In addition, we added an additional paragraph in A.3 that explicitly says that further empirical experiments are needed to evaluate chain of thought prompting on non-reasoning tasks (e.g., machine translation), and that we leave it for future work. Please let us know if you have any suggestions for further clarifying this!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-kJ1LgLVyGp",
                "writer": "author",
                "reply_to": "61aKx0W4wkt",
                "title": "Response to review UpwE",
                "comment": " Thank you for the positive review! We aimed to provide a comprehensive empirical evaluation of chain of thought prompting and we are very encouraged that you found the method to be simple and effective. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "w8sX2wPYe0",
                "writer": "official_reviewer",
                "reply_to": "nips_2022__VjQlMeSB_J",
                "title": "",
                "comment": " This paper introduces Chain of Thought (CoT) prompting, which augments few-shot training samples for in-context learning with explicit reasoning steps. They evaluate the effectiveness of CoT on math word problems, commonsense reasoning and symbolic reasoning and shows performance improvements from the standard prompting as well as fine-tuned GPT-3 with smaller numner of parameters. In addition, a 540B parameter language model prompted with CoT outperforms fine-tuned 175B GPT-3.  ## Strengths\n- The proposed idea is easy, simple, (potentially) widely-applicable and well-motivated.\n- They show that CoT gives large performance improvements when the LMs are large enough, although performance improvements are somewheat limited when the base LMs are smaller than 100B. \n- Their through analysis shows that (i) in many cases the models' generated explanations are correct, (ii) removing sub part of CoT or changing the answer / CoT order largely degrades models' performance, indicating the necessity of using the reasoning process writte in natural languages, and (iii) CoT is robust to the different expalanation written by different annotations (Author A, B and C). \n\n## Weaknesses\n- Although the performance improvements given by CoT is impressive when the LMs are big enough, with the models smaller than 175B, CoT prompting does not provide sigifincat improvements from the standard prompting. Many groups cannot afford to use 175B or larger models, which may make the aplicability and usefulness of CoT limited. Moreover, the evaluations are done in mostly relatively limited domains (e.g., mathword problem). Recent work (Ye and Durret, 2022)* shows that explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations. \n- All of the models used in this papers are only particially publically available (GPT-3 APIs) or completley publically unavailable (LM1 and LM2), which makes it difficult to verify the experimental results and findings in this paper.   \n\n======= \n[2022/08/08] I updated my score given the new results and dicussions with the authors. \n\n*Ye, Xi, and Greg Durrett. \"The Unreliability of Explanations in Few-Shot In-Context Learning.\" arXiv preprint arXiv:2205.03401 (2022). - Is it possible to test CoT with the publiclly available models with the same scale (e.g., Meta OPT)?  The authors mention several limitations of CoT (e.g., writing CoT prompts). ",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "EOkXV8TeeZ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022__VjQlMeSB_J",
                "title": "",
                "comment": " The paper proposes chain-of-thought prompts for in-context few-shot learning. The authors found that with properly designed prompts, which are decomposed reasoning sub-procedures, large language models can achieve substantially higher performances on the arithmetic reasoning, commonsense reasoning, and symbolic reasoning tasks that are tested in the experiments. For many tasks studied in this paper, the performance exceeds that of the supervised finetuning methods.  Analyses show that the chain-of-thought prompts significantly improve few-shot performance compared to the standard prompts. The performance of chain-of-thought prompts has been shown to be robust in different perspectives (e.g., different annotators and various language models).  Strength:\n\n-\tThe proposed model that incorporates chain of thought into prompts achieved substantial and consistent improvements on different tasks with different language models. Although the originality of the paper is a bit of concern, I do think these results are valuable to the community.\n-\tThe paper performs comprehensive and detailed analysis to help establish the claims and draw the conclusions with solid evidence and justification. The detailed analyses provide insights and interesting discussions on the property of the model, e.g., the property with regard to language model sizes and complexities of the target problems, which may invite more studies along the line (although unfortunately, many of such studies may only be conducted in a limited number of organizations).\n-\tThe paper is clearly written and I like the detailed discussions in the FAQ session, which further addressed my questions.\n\nWeakness:\n\n-\tThe originality and novelty, particularly from the methodology perspective, is limited. (However, due the above strengths, I would think the paper\u2019s overall contributions outweighs this limitation.)\n-\tIn terms of generalizability, the paper claims that the proposed approach is applicable in principle to any tasks that humans can solve via language. This is not well supported in the paper and may need further justification. (The contributions of the paper, though, are not dependent on that claim.). For example, the paper mentioned that spurious paths (wrong reasoning steps leading to correct answers) seem to be less of a concern in some of the datasets used in this paper, but that problem could be more serious in other complicated reasoning tasks. One may think that the reasoning problems that need more expressive/powerful model for commonsense may put challenges to the proposed approach.\n -\tWhen annotator A, B, and C wrote chain of thought, what\u2019re their strategies? \n\n-\tThe paper mentioned that one contribution of the proposed model is facilitating interpretation and debugging of the few-shot learning models. Can the authors add some discussion on how to perform the debug?\n\n please refer to the weakness comments.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "61aKx0W4wkt",
                "writer": "official_reviewer",
                "reply_to": "nips_2022__VjQlMeSB_J",
                "title": "",
                "comment": " This paper proposed chain of thought prompting, which can significantly improve the ability of large language models to perform complex reasoning. The proposed method is simple and effective, and also can generalize to various arithmetic, commonsense, and symbolic reasoning tasks.  The paper is well motivated -- scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning.\n\nThe proposed method \u2013 the chain of thought prompting \u2013 is novel, simple, and effective. The prompts help large language models to generate intermediate natural language reasoning steps that lead to the final output. \n\nThe experiments demonstrated the chain of thought prompting could generalize to various arithmetic, commonsense, and symbolic reasoning tasks.\n\nOverall, there is no apparent weaknesses found in the paper. I champion for accepting this paper.\n n/a n/a",
                "rating": 9,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "solid work",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            }
        ]
    },
    "INBO6h9gtG": {
        "paper_id": "nips_2021_INBO6h9gtG",
        "paper_title": "Covariance-Aware Private Mean Estimation Without Private Covariance Estimation",
        "paper_abstract": "Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia Zakynthinou",
        "paper_acceptance": "accept",
        "meta_review": "All reviewers agree that this paper provides a non-trivial advancement for the important problem of differentially private mean estimation. Reviewers 6nea, 8L9y, and ubCi found the proof techniques and algorithmic strategies insightful and novel. Reviewer ubCi found the assumptions underlying this paper to be significantly more realistic than prior work.  The only complaint about the paper is that the proposed algorithms have exponential running time, but reviewers 8L9y and ubCi feel that these algorithms could be the starting point for more practical algorithms. I therefore recommend that this paper be accepted.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "PwC4lwvUDr",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_INBO6h9gtG",
                "title": "",
                "comment": "This paper investigates mean estimation of high dimensional Gaussian (and sub-Gaussian) distributions under the constraint of differential privacy (DP). More specifically, the authors aim at privately estimating the mean of a high-dimensional Gaussian in the Mahalanobis distance (w.r.t. to the covariance matrix). \n\nOne simple approach for this task is to first privately estimating the covariance matrix (say up to a constant factor in spectral distance) after which the problem has known solutions. Unfortunately, this approach is suboptimal as estimating the covariance matrix is harder task. Thus, the authors provide a way to circumvent the issue and construct simple estimators whose sample complexity scales only linearly with the dimension.\n\nThe first algorithm is based on combination of the propose-test-release (PTR) framework, exponential mechanism, and the concept of Tukey depth. Roughly speaking, the authors show that by sampling from the distribution defined by the exponential mechanism with a score function based on the Tukey depth restricted to a data dependent set of possible outputs (some points with high Tukey depth), one can get an accurate and private algorithm for mean estimation. In order to maintain privacy while restricting the output to be a data-dependent output set, the authors make use of the PTR framework to test whether the specific dataset is ``safe\u2019\u2019 for use. They then show that when sampled from Gaussian data, a dataset will be safe with high probability, proving that the algorithm will often output an estimate of the mean. Finally, upon success the output will w.h.p. be a point of high Tukey depth, so the estimator is also accurate. \n\nThe second algorithm relaxes the requirement of strict Gaussian data to sub-Gaussian data. This algorithm is also based on PTR framework, with a clever application of \"skewed\" Gaussian noise addition. The standard Gaussian mechanism used in DP adds Gaussian noise scaled equally in all directions. The authors propose to use an additive Gaussian mechanism (to the empirical mean) that adds noise proportional to the empirical covariance of the data instead. Intuitively, this preserves the scale of the data (and thus preserves accuracy w.r.t. to the Mahalanobis distance). While this is not private for worst case datasets, the authors circumvent this by using the PTR to first test whether the dataset is ``good\u2019\u2019 (appropriately defined), and if need be, project the dataset to the set of good datasets before running the algorithm. Finally, they show that with high probability, sub-Gaussian data will be good and there will be no projection, thus the algorithm will output the empirical mean (perturbed with skewed Gaussian noise), that will be accurate w.r.t. the Mahalanobis distance. \n  The authors have applied techniques such as the exponential mechanism, propose test release framework, and additive Gaussian noise in very clever ways to prove new results for a fundamental problem in DP statistics. The arguments about the privacy and accuracy of the method are sound. The paper is well written and the high level discussions helps with understanding the big picture.\n \nBefore this paper, it was not known whether it is possible to estimate the mean of a Gaussian/sub-Gaussan distribution w.r.t. the Mahalanobis distance under the constraint of DP without first approximating the covariance. Non-privately, the empirical mean works well for this task. Thus, this paper is the first to prove that it is possible to achieve the same in the private case.\n\nWhile the methods proposed in this paper are not computationally efficient, the approach is of broad interest to theoreticians and can be a starting point for the design of more practical approaches.\n\n\n-----------\nupdate after rebuttal: the authors did not specifically respond to my review and I am happy to keep the score after checking out the other reviews/responses. not applicable.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "MZ_LV_-Dv1f",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_INBO6h9gtG",
                "title": "",
                "comment": "The paper studies the problem of private mean/median estimation for Guassian and subGaussian variables. In order to reduce the noise required to satisfy privacy, the algorithms tailor the noise to the (unknown) covariance of the distribution, without directly estimating the covariance matrix. This is an important distinction from some prior methods, as privately estimating the covariance matrix is expensive. The methods used combine techniques including the exponential mechanism with Tukey depth, the Gaussian mechanism, and propose-test-release(PTR). While the authors mention that the privacy analysis uses standard techniques, they give a rigorous proof of the utility of the mechanism which they mention uses novel techniques. \n  Originality:\nMean estimation is one of the oldest problems in DP, and there have been several works developing techniques to improve the utility of mean estimation in DP. One of the key distinctions of this paper is that the noise is scaled according to the (unknown) covariance matrix, without privately releasing the estimated covariance function. As the authors mention, this greatly improves the sample complexity. It also offers much more realistic assumptions on the data generating distribution, rather than requiring tight apriori bounds on the covariance matrix, such as in Karwa & Vadhan. \n\nThe proposed mechanism builds upon several existing DP techniques, including the exponential mechanism with Tukey depth, the Gaussian mechanism, and propose-test-release(PTR). Still, these techniques are combined in a novel manner. More importantly, the utility analysis is novel and this is where the main contributions of the paper are. \n\nQuality:\nThe theoretical results in the paper support both the privacy and utility of the proposed mechanisms. \n\nThe authors suggest that the proposed mechanism is primarily of theoretical interest, as it has exponential running time in both the dimension and sample size. The lack of a practical algorithm limits the work, making it seem somewhat incomplete. \n\nFurthermore, the paper does not have any empirical evaluations of the mechanism. The inclusion of simulations would help to demonstrate the utility of the proposed mechanism compared to prior approaches. \n\nSignificance:\nFrom a theoretical standpoint, the paper provides a novel and important result demonstrating that mean estimation can be achieved privately with much improved utility compared to previous results. Furthermore, the results and techniques could provide building blocks for future researchers to design efficient DP algorithms for other problems. Or, potentially future researchers could modify the techniques of this paper to produce a computationally efficient mechanism with similar utility.\n\nAs mentioned earlier, the lack of a computationally efficient algorithm limits the usefulness of this paper. As the authors mentioned, the proposed algorithm has running time exponential in both n and d, making it entirely impractical for the analysis of real datasets. As such, the results of the paper are currently only of interest to other privacy experts rather than to practitioners looking to implement DP methods. \n\nAnother limitation of the result is that it is framed in approximate-DP. Approximate-DP has been criticized since it allows for a catastrophic failure of privacy with a small probability (approx  delta). Due to this weakness, there has been increasing interest in Renyi-divergence based DP frameworks, which also allow for a relaxation of pure DP, but protect against the catastrophic failure that approx-DP allows. Since the authors are using the PTR technique for their results, I suspect that their results cannot be analyzed using Renyi-DP. \n\nClarity:\nThe authors do a good job of communicating the prior approaches to the problem, and the sample complexity & assumptions of the previous methods. They effectively communicate how their proposed method improves over the prior work in terms of both sample complexity and more realistic assumptions. \n\nThe Tukey depth is introduced in equation (4), but little intuition is given about why it is a useful score function. Some exposition discussing the intuition of Tukey depth, both why it is useful to estimate the location as well as why it is useful in the exponential mechanism would be helpful. \n\nIn Theorem 3.2, it is not clear what is meant by the superscript $\\otimes n$ means.\n\nIn line 303, it is not clear what is meant by Y_{t,x}. I gathered later that this may be the support of M_{epsilon,t}(x), but this should be clearly defined.\n\nConclusion:\nThe paper proposes new DP mean estimation algorithms that improve the sample complexity over previous techniques, and have less stringent assumptions. However, the algorithm proposed in the paper is not computationally practical, limiting the results to only theoretical interest. Nevertheless, the methods of the paper could be of interest to the DP community, and could be potentially used to develop new DP algorithms with high utility. Because I believe that this paper would be of interest to the DP community, I rate it 7 \"good paper, accept\".\n\nUpdate: After reading the other reviews and the author feedback, my opinion of the paper is unchanged. My score remains 7.\n No issues.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "CROUCPP6Z59",
                "writer": "author",
                "reply_to": "MZ_LV_-Dv1f",
                "title": "Response",
                "comment": " We thank all Reviewers for their thoughtful reviews and Reviewer ubCi for the detailed comments on notation and suggestions on the presentation of our paper, which we will address in preparing future versions.\n\nWe agree that important future directions include extending this work to algorithms that are provably accurate under milder distributional assumptions, satisfy stronger privacy guarantees (such as RDP), and are computationally efficient. We note that although extending to RDP is an interesting direction, it would necessarily require a priori bounds on the parameters which would make the two results incomparable. In both algorithms, the preprocessing steps, that is, the \u201csafety\u201d check in the Tukey Depth mechanism and the projection to \u201cgoodness\u201d in the Empirically Rescaled Gaussian mechanism, are the main obstacles to making these mechanisms computationally efficient. \n\nClarifications:\n- $Y_{t,x}$ is indeed the support of $M_{\\epsilon,t}(x)$, i.e., all points with Tukey depth at least t with respect to dataset x. \n- The superscript $\\otimes n$ on a distribution $P$, as used in Theorem 3.2, means that the random variable consists of $n$ i.i.d. draws from $P$.\n- Tukey Depth: We will provide more explanation of why the Tukey depth is useful (both in classical/non-private settings and in prior work that uses it as part of the exponential mechanism). We had omitted that discussion due to space constraints. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PBRZRVmZ1L",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_INBO6h9gtG",
                "title": "",
                "comment": "This paper considered the DP mean estimation problem with unknown covariance under Mahalanobis norm. In the known covariance setting, previous work can estimate gaussian mean near optimally. But in the unknown covariance setting, previous algorithms either need to pay an additional conditional number factor, or require d^3/2 samples to estimate the covariance which are both unsatisfactory. The key difficulty of the problem is getting Mahalanobis norm guarantee without first estimating the covariance privately. \n\nThe authors present two different (exponential time) algorithms for Gaussian and sub-Gaussian settings. The gaussian algorithm is based on an exponential mechanism with Tukey depth score. Although this approach has been used in [45] before, they need to choose the sampling region adaptively based on the data in order to achieve a good Mahalanobis guarantee. This brings additional difficulty in making the algorithm private, and they use the classical propose-test-release framework to obtain privacy guarantees. The second estimator uses the empirically rescaled Gaussian mechanism, where the gaussian noise is scaled by the empirical covariance such that Mahalanobis guarantee is naturally achieved. This estimator can be applied more generally on sub-gaussian distribution, though the sample complexity has a d/\\alpha\\eps^2 instead of the optimal d/\\alpha\\eps. \n  In summary, this paper answered a fundamental question in DP and brings interesting new ideas along the way. I will be interested in the authors\u2019 thoughts on the more efficient algorithms on this problem. \n Yes",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "G7rxjnadv4M",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_INBO6h9gtG",
                "title": "",
                "comment": "The authors considered the problem of estimating the mean of d-dimensional Gaussian and sub-Gaussian distribution under DP constraint. The error bounds for the mean estimate is given in terms of the Mahalanobis distance, however, interestingly enough, the proposed method does not estimate the covariance matrix. The tool proposed by the authors is a sampling mechanism which is based on Tukey depth: the probability of choosing a point is exponentially proportional to the negative Tukey depth. Then one can run a mean estimator on the sampled data. This alone is not enough to accomplish DP, therefore the authors make use of a propose-test-release framework which is basically checks whether the sampled data contains too many points for which the Tukey depth is too small. The proposed methods of order D^{2/3} and in addition, it does not assume anything about the covariance matrix, and does not need a prior estimate for the covariance matrix as previous methods required. The sub-gaussian case basically find the closest gaussian model and applies the same mechanism.\n  I am on the positive side with this paper pretty much. It is really dense. Based on the main paper, one can have only a high level impression on how complex the technical details are. Nevertheless, the appendix is well-structured, and self-contained ( however I have not read all 42 pages of Appendix yet). The basic idea of the paper is very nice and novel as far as I know. \n There is a dedicated paragraph to address the societal impact.",
                "rating": 8,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "this paper provides a non-trivial advancement for the important problem of differentially private mean estimation",
                "Sentiment Expression": "All reviewers agree",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the proof techniques and algorithmic strategies",
                "Sentiment Expression": "insightful and novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the assumptions underlying this paper",
                "Sentiment Expression": "significantly more realistic than prior work",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the proposed algorithms have exponential running time",
                "Sentiment Expression": "The only complaint",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "these algorithms could be the starting point for more practical algorithms",
                "Sentiment Expression": "feel",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "H1ldzA4tPr": {
        "paper_id": "iclr_2020_H1ldzA4tPr",
        "paper_title": "Learning Compositional Koopman Operators for Model-Based Control",
        "paper_abstract": "Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. The learned dynamics can quickly adapt to new environments of unknown physical parameters and produce control signals to achieve a specified goal. Our experiments on manipulating ropes and controlling soft robots show that the proposed method has better efficiency and generalization ability than existing baselines.",
        "paper_acceptance": "accept-spotlight",
        "meta_review": "This paper proposes using object-centered graph neural network embeddings of a dynamical system as approximate Koopman embeddings, and then learning the linear transition matrix to model the dynamics of the system according to the Koopman operator theory. The authors propose adding an inductive bias (a block diagonal structure of the transition matrix with shared components) to limit the number of parameters necessary to learn, which improves the computational efficiency and generalisation of the proposed approach. The authors also propose adding an additional input component that allows for external control of the dynamics of the system. The reviewers initially had concerns about the experimental section, since the approach was only tested on toy domains. The reviewers also asked for more baselines. The authors were able to answer some of the questions raised during the discussion period, and by the end of it all reviewers agreed that this is a solid and novel piece of work that deserves to be accepted. For this reason I recommend acceptance.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "HJlw9pVtir",
                "reply_to": "H1lOXuJacS",
                "title": "Response to Reviewer #3",
                "comment": "Thank you for your thoughtful and constructive comments. \n\n1. Real-world experiments\n\nWe agree that showing real-world experiments would be beneficial. As a first step, we are starting with synthetic environments, which allow us to systematically evaluate and ablate on our model to fully understand its capability. We will explore ways to extend our model to the real world in the future.\n\n2. Comparisons\n\nIn our experiment, we make our environments as close to those used in the related works as possible. For example, our Rope environment is very similar to \u201cstring\u201d in [Battaglia 2016]. We cannot directly use the same environment, because of the different problem setups: we study modeling and controlling a system with unknown physical parameters, while they focus on predicting future physical states given physical parameters.\n\n3. Platform\n\nWe also agree that building a platform such as the OpenAI gym that can benchmark different methods in the same environments would be valuable for the whole community. It can help to ensure that we are making concrete progress. As establishing such a benchmark has been beyond the focus of this submission, we leave it as future work.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1Mdmj9hoS",
                "reply_to": "H1lxiTWjir",
                "title": "Thank you for your suggestions.",
                "comment": "Thanks again for your suggestions, which have made the paper much stronger. We are glad to see that you find the paper now looks better. We\u2019ll look into these additional baselines and hope to include them in the revised paper.\n\nThanks!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1eClp0acr",
                "reply_to": "iclr_2020_H1ldzA4tPr",
                "title": "Official Blind Review #4",
                "comment": "The paper proposes a novel method for modelling dynamical systems over graphs. The main idea investigated by the authors is to combine Graph Neural Networks together with approximate Koopman embedding. The GNN encodes the input graph to what the authors call \"object-centric embedding\", whose concatenation over all objects is defacto the approximate Koopman embedding of the system.\nOne of the key contributions is the reduction in parameters, by assuming that the interactions between different objects in the Koopman space are limited to some fixed number of types, or in other words given the object-centric embedding the Koopman matrix is a block matrix, where each block can only be one of K matrices. In this way the number of parameters is fixed and does not scale with the number of objects, compared to the naive way where it will scale as N^2. In addition to the dynamical modelling the paper adds an extra linear-\"control\" input in the Koopman embedding space which to affect the dynamics of the system and allow for modelling systems where there is external control being applied. The models are than compared on three small scale tasks, showing better results in mean squared error prediction compared to the three baseline approaches. Additionally, when used for controls on the environments the methods outperforms the one baseline method it is compared to. \n\n\nI'm quite borderline on whether the paper should be accepted or rejected, but currently I'm leaning towards a rejection. The main reason for this decision is that in my opinion the experiments presented are somewhat limited with respect to the baselines used and I have some reservations regarding the results presented for IN and PN discussed below.\n\n\nDetailed comments on paper: \n\n1. I personally like the main idea of the paper, which is to use previous results from approximating the Koopman operator and combining it with GNNs for more accurate physical modelling of object-object interactions. Additionally, the idea of reducing the parameters is quite important. \n\n2. Linear control theory - although it is quite natural to add the control as a linear affect in the latent space, and this has been done numerous times before in the literature, I don't recall there to be any theory on Koopman embedding when there is a control signal. Additionally, if the policy that has been used in practice is stochastic, the resulting \"induced\" dynamical system then also becomes stochastic, and to my knowledge, at least in theory, learning Koopman embedding for such systems has more challenges are requires certain assumptions about the true system, such as co-diagonalization and few others. I think this should be discussed in more detailed by the authors (and please do correct me if I'm wrong on any of these statements) as currently for the readers who are not too familiar I think the text might come across as though that the Koopman theory extends naively to these scenarios as well, which I do not think is the case.\n\n3. It is not very clear how does the \"metric\" loss affects the solution. I would encourage the authors to provide comparison (only in terms of dynamical modelling, without active control) of whether this metric helps, or have some negative effects on the prediction. I think that for instance if the GNN has some form of weight regularization than this indeed would have some non-trivial effect on the resulting representation. Also, it would useful to have plots of how accurately does the embedding preserve the distance to the true states in order to understand this better.\n\n\n\nComments on the experiments:\n\n1. In the paper there is no discussion about what are the actual observation space of the environments, could these be clarified better.\n\n2. The block diagonal structure approach in general has been presented as working with multiple types of interactions. However, in practice it seems that the authors have only used two types of interaction -> object-same-object and object-other-object interaction. This however, has never been discussed and is maybe false. Could you clarify these details?\n\n3. The results showed in Figure 3 are somewhat in contrast than the results in the original PN paper, specifically the PN paper states that it can achieve MSE of 7.85 for 1000 time steps, and from figure 6 of that paper it shows about 0.05 MSE over 100 steps on a similar rope environment. These results compared to the one presented here in Figure 3 makes me wonder how well did the authors actually managed to reimperilment the IN and PN paper? Could there be any comments on this as this makes many claims of the proposed method being better questionable and hard to understand its significance in relation to previous work. \n\n4. For the control tasks, it would have been useful to have more than just the single baseline used. There are plenty of algorithms for Reinforcement Learning that could have been used in order to put the method in perspective. E.g. one can apply MPC with ground-truth model (e.g. the simulator) to show the discrepancy with an ideal case. In the RL literature there are plenty of methods for solving smaller problems, parametric and non-parametric: Q-learning, PPO etc... I think this is very important from the reader perspective. \n\n\nPS: Please refer to the discussion below with the authors as I have increased my score from 3 to 6.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "H1lxiTWjir",
                "reply_to": "r1gn76NYoB",
                "title": "Response to rebuttal ",
                "comment": "Firstly, thanks a lot for addressing many of the details I requested in my original review. I've read the updated version and I think the paper looks much better with these included. \n\nFor me the weakest point of the paper still remains the fact that there does not seem to be enough baseline comparisons to other methods. This includes both in the dynamical modelling (e.g. there is plenty of literature on Deep Kalman Filters [1], SVAE [2] and follow up on those) and comparison to more standard RL algorithms for control. Of course, as this being research work I might be being a bit too harsh here. \n\nNevertheless, because of the above reason I will increase my score to 6 rather than 8. \n\n[1] Rahul G. Krishnan, Uri Shalit, David Sontag, \"Deep Kalman Filter\"\n[2] Matthew J. Johnson, David Duvenaud, Alexander B. Wiltschko, Sandeep R. Datta, Ryan P. Adams - \"Composing graphical models with neural networks for structured representations and fast inference\"\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HklsJA4tsS",
                "reply_to": "SklAzVndYH",
                "title": "Response to Reviewer #1",
                "comment": "Thank you for your thoughtful and constructive comments!\n\n1. Extending the approach to objects with different properties.\n\nOur current model can naturally handle objects with different properties by treating interactions having different physical properties as different interactions. For example, our model will treat springs with different stiffness as different interactions. It would be an interesting future direction to exploring formulations that can further reduce the parameter by grouping similar interactions. For example, the matrix blocks representing spring relations with different stiffness can share a similar structure. \n\n2. Details on the training procedure.\n \nOur model is trained end-to-end. In every optimization step, we first use least-square regression to fit the Koopman matrix K and control matrix L, which are then used to calculate the loss functions. We then update the parameters in the graph encoder and decoder by backpropagating the gradient from the loss. \n\n3. Details on the MPC procedure.\n\nIn our MPC process, we re-evaluate the control after 32 steps. It is mainly due to the trade-off between the time used for control synthesis and the accuracy of the control result. In our three environments, our learned model is accurate enough that doing open-loop control gives reasonably accurate results. The benefits of replanning more frequently than once every 32 steps are marginal. \n\n4. Computational time used for online adaptation.\n\nThe statement in the introduction that \u201cour model is 20 times faster when adapting to new environments of unknown physical parameters\u201d is referring to the time used to do the online adaptation. The evaluation is performed in the Rope environment. Our model takes 0.43 +/- 0.11 seconds to identify the transition matrices using least-square regression while the IN/PN model needs 8.9 +/- 1.2 seconds for running 500 iterations of gradient descent using a learning rate of 1e-5. The statistics reported here are computed over 100 trials.\n\nPlease let us know for any additional questions. Thanks!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bke42TNtor",
                "reply_to": "H1gWd_J4cS",
                "title": "Response to Reviewer #2",
                "comment": "Thank you for your thoughtful and encouraging comments! We are glad that you find our work novel, interesting and inspiring. We also appreciate that you find our experiments well designed and the result convincing.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1gn76NYoB",
                "reply_to": "r1eClp0acr",
                "title": "Response to Reviewer #4",
                "comment": "Thank you very much for your constructive and thoughtful comments,  and we would like to address your concerns as follows:\n\n1. Ablation study on the effect of the metric loss.\n\nWe have revised the paper and shown in Appendix B that the simulation performance is comparable with or without a metric loss. However, including metric loss effectively preserves the distance of the original state in the embedding space, which is a desirable character for control synthesis.\n\nOur objective function for control is defined in the embedding space (please see Section 3.4). The derived control signals aim at minimizing the L2 distance to the target embedding. Only when the distance is better preserved can minimizing the distance in the embedding space effectively minimizes the distance between states.\n\nIn the newly added Figure 5(c), we show the control results in the Rope environment, which demonstrates that the model trained with metric loss has a better performance.\n\n2. The correctness of the reimplementation of the baselines.\n\nOur setting is different from the settings in the original IN and PN papers since we do not assume we know the physical parameters and their values, e.g., stiffness, mass, gravity, etc. Instead, the parameters are embedded in the transition matrices during the system identification stage (Section 3.3).\n\nTo check the fidelity of our reimplementation of IN/PN, we test our implementation in settings that are similar to the original IN/PN papers where the physical parameters are known. As shown in Appendix B in our revised paper, the simulation errors of our reimplementation are consistent with the errors reported in the original PN paper of around 0.05 at 100 time-steps. IN and PN slightly outperform our method as the internal linear structure limits our model's expressiveness. However, in the real world, we do not always know the physical parameters and their values, which makes our method preferable when adapting to new environments.\n\n3. Observation space.\n\nIn the Rope environment, each mass on the rope is considered as an object. The observation of each mass is its position and velocity, which has a dimension of 4. In total, a rope with N masses has an observation space of dimension 4N.\n\nIn both the Soft and the Swim environments, each quadrilateral is considered as an object. For each quadrilateral, we have access to the positions and velocities of the four corners. Thus for a soft robot containing N quadrilaterals, we have a 4 * 4 * N = 16N dimensional observation.\n\n4. Types of interactions.\n\nIn our experiments, interactions are considered different if the types are different or the objects involved have different physical properties.\n\nIn the Rope environment, the top mass has a fixed height and is considered differently from the other masses. Thus, we have 2 types of object-same-object interactions for the top mass and the non-top masses. In addition, we have 8 object-other-object interactions. The objects on a relation could be either top mass or non-top mass. It is a combination of 4. And the interaction may happen between two nearby masses or two masses that are two-hop away. In total, the number of object-other-object interactions is 4 * 2 = 8.\n\nIn the Soft and the Swim environments, there are three types of quadrilaterals: rigid, soft and actuated. We have three object-same-object interactions correspondingly. For the object-other-object interactions, we add edges between two quadrilaterals only if they are connected by a point or edge. We are using different relation types if the types of the connected objects are different. In total, there are 3 * 3 * 2 = 18 object-other-objects. \n\n5. The connection to linear control theory.\n\nThe original Koopman theory paper did not consider dynamical systems with control. However, the lack of theory does not hamper the practical usage of the Koopman operator in many control tasks. Many previous papers [1, 2] followed the paradigm that adds the control as a linear effect in the latent Koopman space. While the optimality of these controllers has yet to be proved, \u201cthe numerical performance is striking\u201d [1].\n\nThe Koopman theory is developed for the deterministic dynamical system, which is also the focus of this paper. For the stochasticity introduced by the policy, i.e., we are uncertain about the action given the current state, our model can still work as long as the underlying dynamics are deterministic. \n\nWe have revised the paragraph in section 3.1 to include the discussion. \n\nPlease let us know for any additional questions. Thanks!\n\n[1] Steven L. Brunton, Bingni W. Brunton, Joshua L. Proctor, J. Nathan Kutz, \u201cKoopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control,\u201d PloS one 11.2 (2016): e0150171.\n[2] Daniel Bruder, Brent Gillespie, C. David Remy, Ram Vasudevan, \u201cModeling and Control of Soft Robots Using the Koopman Operator and Model Predictive Control,\u201d in RSS 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkghAoVYiS",
                "reply_to": "iclr_2020_H1ldzA4tPr",
                "title": "General Response",
                "comment": "We thank the reviewers for their constructive comments. We have revised the paper to address the concerns on the presentation and included two additional experiments in the appendix as suggested by the reviewers.\n\n1. Ablation study on the effect of the metric loss.\n\n2. Sanity check on the correctness of the reimplementation of the baselines.\n\n3. Observation space and interaction types.\n\nPlease let us know if you have any questions. Thanks again for all the suggestions, which have made this submission stronger.\n\nBest,\nAuthors.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklAzVndYH",
                "reply_to": "iclr_2020_H1ldzA4tPr",
                "title": "Official Blind Review #1",
                "comment": "This paper introduces an approach to learning compositional koopman operators to efficiently model the dynamics of non-linear systems, consisting of an unspecified number of objects with repetitive dynamics.\n\nThe key contribution of this work is the use of a graph neural network that allows the koopman operator to be learned for systems of  multiple objects, and the incorporation of blockwise structure in the koopman gain and control matrices that improves parameter estimation process and is shown to reduce over-fitting.\n\nResults show the proposed approach is effective, although only simple toy problems are examined and controlled. Nevertheless, this is a useful demonstration of learning for soft robot systems, and the idea of using koopman embeddings is likely to be of value to the ICLR community. \n\nThe paper is well written, and I like the idea of incorporating additional structure into the learning process through the expected blockwise structure.\n\nAs I understand it, the proposed approach is able to exploit this natural blockwise structure due to the assumption that the same physical dynamics are followed by each block (although objects can also be labelled as rigid/moving). How would this approach extend to objects with vary different properties (eg. 5 objects with connected with springs of different stiffness)?\n\nCould you provide more details on the training process, is the model trained in an end-to-end fashion, or in parts?\n\nFor the control experiment, the choice was made to re-evaluate the control after 32 steps. Why was this the case? Is this due to the time taken to simulate/ find controls? MPC would typically re-plan faster than this.\n\nOn a related point, the introduction states that the proposed approach is 20 times faster than baselines, but no evidence of this is provided. How was this assessed, and is this at prediction or training time? How long does it take to replan using the proposed model and SQP?",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "H1gWd_J4cS",
                "reply_to": "iclr_2020_H1ldzA4tPr",
                "title": "Official Blind Review #2",
                "comment": "This paper proposes to learn compositional Koopman operators using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects.  The combination of deep Koopman operator with graph neural nets is very novel and interesting. The experiments are also well designed and the results shown in the paper also indicate the effectiveness and efficiency of the algorithm in both simulation and control. In conclusion, I think this work can inspire more wok into modeling larger and more complex systems by integrating the power of the Koopman theory and the expressiveness of neural networks.",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "H1lOXuJacS",
                "reply_to": "iclr_2020_H1ldzA4tPr",
                "title": "Official Blind Review #3",
                "comment": "The paper is well written and the proposed idea is novel and builds on a sound theoretical framework of Koopman operator theory. A physical system is a represented as a graph; graph neural network is used to encode the current state to an object-centric embedding where the dynamics are assumed to be linear (Koopman operator theory) and modeled as a transition matrix. The key contribution is to recognize that similar physical interactions can be modeled using same parameters which constraints the transition matrix to be block-wise with shared parameters. Furthermore, the model is extended to add a control matrix to model external control. Experiments are conducted on simulations as well as control for 3 different settings - a a hanging rope/string anchored at the top, soft robot with an anchor, and soft robot in fluids.\n\nStrengths\n* Proposed method builds on a sound theoretical basis; although the linear dynamics model appear somewhat limited compared for the complex dynamics, thorough experiments are conducted to demonstrate the effectiveness of the method. The efficiency of the proposed algorithm compared to prior work makes is much practically useful.\n* Well written with examples and illustrative figures.\n* Quantitative analysis together with ablation studies on the structure are insightful.\n\nWeaknesses\n* My primary concern are with the evaluation.\n- Experiments are only conducted on synthetic datasets. While experiments on real datasets are understandably difficult especially for quantitative validation, it would help to map the experiments to real problems to gain a more intuitive understanding and thus cater to a broader community.\n- This paper and several related works are all evaluated on different problems; it would be useful to evaluate on similar tasks; for instance, strings [Battaglia 2016]. It would make it easier to draw comparisons.\n- It's clear that the community would benefit significantly by having a benchmark or a web-based evaluation methodology (similar to OpenAI gym used actively reinforcement learning community). Unfortunately, this paper does not seem to offer a solution to this issue but continue to evaluate in ways similar to previous papers.\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the experimental section",
                "Sentiment Expression": "had concerns",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "adding an inductive bias",
                "Sentiment Expression": "improves",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "this is a solid and novel piece of work that deserves to be accepted",
                "Sentiment Expression": "solid and novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The authors were able to answer some of the questions raised during the discussion period",
                "Sentiment Expression": "were able to answer",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "BJl6bANtwH": {
        "paper_id": "iclr_2020_BJl6bANtwH",
        "paper_title": "Detecting Extrapolation with Local Ensembles",
        "paper_abstract": "We present local ensembles, a method for detecting extrapolation at test time in a pre-trained model. We focus on underdetermination as a key component of extrapolation: we aim to detect when many possible predictions are consistent with the training data and model class. Our method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. We compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian, and provide a tractable method for estimating this quantity. Experimentally, we show that our method is capable of detecting when a pre-trained model is extrapolating on test data, with applications to out-of-distribution detection, detecting spurious correlates, and active learning.",
        "paper_acceptance": "accept-poster",
        "meta_review": "This paper presents an ensembling approach to detect underdetermination for extrapolating to test points. The problem domain is interesting and the approach is simple and useful. While reviewers were positive about the work, they raised several points for improvement. The authors are strongly encouraged to include the discussion here in the final version.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "ByeyHIaqtr",
                "reply_to": "iclr_2020_BJl6bANtwH",
                "title": "Official Blind Review #3",
                "comment": "# Summary of contribution\n- The paper provides a novel fast and simple approximation of second-order local parameter sensitivity of neural networks, to estimate a form of uncertainty wrt to a test sample, which is further used and tested as a novelty detector. \n- The method analyzes the most significant eigenvector/eigenvalues of the Hessian (of training loss), and use the compliment of their span to get directions of local perturbations to network parameters that affect training loss little (\"ensemble subspace\"). The novelty score is then based on how much the prediction is influenced by these perturbations.\n- The idea of estimating \"ensemble subspace\" is interesting and computationally effective. Compared to other recent methods that also use second-order gradients for uncertainty, this paper is more generally applicable, and can be faster at test time. The paper demonstrates good performance on both simulated data and real data (CelebA faces with CNN, etc.).\n\n# Decision TL;DR\nI am giving a weak reject. The paper is strong in its idea, formulation, and theory, but is too similar to recent related works which this paper is reluctant to compare to (either in theory, efficiency, or performance). Since the contribution of the paper lies in the efficient approximation of local ensemble methods, readers cannot gauge how beneficial the contribution is compared to other approximations of ensembles.\n\n\n# Pros\n- Novel way to estimate a local neighborhood that affects training loss little (Note: not an expert in this line of research, not sure if it is completely novel) by estimating significant eigenvectors and using their compliment space.\n- The paper is well-written, and relatively easy to understand, despite a few hard-to-follow spots\n- Widely applicable post-hoc to any trained neural networks, and potentially faster training than ensembles / Bayesian approximated ensembles\n- (Theoretical) stability compared to full Hessian inversion\n\n# Cons\nMotivation wrt other papers unclear, and a lack of comparison.\n- Two of the cited papers (Gal & Ghahramani, 2016; Blundell et al., 2015) both work on local ensembles. The former uses MC dropout, the latter estimates a diagonal covariance of a Gaussian distribution of network parameters. These methods are not mentioned in the motivation or related work, which makes it hard to say this paper is well-placed in the literature.\n- The reason that these methods are not compared to is insufficient. The paper only argues that they are not \"post-hoc\" methods. It is very unclear why in any circumstance (or use case) a post-hoc estimation of local ensemble must be (or is preferred to be) used, rather than having network parameters and local neighborhood jointly estimated. If it is for efficiency reasons, the paper does not provide any experimental comparison of the efficiency. Also, it is hard to argue that the \"post-hoc\" nature of this paper makes it so different from the two prior work. For the first prior work, the only time it is not post-hoc is when the original network does not have any dropout layer, and that circumstance is not very common. For the second prior work, one can easily make it post-hoc by training the network first, and estimate the diagonal covariance post-hoc using their loss. \n- The advantage of this paper is that it is more efficient and stable than alternatives, but only  the full hessian inversion is discussed. In particular, it may be necessary to discuss this paper's efficiency against MC-dropout (Gal & Ghahramani, 2016). This method can be done in mini-batches, while the proposed method has to run forward and back-propagation separately for each sample to get g\u03b8*(x'), and it is unclear how well that scales.\n\nEfficiency analysis lacking\n- As discussed above, the proposed method seems to need to back-prop for each test sample separately without using a batch. How much this affects test efficiency is unclear.\n\nExperimental comparison with similar methods missing.\n- The paper would benefit from comparing to the two cited papers (among which MC-dropout is so easy to implement) as well as a full hessian estimation (for toy datasets at least).\n- The paper poses itself as an efficient alternative, so it would be essential to gauge experimentally how fast each method is.\n\nOthers. (not crucial issues) \n- The performance of the paper's main method (LE w/ predictions) underperforms in Table 2, and a variant had to be proposed to make up for the performance drop. This suggests instability of the proposed method wrt new datasets.\n- The claim in contribution \"We identify underdetermination as a key factor in the unreliability of predictions\" is not verified.\n- Inability to scale up to large networks with larger m needed, compared to real ensemble methods or Bayesian networks with Gaussian distributions.\n\n\n# Room for improvement (decreasing order of importance)\n- Improve placement in the literature by discussing when this paper is more useful than prior work (Gal & Ghahramani, 2016; Blundell et al., 2015).\n- Detailed theoretical or experimental analysis of efficiency against alternative approximations of ensembles.\n- Performance comparison to ensemble and local Bayesian methods.\n\n# Editorial issues\n- Figure 3(c) x axis meaning unclear\n- Figure 4 not mentioned in text, and unclear which experiment this refers to \n- Table 2 experiment's loss gradient version is not explained.\n\n\n#############################################################\nPOST REBUTTAL\n#############################################################\n\nTL;DR: The rebuttal addresses some but not all of my concerns. The MC-dropout comparison especially shows the difference between some approximate ensemble methods and ensembles that specifically changes the loss little (this paper). In the end, it is a good paper in terms of theory, although the experiments is lacking in crucial places (no comparison with many existing papers that do attempt to invert hessian) and lack of analysis of claimed efficiency, which I can only hope don\u2019t turn out to be a big deal. I am increasing the score, to marginally above borderline, but please consider the following feedback for the camera-ready version.\n\n> MC Dropout and Bayes by Backprop effectively measure different types of uncertainty from what our method targets, and so they should not be considered competing methods.\nI disagree; by the same logic we can never compare SVM with random forest because they are so different.\n\n> However, as we discussed in 4.1, small eigenvalues make turning this representation into a proper posterior distribution difficult, because these eigenvalues need to be inverted to obtain a covariance matrix, but these inverted eigenvalues can be numerically infinite.\nI agree; but the main difference is the full hessian is hard to implement while there are papers that have implemented the diagonal hessian. It would benefit the paper to compare to that and prove the issue of instability on top of arguing theoretically.\n\n> First, we clarify that our method can in fact be performed using mini-batches of test points (both the forward and backward passes).\nAs far as I know, with mini-batches, the gradients averaged over all samples in the mini-batch are computed for each network parameter. But this method needs the gradient wrt each sample separately.\nIf the authors have some implementation trick that allows computing network parameters\u2019 gradients wrt each sample in the mini-batch separately, please include in the implementation details. Otherwise, please mention this drawback in any \u201cfast\u201d claim.\n\nI will change the score upwards due to the informative rebuttal. Please include much of the discussion in the paper or appendix.\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HyeaUbbior",
                "reply_to": "iclr_2020_BJl6bANtwH",
                "title": "(Brief) General Note - Typo Correction",
                "comment": "The value 0.738 for the MaxProb baseline, M/E task was erroneously bolded in Table 2, due to an error in confidence interval calculation. We have un-bolded it in the new draft. This means that the baseline is *not* in the 95% CI around the LE (Loss) performance (i.e. a significant improvement in performance by our method exists on this task).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1gXvgDqiB",
                "reply_to": "ByeyHIaqtr",
                "title": "Response to Reviewer 3 (part 3)",
                "comment": "> Efficiency compared to MC Dropout and \u201c[MC Dropout] can be done in mini-batches, while the proposed method has to run forward and back-propagation separately for each sample to get g\u03b8*(x'), and it is unclear how well that scales.\u201d\n\nFirst, we clarify that our method can in fact be performed using mini-batches of test points (both the forward and backward passes).\n\nThe reviewer is correct that MC Dropout is probably more computationally efficient than our proposed method, because it only requires stochastic forward passes through the network. However, as we have argued above, MC Dropout does not measure a comparable notion of uncertainty. From this perspective, our point of comparison is fully retrained ensembles, and our method is computationally cheaper.\n\n# Minor Points\n> The performance of the paper's main method (LE w/ predictions) underperforms in Table 2, and a variant had to be proposed to make up for the performance drop. This suggests instability of the proposed method wrt new datasets.\n\nWe agree that the difference in performance on this task is curious, and it does suggest that our method has some failure modes. This proposed task is quite challenging, and may not map neatly to uncertainty due to underdetermination \u2014 indeed, the strong performance of MaxProb here suggests otherwise. Some other possible issues are discussed in Appendix E.4.\n\nThe variant that we propose here is a natural variant more in line with prior work such as influence functions. It is briefly described at the end of section 2, and in more detail in Appendix E.4.\n\n> The claim in contribution \"We identify underdetermination as a key factor in the unreliability of predictions\" is not verified.\n\nWe agree: previous work cited in the introduction identified this factor. We have removed this claim from our contributions. \n\n> Inability to scale up to large networks with larger m needed, compared to real ensemble methods or Bayesian networks with Gaussian distributions.\n\nWe agree this is an issue. Our demonstrations in this paper are very much proofs of concept, and in principle, there is no reason this could not scale to larger networks and larger m. We also note that for many networks, taking m << #parameters may be sufficient, given the highly ill-conditioned hessians in many deep models.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Skx8NgPcsS",
                "reply_to": "ByeyHIaqtr",
                "title": "Response to Reviewer 3 (part 2)",
                "comment": "\n> Advantages of post-hoc methods, and comparison to post-hoc-ness of MC Dropout and Bayes by Backprop.\n\nWe would like to push back a bit on the reviewer\u2019s points about the lack of importance of post-hoc methods. Especially for large models, it is quite common in practice to make use of a pre-trained model even if the dataset is available. Often, the person deploying the model does not have full control over the procedure used to train the model, or would prefer not to retrain a performant model from scratch. In these cases, post-hoc methods are appealing.\n\nWe do agree that if Dropout happens to be used in network training, that MC Dropout is an effective technique for cheaply generating an ensemble of predictors. However, not all neural networks are trained with Dropout and, as we discussed above, MC Dropout is targeting a very different form of uncertainty. In fact, the reliability score generated in this way would be at the mercy of the choices made by the person who trained the model -- the choice of where to include dropout layers, and the dropout probabilities all change the type of uncertainty being estimated. On the other hand, the interpretation of our method does not depend on these choices; it only depends on the learning problem specification, and choices made by the person deploying the model (i.e., the choice of m).\n\nIf we understand correctly, the reviewer also suggests deploying Bayes by Backprop in a post-hoc fashion, by estimating only the diagonal elements of the Hessian and using this as an approximation to the inverse covariance matrix of a posterior distribution. Our method could be seen as a way to fix some of the pathologies of this proposed method. First, we note that this suggestion is quite similar to the Laplace approximation approach discussed in Section 4.1. In fact, the Laplace approximation method is one step more sophisticated than this proposal, because (by the eigendecomposition argument we make) it effectively constructs the basis in which the model Hessian can be represented _exactly_ as a diagonal matrix. However, as we discussed in 4.1, small eigenvalues make turning this representation into a proper posterior distribution difficult, because these eigenvalues need to be inverted to obtain a covariance matrix, but these inverted eigenvalues can be numerically infinite.\n\nOur method effectively patches this approach by noting that is is the alignment of the prediction gradient with these \u201cflat\u201d directions that translates to instability in predictions (see the second-to-last paragraph of 4.1). By settling for measuring the projection of the prediction gradient into the \u201cflat\u201d subspace, we abandon some of the full Bayesian interpretation that could come from the Laplace approximation approach, but retain most of the key information about the underdetermined predictions.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1g3hkD9jB",
                "reply_to": "HylycJP9jB",
                "title": "Response to Reviewer 3 (part 1b)",
                "comment": "For example, variational approximations such as Bayes by Backprop with a variational Gaussian posterior will impose curvature on the loss surface because the approximating distribution is unimodal. Thus, the variational posterior distribution will either artificially introduce curvature to underdetermined dimensions in the parameter space, or it will become nearly improper, and thus numerically unstable. In practice, a user is likely to add strong regularization until evidence of underdetermination has been erased (similarly to fixes to ill-conditioned Hessians discussed in Section 4.1). Likewise, in the case of MC Dropout, the implicit prior distribution that is used to marginalize over weights in the Dropout scheme can result in a posterior distribution that does not represent underdetermination well. In high dimensions, it is often the case the prior distributions will concentrate mass in counterintuitive places, resulting in posterior distributions that do not preserve salient properties of the log-likelihood (i.e., negative loss). In the case of Dropout, there is no guarantee that the posterior distribution will retain the \u201cflat\u201d directions in the loss surface that we are interested in exploring. This is borne out in the experiments cited above.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HylycJP9jB",
                "reply_to": "ByeyHIaqtr",
                "title": "Response to Reviewer 3 (part 1)",
                "comment": "We thank the reviewer for their thorough engagement with our paper. The points raised here helped us better contextualize our work in the existing literature, and motivated a number of experiments that we found to be enlightening. We address specific points below.\n\n# Major points\n\n> \u201c[MC Dropout (Gal & Ghahramani, 2016) and Bayes by Backprop (Blundell et al., 2015)] are not mentioned in the motivation or related work, which makes it hard to say this paper is well-placed in the literature.\u201d\n\nWe appreciate the reviewer\u2019s concern that we do not spend as much space in the paper contrasting against these two existing approximate ensemble-like methods. We focused mostly on second-order approximate uncertainty quantification methods (Section 4.1) because our method resembles these most closely. However, the requested comparison is also useful.\n\nOur general thoughts here are that MC Dropout and Bayes by Backprop effectively measure different types of uncertainty from what our method targets, and so they should not be considered competing methods. We have clarified this position in the paper, in both the introduction and the related work sections. We have also added an appendix that demonstrates this qualitative distinction empirically.\n\nMC Dropout constructs an ensemble of predictors by performing a set of stochastic forward passes through the network by applying dropout at test time. MC Dropout is expected to work well when the network was trained with dropout. In particular, Gal and Ghahramani show that this procedure approximates the posterior predictive distribution of a variational approximation to a deep GP. Similarly, Bayes by Backprop is a method for constructing a variational posterior over model weights. This method requires that the network be trained to minimize a variational ELBO. Thus, as mentioned in the paper, both methods require some assumptions on the training process (more about the post-hoc question later).\n\nSimilarly to our method, both of these methods can be framed as constructing a (weighted) ensemble of predictors and quantify predictive uncertainty in terms of the variation of predictions across this ensemble. However, the motivations for these ensembles are quite different from the method that we propose. Specifically, the ensembles constructed under these approximate Bayesian methods have no guarantee of being loss-preserving (that is, the members of the ensemble are not guaranteed to have near-identical loss) because they are not constructed with this being the goal. \n\nWe have implemented some experiments with MC Dropout on the small UCI datasets to demonstrate how these targets of estimation differ. These are now included in Appendix F. We implement a dropout scheme, and measure how much the training loss is perturbed across a Dropout ensemble. We then measure how a parameter perturbation with the same norm as the Dropout perturbation affects the training loss when that perturbation is constrained to lie in an ensemble subspace, for many values of our tuning parameter m (larger m corresponds to smaller eigenvalues in the ensemble subspace, and thus a flatter subspace). The figures clearly show that the variation in training loss in the Dropout ensemble is much larger than the variation in training loss within our constructed local ensembles when m is set so that the corresponding eigenvalue \\lambda^{(m)} is reasonably small (in the experiments in the main test, we set m = 2000).\n\nThis is in line with some of the arguments we make to motivate our method. We can expand on them here. In general, underdetermination is a narrower notion of uncertainty than Bayesian uncertainty \u2014 Bayesian uncertainty also incorporates uncertainty from dimensions of the parameter space that are well-constrained by the training data. In principle, exact Bayesian inference with appropriate priors could capture the notion of underdetermination that we aim to capture with our method. However, approximations to Bayesian inference and high-dimensional prior distributions often result in poor representations of underdetermination.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HylPlkv5sS",
                "reply_to": "S1gq0LRRKH",
                "title": "Response to Reviewer 2",
                "comment": "We thank the reviewer for their thorough engagement with our paper. The reviewer raises several valid and interesting points that have helped us strengthen the paper and inspired us to consider extensions. We address specific points below.\n\n# Major Points\n\n> Measuring sensitivity to choice of eigenvalue cutoff\n\nWe agree that this is an important point, and it was brought up by other reviewers as well. For our experiments in this paper, as a proof of concept, we fixed m to be 2000 for the tabular datasets, and m = 3000 for the CelebA dataset. In response to reviewer comments, we tried some other heuristics for setting cutoffs and found our results to be relatively insensitive.\n\nThis being said, we would ideally aim for the largest subspace such that the curvature of the loss surface in any direction in that subspace is smaller than some tolerance. Because we use a method that iteratively calculates the highest-magnitude eigenvectors, we can (up to numerical error) upper bound the curvature in any single direction in the ensemble subspace by the magnitude of the m-th eigenvalue. This can be used to develop heuristics for what constitutes an ensemble subspace that is \u201cflat enough.\u201d For example, one could choose m such that a parameter perturbation of a given norm does not change the training loss by a given percentage. We have added some of this discussion to the body of the paper in Section 3. We may attempt to implement such a heuristic for the camera ready version of the paper for consistency.\n\n# Minor Points\n\n> \u201cIn the experimental section with label, class prediction task, how correlated are the confounders Eyeglasses and Hat? What happens if the models are allowed to train for a longer; does the inconsistency in the behavior of AUC over more eigenvalues change?\u201d\nEyeglasses and Hat have Pearson correlation coefficient 0.07 - they are not highly correlated.\n\nIt is not clear to us if training models longer has a consistent effect on the results: we ran some experiments with the smaller models and observed inconclusive results. We agree that exploring these dynamics throughout training is definitely an interesting direction for future work.\n\n> \u201cIn section 5.4, I think a comparison with the Resampling Under Uncertainty baselines is imperative.\u201d\n\nUnfortunately, Resampling Uncertainty Estimation (RUE) here is not well-defined for the active learning task: influence functions are intended to be calculated for model parameters which are trained to a local (convex) minimum. In active learning, this assumption does not hold since the parameters are not fully trained at each step. Additionally, implementation of RUE on larger models is non-trivial (note that no image models were evaluated in the RUE paper). This is because of the issues discussed in Section 4.1.\n\n> \u201cThere is lack of clarity in how similar the models are during training. Although, the ensemble is used post-hoc, its unclear if the models during training differ in initialization only?\u201d\n\nAssuming you are referring to the ensemble experiments (e.g. Fig 2). Yes, the models are identical, differing in initialization only (specifically, model random seed changed. Data random seed was held constant).\n\n> \u201cWhat are the implications of the method in the case of finetuning models especially is the training data available for fine-tuning is low.\u201d\n\nThis method could be quite useful in the case of finetuning, since this is a specific case where the model\u2019s performance in the new (finetuning) domain could likely be underspecified due to a lack of relevant data in the original (non-finetuning) training set.\n\n> \u201cFurther, is there any notion of how the method scales with increasing depth in the neural network models? A comparison with larger test set and models trained on deeper architecture such as ResNet and the like will be interesting to see.\u201d\n\nThere is no reason why depth in particular would be an issue for our method. We agree that scaling up to models with larger parameter sizes is an important direction for improvement. However, in our current work we demonstrate that our approach can be successful on non-trivial models, and while we do not present results on SOTA image models, we believe our approach is still promising, and worthy of further investigation.\n\n> \u201cWith noisier data and the inconsistencies in the expected behavior of the method, is there a way of quantifying the amount of noise and the extrapolation? I do see the empirical experiments demonstrating this but some more insight into this is perhaps important.\u201d\n\nI am a little unclear on the exact meaning of this question, but what I think you\u2019re asking about is how noise in the data affects our method\u2019s ability to detect extrapolation (as in Section 5.2). The place where noisy data might present the largest challenge would be in the minibatch calculation of the HVP. In this case, one can make a tradeoff with time if necessary - increasing the number of minibatches used to estimate the HVP.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJxqsALqiS",
                "reply_to": "BJxhlmtn9H",
                "title": "Response to Reviewer 1",
                "comment": "We thank the reviewer for their thorough engagement with our paper. The reviewer raises several interesting points that have helped us strengthen the paper. We address specific points below.\n\n# Major Points\n\n> Clarity of Proposition 1\n\nWe have reworded the statement and proof of Proposition 1 to make the thinking a little more transparent. Please do let us know if there is still some ambiguity here.\n\n> Relation between found and optimal ensemble subspaces\n\nThis is an important question. The notion of an \u201coptimal\u201d subspace could be formalized as the largest subspace such that the curvature of the loss surface in any direction in that subspace is smaller than some tolerance. Because we use a method that iteratively calculates the highest-magnitude eigenvectors, we can (up to numerical error) upper bound the curvature in any single direction in the ensemble subspace by the magnitude of the m-th eigenvalue. This can be used to develop heuristics for what constitutes an ensemble subspace that is \u201cflat enough.\u201d For example, one could choose m such that a parameter perturbation of a given norm does not change the training loss by a given percentage. We have added some of this discussion to the body of the paper in Section 3.\n\nFor our experiments in this paper, as a proof of concept, we fixed m to be 2000, and found that in most cases, the corresponding eigenvalue was small (for the tabular experiments < 1e-4). A more thorough analysis would factor numerical error into this determination.\n\n# Minor Points\n\n> \u201cMany of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved\u201d\n\nThis is a good point and we have adjusted the figures in Sections 5.2 and 5.4 accordingly.\n\n> \u201cSome explanation about the choice of AUC as a metric would be informative and could help connect to the initial motivation of the method\u201d\n\nWe chose to focus on this OOD detection task because this is that task that most previous methods have proposed. This being said, tasks with a more continuous notion of ground truth would also be useful to consider.\n\nWe have added a sentence to Sec 5.1 commenting that we use AUC since OOD data may be rare.\n\n> \u201cExperiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a \"neural network\" is trained, the architecture should be specifically given alongside the discussion of the experiment\u201d\n\nWe agree with the reviewer that experimental details should go in the main body - however, we are somewhat space-constrained by the ICLR page limit. We have added a sentence to each experiment subsection stating the architecture of each model.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1gq0LRRKH",
                "reply_to": "iclr_2020_BJl6bANtwH",
                "title": "Official Blind Review #2",
                "comment": "The paper focusses on underdetermination as being key to extrapolation. In the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.\nThe underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.\n\nThe authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.\n\nThe score approximates the variance of predictions by estimating the norm of the component of the test point\u2019s gradient that aligns well with the low curvature directions of the Hessian, thus providing a tractable quantity in quantifying uncertainty in predictions. The motivation is that if the models have been trained to a local minimum or saddle point, then parameter perturbations in flat directions (small eigenvalues of the Hessian) do not change the training loss substantially.  These models with small perturbations on the flat regions then form the local ensemble for measuring the extrapolation and predicting on out of distribution samples, spurious correlated samples and for  active learning on uncertain data.\n\nThe authors prove that the extrapolation score is proportional to the standard deviations of predictions across a model ensemble with similar training loss. The math in the derivation checks out.\n\n \n\nOne of the novel contributions of the paper is in using computationally cheap post-hoc local ensembles over fully trained ensembles in the baselines that require complicated training procedure. The other key differentiation over baselines is their method leverages the ill conditioned Hessian where  the baselines struggle in requiring an inverse of that ill conditioned Hessian.\n\nThe limitations of their method is in the determination of sufficiently small eigenvalues from the ensemble subspace. Further, the sensitivity of the small set of eigenvalues towards overestimating the prediction\u2019s sensitivity to loss preserving perturbations and being less sensitive to some other under-constrained directions.\nBelow are the potential places where more clarity will help:\nIt would have been good to see a way of measuring the sensitivity in the set of small eigenvalues determination. I urge the authors to think of a way  of quantifying this sensitivity if possible especially since the model class is low dimensional.\n\nIn the experimental section with label, class prediction task, how correlated are the confounders Eyeglasses and Hat? What happens if the models are allowed to train for a longer; does the inconsistency in the behavior of AUC over more eigenvalues change?\n\nIn section 5.4, I think a comparison with the Resampling Under Uncertainty baselines is imperative. \nThere is a typo in E.4 label Attribute->Attractive.\n\nThere is lack of clarity in how similar the models are during training. Although, the ensemble is used post-hoc, its unclear if the models during training differ in initialization only? \n\nWhat are the implications of the method in the case of finetuning models especially is the training data available for fine-tuning is low. \n\nFurther, is there any notion of how the method scales with increasing depth in the neural network models? A comparison with larger test set and models trained on deeper architecture such as ResNet and the like will be interesting to see.\n\nWith noisier data and the inconsistencies in the expected behavior of the method, is there a way of quantifying the amount of noise and the extrapolation? I do see the empirical experiments demonstrating this but some more insight into this is perhaps important.\n\nOverall, its an extremely well written paper with great clarity. The method described by the authors is well differentiated from the baselines in making the clever use of the projection of the ill conditioned Hessian on the low curvature directions of the test point\u2019s gradient. \n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BJxhlmtn9H",
                "reply_to": "iclr_2020_BJl6bANtwH",
                "title": "Official Blind Review #1",
                "comment": "This paper presents local ensembles, a method for detecting underdetermination when extrapolating to test points. The authors define an extrapolation score which is used to estimate the standard deviation of predictions at test points. The extrapolation score is chosen to represent the variability in predictions that would be generated by models with similar training loss. By considering the eigenvectors of the Hessian that are associated with minimum eigenvalues the directions of the loss surface with minimal curvature are found, and perturbations of the parameters in the subspace of minimal curvature correspond to models with similar training loss. \n\nThe authors show that their extrapolation score is proportional to the first order approximation to the change in prediction under a perturbation of the parameters with minimal change in loss. In practice the minimum eigenvalue/eigenvector pairs are computationally challenging to compute for large matrices. For this reason the subspaces with minimal change in loss are computed by finding sets of vectors that are mutually orthogonal to the eigenvectors associated with dominant eigenvalues of the Hessian. \n\nThe method is validated experimentally first through out-of-distribution detection on synthetic data. The authors then test performance by constructing a \"blind spot\" by generating features that are a linear combination of existing features. Data can be generated as either within or out of distribution and the AUC metric can be applied to test model performance. In the final experiment the authors demonstrate the use of local ensembles in active learning. By determining which of the training samples are in the model's blind spots at each iteration and training based on these examples rather than randomly selecting training examples rates of convergence can be increased. \n\nI vote to accept this paper as the proposed local ensemble method builds on a growing body of literature regarding loss-surface inference, providing a new way to connect the shape of the loss surface to extrapolation detection. The theoretical result showing the first order relationship between the standard deviation of extrapolation predictions and perturbations in solutions is a useful insight. \n\nThere are some points that should be addressed for clarity however. Firstly the proof of proposition 1 should be made clearer. This is central to the work of the paper and a more full treatment of the proof here could help illuminate some intuition about the connection to perturbations and variance of predictions. \n\nThe other main point that is not addressed is that in principal we aim to find the subspace associated with minimal eigenvalues, but in practice this is computationally prohibitive. Therefore a space that has a basis that is mutually orthogonal to the dominant eigenvectors is sought (the found subspace), and this could have minimal relation to the subspace that is actually sought (the optimal subspace). Some experimentation showing how the found subspace relates to the optimal subspace would be informative, as well as how sensitive the results are to how much the found and optimal subspaces differ.\n\nSome minor points:\n- Many of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved\n- Some explanation about the choice of AUC as a metric would be informative and could help connect to the initial motivation of the method\n- Experiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a \"neural network\" is trained, the architecture should be specifically given alongside the discussion of the experiment\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "an ensembling approach to detect underdetermination for extrapolating to test points",
                "Sentiment Expression": "presents",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "The problem domain",
                "Sentiment Expression": "is interesting",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the approach",
                "Sentiment Expression": "is simple and useful",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the work",
                "Sentiment Expression": "reviewers were positive",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "several points for improvement",
                "Sentiment Expression": "they raised",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the discussion here",
                "Sentiment Expression": "The authors are strongly encouraged to include",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "BJeapjA5FX": {
        "paper_id": "iclr_2019_BJeapjA5FX",
        "paper_title": "GEOMETRIC AUGMENTATION FOR ROBUST NEURAL NETWORK CLASSIFIERS",
        "paper_abstract": "We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming traditional deep (convolutional) neural networks into adversarially robust classifiers. Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers (BNP-MFA) over the input space are used to design soft decision labels for feature to label isometry. Classconditional distributions over features are also learned using BNP-MFA to develop plug-in maximum a posterior (MAP) classifiers to replace the traditional multinomial logistic softmax classification layers. This novel unsupervised augmented framework, which we call geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and to Radio-ML (a time series dataset for radio modulation recognition). We demonstrate the robustness of GRN models to adversarial attacks from fast gradient sign method, Carlini-Wagner, and projected gradient descent.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "All three reviewers feel that the paper needs to provide more convincing results to support their robustness claim, in addition to a number of other issues that need to be clarified/improved. The authors did not provide any response. ",
        "meta_review_title": "More convincing experiments are needed",
        "reviews": [
            {
                "review_id": "S1xets1S6Q",
                "reply_to": "iclr_2019_BJeapjA5FX",
                "title": "The paper can be much improved by providing more evidence of the robustness to adversarial attack and advantages over other models.",
                "comment": "The paper is working on a robust classifier that consists of two stages. The first stage performs unsupervised conditional kernel density estimates (KDE) of the covariate vectors, and the second stage is feature extractions and classification. I appreciate the authors' efforts to clarify the intuition, but more technical details and experiments can be provided to support their arguments. My questions and comments are below.\n\n1. Page 2. \"this means the stochastic gradient descent training algorithm minimizing...\" Is the problem because of SGD or the structure of NN? I think the reason might be the latter, consider logistic regression, which can be seen as a single-layer NN, does not suffer such a problem. \n2. I know the KDE part is from an existing paper, but more technical details can make the paper clearer and some statements are questionable. Specifically, what basis vectors are used for (3)? Is it really speedy and scalable (Page 4, Section 3.1) for BNP-MFA if using Gibbs sampling? Is it the reason why the experiments in Table 1 is incomplete?\n3. For Eqn (7), how do you calculate \\beta's to \"scale the correct class label higher than incorrect classes for the cases...?\"\n4. Is the proposed model robust to all kinds of attacks, like gradient based noise, and outliers which locates far away from the corresponding cluster?\n5. Can you provide some experiments to show the advantage over other approaches?[1]\n\n\nI highly encourage the use of BNP KDE which has many advantages as stated in the paper. But the authors may have to solve the problem of scalability and show advantages over other approaches.\n\n[1]Uli\u010dn\u00fd, Matej, Jens Lundstr\u00f6m, and Stefan Byttner. \"Robustness of deep convolutional neural networks for image recognition.\" International Symposium on Intelligent Computing Systems. Springer, Cham, 2016.",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HkewPPDF3Q",
                "reply_to": "iclr_2019_BJeapjA5FX",
                "title": "Interesting work but more comprehensive evaluations needed",
                "comment": "This paper proposes geometrically robust networks (GRN), which applies geometric perspective and unsupervised model augmentation to transform traditional deep neural networks into adversarial robust classifiers. Promising experimental results against several adversarial attacks are presented as well.\n\nThe BNP-MFA are applied twice in the framework: one for getting the soft labels, and the other for getting the predictions through MAP estimation. There are existing works which are in the same line as the second part: deep kNN [1], and simple cache model [2] for example, where similarities to training examples are used to derive the test prediction and substantial increase of the robustness against adversarial attacks considered in this work have also been shown. \n\nThese raise two questions:\n(1) How much does the soft label encoding help increase the robustness?\n(2) How does the proposed model compare with the deep kNN and the simple cache model, which are much simpler?\n\nSome minor issues:\n- The unsupervised learning for label encoding is performed on the input space, the image pixel for example. But it is known that they are not good features for image recognition.\n- It is unclear which part of the network is considered as \"feature extraction\" part which is used for MAP estimation in the experiments.\n- It would be nicer to have results with different architectures.\n\n\n[1] N. Papernot and P. McDaniel. Deep k-nearest neighbors: towards confident, interpretable and robust deep learning. arXiv:1803.04765.\n[2] E. Orhan. A simple cache model for image recognition. arXiv:1805.08709.",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ByglSPbvnX",
                "reply_to": "iclr_2019_BJeapjA5FX",
                "title": "This work lacks any convincing experimental result to support the claims",
                "comment": "This work proposes a defence based on class-conditional feature distributions to turn deep neural networks into robust classifiers.\n\nAt present this work lacks even the most rudimentary evidence to support the claims of robustness, and I hence refrain from providing a full review. In brief, model robustness is only tested against adversarials crafted from a standard convolutional neural network (i.e. in a transfer setting, which is vastly different from what the abstract suggests). Unsurprisingly, the vanilla CNN is less robust than the density-based architecture introduced here, but that can be simply be explained by how close the substitute model and the vanilla CNN are. No direct attacks - neither gradient-based, score-based or decision-based attacks - have been used to evaluate robustness. Please check [1] for how a thorough robustness evaluation should be performed.\n\n[1] Schott et al. \u201cTowards the first adversarially robust neural network model on MNIST\u201d.",
                "rating": 3,
                "confidence": 5,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "needs to provide more convincing results to support their robustness claim, in addition to a number of other issues that need to be clarified/improved",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "8pOPKfibVN": {
        "paper_id": "nips_2021_8pOPKfibVN",
        "paper_title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time",
        "paper_abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Adding auxiliary losses to the main objective function is a general way of encoding biases that can help networks learn better representations. However, since auxiliary losses are minimized only on training data, they suffer from the same generalization gap as regular task losses. Moreover, by adding a term to the loss function, the model optimizes a different objective than the one we care about. In this work we address both problems: first, we take inspiration from transductive learning and note that after receiving an input but before making a prediction, we can fine-tune our networks on any unsupervised loss. We call this process tailoring, because we customize the model to each input to ensure our prediction satisfies the inductive bias. Second, we formulate meta-tailoring, a nested optimization similar to that in meta-learning, and train our models to perform well on the task objective after adapting them using an unsupervised loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on a diverse set of examples.\n",
        "paper_acceptance": "accept",
        "meta_review": "This paper proposes tailoring, a technique for incorporating unsupervised objectives during test time. The main distinction and past work is that tailoring is applied to each individual test datapoint. The paper includes some theoretical justification of the approach as well as rather extensive experiments. Reviewers were generally positive on the paper. The main criticism of the paper came down to lack of clarity in writing, but the authors have addressed this concern through their rebuttal. Before the camera-ready version, please incorporate the proposed changes to writing.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "2ghKvYiZWwd",
                "writer": "author",
                "reply_to": "PzIlTCeY5kr",
                "title": "Selecting a good tailoring loss for tailoring and why meta-tailoring is robust to this choice",
                "comment": " That's a good question! In this paper, we suggest tailoring losses serve a similar role to auxiliary losses, but better serve the outer objective and, in the experiments shown in the paper, work better. Intuitively, you want tailoring losses to be informative of the true task, so that the gradient minimizing the tailoring loss is aligned with the gradient minimizing the task loss (because at test time you can only see the former, but not the latter). This was analyzed for auxiliary losses [which are widely used] in [1].\n\nFinally, one notable point is that in meta-tailoring the network will be trained to perform well _after_ the update. Therefore, in the same way Figure 2 shows it doesn't overfit to the tailoring loss, if the tailoring loss is detrimental meta-tailoring is likely to ignore it.\n\nWe will add a comment (and pointer to [1]) in the main text.\n\n[1] Adapting auxiliary losses using gradient similarity; Du*,Czarnecki* et al.  '2020; https://arxiv.org/abs/1812.02224",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PzIlTCeY5kr",
                "writer": "official_reviewer",
                "reply_to": "15M3dFN7DjK",
                "title": "Response to Rebuttal",
                "comment": " Thanks for your responses.\n>Avoiding catastrophic forgetting Partial catastrophic forgetting ........ There, the model is trained to have a low task loss after the tailoring update and thus learns not to forget about the task loss after the inner optimization. You can see that in the monotonicity of the blue curve.\n\nI see.  Selecting good tailoring loss still seems to be a little tricky, or it could be contrastive to the task loss. Is it possible to provide more guidance on how to select tailoring loss? \n\nThe rebuttal fixed most of my concerns, I prefer to keep my score and vote for acceptance.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QwklLM0GxKo",
                "writer": "author",
                "reply_to": "QPw6RV22B-4",
                "title": "Update: pseudo-code added to the main text",
                "comment": " Following your suggestions, we've now changed theory content for method content.\n\nTo make space, we have moved some math(Assumption 1, Theorem 2, and details from section 3) to the appendix. These were mostly math technicalities and not part of the main punchline. Therefore, we think the clarity of the theory has not decreased.\n\nThis allowed us to move the entire Algorithm pseudo-code (both training and prediction) from App D to the main text along with some extra comments on the method. We think this will greatly benefit clarity as it will make the method much more concrete to the reader.\n\nThanks for the suggestion!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Q3MeZG7G9E",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_8pOPKfibVN",
                "title": "",
                "comment": "The paper proposes a new method to adapt neural networks at inference time to the given input such that the model minimizes some given unsupervised loss. This method is called 'tailoring'. Additionally, 'meta-tailoring' is proposed that trains the model using 'tailoring', too. This way the gap between training and inference distribution is removed that 'tailoring' introduces.  The paper describes a, to the best of my knowledge, novel method. I believe this method is interesting for niche applications of ML. The model is replaced by a short SGD loop of the model, such that given x a few iterations of SGD are performed, before making the prediction. I do not understand why there is no possibility to make the model behave straight away like the model after a few steps.\n\nMain Points:\n\n1. Method Description. The method is not described enough in the main paper. One has to read the appendix to really understand what the method is, especially for meta-tailoring. There should be pseudo-code or at least a more detailed text description in the main paper. For example, I do not understand what the difference of Meta-Tailoring (0 st.) is to the baseline, even after looking into the appendix. I am pretty confident that algorithm 1 would actually not train at all with steps=0. I also find the naming confusing with the introduction of CNGrad, but naming the application of CNGrad in the experiments Meta-tailoring again.\n\n2. First-order and detached CNGrad. You only consider detached CNGrad in all experiments (line. This is not the algorithm you provide guarantees for in Section 3. I think this is a severe short-coming, as the detached variant, also does not agree with the intuition one develops around meta-tailoring. For 2/4 experiments you write that you use first-order CNGrad, for the others it is not known. First-order CNGrad, goes even further away from meta-tailoring and detaches \\gamma and \\beta even earlier. This, thus makes the above problem even more severe.\n\n3. Focus. I think this paper could benefit a lot of a more focused structure. You try to keep everything as general as possible, but there you also have to make changes for each experiment. I believe the paper might benefit from more focus. Only considering one or two CNGrad variants with fixed hyper-parameters across tasks, a main evaluation on which you reach something close to state-of-the-art and more focus on the method presentation.\n\n4. Experiments. This paper proposes a broad method, thus I understand the motivation of the authors to include a diverse set of benchmarks. The problem with this setup is that, the evaluations themselves suffer from this. None, of the evaluations is described to a sufficient level, such that it is really hard to understand the effect.\n\n5. Baselines. The baselines are not easily understood. While, I very much appreciate that you add the tailoring loss to the inductive baseline for the first experiment, I am still a little critical regarding the baselines. You now put a lot of work in making CNGrad work including selecting where to apply CN layers, what inner-lr to use, whether to use first-order CNGrad or detached CNGrad, but for the baselines it seems you performed less of this tuning. For some of the baselines my feeling is that it is reasonably easy to come up with a way to make the inductive baseline perform well and consider the constraints at testing time. Like you wrote for Adverserial Examples.\n\n6. Costs. As this method introduces a considerable amount of extra gradient steps to the training loop, I believe there should be some considerations of the costs of this method in the main paper.\n\nSummary: The paper proposes an interesting new method. It does not show conclusive evidence that it improves state-of-the-art models in any domain, though, and it uses a different algorithm compared to the described algorithm. Additionally, parts of the paper seem to not be quite ready for a main conference and rather in a draft-stage. Nevertheless, I believe this paper might be a great contribution after some more work on the experiments and the presentation.\n\nDISCLAIMER: I did not take the time to read section 3 and the corresponding proofs in detail, as I do not believe that this changes my view of this paper too much, it is hard to follow and does, to my understanding, not apply to the algorithm actually used practically in the experiments as pointed out in 2.\n\nDetails:\n\ni) Line 720 'key' -> 'this is key'\n\nii) Line 105 '. Losses' -> '.\\n\\nLosses'\n\niii) Line 63 -> Notice that the outer process now only optimizes the objective we care about, ...\n\niv) Confusing notation in algorithms, with var assignments and 'for' construct in the same line.\n\nv) Table 1 description 'over-performs' -> 'outperforms'\n\nvi) You used /begin{figure} for many tables where /begin{table} should be used.\n\nvii) Line 356 ' Improving' -> 'Improving' The authors consider interesting societal impacts and the limitations of their algorithm.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "QPw6RV22B-4",
                "writer": "author",
                "reply_to": "SbTMGwEd6AT",
                "title": "Thank you for your reply",
                "comment": " Thank you for your fast reply.\n\n**0-step meta-tailoring**: Yes, exactly!\n\n**CNGrad vs meta-tailoring**: All experiments, including contrastive learning, apply meta-tailoring using first-order CNGrad (which allows it to be run efficiently). It\u2019s similar to saying a paper does meta-learning using FO-MAML[1]. In the same way, CNGrad is a meta-tailoring algorithm. \n\nYour observation about algorithm 1 is a great and subtle one; both options are correct! Taking a single outer step after all inner steps is similar to MAML[1].  Taking an outer step after every inner step is similar to WarpGrad[2], which got better results than MAML by having some weights $w$ trained in the outer loop and some weights $w\u2019$ trained in the inner loop. CNGrad builds on WarpGrad with the customization of the weights $w\u2019$ being CN layers, which allows the efficient parallelization. Preliminary experiments didn\u2019t show much difference between both approaches (MAML-like vs. WarpGrad-like). Therefore, we chose the latter for consistency. \n\n**Math applies to all the experiments**: we want to emphasize that section 3 does apply to all the experiments. As described in theorem 1, it applies to any meta-tailoring algorithm (where the prediction function is the same for test and train). Theorem 1 and Remark 1 provide upper-bounds of the test task loss that depend on $\\mathcal{L}^{tailor}(x,\\theta_{x,S})$. The same bounds apply to regular inductive learning algorithms by just changing $\\theta_{x,S}$ to $\\theta_S$. The key insight then is that meta-tailoring algorithms can optimize $\\mathcal{L}^{tailor}(x,\\theta_{x,S})$ for each $x$ at prediction time, lowering the upper-bound.  Because all experiments use meta-tailoring, they are all within the scope of the theory results.\n\n**Clarity** Thank you for all your questions and comments. We believe they will greatly improve the clarity of the manuscript. For instance, we will move some parts of the theory to the appendix and bring information on the algorithm from appendix D to the main text.\n\n[1] Model-agnostic meta-learning for fast adaptation of deep networks; Finn et al. \u201817\n\n[2] Meta-Learning with Warped Gradient Descent; Flennerhag et al. \u201819",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SbTMGwEd6AT",
                "writer": "official_reviewer",
                "reply_to": "w_pvJf8QNkZ",
                "title": "Updated Opinions",
                "comment": " On 0-step meta-tailoring: Oh, I understand now. That is interesting. Updating the BatchNorm parameters during training for each example yields a neural network that generalizes better even without these updates.\n\nCNGrad vs meta-tailoring: It seems to only be a communication problem, e.g. in the contrastive learning experiments you don't do CNGrad, but aren't explicit about it. But I am confused again: in algorithm 1 you actually take steps with respect to $w$ $steps$ times. Shouldn't you only take one final step with $w$, and $steps$ many steps with $\\beta$ and $\\gamma$ only?\n\nThe Math: I believe the math of the method you do not even use has too big a part of this paper. If you had the extra page to explain your method and the experiments in detail the paper would be stronger in my opinion.\n\nThe hyper-parameters: The monotonicity, of course, is beautiful and makes my arguments less strong. \n\nOverall, I am scared this paper does not provide a contribution that can be built on. This is mostly due to the communication and could therefore likely be fixed without further experiments.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "w_pvJf8QNkZ",
                "writer": "author",
                "reply_to": "Q3MeZG7G9E",
                "title": "Thank you for your review",
                "comment": " Thank you for the detailed review and constructive comments.\n\n**Clarifications on 0-step meta-tailoring** We agree that the notation was confusing; we will clarify it. The number of steps within the parentheses of table 1 corresponds only to the number of inner steps taken at test time. For meta-tailoring, we performed 2 inner steps during training and then analyzed the performance of varying the test-time inner steps. This analysis is common for gradient-based meta-learning algorithms, where we meta-train with a fixed number of inner steps and observe how performance evolves w.r.t. the number of meta-test inner steps. In table 1, 0-step meta-tailoring refers to the case where we perform 0 inner steps at test time, but have performed 2 inner steps at training time. We will clarify this in the main text.\n\n**Devoting more space to algorithm, experiments and compute costs** We agree that more detail in the method section and the experiments would be fruitful. Given the amount of theoretical content and number of experiments, we had to move many details to the appendix, as well as an entire toy-but-insightful experiment (appendix H). We are considering moving other subsections to the appendix to increase the details in the method and experiments sections. In particular, we will summarize the relationship between meta-tailoring and meta-learning on page 3 and the broader impact statement. We welcome other suggestions!\nWith respect to computational costs, note that we discuss them in section 6.2 in the main text, with extra details in lines 725-738 in appendix D. We will move some of these details to the main text.\n\n**CNGrad vs meta-tailoring** Meta-tailoring computes a different set of weights for each example. In PyTorch and TensorFlow, we cannot parallelize this process in the general case (it is possible to do it in JAX). CNGrad allows us to parallelize meta-tailoring in all these platforms, by only customizing the conditional normalization layers to each example. Tailoring and meta-tailoring are the frameworks, CNGrad is an architectural trick to make them efficient. This is why we simultaneously refer to meta-tailoring and CNGrad in our experiments. We will make it clearer in the text.\n\n**First-order CNGrad** The reviewer correctly points out that all results come from first-order CNGrad (which is the same as detached CNGrad). However, note that we also tried CNGrad\u2019s  second-order version for the first experiment, which actually performed slightly worse. This was not entirely surprising for the following reasons:\nAs observed in the meta-learning literature, second-order gradients are often unstable, requiring us to use a smaller inner learning rate.\nAs in the case of FO-MAML[1] or REPTILE[2] for meta-learning, first-order CNGrad does not warp the inner optimization but still takes the adaptation into account. Therefore it still satisfies the essence of meta-tailoring captured in the theory section(#3).\nFirst-order versions have proven successful in meta-learning[1,2]. In meta-tailoring, all networks tackle the same task. Therefore, we expect the differences between the tailored weights of each example to be smaller than the differences between the adapted weights of different tasks in meta-learning. Since the second-order part of the optimization concerns itself with these small differences between examples, the importance of second-order gradients in meta-tailoring will be even smaller than in meta-learning.\nGiven their similar performance, we used the first-order version of CNGrad because it is easier to implement and faster. An in-depth comparison between both options can be found in lines 693-710 in appendix D.\n\n**Varying hyper-parameters and strength of the baselines** The reviewer rightfully points out that meta-tailoring with CNGrad adds hyper-parameters: number of inner steps, inner learning rate, and whether to use first-order or second-order optimization in CNGrad. However, this doesn\u2019t result in stronger hyperparameter tuning w.r.t the baselines for the following reasons:\n- **Good baseline tuning**: all but four baselines come from impactful published works, which were optimized by their corresponding authors. We took great care with the remaining four baselines implemented by us. We thank the reviewer for noticing that we appropriately tuned the corresponding hyperparameter for the two inductive baselines in the physics experiment. In that experiment, we also allowed the TTT baseline to do the inner loop up to convergence. This used 5 times more steps than meta-tailoring (the more steps the better for both methods, as seen in table 1 and figure 2). Finally, for the TTT baseline for the contrastive learning experiments we searched over steps and learning rate in the exact same way as for meta-tailoring (this tuning is shown in figure 5 in appendix G).\n- **Monotonic dependence w.r.t. hyperparameters**: as long as the optimization is stable (which can be easily checked during training) we observe a mostly monotonic relationship between the final task loss and the three hyper-parameters. A higher learning rate is better, more steps are better, and second-order is better than first-order. Increasing the inner learning rate eventually resulted in instabilities, both the steps and second-order CNGrad increase compute, and the second-order CNGrad increased code complexity. Since we were not seeking the state-of-the-art, we always used the first-order version and a small number of steps to reduce compute and code complexity. Using more steps or the second-order version could have improved results further.\n- **Diversity of experiments results in varied hyperparameters**: all experiments use first-order CNGrad. The reviewer is correct in mentioning that the number of steps and inner learning rate vary across experiments. This is because this paper includes a wide diversity of applications. Some networks are very small (3-layers) and some are large (ResNet-50 in the contrastive and adversarial experiments). Having a small network allows having a larger number of steps. Similarly, losses also vary greatly: some are MSE and some are cross-entropy, with varied scales. These different magnitudes affect the range of effective inner learning rates.\n\n\n[1] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks; Finn et al. \u201817\n\n[2] On First-Order Meta-Learning Algorithms; Nichol et al. \u201818",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Dt45tj4RMl3",
                "writer": "author",
                "reply_to": "oBkVTM6TU10",
                "title": "Thank you for your review",
                "comment": " Thank you for your review.\n\n\n**Complexity of the writing** We will improve the clarity of the text. In particular, after seeing the feedback from all reviewers we will devote more space to the method and experiment sections. Since space is limited, we plan on moving to the appendix some details of the comparison between meta-tailoring and meta-learning as well as the broader impact statement.\n\n\n**Code and pseudo-code** Pseudo-code can be found in appendix D, pages 22 & 23. We will open-source the code for the paper to facilitate reproducibility.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4zzDMZEUgX7",
                "writer": "author",
                "reply_to": "0Tsm-tzLN9H",
                "title": "Thank you for your review",
                "comment": " Thank you for your review.\n\n**Breadth of applicability** We decided to do many experiments to display the breadth of applicability of tailoring as a way to encode a wide variety of inductive biases in the standard ML setting. It is worth noting that we chose to build on some of the most impactful papers in contrastive learning, certified adversarial examples, and physics modeling to facilitate understanding instead of using the state-of-the-art works, which often have more bells and whistles. Moreover, all three are still close to SOA in their respective fields. We also chose to focus on the same-distribution setting, where our theoretical guarantees apply, to keep the message clear and consistent. \n\n**Practical implementation in deep learning frameworks** Both tailoring and meta-tailoring are already efficiently implementable in JAX. This is because JAX allows the evaluation of multiple networks with the same architecture but different weights in a batch. To the best of our knowledge, this is not yet possible in PyTorch and TensorFlow. For these frameworks, we constructed CNGrad, where parameter updates only occur in conditional normalization layers, which can be efficiently parallelized. Therefore, CNGrad can be efficiently deployed in all popular deep learning frameworks. We will clarify this in the text.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "15M3dFN7DjK",
                "writer": "author",
                "reply_to": "QUbG7t7V2O",
                "title": "Thank you for your review",
                "comment": " **Avoiding catastrophic forgetting** Partial catastrophic forgetting of the task loss can occur when doing tailoring with a large learning rate or for many inner steps. However, it does not occur when doing meta-tailoring. This can be seen in figure 2. The green curve (representing tailoring) begins at the top right and, at first, goes down and to the left: as we minimize the physics tailoring loss, the task loss also decreases. However, as you mention, the task loss eventually starts to increase after many inner steps. Therefore, tailoring is mostly useful for a few inner steps (which will also be faster). In the case of meta-tailoring, catastrophic forgetting is not a problem because meta-tailoring takes the tailoring process into account during training. There, the model is trained to have a low task loss after the tailoring update and thus learns not to forget about the task loss after the inner optimization. You can see that in the monotonicity of the blue curve.\n\n**Relation to semi-supervised learning** We agree about the relevance of the semi-supervised learning literature and virtual adversarial training. We will add more discussion on both topics. It is worth noting that transductive learning (which we list as one of our main inspirations) is closely related to semi-supervised learning, as it assumes unlabeled test data is available at training time. We will clarify this relationship in the paper; following the great analysis of chapters 24&25 in [1].\n\n**Tailoring with pseudo-labels** Allowing the model to make an initial prediction, then tailoring the model to maximize its confidence makes a lot of sense. In particular, minimizing entropy at the test points was the main loss used in the classic transductive learning literature. Your idea would be to bring this loss to the meta-tailoring setting, which is likely to be useful!\n\n[1] Semi-Supervised Learning; Chapelle, Scholkopf, Zien",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QUbG7t7V2O",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_8pOPKfibVN",
                "title": "",
                "comment": "This paper proposed tailoring, which is a general framework that could help to finetune the prediction on each test sample according to some specific inductive biases. Tailoring provides a different perspective that avoids involving extra loss function in the proxy fashion.  Besides, the authors proposed meta-tailoring, which integrates the unsupervised loss in a way similar to meta-learning. The theoretical discussion and empirical results demonstrate the effectiveness of the proposed tailoring.   Strengths:\n1. The idea of tailoring is interesting. The current neural networks mainly conduct amortized optimization, how to reduce such gap especially on the test data is an important research direction. I believe the proposed framework could provide a different perspective on understanding the \"generalization\" gap and different inductive biases. \n\n2. The limitation of tailoring lies in the increased computational cost. To reduce such extra cost, the authors thus introduce CNGRAD which could efficiently parallel the evaluation of the model over multiple samples.  And there is detailed and sound theoretical justification on the CNGRAD provided. \n\n3. The authors provided extensive examples of inductive biases. And correspondingly the experiment results on symmetry constraints, inductive biases, contrastive loss, and adversarial examples justify the effectiveness of both tailoring and meta-tailoring.  The ablation study is well designed and the results are promising.\n\nWeakness:\n1. One particular concern of mine is when conducting tailoring the parameters of the model change according to a single sample, this is similar to the continual learning setting. I wonder how the methods could avoid catastrophic forgetting. It seems that the authors constrain the steps of the tailoring while small changes in parameter space could result in the relatively large change of the model output. I suggest the authors add more discussions on this part. \n\n2. Though the method does no limit the application scenarios, I feel that the method is highly related to the field of semi-supervised learning. Therefore, I suggest more discussions on the related works. For example, the adversarial examples setting of tailoring is related the virtual adversarial learning in semi-supervised learning [1].\n\n[1]. Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning\n\nQuestions:\nI am curious about the setting when evaluating the test sample, we first assign a pseudo label according to the initial output. And minimizing the loss towards this pseudo label, I wonder whether tailoring in this setting could work.  Yes.\nRefers to main review",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "0Tsm-tzLN9H",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_8pOPKfibVN",
                "title": "",
                "comment": "The paper proposes tailoring - a general framework of algorithms that can combine ideas for test-time generalization, self supervision, meta learning and transductive learning. Although the experiments are quite limited overall, the paper is a good addition for the ML community.   The paper introduces tailoring and meta tailoring as a means to add inductive biases at test-time using contrastive losses. The paper overcomes some of the shortcomings of TTT and meta-tailoring seems like an interesting improvement over TTT. The idea of encouraging soft inductive biases (5.2) is very interesting and practical. I would like the authors to include experiments such as domain generalization (like the sort done by TTT) since I believe that those set of experiments are very good tests of how such algorithms can adapt to novel data distributions (not just adversarial samples as they are a very specific type of generalization. Although the authors mention the practical problems with implementation in popular deep learning frameworks (pytorch and tensorflow), it would be good for the authors to provide means to overcome such problems so that CNGrad can become a staple in the deployment of ML models.\n\nThe authors tend to focus on the breadth of results to show the generality of the solution, rather than depth in one or two fields, it might be useful to show more difficult tasks in any of the tasks to show a strict improvement and the scalability of the solution. The most practical problems with the frameworks (as the authors discuss) is with privacy as the models are \"tailored\" for the given example. This can have negative impact and which the authors correctly discuss. ",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "oBkVTM6TU10",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_8pOPKfibVN",
                "title": "",
                "comment": "In this paper, the authors propose optimizing an unsupervised loss function at test time as an inductive prior on a neural network. They propose two schemes for achieving this: training a network in a regular fashion and then applying the unsupervised loss at inference time only, or applying the unsupervised loss during training as a meta-learning scheme. The authors claim that this method can improve robustness, as well as improve train-test generalization gap in cases where there is a known inductive prior on the expected output (e.g. when modelling physical systems)\n  The paper's writing could be improved, as the writing is complex and hard to follow. Furthermore the model details were mixed with the introduction, making it hard to understand what is the author's proposed work. It is also hard to understand the implementation details of this idea, as the authors share neither pseudo-code nor actual code to illustrate their implementation. With such an idea, it is often the case that the devil is in the details, and efforts to replicate it could show very different results to the ones shown on the paper.\n\nThe idea is very interesting and novel. It is well motivated in learning theory and seems almost obvious in hindsight (a good thing!). The authors provide a wide variety of experiments to show the effectiveness of the idea, not just for different datasets, but for different types of inductive bias. Especially in the case of physical systems modelling, this idea could be very useful in the applied setting and have a broad impact on further research. The authors have adequately addressed the limitations of their work and Societal impact.",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "clarity in writing",
                "Sentiment Expression": "lack",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "qRDQi3ocgR3": {
        "paper_id": "iclr_2022_qRDQi3ocgR3",
        "paper_title": "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective",
        "paper_abstract": "Deep neural networks (DNNs) often rely on easy\u2013to\u2013learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current generation of machine learning models. In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications. We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand. Even under equal opportunities, we observe that (1) certain cues are preferred to others, (2) solutions biased to the easy\u2013to\u2013learn cues tend to converge to relatively flat minima on the loss surface, and (3) the solutions focusing on those preferred cues are far more abundant in the parameter space. We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs. Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the inborn bias of models leans toward simple cues, such as color and ethnicity. Our findings emphasize the importance of active human intervention to remove the inborn model biases that may cause negative societal impacts.",
        "paper_acceptance": "Accept (Poster)",
        "meta_review": "The reviewers were split, with one of them leaning towards rejection, primarily due the (perceived) limited impact of the study. I tend to agree with the other reviewers that this paper provides an interesting and original framework for analysis of learning models, and while there are substantial shortcomings, they are outweighed by the positives (including the promise this approach may hold for analysis of learning in more realistic scenarios). I therefore recommend acceptance, if space in the proceedings allows.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "XPOiPxLU6FG",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_qRDQi3ocgR3",
                "title": "",
                "comment": "The authors propose a framework for studying the tendency of deep neural networks to preferentially adopt \"cues\". Specifically, they focus on settings where multiple cues are equally likely, though not all of them are equally exploited. To set up such a scenario, they introduce the WCST-ML task, in which the prevalence of cues can be parametrically controlled. They also conduct empirical studies on the more naturalistic UTKFace dataset. The authors introduce a set of metrics, such as path connectivity, attractor basin properties, etc. to analyze cue preferences from a loss landscape perspective. The authors also explain these observations based on the \"complexity\" of cues.    Strengths:\n\nOverall this is a well-written paper, clearly motivated and carefully constructed. The finding that the number of solutions (i.e. parameter configurations) that rely on preferred cues are also abundant is a good corroboratory result.\n\nThe motivations for this study are also exactly what the field needs at this point, given the abundant use of deep convolutional networks for a variety of applications with minimal understanding of its exact decision-making mechanisms.\n\nThough the synthetic task (Wisconsin card sorting) has been widely adopted in the cognitive neuroscience community, their formulation for mainstream ML is nifty and allows for systematic empirical evaluation.\n\nWeaknesses:\n\nHaving said that, this paper fails to deliver on its promise of intricate analysis.\n(i) There is an inherent question that authors fail to sufficiently address. The selected \"cues\" for analysis have intrinsically different extraction demands from the stimuli, thus making the argument about them being \"equally prevalent\" moot. For example, accessing color is more direct (pixel-level information is directly available) than accessing, say, shape (for which a network needs to build sufficiently large receptive fields to understand the global geometry of a scene).\n\n(ii) The experiments on the UTKFace dataset are not convincing. The authors themselves acknowledge that there might be other \"shortcut cues\" outside of the selected ethnicity and age cues. This brings up a subtle (yet important) question. In naturalistic datasets, how can one determine or interpret a basis set of cues that are truly orthogonal dimensions? The experiments on WCST-ML work because the shape, color, scale, and orientation are by definition orthogonal. For all we know, the \"cues\" in naturalistic datasets could be abstract and not human-interpretable too.\n\n(iii) Treating the number of model parameters as a proxy for the Kolmogorov complexity of the input-output mapping seems ill-advised. If one trains a parameter-shared recurrent neural network it can perform the same effective computations as a ResNet, but with much fewer parameters (though the task per se has not changed, and by extension the cues). I would appreciate it if the authors can offer some insight into this.\n\nMinor:\n(i) Figures need to be made more legible. Axis labels and text insets are barely visible without zooming in.\n(ii) Particularly, Figure 4 could be refined further. The caption isn't very descriptive, and the section on \"Qualitative view on the loss surface\" is a bit misleading. The ethnicity solution doesn't seem to be \"characterized with a flatter and wider surface\" as compared to age and gender solutions in Fig. 4. If there are more obvious examples, the authors should use them instead. This is a well-written paper trying to address an important problem. However, as I've expressed in the main review, some of the claims are unsupported and hence my initial assessment. I am willing to update my score if the authors are able to provide a convincing response!",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "sQfS9uqtZff",
                "writer": "official_reviewer",
                "reply_to": "48MreOloxmC",
                "title": "Response",
                "comment": " I thank the authors for their thorough consideration of my review. While the authors responses and updated draft definitely point in the right direction, I remain of the opinion that the current version is not quite ready for exposure to a larger audience on the main track. I have upgraded my rating of technical novelty from 2 to 3 to recognize the components the authors point out in the response: (a) toolbox for KC estimation and zero-loss path finding, (b) added outlook on the thinking around mode-connectivity for potential impact in ensemble models.\n\nIn case of final rejection of this submissions, I'd suggest to the authors to strengthen a resubmission by going deeper on the empirical side on questions they have raised in the rebuttal, e.g. \n\n\"We can extend this idea further in our setup where the endpoints of the path correspond to solutions attending to different cues. Samples along such a heterogeneous curve are likely to be much more diverse and could result in a more effective ensemble. We may also control the type and amount of bias in the resulting ensemble by adjusting the concentration of sampling probabilities along the curve.\"\n\nI feel that convincing empirical results from the above in a practical, \"reasonably real-world\" case would be very well received by the parts of the community that are concerned with generalization and robustness.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gXTSZocs01",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_qRDQi3ocgR3",
                "title": "",
                "comment": "The paper discusses biases in inductive learning in deep neural networks that stem from pathologically sampled data. The authors pose a problem set where a trainer only sees samples where two or more latent values can only be observed in a fully correlated fashion (e.g. scale, color, shape). They then design various criteria and protocols to gain insight in the behavior of the learned model when the correlation does not hold any more (i.e. samples that have not been seen in the training data). They conclude that in this case of generalization to unseen data, the trained model has an implicit bias towards choosing (1) mostly single cues to determine the predicted label (e.g. color only), (2) that more preferred cues are simpler than less preferred cues and (3) that the underlying solution space of possible parameters has more solutions that prefer the simple cues. They demonstrate empirical results on variations of existing datasets (DSprites, UTKFace).    The paper is written well, the claims are laid out clearly, and the method is largely well described. The approach seems methodically sound given the problem statement  (caveat in W2 below). The path of investigation is fundamentally valuable in the sense that models of biases and generalization of networks are helpful tools.\n\nWeaknesses:\n[W1] Some parts of the exposition need minor clarification:\n(a) p6: \"The trend is clear for ResNet20 and ViT, while the ethnicity and gender preferences are within the error bars for FFnet.\" This refers to Figure 3, and I cannot understand this from Figure 3, in the least it is confusingly stated. The ethnicity and gender graphs for FFNet in figure 3 seem separated by several standard deviations according to the error bars.\n(b) Figure 6, three right columns: What does the dot and the black arch signify? I was not able to gain this from text or caption. I assume that the zero loss path is the dashed line?\n\n[W2] A somewhat methodical weakness of the paper seems to be the problem statement. For instance in the introductory example of section one/figure 1, the authors imply that the model is biased towards scale (then shape, then color). I do agree that this is consistent with the model preferring the property of scale in making the decision. Is this bias, though? In order to evidence bias, I'd need a truth/label assignment on the off diagonals (e.g. indicating that blue small triangle is not supposed to be class 1). Without this truth assignment I do not see the bias of the model.\n\n[W3] The main weakness of the paper seems to me that I am asking myself: What have I learned after reading the paper? I feel that the experiment setup is sound in the stated sense: what does the network learn if it only ever sees pathological samples (i.e. where all latent variables are exactly correlated). However, the presented answers that networks tend to learn simpler cues, where simpler can be measured by Kolmogorov complexity are not overly surprising or counter-intuitive. Perhaps the authors can rephrase the contributions more clearly towards the benefits? For instance: With the tools, analysis and exposure of this paper, what can I now see different, do different, etc. What are possible next steps where stronger impact is on the horizon? The paper addresses an important topic, but I feel that the overall impact, measured by depth of presented contributions is too shallow.",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "TtQuFMuC8Lc",
                "writer": "author",
                "reply_to": "XPOiPxLU6FG",
                "title": "Response 1/2",
                "comment": " We thank the reviewer for the thoughtful and insightful review. We are glad that the reviewer recognises the significance of the topic and the benefit of our approach and analysis. Now, let\u2019s focus on the weaknesses pointed out by the reviewer. The pointed weaknesses are great points too; we have been able to deepen the arguments in the paper thanks to the reviewer\u2019s comments.\n\n> There is an inherent question that authors fail to sufficiently address. The selected \"cues\" for analysis have intrinsically different extraction demands from the stimuli, thus making the argument about them being \"equally prevalent\" moot. For example, accessing color is more direct (pixel-level information is directly available) than accessing, say, shape (for which a network needs to build sufficiently large receptive fields to understand the global geometry of a scene).\n\nTo start, we\u2019d like to differentiate what we observe from what we control. We have used terminologies like below throughout the paper:\n- equally conducive\n- equally valid\n- equally plausible\n- equally correlates with the targets\n- equally represented\n\nThey synonymously indicate the condition that the *degrees of correlation with the target label* for every cue are *identical*. This is what we control. What we observe and analyse, instead, is that there are \u201cdifferent extraction demands\u201d for each cue, as seen by the highly skewed likelihood of each cue being chosen by a deep model.\n\nThe reviewer is totally right about the fact that cues do have different intrinsic easiness of accessibility. Our work is focused on exactly this phenomenon. For example, when comparing cues such as color vs shape, color recognition only necessitates reading off pixel values (i.e. \u201cpixel-level information is directly available\u201d), while shape recognition requires something far more complex. So color is indeed easier. However, the question becomes highly non-obvious for other cues, such as shape versus orientation, or in the case of UTKFace, Age vs Gender. Both are high-level concepts. How do we generalize the concept of certain information being \u201cmore directly available\u201d to an arbitrary pair of cues? \n\nThis is where the mathematical framework comes in. The mathematical tool in our case is the *Kolmogorov complexity (KC)*. We propose a formal way to characterise the \u201cdirect availability\u201d of a cue via its readiness to be represented by a simple model. That is, a cue is \u201cdirectly available\u201d if and only if a small model ($f$ with small $K(f)$) can already effectively memorise the cue pattern (i.e. achieve $\\mathcal{L}(f;X,Y)<\\delta$); see Equation 6. The benefit of this characterisation is that one can apply the KC concept to an arbitrary cue, as long as we have access to some labelled data for the cue (X-Y pairs).\n\n> In naturalistic datasets, how can one determine or interpret a basis set of cues that are truly orthogonal dimensions? The experiments on WCST-ML work because the shape, color, scale, and orientation are by definition orthogonal. For all we know, the \"cues\" in naturalistic datasets could be abstract and not human-interpretable too.\n\nThanks for raising this point. WCST-ML experiments do not require the cues to be orthogonal. The point we try to convey is that deep models prefer certain cues over others. On UTKFace, we have shown that the ethnicity cues are preferred to gender or age, without relying on the orthogonality of the cues. It is also important to point out that, within WCST-ML, human interpretability is also not a requirement. We need a much weaker assumption: that we can put integer cue labels on samples. In the extreme case, we may also define cues A and B on the dataset by uniform-sampling the labels (random noise labels) twice for each input. We may still compare a model\u2019s preference between the cues A and B defined through the respective (X,Y) samples by performing the identical analysis as we have in Section 3. In this work, we chose orthogonal (e.g. shape and color) and interpretable (e.g. gender and ethnicity) cues as an illustrative and meaningful example to the readers, but should labels be available, WCST-ML and its analysis can also be used for an entangled and non-interpretable set of cues. We believe we had not appropriately discussed these points in our previous version, and we have thus updated the paper with this discussion (Section 2.1, below Proposition 1).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dBc32rfGaW",
                "writer": "author",
                "reply_to": "XPOiPxLU6FG",
                "title": "Response 2/2",
                "comment": " > Treating the number of model parameters as a proxy for the Kolmogorov complexity of the input-output mapping seems ill-advised. If one trains a parameter-shared recurrent neural network it can perform the same effective computations as a ResNet, but with much fewer parameters (though the task per se has not changed, and by extension the cues). I would appreciate it if the authors can offer some insight into this.\n\nThis is a great point. A crucial detail that we failed to emphasise in the submission is that we need to confine the computation of the number of parameters to a single model family. In our case, we have fixed the space of models to *ResNet20 with a varying number of channels* (Section 4, under paragraph \u201cMeasuring KCC\u201d). This prevents the ill-defined ordering of network simplicity that arises from comparing e.g. ResNet family versus RNN family, as pointed out by the reviewer. We have emphasised this strategy in the revised manuscript (Section 4, under paragraph \u201cMeasuring KCC\u201d).\n\n\n>  Figures need to be made more legible. Axis labels and text insets are barely visible without zooming in. Figure 4 could be refined further. The caption isn't very descriptive.\n\nWe have updated the figures as per the reviewer\u2019s comment.\n\nFigure 3, 4, 5, 6, B.1, C.1: Increased font sizes by 20%.\nFigure 4: We have updated the caption with more information.\n\n> The ethnicity solution doesn't seem to be \"characterized with a flatter and wider surface\" as compared to age and gender solutions in Fig. 4.\n\nOne can observe the flatter and wider surface for the ethnicity solution by comparing the size of the blue area against that for the other cues. This is more visible when viewed from the top (right plots for each cue). Albeit minute, we do see that the blue area is slightly wider for ethnicity than age and gender. We agree our description is quite strong, especially given the high subjectivity of the observation. We have fixed the description in the manuscript (Section 3.2, under paragraph \u201cQualitative view on the loss surface\u201d). As a means to complement the subjectivity of qualitative observations, we have introduced *quantitative metrics* measuring the width of the basin of attraction in Section 3.2 and Figure 5.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "pn-J6PWz1p2",
                "writer": "author",
                "reply_to": "gXTSZocs01",
                "title": "Response 1/2",
                "comment": " We thank the reviewer for the thorough review and insightful comments. We are glad the reviewer sees fundamental value in this path of investigation. We lay below our response to the main comments in the review.\n\n>  [W1]: Some parts of the exposition need minor clarification:\n\n>  (a) p6: \"The trend is clear for ResNet20 and ViT, while the ethnicity and gender preferences are within the error bars for FFnet.\" This refers to Figure 3, and I cannot understand this from Figure 3, in the least it is confusingly stated. The ethnicity and gender graphs for FFNet in figure 3 seem separated by several standard deviations according to the error bars.\n\nAs mentioned by the reviewer, the description referring to the trends of the FFNet model in Figure 3 was misleading and was referring to an older version of the figure. The up-to-date figure shows indeed a clear trend for all models. While the FFNet model shows a more variable performance across runs, the ranking of cues is preserved throughout. The statement has been updated accordingly in the manuscript (Section 3.1, under paragraph \u201cModels adopt cues with uneven likelihood\u201d).\n\n> (b) Figure 6, three right columns: What does the dot and the black arch signify? I was not able to gain this from text or caption. I assume that the zero loss path is the dashed line?\n\nThe three rightmost columns in Figure 6 are mainly based on Figure 1 of [1]. [1] has found that it is almost always possible to find a zero-loss **quadratic curve** between two solutions, while finding a zero-loss linear path is not as easy. A quadratic curve (more precisely a Bezier curve [1]) is parameterized with three points: two end points and the third point that determines the shape of the curve. In our plots, we have shown all three points as black dots, the zero-loss quadratic curve as the solid line, and the linear segment connecting the solutions as a dashed line (which does not guarantee zero loss). We have improved the explanation in the caption of Figure 6.\n\n[1] Garipov et al. Loss surfaces, mode connectivity, and fast ensembling of dnns. NeurIPS 2018.\n\n> [W2] A somewhat methodical weakness of the paper seems to be the problem statement. For instance in the introductory example of section one/figure 1, the authors imply that the model is biased towards scale (then shape, then color). I do agree that this is consistent with the model preferring the property of scale in making the decision. Is this bias, though? In order to evidence bias, I'd need a truth/label assignment on the off diagonals (e.g. indicating that blue small triangle is not supposed to be class 1). Without this truth assignment I do not see the bias of the model.\n\nWe believe the description for Figure 1 and the related text have not been exceptionally clear in showcasing the focal takeaways in WCST-ML, and could have led to misinterpretations of the Figure. In Figure 1, we show objects which vary in shape, scale and color. In the diagonal examples, these object cues are fully correlated, thus we have a small-red-circle (label 1), a medium-green-triangle  (label 2), and a large-blue-square (label 3). In the lower right corner of the figure, instead of implying which cue the example model is biased towards, we instead only show different possible scenarios:\n- If we observe that $f$(small-blue-triangle) = 1, then we can infer that $f$ is biased to scale.\n- If $f$(small-blue-triangle) = 2, then $f$ is biased to shape.\n- If $f$(small-blue-triangle) = 3, then $f$ is biased to color.\n\nThis is to illustrate how, by experiment design, we can determine which cue the model $f$ has been biased towards by looking at the off-diagonal samples. As the reviewer said, it is possible to reveal the bias for $f$ **if and only if** you have access to off-diagonal samples. If such off-diagonal samples are furthermore labelled, then we can even make a judgment on the model, for example on whether the model is biased towards helpful cues or wrong cues.\n\nThanks to this comment we have revised the caption for Figure 1 and the related paragraph to clarify these important aspects and improve the overall presentation of WCST-ML.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "48MreOloxmC",
                "writer": "author",
                "reply_to": "gXTSZocs01",
                "title": "Response 2/2",
                "comment": " > [W3] The main weakness of the paper seems to me that I am asking myself: What have I learned after reading the paper? I feel that the experiment setup is sound in the stated sense: what does the network learn if it only ever sees pathological samples (i.e. where all latent variables are exactly correlated). However, the presented answers that networks tend to learn simpler cues, where simpler can be measured by Kolmogorov complexity are not overly surprising or counter-intuitive. Perhaps the authors can rephrase the contributions more clearly towards the benefits? For instance: With the tools, analysis and exposure of this paper, what can I now see different, do different, etc. What are possible next steps where stronger impact is on the horizon?\n\nMany thanks for this comment. The reviewer\u2019s concerns are well received, and we take it upon ourselves to improve the discussion and conclusion section to better explain the practical benefits deriving from the experiments in this work.\n\nThe current form of the paper is focused more on conceptual contributions. As the reviewer has summarized well, we show that shortcut learning may not only stem from the disparities in cues\u2019 statistical correlations to the target task but also from their very nature: cues have different inherent Kolmogorov complexities (KC). An interesting implication of our conceptual contribution is that our conclusion goes against the common wisdom in machine learning: given simple and complex solutions, it is better to choose the simple one (Occam\u2019s razor). There are common DNN-training recipes designed to encourage simpler solutions, such as stochastic gradient descent, dropout, stochastic depth, and weight decay. An immediate future work would be to re-assess their utilities when applied to de-biasing or fairness scenarios, as they may encourage unwanted shortcuts.\n\nThe less pronounced yet crucial contribution of this paper is the toolbox for analysis. We have introduced the measurement of KC for generic cues, which will correlate well with the models\u2019 preference for the cues. There is nothing that stops us from computing the KC even for real-world cues; the only requirement is access to samples labelled according to the cue of interest. The estimated KCs may be useful for many downstream applications. For example, one may optimize the needed amount of the off-diagonal samples to de-bias a model based on the KC estimates. This will be highly useful in practice for reducing the data collection costs; collecting or synthesising the off-diagonal samples is typically expensive as they would seldom appear in nature (e.g. trucks on lakes).\n\nThe combination of the mode-connectivity technique [1] and the bias analysis technique in our paper provides further avenues for future research directions. In our paper, we have used the mode-connectivity technique to find a path from a preferred solution to an averted one along which the original-task loss (i.e. diagonal loss) is effectively zero. A particularly successful application of the mode-connectivity technique has been the ensemble of models along the path to improving generalisation [1]. Such a path ensemble tends to generate a more diverse set of models than locally perturbing the solutions [1]. We can extend this idea further in our setup where the endpoints of the path correspond to solutions attending to different cues. Samples along such a heterogeneous curve are likely to be much more diverse and could result in a more effective ensemble. We may also control the type and amount of bias in the resulting ensemble by adjusting the concentration of sampling probabilities along the curve.\n\nAbove are only a subset of possible research directions that our analyses and tools may inspire. We have revised the manuscript to aid readers in quickly grasping the contributions and gaining insights like the above. Please take a look at the added \u201ctoolbox contribution\u201d paragraph in Section 5 that summarises the above possibilities. Let us know if the reviewer believes further improvement will be necessary - we will be happy to engage in further discussions and revisions.\n\n[1] Garipov et al. Loss surfaces, mode connectivity, and fast ensembling of dnns. In NeurIPS 2018.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xsyV6BdF0OY",
                "writer": "author",
                "reply_to": "NRueFjy40Z_",
                "title": "Response 1/2",
                "comment": " We thank the reviewer for providing a thorough review of the paper, for highlighting relevant and important aspects of this work, and for providing us with valuable feedback to improve its presentation and engage a wider audience.\n\nBelow we will address the main comments/concerns in order. \n\n> Averted solution or averted cues are not introduced, before being used in section 3.2. What is the difference between averted and preferred? Section 3.2 seems to imply that preferred solutions are computed by optimizing the model using D_diag and averted solutions computed using D_i for a specific property i that is not deemed preferred. Am I correct? If this is the case, it seems unreasonable to be comparing the two solutions directly (e.g. in Fig 4) since they are computed using datasets of different sizes.\n\nThank you for asking for clarifications. The notion of preferred versus averted cues (and solutions likewise) is relative. Given a pair of cues A and B, we determine which one is preferred and averted based on a model\u2019s tendency to learn one over the other when trained on the diagonal set $\\mathcal{D}_\\text{diag}$. We had not previously introduced the terms unequivocally before using them in Section 3.2. We have inserted this definition at the end of Section 3.1 and added appropriate text to clarify the use of \u201cpreferred\u201d and \u201caverted\u201d in the manuscript. \n\nIt is true that one could often obtain the preferred solution by simply training on the $\\mathcal{D}_\\text{diag}$. However, as the reviewer has pointed out, this will lead to an imbalance in the number of training data for the preferred and averted solutions, introducing an uncontrolled factor. Instead of doing this, we always use the union of the diagonal and off-diagonal sets, denoted $\\mathcal{D}_k$ (Equation 2), for finding both preferred and averted solutions. Though redundant, this ensures a fair comparison. We have updated the paragraph \u201cHow to find the averted solutions\u201d in Section 3.2 with this description.\n\n\n\n> In Fig 4, it also is not clear what the loss landscape represents. Section 3.2 mentions that it's the loss around averted and preferred solutions, but Figure 4 does not mention which ones are averted and which ones are preferred. Also, Figure 4 says that the two directions of parameter variation were chosen at random. Given the high dimensionality of DNN parameters, it would be much more informative to depict several of these plots per solution, to cover more directions of variation.\n\nAs \u201caverted\u201d and \u201cpreferred\u201d solutions are relative concepts, it is not possible to label each cue in Figure 4 as either \u201caverted\u201d or \u201cpreferred\u201d without reference points. Instead, we have sorted the cues in the descending order of preference. For example, scale is a preferred cue against orientation but is an averted cue against shape.\n\nIt will help the reader to further clarify that we use different datasets, and consequently different loss functions, for finding the solutions versus for plotting the loss landscape in Figure 4. For finding the solutions (the center of the X-Y plane for each plot in Figure 4), we use the union of diagonal and off-diagonal sets, or $\\mathcal{D}_k$; see also the response above. For plotting the loss values (loss landscape for each plot in Figure 4), we only use the diagonal set $\\mathcal{D}_\\text{diag}$, which is shared by all cues. It important to notice that the solutions found with each union set $\\mathcal{D}_k$ are still meaningful to the original task $\\mathcal{D}_\\text{diag}$. The solutions found with $\\mathcal{D}_k$ **are** simultaneously the solutions to $\\mathcal{D}_\\text{diag}$, by Corollary 3. In other words, we have discovered a diverse set of solutions for the task $\\mathcal{D}_\\text{diag}$ with the help of off-diagonal augmentation $\\mathcal{D}_k$ with different $k$\u2019s. Now, since all the plots in Figure 4 represent the loss values for the diagonal set $\\mathcal{D}_k$, we can directly compare the loss landscapes across the solutions biased to different cues. Following the reviewer comment, we have revised the Figure\u2019s caption to clarify this important aspect.\n\nThe main purpose of Figure 4 was to share our observations and some intuition with the readers. We decided that supplying multiple samples of the 3D plots in a paper was not the best use of the medium. Instead, after sharing the intuition in Figure 4, we have presented a quantitative version of the observations in Section 3.2 and Figure 5, where we measure the breadth of the basin of attraction for each solution type. The breadth is measured in terms of the mean across multiple directions.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6zw0QvWvyx",
                "writer": "author",
                "reply_to": "NRueFjy40Z_",
                "title": "Response 2/2",
                "comment": " > Section 4 provides evidence to the idea that certain cues have higher complexity than others. This is measured by searching for the smallest model that can memorize the training set. Color could be memorized by a model with only 1.2K parameters, and orientations needed 273 K parameters. A natural question is then whether large models present a less degree of biases towards simpler cues. As the parameter count increases, the solution set for the orientation task must also increase. It would be interesting to see if the model preference for more complex cues would also increase, or if solution sets for simpler cues would still dominate for very large models.\n\nGreat question. We believe this is a good chance to clear up our conclusions at different model sizes. Let us consider, for example:\n- Model **S**mall: 10K parameters\n- Model **L**arge: 1M parameters\n\nModel **S** is capable of representing color but not orientation. In the parameter space, there are color-biased solutions but no orientation-biased ones. \n\nModel **L** is fully capable of representing both color and orientation, with both types of solutions in the parameters space. Thus, the reviewer is correct to say that compared to model **S**, the larger model **L** has a greater volume of orientation-biased solutions in the parameter space.\n\nThe main setup the paper is concerned with is the models of type **L**, when a model is capable of representing both simple and complex cues. Our empirical and theoretical conclusion is that even with the stated capabilities, such models prefer simple cues. In the parameter space, color-biased solutions are exponentially more abundant than orientation-biased ones (Section 4, $p(f)\\lesssim 2^{-a\\widetilde{K}(f)+b}$). We expect it to be difficult to reverse models\u2019 natural inclinations towards simplicity by merely increasing the model size.\n\nWhat is not inferred from our studies is whether the preference to simple cues will become less glaring as the model size increases far beyond **L**. It would be a great finding if there is a continual increase in the relative volume of complex solutions in larger models. But at the same time, one needs to keep in mind that the relative volume of complex solutions is always bound to be exponentially smaller than that of simple solutions (Section 4, $p(f)\\lesssim 2^{-a\\widetilde{K}(f)+b}$).\n\n\n> Although the diagonal dataset construct is appropriate to see which cues are preferred by a model, in practice this construct rarely applies. Real data may present correlations similar to those found in the diagonal construct, but will also contain off-diagonal samples. Thus, an interesting analysis would be to measure how much you'd need to deviate from D_diag to obtain an \"averted solution\". In other words, if we train a model with (100-p)% diagonal samples and p% off-diagonal samples labelled according to cue i (e.g. labelled for orientation), how likely is the model to correctly predict cue i for increasing values of p?\n\nThank you for this interesting comment. The setup of the experiment is indeed purposefully extreme to let us understand the results better. The experiment proposed by the reviewer is very pertinent and practically meaningful. We expect to observe that an averted cue will require more off-diagonal samples (p%) to increase its ranking, and vice versa.\n\nAlthough pressed for a limited time, we are currently attempting to generate additional results following the reviewer\u2019s interesting thought experiment. We will include them in the supplementary materials should this be possible within the rebuttal deadline.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gZoaeAqqTa1",
                "writer": "author",
                "reply_to": "iclr_2022_qRDQi3ocgR3",
                "title": "Summary of changes in the revision.",
                "comment": " We summarise the changes made in the revision. In the updated PDF, the changes are marked with **red text**.\n\n### Section 1 (Introduction)\n\n* **Figure 1**. Revised the caption and the related paragraph to clarify the conclusion from the illustration of WCST-ML. [Reviewer QJQp] \n\n\n### Section 2 (Setup)\n\n* **Section 2.1**, below Proposition 1: Elaborate on the conditions for the cues to be analysed with WCST-ML. [Reviewer QJQp]\n\n\n### Section 3 (Observations)\n\n* **Section 3.1, under paragraph \u201cModels adopt cues with uneven likelihood\u201d**: Fixed the description for Figure 3 to correctly capture the observation. [Reviewer QJQp]\n\n* **End of Section 3.1**: Inserted definitions for \u201cpreferred\u201d and \u201caverted\u201d cues. [Reviewer s3vA]\n\n* **Section 3.2, under paragraph \u201cHow to find the averted solutions\u201d**: Explain how we match the amount of training data for preferred and averted solutions. [Reviewer s3vA]\n\n* **Section 3.2, under paragraph \u201cQualitative view on the loss surface\u201d**: Downtoned the description. [Reviewer tkjD]\n\n* **Figure 4**: Updated the caption with more information. [Reviewer tkjD] Clarified the underlying dataset $\\mathcal{D}_k$ based on which we plot the loss values. [Reviewer s3vA]\n\n* **Figure 6**: Included description of markers and curves in the caption. [Reviewer QJQp]\n\n\n### Section 5 (Discussion and Conclusion)\n\n* **Paragraph \u201cToolbox contribution\u201d**: Inserted a discussion on our toolbox contributions. [Reviewer QJQp]\n\n\n### Overall\n\n* **Figure 3, 4, 5, 6, B.1, C.1**: Increased font sizes by 20%. [Reviewer tkjD] \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "NRueFjy40Z_",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_qRDQi3ocgR3",
                "title": "",
                "comment": "The paper conducts a study of which visual cues are preferred by current vision models. The paper designs a training setup with several cues where each cue is equally correlated with the image label. The paper shows that visual cues like color are much easier to be learned by a vision model, than other cues such as orientation and shape. The paper also provides evidence that easy-to-learn cues tend to converge to relatively flat minima and models that prefer these cues are more abundant in parameter space. The paper is a very interesting read. It provides an interesting analysis of model preferences for various visual cues. The analysis shows the preference of visual models towards low complexity visual cues, such as color and ethnicity. \n\nMain comments:\n\n- Averted solution or averted cues are not introduced, before being used in section 3.2. What is the difference between averted and preferred? Section 3.2 seems to imply that preferred solutions are computed by optimizing the model using D_diag and averted solutions computed using D_i for a specific property i that is not deemed preferred. Am I correct? If this is the case, it seems unreasonable to be comparing the two solutions directly (eg in Fig 4) since they are computed using datasets of different sizes.\n- In Fig 4, it also is not clear what the loss landscape represents. Section 3.2 mentions that it's the loss around averted and preferred solutions, but Figure 4 does not mention which ones are averted and which ones are preferred. Also, Figure 4 says that the two directions of parameter variation were chosen at random. Given the high dimensionality of DNN parameters, it would be much more informative to depict several of these plots per solution, to cover more directions of variation.\n- Section 4 provides evidence to the idea that certain cues have higher complexity than others. This is measured by searching for the smallest model that can memorized the training set. Color could be memorized by a model with only 1.2K parameters, and orientations needed 273K parameters. A natural question is then whether large models present less biases towards simpler cues. As the parameter count increases, the solution set for the orientation task must also increase. It would be interesting to see if the model preference for more complex cues would also increase, or if solution sets for simpler cues would still dominate for very large models.\n- Although the diagonal dataset construct is appropriate to see which cues are preferred by a model, in practice this construct almost never applies. Real data may present correlations similar to those found in the diagonal construct, but will also contain off-diagonal samples. Thus, an interesting analysis would be to measure how much you'd need to deviate from D_diag to obtain an \"averted solution\". In other words, if we train a model with (100-p)% diagonal samples and p% off-diagonal samples labeled according to cue i (e.g. labeled for orientation), how likely is the model to correctly predict cue i for increasing values of p? The paper is a very interesting read. It provides an interesting and insightful analysis of model preferences for various visual cues. The analysis shows the preference of visual models towards low complexity visual cues, such as color and ethnicity. Given the potential impact of the analysis, I recommend the paper to be accepted. I would nevertheless strongly encourage the authors to address my main comments/concerns.\n",
                "rating": 8,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "this paper provides an interesting and original framework for analysis of learning models",
                "Sentiment Expression": "interesting and original",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "SkgVRiC9Km": {
        "paper_id": "iclr_2019_SkgVRiC9Km",
        "paper_title": "Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations",
        "paper_abstract": "Deep networks have achieved impressive results across a variety of important tasks. However, a known weakness is a failure to perform well when evaluated on data which differ from the training distribution, even if these differences are very small, as is the case with adversarial examples.  We propose \\emph{Fortified Networks}, a simple transformation of existing networks, which \u201cfortifies\u201d the hidden layers in a deep network by identifying when the hidden states are off of the data manifold, and maps these hidden states back to parts of the data manifold where the network performs well. Our principal contribution is to show that fortifying these hidden states improves the robustness of deep networks and our experiments (i) demonstrate improved robustness to standard adversarial attacks in both black-box and white-box threat models; (ii) suggest that our improvements are not primarily due to the problem of deceptively good results due to degraded quality in the gradient signal (the gradient masking problem) and (iii) show the advantage of doing this fortification in the hidden layers instead of the input space.  We demonstrate improvements in adversarial robustness on three datasets (MNIST, Fashion MNIST, CIFAR10), across several attack parameters, both white-box and black-box settings, and the most widely studied attacks (FGSM, PGD, Carlini-Wagner).  We show that these improvements are achieved across a wide variety of hyperparameters.  ",
        "paper_acceptance": "rejected-papers",
        "meta_review": "This paper suggests a method for defending against adversarial examples and out-of-distribution samples via projection onto the data manifold. The paper suggests a new method for detecting when hidden layers are off of the manifold, and uses auto encoders to map them back onto the manifold. \n\nThe paper is well-written and the method is novel and interesting. However, most of the reviewers agree that the original robustness evaluations were not sufficient due to restricting the evaluation to using FGSM baseline and comparison with thermometer encoding (which both are known to not be fully effective baselines). \n\nAfter rebuttal, Reviewer 4 points out that the method offers very little robustness over adversarial training alone, even though it is combined with adversarial training, which suggests that the method itself provides very little robustness. ",
        "meta_review_title": "The ideas are quite novel and promising, but there is no sufficient justification of claims made",
        "reviews": [
            {
                "review_id": "ryxxQlaWy4",
                "reply_to": "BylDnFpep7",
                "title": "New Results Summary",
                "comment": "Hello, \n\nWe've updated the paper with new results on the PGD attack with many more iterations, architectures (including large architectures like wideresnet), and setups (especially see Tables 2 and 3).  This directly addresses the over-reliance on FGSM as an attack, which was the focus of Ian Goodfellow's comment.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1gedxOiCX",
                "reply_to": "S1gcR8XcAX",
                "title": "New experiments show significantly lower gains from fortification",
                "comment": "In the new experiments conducted by the authors on the two ResNet models, the additional benefits of fortification are even less significant (about 1%, close to error margins). \n\nIf this degree of robustness was attained by a technique which completely *replaces* adversarial training, I think it would indeed be valuable. But in this paper, the proposed method *augments* adversarial training with additional loss terms, and so one can argue that most of the robustness comes from adversarial training itself and the benefits of the fortified layers are marginal. \n\nThus, based on the empirical results, I do not think that the contribution of the proposed approach as a defense against adversarial attacks is sufficient.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1gcR8XcAX",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Rebuttal Summary and Highlights",
                "comment": "We thank all of the reviewers and commenters for their feedback, which has done a great deal to improve the quality of the paper. The main points raised by reviewers and commenters were related to the experimental results, the motivation, gradient obfuscation tests, and related work. All points have been addressed in the revised manuscript, and are summarized in the following.\n\n1.  Stronger Attacks: We strongly agree that the FGSM attack is not a strong attack and to that end we have conducted new experiments against the PGD attack with up to 200 steps as well as a range of epsilons from 0.03 to 0.3 (Table 2).  The improvements from Fortified Networks with 200 steps are similar to the improvements over baseline with 7 steps, and also Fortified Networks improve results over the baseline when using larger epsilons.  \n\n2.  Motivation for Fortified Networks: we have clarified that our motivation for fortified networks is that the autoencoders map some points from off of the manifold back onto the manifold.  This in turn reduces the potential space of adversarial examples (because most of the space is off-manifold), which then makes adversarial training more efficient. These off-manifold points are not necessarily adversarial examples and not all adversarial examples are off the manifold (Gilmer 2018).  However, our main claim is that some of the adversarial examples are off of the manifold, and thus when we use adversarial training, it is more effective and efficient when we have the autoencoders in the network.  \n\n3.  Gradient Obfuscation and Masking: We strongly agree that it is important to show that the improvements are due to actual improvements in robustness and not merely a degradation in the quality of the gradient signal.  To address this, we have run PGD with a greater number of steps (up to 200).  We have also run some variants of the attack which address issues related to gradient obfuscation (Table 2).  For example we have run with larger epsilons and found that the model is still able to find adversarial attacks.  Additionally we have run the network without noise and with attacks where gradient skips the autoencoder (BPDA), and found that Fortified Networks still improve robustness in both cases.  \n\n4.  Baselines and Related Work: We have added new results with PreActResNet18 and WideResNet28-10 on CIFAR-10 (Table 3), which are relatively competitive architectures.  In both cases we found significant improvements using Fortified Networks and intriguingly we saw almost no change in the clean test accuracy.  This is strong evidence that the resulting improvement does not trivially come from added capacity, as was suggested as a possibility by R4.  Additionally we conducted an experiment where we simply added a square loss on the hidden layers (similar to ALP except on all layers) and found that this did not improve results.  \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkeHBHy50Q",
                "reply_to": "SklG26PgAm",
                "title": "Clarification",
                "comment": "\"- Table 2 CIFAR-10 argues PGD eps=0.03 error of the baseline network is 38.1%.\n- Table 8 CIFAR-10 argues PGD eps=0.03 error of the baseline network is 33.0% or 31.4% for 7 or 200 (respectively) iterations of gradient descent. Why is this different? How many iterations did you use in Table 2?\n\n- Table 7 argues 100 iterations of PGD at eps=0.03 has an error rate of 35.3% on \"basline with extra layers\"\n- Table 8 argues 50/200 iterations of PGD at eps=0.03 has an error rate of 32.5/32.2 (respectively for the same model. Because 50<100<200 I would expect that the 35.3 should be something smaller. Why is this?\u201d\n\nBecause Fortified Networks adds capacity to the model, using the network without the fortified layers is a weak baseline.  The discrepancy that you point to results from two different ways of adding activations to the baseline model.  Essentially, the lower result uses the same number of layers with activations as fortified networks, but the higher number has more activations, and in some sense this makes it a higher capacity model.  Nonetheless the paper has been updated with the discrepancy explained (Table 2).  \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bkgo0MaF0m",
                "reply_to": "SyeyA7BlA7",
                "title": "Thanks for the Feedback - Response",
                "comment": "\u201c1. The proposed method is not an alternative to adversarial training, but instead augments it with an additional objective from the denoising autoencoder. The authors are also claiming only ~5% improvement over the baseline. One might argue that the benefits of the proposed approach over adversarial training are marginal. Even if we assume that the 5% is significant, it is not clear how accurate the baseline evaluation is. I agree with one of the anonymous comments in this regard. The authors use a non-standard model, and their PGD baseline is quite a bit lower than the state-of-the-art. I would really like to see the results on a state-of-the-art model to be convinced that the benefit is not just an artifact of a weak baseline.\u201d\n\nWe conducted experiments using two much stronger models: PreActResNet18 and WideResNet28-10.  All experiments ran for 200 epochs.  \n\nPreActResNet18\nBaseline: 37.87 (20 step PGD), 84.93% (clean test accuracy)\nFortified Networks: 39.2% (20 step PGD), 84.84% (clean test accuracy)\n\nWideResNet28-10:\nBaseline: 43.28% (20 step PGD), 87.42% (clean test accuracy)\nFortified Networks: 44.06% (20 step PGD), (87.40% clean test accuracy)\n\n\u201c2. If I correctly understand the new results posted by the authors, their model obtains ~10-13% accuracy against an Linf adversary of eps>0.1 on CIFAR-10. It has been shown that an eps~0.125 is already too large - one can perturb the image to actually be from another class (also shown in the ICLR submission that the authors linked - \u201cRobustness may be at odds with accuracy\u201d https://openreview.net/forum?id=SyxAb30cY7). I do not understand how the fortified model can get an accuracy > 0% for such large epsilons, which are probably impossible to be robust to. Have the authors checked what the adversarial examples look like for these large eps? What about trying a nearest neighbor attack from the test set? Seeing a non-zero robust accuracy to such large epsilons makes me doubt the correctness of the attack setup within the experimental evaluation.\u201d\n\nOur robustness with an epsilon of 0.3 is very similar to what\u2019s reported in (Madry 2018), especially Figure 6c: \n\nhttps://openreview.net/pdf?id=SyxZJn05YX\n\n\nOne possibility is that for some examples, it is possible to find a real example with a different class within an epsilon ball of size 0.3 - but there is a small fraction of examples where this isn\u2019t possible.  \n\n\u201c3. The proposed defense seems to use random noise (as part of the denoising stage). Have the authors tried multiple gradient queries per PGD step? \u201c\n\nWe conducted a very similar experiment to this where we ran both the forward and the backward pass without any injected noise and we showed that Fortified Networks retained a significant improvement over the baseline.  \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJe8N-s_CQ",
                "reply_to": "HyxDFKXdC7",
                "title": "Difference Between the Papers",
                "comment": "Hello, \n\nOur method and the \"High-Level Representation Guided Denoiser\" are very different.  We ran our attacks and evaluate on the full model, end-to-end, including the autoencoders. This is a major difference from [1], and that change is what broke the paper you referenced.  The paper that you referenced did not perform adversarial training on the main part of the network, and only trained the autoencoder, keeping the classifier network itself fixed.  \n\nWe also conducted an experiment with BPDA (Athalye 2018), where we consider skipping the autoencoders in the backward pass (i.e. using the identity function to compute the gradients) as well as running the forward and backward pass of the network with no noise injected and we produced, and the advantage of fortified networks was preserved.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyxDFKXdC7",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "A closed related paper",
                "comment": "Hi,\nI found the idea is very similar to \"Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser\"\nhttp://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Liao_Defense_Against_Adversarial_CVPR_2018_paper.pdf\n\nCould you please clarify the difference between your work and this paper? Thanks.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkenZJ6URQ",
                "reply_to": "SyeyA7BlA7",
                "title": "Comment 2. does not seem fully accurate",
                "comment": "https://arxiv.org/pdf/1706.06083.pdf \n\nThe original paper linked above claims an accuracy of ~10% at eps=30/255 (eps~0.118). I do not think the argument that \"adversarial examples exist => PGD will find it\" is accurate. In fact, https://arxiv.org/pdf/1706.06083.pdf  has a disclaimer that suggests such points may well be present and PGD may well be unable to find them.\n\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklESyJGCm",
                "reply_to": "r1gVS6NaaX",
                "title": "This paper seems to indicate ALP results in robustness, comparable to your approach",
                "comment": "https://arxiv.org/pdf/1810.12042.pdf\n\nHowever, they may be complementary and can be combined?",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklG26PgAm",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Confusion over new results",
                "comment": "I'm having a hard time interpreting the new results.\n\n- Table 2 CIFAR-10 argues PGD eps=0.03 error of the baseline network is 38.1%.\n- Table 8 CIFAR-10 argues PGD eps=0.03 error of the baseline network is 33.0% or 31.4% for 7 or 200 (respectively) iterations of gradient descent. Why is this different? How many iterations did you use in Table 2?\n\n- Table 7 argues 100 iterations of PGD at eps=0.03 has an error rate of 35.3% on \"basline with extra layers\"\n- Table 8 argues 50/200 iterations of PGD at eps=0.03 has an error rate of 32.5/32.2 (respectively for the same model. Because 50<100<200 I would expect that the 35.3 should be something smaller. Why is this?\n\nBecause the improvement gain for fortified networks is relatively small, these ~5% differences add up. Are they due to random initializations? In that case, could we get some margin-of-error results for these tables?",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Syell2DxRQ",
                "reply_to": "ByxB567yRQ",
                "title": "Makes sense",
                "comment": "Thank you, that makes sense.\n\nI agree running a resnet would be very important. Madry et al. state that one of the reasons their defense works is that you have to have large network capacity.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyeyA7BlA7",
                "reply_to": "H1e2sO9567",
                "title": "Response to Experiments",
                "comment": "I am still not convinced by the empirical evaluation performed by the authors. My concerns are:\n\n1. The proposed method is not an alternative to adversarial training, but instead augments it with an additional objective from the denoising autoencoder. The authors are also claiming only ~5% improvement over the baseline. One might argue that the benefits of the proposed approach over adversarial training are marginal. Even if we assume that the 5% is significant, it is not clear how accurate the baseline evaluation is. I agree with one of the anonymous comments in this regard. The authors use a non-standard model, and their PGD baseline is quite a bit lower than the state-of-the-art. I would really like to see the results on a state-of-the-art model to be convinced that the benefit is not just an artifact of a weak baseline.\n\n2. If I correctly understand the new results posted by the authors, their model obtains ~10-13% accuracy against an Linf adversary of eps>0.1 on CIFAR-10. It has been shown that an eps~0.125 is already too large - one can perturb the image to actually be from another class (also shown in the ICLR submission that the authors linked - \u201cRobustness may be at odds with accuracy\u201d https://openreview.net/forum?id=SyxAb30cY7). I do not understand how the fortified model can get an accuracy > 0% for such large epsilons, which are probably impossible to be robust to. Have the authors checked what the adversarial examples look like for these large eps? What about trying a nearest neighbor attack from the test set? Seeing a non-zero robust accuracy to such large epsilons makes me doubt the correctness of the attack setup within the experimental evaluation.\n\n3. The proposed defense seems to use random noise (as part of the denoising stage). Have the authors tried multiple gradient queries per PGD step? \n\n4. I would also like to see the standard (non-robust) accuracies of the models (specifically the  baseline model, baseline with extra layers and fortified networks) to make sure that the 5% gain in robustness is not an artifact of larger expressivity of the proposed model.\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ByxB567yRQ",
                "reply_to": "B1ekccmyAQ",
                "title": "Reason",
                "comment": "Thanks, the reason is that the baseline is a 4-layer CNN and not a resnet.  When we run with the resnet our results are about the same as Madry, but our goal in the rebuttal has been to get the results with as many types of attacks/setups as possible to ensure that the improvements are a not result of gradient masking.  \n\nWe can add more experiments with ResNets as well.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1ekccmyAQ",
                "reply_to": "H1e2sO9567",
                "title": "Why is your baseline weaker than Madry et al.?",
                "comment": "You claim that PGD adversarial training as a baseline gives a robustness of 35% at eps=0.03. However, to the best of my knowledge, no prior paper has reduced the accuracy below 44%.\n\nCan you account for this difference? Are you able to lower the accuracy the Madry et al. defense to 38%? \n\nWhile a gap of ~6% might not typically be important, you are only claiming a gain of about 5%. So in this case, it's absolutely critical that we can be sure it's not just that you have a weak baseline you're comparing against.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1gVS6NaaX",
                "reply_to": "ByxZaNAChm",
                "title": "Thanks for your feedback",
                "comment": "We thank the commenter for their valuable feedback and suggestions for more thorough experimentation. We have run many of the suggested tests to address the question of gradient obfuscation, which was also raised by others.\n\n\u201cWhy is CIFAR only evaluated against FGSM? Shouldn't you at least try PGD on CIFAR-10? Why not try out PGD/CW on CIFAR-10?  It is not obvious that the method will scale to complex datasets such as CIFAR-10 (leave alone Imagenet).\u201d\n\nWe have added new results with PGD on CIFAR-10 with many more iterations at evaluation time.\n\n# steps | Baseline | Baseline w/ extra layers | Fortified Networks\n    7 steps | 33.0 | 34.2 | 45.0\n  50 steps | 31.6 | 32.5 | 42.1\n200 steps | 31.4 | 32.2 | 41.5\n\n\u201cFor how many iterations was PGD run? I think this information is critical. How many random restarts? There is some recent work (https://arxiv.org/abs/1810.12042) that indicates large number of restarts/iteration steps might be necessary for a meaningful evaluation\u201d\n\nWe have added new results with PGD run for many iterations (up to 200), and with several restarts (up to 50), as well as for different epsilon values (0.03 to 0.3). Our model outperforms baseline models in all cases, demonstrating effectiveness of the method even under these more difficult conditions.\n\n\u201cWhy not baseline against adversarial logit pairing? (investigations by third parties have shown that while ALP does not help as much as claimed with Imagenet, it does help with CIFAR and MNIST).\u201d\n\nWe ran ALP-like experiments, wherein we added an adversarial loss on the hidden layers instead of adding fortified layers . We could not achieve competitive performance with this system. In fact, it did not perform better than just an adversarially trained baseline system, however, we have not exhaustively explored this approach.  \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgZu_ba6X",
                "reply_to": "H1e2sO9567",
                "title": "SPSA?",
                "comment": "Can you try out SPSA and confirm your results? The public discussion on the link below shows that PGD with large iterations cannot actually detect masked gradients fully, but SPSA sort of pretty much cancels all their gains from their method!\n\nSee discussion on this page. \nhttps://openreview.net/forum?id=Bylj6oC5K7&noteId=H1leI9Iah7",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJxi0d9q6Q",
                "reply_to": "B1li57iX6Q",
                "title": "Motivation for Fortified Networks",
                "comment": "\u201c I do not really understand the motivation behind using an autoencoder here. Firstly, it is not clear that adversarial examples lie off the data manifold - they could form a very small set on the data manifold and thereby not affect standard generalization.\u201d\n\nOur main claim is that using a model which can perform reconstruction can map points off of the data manifold back onto the manifold.  For example, we can imagine that unusual noise patterns would not appear in the reconstructions.  These off-manifold points are not necessarily adversarial examples and not all adversarial examples are from off of the manifold (Gilmer 2018).  However, our claim is only that *some* of the adversarial examples are off of the manifold, and thus when we use adversarial training, it is more effective and efficient when we have the autoencoders in the network, as it reduces the space that we need to search over.  \n\nEvidence that some adversarial examples are off of the manifold (at least for an undefended network) is in our paper in figure 1.  Some additional qualitative evidence supporting this claim is provided by another submission.  Figure 2 and Figure 3 of the \u201cRobustness May be at Odds with Accuracy\u201d paper (https://openreview.net/pdf?id=SyxAb30cY7), show the perturbations for a defended model appear to be somewhat unrealistic (although much less so then for an undefended model).  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1e2sO9567",
                "reply_to": "B1li57iX6Q",
                "title": "Thanks - Response to Comments on Experiments and Gradient Obfuscation",
                "comment": "Thank you for your feedback.  We strongly agree that it is absolutely essential to show that the improvements are not a result of gradient obfuscation.  \n\n\u201cThe authors mostly evaluate their defense using FGSM (particularly on CIFAR). To truly establish the merit of a new defense, the authors must benchmark against state-of-the-art defenses such as PGD. \u201d\n\nWe added new results with PGD on CIFAR-10 with many more PGD-steps for evaluation.  We evaluated a convolutional network on CIFAR-10 with 4 convolutional layers followed by a single fully-connected layer.  We trained fortified networks, where we added an autoencoder following each hidden layer.  We also added a baseline \u201cExtra Layers\u201d where we trained with the layers added to match the capacity of Fortified Networks (same number of parameters).  \n\n# steps | Baseline | Baseline w/ extra layers | Fortified Networks\n    7 steps | 33.0 | 34.2 | 45.0\n  50 steps | 31.6 | 32.5 | 42.1\n200 steps | 31.4 | 32.2 | 41.5\n\nEven when running PGD for 200 steps, we found large and consistent advantages for fortified networks, which are not primarily attributable to adding additional layers.  \n\n\u201cIt also seems like the epsilon values used for the PGD attacks are fairly small. The authors should report accuracies to a range of epsilon values for the PGD attack, as is standard.\u201d\n\nThis is a great point and we performed an additional experiment using the same convolutional neural network discussed above.  Using 7 and 100 steps of PGD, we attacked our fortified nets model with varying epsilons: \n\nPGD, 100 steps\nEpsilon | Baseline with extra layers | Fortified Networks\n0.03 | 35.3 | 39.2\n0.04 | 24.8 | 28.0\n0.06 | 14.3 | 15.6\n0.08 | 12.0 | 13.0\n  0.1 | 11.7 | 12.9\n  0.2 | 10.2 | 11.3\n  0.3 |   8.4 | 9.6\n\n\u201cWhen the authors attack their models using PGD/FGSM, is this only on the classification loss or does this also include the denoising terms? Similar defenses which use denoisers have been broken once you run PGD on the full model [1].\u201d\n\nWe run our attacks on the full model, end-to-end, including the autoencoders. This is a major difference from [1], and that change is what broke the paper you referenced.  The paper that you referenced did not perform adversarial training on the main part of the network, and only trained the autoencoder, keeping the classifier network itself fixed.  \n\nWe also conducted a new experiment with BPDA (Athalye 2018), where we consider skipping the autoencoders in the backward pass (i.e. using the identity function to compute the gradients) as well as running the forward and backward pass of the network with no noise injected.  \n\nWe also ran some new experiments for this using eps=0.03 and 100 steps of PGD using the same CNN architecture discussed earlier.  \n\n33.4 (baseline, normal attack)\n40.1 (Fortified Networks, normal attack)\n38.2 (Fortified Networks, no noise during attack)\n67.1 (Fortified Networks, skip DAE during attack, BPDA)\n\nThis is strong evidence that skipping the autoencoders while generating the attacks significantly weakens them, but turning the noise off slightly strengthens the attack, but it is still much stronger as a defense than the baseline adversarially trained model with the same number of parameters and capacity.  \n\n\u201cSecondly, have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline\u201d\n\nYes, we conducted new experiments to directly address this issue (Adversarial Logit Pairing is a special case of the regularizer that you describe), but we also note that our method provides improvements even when we don\u2019t use the L_adv loss comparing the adversarial input\u2019s hidden states to the clean input\u2019s h.  This is with the same CNN architecture discussed earlier.  \n\nPGD, 7 iterations:\n43.3 (Fortified Networks)\n38.1 (Adv. training baseline)\n34.2 (Penalty between layers)\n\nPGD, 100 iterations:\n39.2 (Fortified Networks)\n35.3 (Adv. training baseline)\n32.2 (Penalty between layers)\n\nWe found that this penalty between the hidden states, where we attracted the hidden states in the network on adversarial inputs to the hidden states of the network on clean states (applied at every layer), hurts robustness somewhat, but it may be possible that such an approach depends on exactly how it\u2019s used.  We also add that unlike adversarial logit pairing, our improvements hold up after running PGD for a large number of iterations, whereas the benefits from adversarial logit pairing almost entirely disappear.  \n\n\u201cAlso, which approach are the authors denoting as \u2018baseline adv. Train\u2019 in the tables?\u201c\n\nThis refers to the PGD training of Madry 2017.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bye09P9caQ",
                "reply_to": "rygsxS39nm",
                "title": "Thanks for the feedback",
                "comment": "\u201cThe major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved.\u201c\n\nWe thank the reviewer for the positive and constructive feedback.\n\nWe also like to point out that we\u2019ve conducted new experiments to help to demonstrate that our method isn\u2019t benefiting from obfuscated gradients and additionally we ran PGD attacks with many more iterations (200) on CIFAR-10 (see the response to reviewer 4).  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1eCNv5c6X",
                "reply_to": "BylDnFpep7",
                "title": "Thank you for your feedback.",
                "comment": "\u201cThis paper presents an approach of fortifying the neural networks to defend attacks. The major component should be a denoising autoencoder with noise in the hidden layer.\nHowever, from the paper, I am still not convinced why this defends the FGSM attack. From my perspective, a more specifically designed algorithm could attack the network described in the paper as the old way\u201d\n\nIn our experiments (except where we explicitly test against a special BPDA) we backpropagate errors through the autoencoders, such that the the autoencoders are not hidden from the attacker.  Indeed we found that skipping the autoencoders when running the attacks makes them significantly weaker, but in our main experiments we backpropagate through the autoencoders and allow the attacker to use this information.  \n\n\u201cand what is the insight of defending the attacks, whether this objective function is harder to find to adversarial examples, or have to use more adversarial examples?\u201d\n\nOur main claim is that using a model which can perform reconstruction can map points off of the data manifold back onto the manifold.  For example, we can imagine that unusual noise patterns would not appear in the reconstructions.  These off-manifold points are not necessarily adversarial examples and not all adversarial examples are from off of the manifold (Gilmer 2018).  However, our claim is only that *some* of the adversarial examples are off of the manifold, and thus when we use adversarial training, it is more effective and efficient when we have the autoencoders in the network, as it reduces the space that we need to search over.  \n\nEvidence that some adversarial examples are off of the manifold (at least for an undefended network) is in our paper in figure 1.  Some additional qualitative evidence supporting this claim is provided by another submission.  Figure 2 and Figure 3 of the \u201cRobustness May be at Odds with Accuracy\u201d paper (https://openreview.net/pdf?id=SyxAb30cY7), show the perturbations for a defended model appear to be somewhat unrealistic (although much less so then for an undefended model).  \n\n\u201cAnother problem rise from Ian Goodfellow's comment. I am trying not to be biased. So if the author could address his comments properly, I am willing to change the rating.\u201d\n\nWe strongly believe that it is essential to show that the improvements do not result from gradient obfuscation as well as to demonstrate improvements against strong attacks (such as PGD) on CIFAR-10. We have thus run additional experiments demonstrating effectiveness of the method on CIFAR-10, on a CNN as well as a ResNet architecture. We ran validation experiments to confirm that our method does not simply operate by obfuscating gradients.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyeaoM55aX",
                "reply_to": "Skew_xxphQ",
                "title": "Thanks for the feedback",
                "comment": "\u201cThe method works by substituting a hidden layer with a denoised version. \nNot only it enable to provide more robust classification results, but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution.\nImprovements in adversarial robustness on three datasets are significant.\nBibliography is good, the text is clear, with interesting and complete experimentations.\u201d\n\nThank you for your feedback. We have obtained several new results to address concerns related to gradient obfuscation raised by other reviewers and the public comment.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJeUD3u5a7",
                "reply_to": "SJeYd58kpQ",
                "title": "Thanks",
                "comment": "We have removed the thermometer coding reference from the results table and we have also run new experiments to attack fortified networks using BPDA.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HkeWZ2_qT7",
                "reply_to": "r1eUwHib6Q",
                "title": "Main Claim Clarification",
                "comment": "\u201cIs there any proof that using an autoencoder maps the data back in the manifold? Especially against adversarial perturbations?\u201d\n\nTo clarify: our motivation is that the autoencoders map some points from off of the manifold back onto the manifold.  This in turn reduces the potential space of adversarial examples (because most of the space is off-manifold), which then makes adversarial training more efficient. These off-manifold points are not necessarily adversarial examples and not all adversarial examples are off the manifold (Gilmer 2018).  However, our main claim is that some of the adversarial examples are off of the manifold, and thus when we use adversarial training, it is more effective and efficient when we have the autoencoders in the network.  \n\nEvidence that some adversarial examples are off of the manifold (at least for an undefended network) is in our paper in figure 1.  Some qualitative evidence supporting this claim is provided by another submission.  Figure 2 and Figure 3 of the \u201cRobustness May be at Odds with Accuracy\u201d paper (https://openreview.net/pdf?id=SyxAb30cY7), show the perturbations for a defended model appear to be somewhat unrealistic (although much less so then for an undefended model).  \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1li57iX6Q",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Empirical results are not sufficient to demonstrate the strength of the proposed defense",
                "comment": "This paper proposes a new defense to adversarial examples based on the 'fortification' of hidden layers using a denoising autoencoder. While building models that are robust to adversarial examples is an important and relevant research problem, I am not convinced by the evaluation of the defense.  Specific comments:\n\n- The authors mostly evaluate their defense using FGSM (particularly on CIFAR). To truly establish the merit of a new defense, the authors must benchmark against state-of-the-art defenses such as PGD. It also seems like the epsilon values used for the PGD attacks are fairly small. The authors should report accuracies to a range of epsilon values for the PGD attack, as is standard.\n\n- When the authors attack their models using PGD/FGSM, is this only on the classification loss or does this also include the denoising terms? Similar defenses which use denoisers have been broken once you run PGD on the full model [1].\n\n- I do not really understand the motivation behind using an autoencoder here. Firstly, it is not clear that adversarial examples lie off the data manifold - they could form a very small set on the data manifold and thereby not affect standard generalization. Secondly, have the authors tried a simple regularization loss based on the error between hidden layer representations to a natural examples and the corresponding adversarial example? I think the authors must motivate the use of denoising autoencoders here by comparing to such a simple baseline.\n\nGeneral comment: The results hard to parse given the arrangement of figures and tables. Also, which approach are the authors denoting as \u2018baseline adv. Train\u2019 in the tables? \n\nOverall I feel like building defenses to adversarial examples is a challenging problem and the empirical investigation in this paper is not sufficient to illustrate any real progress on this front.\n\n[1] Athalye, A., & Carlini, N. (2018). On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses. arXiv preprint arXiv:1804.03286.\n",
                "rating": 4,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1eUwHib6Q",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Is there proof for the main claim?",
                "comment": "Is there any proof that using an autoencoder maps the data back in the manifold? Especially against adversarial perturbations? \n\nHave the authors tried their method with networks that include residual connections? It will be interesting to verify that mapping back to the manifold indeed works with such connections that can amplify perturbations through the skip connections.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BylDnFpep7",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "A sensible approach, but needs to justify the experiments more strongly",
                "comment": "This paper presents an approach of fortifying the neural networks to defend attacks. The major component should be a denoising autoencoder with noise in the hidden layer.\n\nHowever, from the paper, I am still not convinced why this defends the FGSM attack. From my perspective, a more specifically designed algorithm could attack the network described in the paper as the old way, and what is the insight of defending the attacks, whether this objective function is harder to find to adversarial examples, or have to use more adversarial examples?\n\nAnother problem rise from Ian Goodfellow's comment. I am trying not to be biased. So if the author could address his comments properly, I am willing to change the rating.",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Skew_xxphQ",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Improving the robustness of deep Networks by modeling the manifold of hidden representations is original, efficient and well motivated",
                "comment": "The method works by substituting a hidden layer with a denoised version. \nNot only it enable to provide more robust classification results, but also to sense and suggest to the analyst or system when the original example is either adversarial or from a significantly different distribution.\nImprovements in adversarial robustness on three datasets are significant.\n\nBibliography is good, the text is clear, with interesting and complete experimentations.",
                "rating": 9,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rygsxS39nm",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "good work",
                "comment": "In this paper, the authors proposed a fortified network model, which is an extension to denoising autoencoder. The extension is to perform the denoising module in the hidden layers instead of input layer. The motivation of this extension is that the denoising part is more effective in the hidden layers. Overall, this extension is quite sensible, and empirical results justify the utility of this extension. The major issue, which was left as an open question in the end of Section 3, is that when and where to use fortified layers. The authors discussed this issue, but did not solve this issue. Nevertheless, I do believe solving this issue requires a sequence of papers. Overall the paper reads very well, but there are a number of minor places to be improved. \n\n \n(1) a grammar error at \"provide a reliable signal of the existence of input data that do not lie on the manifold on which it the network trained.\"\n\n(2) a grammar error at \"This expectation cannot be computed, therefore a common approach is to to minimize the empirical risk\"\n\n(3) The sentence \"For a mini-batch of N clean examples, x(1), ..., x(N), each hidden layer h(1)_k, ..., h(N)_k is fed into a DAE loss\" is a little confusing to me. \"h(1)_k, ..., h(N)_k\" is only for one hidden layer, rather than \"each hidden layer\". Right?",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SJeYd58kpQ",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Thermometer coding does not improve adversarial robustness",
                "comment": "I haven't read this paper but a colleague told me that it quotes the accuracy numbers from the original Buckman et al paper and uses them as a point of comparison. I'm a co-author of thermometer coding, and I'm here to say it's important to understand that a new attack, BPDA, was able to break the model from our paper: https://arxiv.org/abs/1802.00420\n\nIn our own follow-up experiments, we found that if we retrain using BPDA for adversarial training, models that use thermometer coding perform about the same as models that use real numbers for input. Thus it's probably best to just use adversarial training as the baseline.\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkxYnt8JpQ",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "FGSM is not a strong attack",
                "comment": "I'm not trying to weigh in on whether or not the paper should be accepted and I haven't read the paper; I'm just trying to provide the reviewers with good information on how to interpret FGSM experiments. I'm commenting because a colleague told me that this information would be relevant to reviewing this paper.\n\nI developed the FGSM attack, and I'd like to comment that it's not intended to be a strong attack.\n\nThe FGSM was mostly intended to be used for a scientific experiment to show that linear information is sufficient to break undefended neural nets. It's not meant to be a strong attack.\n\nUntil a few years ago, FGSM was also a good \"unit test\" to see if a defense was strong. By now, I personally don't even use FGSM as a unit test anymore. Performance on FGSM does not correlate well with performance on the strongest attacks.\n\nIt's fine if you want to use FGSM as a unit test but success on FGSM shouldn't be regarded as strong evidence that a defense works in a particular threat model. The reviewers should check what specific claims are made in the paper and if there are claims of a strong defense these claims should be supported by something other than FGSM.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ByxZaNAChm",
                "reply_to": "iclr_2019_SkgVRiC9Km",
                "title": "Experimental Evaluation Not Convincing",
                "comment": "I like the writing, but I have some core problems with the experimental evaluation. \n\nSome questions: \n1. Why is CIFAR only evaluated against FGSM? Shouldn't you at least try PGD on CIFAR-10? Why not try out PGD/CW on CIFAR-10?  It is not obvious that the method will scale to complex datasets such as CIFAR-10 (leave alone Imagenet). \n\n2. Why not try out NES/SPSA/ElasticNet attacks as evidence against gradient-masking?\n\n3. Blackbox accuracy seems to be slightly worse than white-box.  Is this a sign that there is some gradient masking going on? \n\n4. For how many iterations was PGD run? I think this information is critical. How many random restarts? There is some recent work (https://arxiv.org/abs/1810.12042) that indicates large number of restarts/iteration steps might be necessary for a meaningful evaluation\n\n5. Why not baseline against adversarial logit pairing? (investigations by third parties have shown that while ALP does not help as much as claimed with Imagenet, it does help with CIFAR and MNIST). \n\nThe evidence against gradient masking given in the paper is also presented by ALP (https://arxiv.org/abs/1803.06373). But, this paper (https://arxiv.org/abs/1807.10272) shows that these signs may very well be present in defenses that rely on gradient obfuscation. \n\nOverall, there are no theoretical guarantees and I am not convinced that there are actually any gains compared to ALP/other SOTA defenses... especially with the fact that the possibility of gradient obfuscation has not been fully explored, and that most experiments are limited to MNIST!",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "well-written",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the method",
                "Sentiment Expression": "novel and interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the original robustness evaluations",
                "Sentiment Expression": "not sufficient",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the method",
                "Sentiment Expression": "offers very little robustness",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "rC3zu-OqnII": {
        "paper_id": "nips_2021_rC3zu-OqnII",
        "paper_title": "A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms",
        "paper_abstract": "One of the key drivers of complexity in the classical (stochastic) multi-armed bandit (MAB) problem is the difference between mean rewards in the top two arms, also known as the instance gap. The celebrated Upper Confidence Bound (UCB) policy is among the simplest optimism-based MAB algorithms that naturally adapts to this gap: for a horizon of play n, it achieves optimal O(log n) regret in instances with \"large\" gaps, and a near-optimal O(\\sqrt{n log n}) minimax regret when the gap can be arbitrarily \"small.\" This paper provides new results on the arm-sampling behavior of UCB, leading to several important insights. Among these, it is shown that arm-sampling rates under UCB are asymptotically deterministic, regardless of the problem complexity. This discovery facilitates new sharp asymptotics and a novel alternative proof for the O(\\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also provides the first complete process-level characterization of the MAB problem in the conventional diffusion scaling. Among other things, the \"small\" gap worst-case lens adopted in this paper also reveals profound distinctions between the behavior of UCB and Thompson Sampling, such as an \"incomplete learning\" phenomenon characteristic of the latter.\n",
        "paper_acceptance": "accept",
        "meta_review": "This is strong paper that I would like to see accepted as a spotlight. There is a minor criticism concerning the restriction to two arms but the reviewers feel that this is not a major concern. There are some suggestions by the reviewers of how to improve the paper further and it would be good if the authors could incorporate these.  ",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "pm_RS3rq2CH",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_rC3zu-OqnII",
                "title": "",
                "comment": "The paper studies the arm sampling behavior of UCB and Thompson sampling algorithms. For the two-arm case, the asymptotic behavior of arm sampling is characterized for different regimes (small, large, and medium) of suboptimality gap. Using this characterization, the minimax regret of UCB is shown to O(n\\logn), where n is the time horizon. They highlight the incomplete learning phenomenon in Thomson sampling where sample-split could be arbitrarily imbalanced along a sample path even when both the arms have the same mean.  The paper highlights the important aspect of arm sampling distribution in the Multi-armed setting. The author nicely built the imbalances in arm sampling through examples for both UCB and Thompson sampling. The insights from the analysis highlight the 'incomplete learning' learning aspect in Thompson sampling.\n\nI have the following question for the authors: \n\nTheorem 1: The result holds aysmptotically and $\\Delta$ (problem instance) is changing with $n$. After $n$ rounds (sufficiently large), we expect UCB to have played the optimal arm exponentially more time than the sub-optimal arm. But the asymptotic result suggests that this need not be the case if $\\Delta \\rightarrow 0$ as $n \\rightarrow$. Can anything be said about the arm-sampling distribution for a fixed instance $\\Delta$. The asymptotic result is good, but since we also know finite time behavior, it is nice to connect.\n\nTheorem 2: What does the symbol $\\implies$ denote (convergence?). The statement assumes $n\\rightarrow \\infty$, but the Proof Sketch says for any $n \\in \\mathcal{N}$, which one is correct.\n\nTheorem 3: Does the result hold with weaker condition $\\Delta \\rightarrow 0$ Instead of thanking $\\Delta \\sim \\sqrt{\\frac{\\theta \\log n}{n}}$.\n\nOverall nice paper. Good insights. If authors could highlight the practical implications of their results, it could be nice. For example, is incomplete learning of Thompson Sampling good or bad. When should UCB be preferred over Thompson sampling and vice versa?\nDoes the incomplete learning aspect explain why Thompson sampling often performs better empirically in practice?\n\nI would be happy to revise the scores post rebuttal. \n-----\n\nPost rebuttal:\n\nThanks for clarifying some of my points. I have increased the score to 7.\n Yes",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "Jok7EEtDcpV",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_rC3zu-OqnII",
                "title": "",
                "comment": "Classical bandit algorithms such as UCB and Thompson Sampling are well-understood in terms of their performance\u2014for example, an upper bound on their regret\u2014but, until recently, little attention has been given to their actual behavior\u2014for example, the rates at which specific arms are sampled. This work seeks to address these questions by analyzing the arm sampling distribution achieved by both UCB and Thompson Sampling. They provide results characterizing these distributions asymptotically and, in addition, characterize the distribution of the rewards obtained by running UCB.  While the regret incurred by UCB and Thompson Sampling (TS) is well-understood, this work seeks to understand the actual behavior of these algorithms. It makes several contributions in this direction, all in the two-armed bandit problem (though some of the results extend to K arms). In particular, they characterize the:\n- Asymptotic distribution of N_{I*}(n)/n achieved by UCB, where N_{I*}(n) is the number of pulls of the optimal arm up to time n, for various regimes of the gap.\n- Asymptotic distribution of N_{I*}(n)/n achieved by TS but in a restricted regime where the rewards are deterministic.\n- Precise regret incurred by UCB asymptotically, establishing that R_n -> c(\\theta)*\\sqrt{n log n} for some problem-dependent constant c(\\theta).\n- Distribution of the random process of the UCB\u2019s reward and regret.\n\nAll results are asymptotic in nature (i.e. the number of samples n -> infinity). In addition, the authors provide several numerical examples illustrating that in practice UCB and TS can exhibit surprising behavior, and show that they are able to explain this behavior with their theoretical results.\n\nPros:\n- This is a timely work. In the last few years, a significant amount of attention has been given to understanding the asymptotically optimal regret of bandit problems (e.g. [4]), as well as properties of UCB (for example, the bias in the estimates of the arm means maintained by UCB [1]). Very recently [5] studied the asymptotic sampling distribution of Thompson Sampling but, to my knowledge, no other works on this exist, though open questions still remain.\n- The results are extensive and provide a full characterization of the behavior of the UCB algorithm, greatly deepening our understanding of its properties. The motivating examples help make clear that there are unusual and little-understood effects that occur when running UCB and TS.\n\nCons:\n- The main results only hold for a two-armed bandit model, though they are partially extended in the appendix to K armed models (the restriction to two-armed bandits has precedence in the literature though, for instance [4]). \n- The results on Thompson Sampling only holds for deterministic bandits. Furthermore, [5] studies the asymptotic distributions of the arm pulls of Thompson Sampling and should be cited and compared against. \n- There\u2019s a line of recent work ([1]-[3]) which aims to understand the bias of optimistic bandit algorithms. While not directly comparable to this work, it is related to the theme of understanding the behavior of bandit algorithms, and should be cited.\n- Some discussion on the applications of these results would be interesting. For example, do these results motivate any improvements to UCB or other algorithms?\n\nOverall, this is a thorough study of the performance of bandit algorithms, greatly deepens our understanding of both UCB, and warrants an accept. \n\n----------------------------\n\nUpdate after rebuttal: I would like to maintain my score after reading the rebuttal. I believe this work makes fundamental, novel contributions to our understanding of bandits and deserves an accept.\n\n\n[1] Shin, Jaehyeok, Aaditya Ramdas, and Alessandro Rinaldo. \"Are sample means in multi-armed bandits positively or negatively biased?.\"\u00a0arXiv preprint arXiv:1905.11397\u00a0(2019).\n[2] Shin, Jaehyeok, Aaditya Ramdas, and Alessandro Rinaldo. \"On the bias, risk and consistency of sample means in multi-armed bandits.\"\u00a0arXiv preprint arXiv:1902.00746\u00a0(2019).\n[3] Shin, Jaehyeok, Aaditya Ramdas, and Alessandro Rinaldo. \"On conditional versus marginal bias in multi-armed bandits.\"\u00a0International Conference on Machine Learning. PMLR, 2020.\n[4] Kaufmann, Emilie, Olivier Capp\u00e9, and Aur\u00e9lien Garivier. \"On the complexity of best-arm identification in multi-armed bandit models.\"\u00a0The Journal of Machine Learning Research\u00a017.1 (2016): 1-42.\n[5] Kalkanli, Cem, and Ayfer Ozgur. \"Asymptotic Convergence of Thompson Sampling.\"\u00a0arXiv preprint arXiv:2011.03917\u00a0(2020). The authors do not discuss potential societal impact. However, the work is primarily theoretical so it is difficult to determine what negative societal impacts it may have. Limitations are discussed. ",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "NgxfVezX3oG",
                "writer": "author",
                "reply_to": "Jok7EEtDcpV",
                "title": "Author-response",
                "comment": " Thank you for your interest in our work and the time spent reviewing our manuscript. We greatly appreciate your feedback; point-wise responses to your main remarks are provided below.\n\n$(1)$ Extension to the K-armed setting. Although a full-scale generalization of Theorem 1 (and other results) will likely involve messy book-keeping (since $K(K-1)/2$ pairwise gaps are involved), this can be done in principle. Overall, we felt such a development is less central than other results derived in the paper given the space limits and scope. \n\n$(2)$ Comparison of Theorem 2 with suggested reference [5]. Thank you for bringing this work to our notice. While [5] indeed characterizes the asymptotic distribution of arm-pulls under Thompson Sampling, they consider the Bayesian setting where a prior distribution exists over problem instances, and information about said prior is baked into the Thompson Sampling algorithm. A sample-path of the algorithm in their model involves, in particular, a $random$ problem instance from the instance-space. In contrast, the derivation of asymptotic distribution of arm-pulls in our work is for specific $(fixed)$ problem instances, viz., reward configurations (I) and (II) described in Theorem 2, and under the classical Beta-Bernoulli version of Thompson Sampling. The two works are not comparable but are complementary, and hopefully combine to aid our understanding of Thompson Sampling. We will remark upon this in the revision.\n\n$(3)$ On suggested references [1,2,3,4]. Thank you for pointing out these works; we will include them in our literature survey as appropriate.\n\n$(4)$ Potential improvements to UCB/Thompson Sampling. As you presciently noted, the broader question posed by our work is indeed whether it is possible to design a \"best of both worlds\" algorithm with desirable properties of Thompson Sampling (better empirical performance in \"well-separated\" instances vis-`a-vis UCB) as well as that of UCB (approximately \"balanced\" sample-split w.h.p. when the gaps are \"small/moderate,\" as opposed to the \"imbalance\" under Thompson Sampling). Such a possibility will give the user a lever to tune an appropriate operating point on the \"Pareto frontier\" of performance based on the desired level of trade-off between the competing objectives of regret minimization and ex post causal inference. This aspect is currently under investigation.\n\nWe have noted the minor points raised as well, and will duly address them in the revision. Thank you again for all the helpful comments. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zcXwIRgIHEW",
                "writer": "author",
                "reply_to": "-qDRi_Jh5oW",
                "title": "Author-response",
                "comment": " Thank you for your interest in our work and the time spent reviewing our manuscript. We greatly appreciate your feedback; point-wise responses to your main remarks are provided below.\n\n$(1)$ Clarification on notation. We regret the confusion our notation might have caused. Indeed, your interpretation in terms of a sequence of two-armed bandit instances indexed by $n$ (the horizon of play) is correct. We will make the notation unambiguous in the revision; thank you for pointing this out.\n\n$(2)$ On publication of codes. We have noted this and will certainly do the needful; thank you for pointing this out.\n\n$(3)$ Generalization to the $K$-armed case. Although a full-scale generalization of Theorem 1 (and other results) to K-MAB will likely involve messy book-keeping (since $K(K-1)/2$ pairwise gaps are involved), this can be done in principle (A simple extension of Theorem 1 to the $K$-armed setting is provided in Appendix B for illustrative purposes.). Overall, we felt such a development is less central than other results derived in the paper given the space limits and scope. \n\n$(4)$ Results for Thompson Sampling. This is still an evolving landscape. A recent paper [Lin Fan and Peter W Glynn. Diffusion approximations for Thompson sampling. arXiv preprint arXiv:2105.09232, 2021] appeared post our submission; cited paper derives a diffusion limit for Thompson Sampling in the Gaussian bandit setting (distinct from our Beta-Bernoulli setting) and does so using a very different framework that is not directly applicable to our setting. Moreover, unlike the closed-form limit for UCB (Theorem 4 in our paper), the limit-process for Thompson Sampling can only be characterized in terms of solutions (possibly non-unique) to a stochastic ordinary differential equation driven by a time-changed Brownian motion. The complex nature of the limit has to do with the non-degeneracy of the distribution of $N_i(n)/n$ under Thompson Sampling as $n\\to\\infty$, when $\\Delta \\asymp 1/\\sqrt{n}$; a special case of this phenomenon is the \"imperfect learning\" observable in instances with \"zero gap,\" which Theorem 2 in our paper formalizes in the deterministic setting. It is readily observable empirically that non-degeneracy in the asymptotic distribution of $N_i(n)/n$ persists up to diffusion-scale $\\mathcal{O}\\left( 1/\\sqrt{n} \\right)$ gaps under Thompson Sampling, both in the Beta-Bernoulli as well as the Gaussian setting (see [25] for a few examples in the Gaussian setting). Theoretical development in this area is still in its initial stages at the moment, and we hope Theorem 2 provides a useful starting iterate for further investigations into this aspect of Thompson Sampling.\n\nWe have noted the minor points raised as well, and will duly address them in the revision. Thank you again for the careful reading. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H8q2sZetPN",
                "writer": "author",
                "reply_to": "K8UhrYHsqZ5",
                "title": "Author-response",
                "comment": " Thank you for your interest in our work and the time spent reviewing our manuscript. We greatly appreciate your feedback; point-wise responses to your main remarks are provided below.\n\n$(i)$ On the connection between Theorem 4 and the derivation in [25]. The distinctions have been discussed in Lines 317-320, and again in Lines 344-354 in the paper. In a nutshell, [25] uses the martingale framework developed by [Daniel W Stroock and SR Srinivasa Varadhan. Multidimensional diffusion processes. Springer, 2007] to characterize the diffusion limit for algorithms satisfying certain regularity conditions, whereas we derive Theorem 4 for UCB [6] directly from first principles. More importantly, our work is independent of [25] as the latter's framework cannot be applied to UCB due to a violation of the aforementioned regularity conditions under UCB. In fact, the framework of [25] is quite limited insofar as its applicability to the study of bandit algorithms; in addition to  UCB, both versions of Thompson Sampling discussed in [2] also remain outside the ambit of the analysis in [25].\n\n$(ii)$ On the difficulty of deriving diffusion approximations for general algorithms. As noted in the paper, the fact that $N_i(n)/n \\xrightarrow{p} 1/2$ for both arms $i=1,2$ under UCB when $\\Delta \\asymp 1/\\sqrt{n}$, is crucial for the diffusion-limit process to admit a closed-form characterization as stated in Theorem 4. Theorem 2 shows that the aforementioned condition $\\left(N_i(n)/n \\xrightarrow{p} 1/2\\right)$ is provably violated under Thompson Sampling in the Beta-Bernoulli bandit setting. In fact, similar empirical observations are available also in the Gaussian bandit setting. The [Fan and Glynn] reference you mention, derives a diffusion-limit process for Thompson Sampling in the Gaussian bandit setting using a very different framework that is not directly applicable to our setting. Moreover, unlike our result for UCB, the limit-process in their work can only be characterized in terms of solutions (possibly non-unique) to a stochastic ordinary differential equation driven by a time-changed Brownian motion. This characterization has to do with the non-deterministic nature of $N_i(n)/n$ under Thompson Sampling when $\\Delta \\asymp 1/\\sqrt{n}$. Presently, we are unaware of a general strategy for deriving diffusion approximations for bandit algorithms. Our approach is tailored to UCB (and UCB-like) algorithms under which $N_i(n)/n \\xrightarrow{p} c$ when $\\Delta \\asymp 1/\\sqrt{n}$, where $c$ is some constant bounded away from $0$ and $1$. The work in [Fan and Glynn] is specific to Thompson Sampling in the Gaussian bandit setting. As discussed earlier, [25] proposes a general framework to derive diffusion approximations. However, their conditions are restrictive and the classical versions of UCB as well as Thompson Sampling remain outside the ambit of their analysis. \n\n$(iii)$ Miscellaneous. A simple extension of Theorem 1 to the $K$-armed setting is  provided in Appendix B. A full-scale generalization involves messy book-keeping since $K(K-1)/2$ pairwise gaps are involved, but this can be done in principle (though we felt it is outside the scope of the present conference paper).  Of the three references suggested, [Fan and Glynn] is certainly relevant to our work; thank you for bringing this to our notice. [Hirano and Porter] and [Araman and Caldentey] study diffusion limits in broader settings involving sequential experiments (which are quite distinct in nature from our bandit paradigm); the intersection with these papers is minor, and it is worth noting that their analysis does not directly pertain to analysis of UCB and similar bandit algorithms. \n\nWe have noted the minor points raised as well, and will duly address them in the revision. Thank you again for the helpful comments. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mXxv-VTSiHz",
                "writer": "author",
                "reply_to": "pm_RS3rq2CH",
                "title": "Author-response",
                "comment": " Thank you for your interest in our work and the time spent reviewing our manuscript. We greatly appreciate your feedback; point-wise responses to your main remarks are provided below.\n\n$(i)$ $Theorem1.$ Finite-time results are indeed well-known for UCB. However, the assertion that the optimal arm is played exponentially more often than the sub-optimal ones is true only when the instance is \"well-separated.\" In the context of the two-armed problem we study, a \"well-separated\" instance corresponds to a fixed value of $\\Delta$ that is bounded away from $0$ and independent of the horizon of play $n$ (basically, $\\Delta$ being some $positive$ absolute constant). Such an instance is an element of the \"large gap\" regime where $\\Delta = \\omega \\left( \\sqrt{{\\log n}/{n}} \\right)$. In the \"large gap\" regime, the finite-time behavior of the right tail of the distribution of $N_i(n)/n$ ($i$ being the inferior arm among the two) under UCB is well-documented in [5] (see the the literature review section of the main paper at Line 120). However, the results in [5] become vacuous if $\\Delta$ scales with $n$ and vanishes at $\\mathcal{O} \\left( \\sqrt{{\\log n}/{n}} \\right)$ rates. One of the technical contributions of our paper is deriving finite-time tail bounds for $N_i(n)/n$ in the $\\Delta = \\mathcal{O} \\left( \\sqrt{{\\log n}/{n}} \\right)$ regime. We derive and use such bounds in the proof of Theorem~1, but the details are currently included only in the supplementary material due to space constraints; we will revise the exposition of the main paper based on your suggestion.\n\n$(ii)$ $Theorem2.$ '$\\implies$' denotes weak convergence, as noted in Line 175. The proof sketch remarks that in the $q=1$ case, $N_1(n)$ is uniformly distributed over $\\{0,1,...,n\\}$ for any $n\\in\\mathbb{N}$. The assertion in part (II) of the theorem follows as a corollary to the aforementioned statement; to the best of our understanding there is no contradiction here. The full technical details can be found in Appendix~G.\n\n$(iii)$ $Theorem3.$ The result as stated is specific to the \"moderate gap\" regime where $\\Delta  \\asymp \\sqrt{{\\log n}/{n}}$. The proof sketch of Theorem~3 (Line 298-305) remarks that the limit of ${R_n^\\pi}/{\\sqrt{n\\log n}}$ is non-trivial only in the \"moderate gap\" regime. In other regimes of $\\Delta$, viz., $\\omega\\left( \\sqrt{{\\log n}/{n}} \\right)$ \"large\" and $o\\left( \\sqrt{{\\log n}/{n}} \\right)$ \"small\" gaps, the limit is $0$.\n\n$(iv)$ $Miscellaneous.$ We will place a greater emphasis on the practical implications of the phenomena of \"incomplete learning\" under Thompson Sampling and \"balanced sample-split\" under UCB; thank you for the suggestion. On your last point regarding \"incomplete learning,\" we do indeed believe that it is strongly tied to Thompson Sampling's better empirical (regret) performance vis-\\`a-vis UCB. Having said that, it seems far from obvious since the state-of-the-art minimax regret bounds in prior work are $\\mathcal{O}\\left( \\sqrt{n\\log n} \\right)$ for both algorithms. For UCB, our present work improves upon the state-of-the-art by showing an $\\Omega \\left( \\sqrt{n\\log n} \\right)$ lower bound; thus the minimax regret of UCB is, in fact, $\\Theta\\left( \\sqrt{n\\log n} \\right)$. For the version of Thompson Sampling studied in this paper (Beta priors and Bernoulli likelihoods), unfortunately, no lower bound on the minimax regret is currently known in the literature. As a result, despite compelling empirical evidence, the claim that Thompson Sampling incurs a smaller regret than UCB is still not theoretically supported. However, it is quite likely that the claim is true, considering well-known related results for the minimax regret of Thompson Sampling with Gaussian priors and Gaussian likelihoods, which is known to be $\\Theta\\left(\\sqrt{n}\\right)$ [2]. Furthermore, \"incomplete learning\" is also observable empirically under said algorithm. The same is true also for other minimax-optimal algorithms such as MOSS [Audibert and Bubeck, Minimax policies for adversarial and stochastic bandits, COLT 2009]; a minimax regret of $\\Theta\\left(\\sqrt{n}\\right)$ and an empirically observable \"incomplete learning\" phenomenon. We do believe based on these observations that the \"incomplete learning\" phenomenon for an algorithm is somehow connected to its minimax optimality. This also suggests that it may very well be possible to \"shave off\" the $\\sqrt{\\log n}$ term from the current state-of-the-art $\\mathcal{O}\\left( \\sqrt{n\\log n} \\right)$ bound for the Beta-Bernoulli version of Thompson Sampling studied in this paper. The theoretical development in this space, however, is in a very nascent stage at the moment, and much remains to be done to iron out these conjectures. We hope the present work helps build some foundations towards these future investigations. \n\nWe have noted other minor points raised as well, and will duly address them in the revision. Thank you again for the many helpful suggestions. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "K8UhrYHsqZ5",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_rC3zu-OqnII",
                "title": "",
                "comment": "The authors study the asymptotic behavior of arm-sampling distributions under the UCB and the Thompson sampling. They provide an asymptotic characterization of the distributions, and show the arm sampling rates asymptotically deterministic regardless of the hardness of instances. With this characterization, focused on canonical UCB algorithm, they provides the first algorithm-specific worst case bound and the first diffusion-limit performance.\n  \nOriginality:\nTo the best of my knowledge, this is the first work to provide algorithm-specific result (Theorem 3). Also, it is interesting that they provide another view of proving the worst-case performance of UCB using a diffusion scaling approach (Theorem 4). Although the paper is not the first work introducing the diffusion scaling to bandit problems, it can deal with more standard bandit algorithms. As for the originality of proof of Theorem 4, proof techniques seem to be similar to that of [25]. Is there any difficulty to apply a diffusion scaling technique to more standard bandit algorithms?\n\nQuality:\nThe proposed theorems seem to be technically sound. The authors honestly discuss about the strengths and weaknesses.\n\nClarity:\nThe paper is well-written and provides appropriate examples. I can easily follow the motivation. Since the diffusion scaling is a key technique of the main contribution, more descriptions about it helps us to understand the details of the contributions.\n\nSignificance:\nAs written in the originality, the provided results are the first works, which is the significant point of the paper. Since the works include some limitations that deals with two-armed bandit settings, it is an important first step to explore other bandit algorithms and settings .\n A limitation of that paper is to deal with two-armed bandit settings, but as described in the concluding remarks in the paper, it may be generalized K-armed bandit. So, I think the contributions outweigh this drawback.\n\nI found the following researches that also consider the asymptotic regime using a diffusion approximation. It would be better to add some if possible.\n\nReferences:\n\n[i] Keisuke Hirano and Jack R Porter. Asymptotic representations for sequential experiments. Cowles Foundation Conference on Econometrics, 2021\n\n[ii] Fan, Lin, and Peter W. Glynn. \"Diffusion Approximations for Thompson Sampling.\" arXiv preprint arXiv:2105.09232 (2021).\n\n[iii] Araman, Victor F., and Rene Caldentey. \"Diffusion Approximations for a Class of Sequential Testing Problems.\" arXiv preprint arXiv:2102.07030 (2021).\n",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "-qDRi_Jh5oW",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_rC3zu-OqnII",
                "title": "",
                "comment": "The paper studies the asymptotical behaviors (with respect to the suboptimality gap $\\Delta$) of key statistics in the standard upper confidence bound (UCB) algorithm in standard multi-armed bandits (MAB). The contributions are as follows.\n\n1. The paper proves that the **asymptotical arm-sampling rate** converges to a constant, and gives its **analytical form** (Eq. 2). In particular, the paper discovers a non-trivial \"**moderate gap**\" regime for UCB. \n2. The paper proves an asymptotic regret lower bound for UCB **up to $(1 + o(1))$**  in two-armed bandits. \n3. The paper proves that the **asymptotical empirical sum** converges to a Brownian motion for UCB in two-armed bandits.\n4. The paper proves the asymptotical arm-sampling rate for Bernoulli Thompson sampling in two-armed *deterministic* bandits. \n5. The authors extend their results to MAB.  \nWhile asymptotical analyses are common in statisics and EE (e.g. mean-field asymptotics), it is not common in CS and rare in bandits literature. Asympotical analyses are important because it could help both researchers and practitioners better understand the algorithmic behaviors, because non-asymptotic analyses are usually not as exact and precise as their asymptotical counterparts. \n\nThis paper studies two-armed bandits, the classical and fundamental task in bandits literature, and analyzes UCB, the classical and fundamental algorithm in bandits literature. The most important statistics maintained by the UCB algorithm are the number of times of each arm being pulled and the empirical sum of each arm. For these two statistics, the paper provides exact asymptotical characterization under the simplest two-armed bandits case, which clearly distinguishes this paper from previous papers. The results are significant and set up seminal directions in bandits literature, and could benefit subsequent researchers, including myself. I believe the related works are cited adequately. \n\n\nThe paper is very well-written. The maths are easy to read and can be clearly verified in a step-by-step manner. I went through Appendices E and F (which contains main technical proofs) and did not find apparent errors. The authors are honest about their weakness about their results in the final section. \n\nThe paper almost completely depicts how standard UCB works in two-armed bandits. The results on other tasks such as multi-armed bandits and other algorithms such as Thompson sampling are, however, still premature. Further will be discussed in the limitations section. Nonetheless, given the novelty, quality, and clarity of this submission, I vote for a strong accept. \n### Limitations\n\n1. The authors' notations confuse me, in that it's not clear that *which limiting regime* the authors are discussing. It seems to me that Theorem 1 is stated for a sequence of two-armed bandits instances, i.e. for each $n$, there's an instance with parameters $(\\mu_{1, n}, \\mu_{2, n})$ such that $\\Delta_n = \\mu_{1, n} - \\mu_{2, n}$ satisfies $\\Delta_n \\asymp 1/\\sqrt{n}$, and that the $N_{i^*}(n) / n$ term in the result of Theorem 1 refers to the $N_{i^*}(n) / n$ in the $n$-th instance. The confusions exist at least for Theorem 1, 3, 4. Since asymptotical style results are not common in bandits literature, I recommend the authors to point out the asymptotical regime more clearly. \n2. The paper includes numerical experiments, so I think the section 3 in the checklist (Line 436) shall not be completed by \"N/A\", and the authors are encouraged to publish their codes, (though numerical experiments might be very simple).\n3. The paper fails to fully extend the results (Theorems 1, 3, 4) to the MAB case. The two-armed case is limited from both theoretical and practical perspective. In particular, in the two-armed case, the arm pulling rates satisfy $N_1(n) = n - N_2(n)$, which makes it possible to fully characterize the bandits by only analyzing one statistics $N_1(n)$. However, in the multi-armed case, at least two statistics need be studied. The paper did not prove the most intriguing \"moderate gap\" part in the multi-armed case.  \n4. The paper's results on Thompson sampling are quite limited, in that it only studies the deterministic case where rewards are constantly $0$ or $1$ for each arm. Although the authors somehow suggest that the results should extend to the stochastic setting by doing experiments (Fig. 1), I am still concerned about the results.  \n\n### Other Comments\n\n* Line 128: \"diffuion-limit\" should be \"diffusion-limit\"\n* Line 144-145: The sentence almost repeats the same sentence at Line 137-138, same thing at Line 149-150; the authors need not repeat the claim of first characeterization three times in the same paragraph\n\n### Societal Impact\n\nThe paper is mainly theoretical, and I don't see any potential negative societal impact.",
                "rating": 9,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This",
                "Sentiment Expression": "is strong paper that I would like to see accepted as a spotlight",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "HJgkx2Aqt7": {
        "paper_id": "iclr_2019_HJgkx2Aqt7",
        "paper_title": "Learning To Simulate",
        "paper_abstract": "Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.",
        "paper_acceptance": "accepted-poster-papers",
        "meta_review": "This paper discusses the promising idea of using RL for optimizing simulators\u2019 parameters. \n\nThe theme of this paper was very well received by the reviewers. Initial concerns about insufficient experimentation were justified, however the amendments done during the rebuttal period ameliorated this issue. The authors argue that due to considered domain and status of existing literature, extensive comparisons are difficult. The AC sympathizes with this argument, however it is still advised that the experiments are conducted in a more conclusive way, for example by disentangling the effects of the different choices made by the proposed model. For example, how would different sampling strategies for optimization perform? Are there more natural black-box optimization methods to use?\n\nThe reviewers believe that the methodology followed has a lot of space for improvement. However, the paper presents some fresh and intriguing ideas, which make it overall a relevant work for presentation at ICLR.",
        "meta_review_title": "Methodology could be improved but ideas are very intriguing",
        "reviews": [
            {
                "review_id": "H1e7y2eA3Q",
                "reply_to": "iclr_2019_HJgkx2Aqt7",
                "title": "Great idea, but I don't think the right problems were selected to showcase the method",
                "comment": "Pros:\n* Using RL to choose the simulator parameters is a good idea. It does not sound too novel, but at the same time I am not personally aware of this having been explored in the past (Note that my confidence is 4, so maybe other reviewers might be able to chime in on this point)\n* In theory, you don't need domain adaptation or other sim2real techniques if you manage to get the optimal parameters of the simulator with this method.\n* Certain attributes of the method were evaluated sufficiently: eg the number of training epochs for each policy iteration, the dataset size generated in each iteration, and whether initialization was random or not in each iteration.\nCons:\n* Experiments were underwhelming, and the choice of problems/parameters to tune was not the right one for the problem.\n* Parts of the paper could be clearer\n\nQUALITY:\n* I believe that although the idea is great, but the quality of the experiments could have been higher. Firstly, better problems could have been selected to showcase the method. I was excited to see experiments with CARLA, but was underwhelmed when I realized that the only parameter of the simulator that the method controlled was the number and the type of cars in the scene, and the task of interest was a car counting task (for which not much detail was provided). This would have been much more interesting and useful to the community if more parameters, including rendering parameters (like lighting, shading, textures, etc) were part of the search space. Similarly, the semantic segmentation task could have used more than one category. But even for the one category, there were no previous methods considered, and the only comparison was between random parameters and the learned ones, where we only see marginal improvement, and what I perceive to be particularly low IoU for the car (although it'd help to know what's the SOTA there for comparison) For both vision applications I could help but wonder why the authors did not try to simply train on the  validation set to give us another datapoint to evaluate the performance of the method: this is data that *is* used for training the outer loop, so it does beg the question of what is the advantage of having hte inner loop. \n\nCLARITY:\n* The writing of the paper was clear for the most part, however the experimental section could have been clearer. I was wondering how model/hyperparameter selection was performed? Was there another validation set (other than the one used to train the outer loop)\n* The proposed policy is dubbed \"its\". What does it mean?\n* It's not clear what is a \"deliberately adversarial\" initialization. Could you elaborate?\n* The letter R is used to mean \"reward\" and \"rendering\". This is confusing. Similarly some symbols are not explicitly explained (eg S) Generally Section 2.3 is particularly unclear and confusing until one gets to the experimental section.\n* Section 3 discusses the technique and states that \"we can thus generate or oversample unusual situations that would otherwise not be part of the training data\" I believe it is important to state that, as the method is presented, this is only true if the \"validation\" data is varied enough and includes such situations. I believe this would be more applicable if eg rendering parameters were varied and matched the optimal ones.\n* Also the method is presented as orthogonal to domain adaptation and other sim-to-real techniques. However, I do not necessarily believe that this paper should be discussed outside the context of such techniques like domain randomization, Cycada, PixelDA etc. Even though these (esp. the latter ones) focus on vision, I do think it sets the right context.\nORIGINALITY:\n* As far as I'm aware noone has tried something similar yet. However, I'm not confident on this.\nSIGNIFICANCE:\n* Although the idea is good, I don't think that the approach to select the simulation parameters presented in the experiments in such a way is significant. I think that eg doing so for rendering parameters would be a lot more powerful and useful (and probably a lot more challenging). Also, I think that a single set of parameters (which seems to be what the goal is in this work) is not what one wants to achieve; rather one wants to find a good range of parameters that can help in the downstream task.\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1gzzcZ91E",
                "reply_to": "HyxPjJwf2Q",
                "title": "paper has improved; authors responded well to reviews",
                "comment": "I upgraded my score from 6 to 7.\n\nThe revision and the responses provided by the authors address some of my concerns.\n\nI still have doubts about the use of RL here. (I don't think it's needed.) And I wish the authors have gone further in the aspects of the simulation they optimize as well as the downstream tasks they tackle. Overall, on the methodological and the experimental fronts, I consider the paper to be rather weak. However, this is counterbalanced by the idea itself, which I find timely and stimulating. This paper may spur others to study this direction, bring more appropriate methods to bear on this problem, and attack more complex and realistic downstream tasks.\n\nAs a kind of \"lightning rod\" that attracts attention and stimulates follow-up work, this paper can be a useful addition to the literature.\n\nI also appreciate that the authors have thoroughly addressed the reviewers' concerns and have added more substantial experimental results to the revision.\n\nOverall, the benefits of publishing the work probably outweigh the drawbacks.\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyxPjJwf2Q",
                "reply_to": "iclr_2019_HJgkx2Aqt7",
                "title": "stimulating idea; potentially flawed method and incomplete evaluation",
                "comment": "The paper explores an interesting idea: automatically tuning the parameters of a simulation engine to maximize the performance of a model that is trained using this simulation engine. In the most interesting scenario, the model is trained using such optimized simulation and then tested on real data; this scenario is explored in Section 4.5.\n\nThe basic idea of optimizing simulation parameters for transfer performance on real data is very good. I believe that this idea will be further explored and advanced in future work. The present submission is either the first or one of the first papers to explicitly explore this idea, and deserves some credit and goodwill for this reason. This is the primary reason my rating is \"marginally above acceptance threshold\" and not lower.\n\nThe paper suffers from some issues in the technical formulation and experimental evaluation. The issues are reasonably serious. First, it is not clear at all that RL is the right approach to this optimization problem. There is no multi-step decision making, there are no temporal dynamics, there is no long-term credit assignment. The optimization problem is one-shot: you pick a set of parameters and get a score. Once. That's it. It's a standard black-box optimization setting with no temporal aspect. My interpretation is that RL is used here because it's fashionable, not because it's appropriate.\n\nThe evaluation is very incomplete and unsatisfactory. Let's focus on Table 1, which I view as the main result since it involves real data. First, is the optimization performed using the KITTI validation set? Without any involvement of the test set during the optimization? I hope so, but would like the authors to explicitly confirm.\n\nSecond, the only baseline, \"random params\", is unsatisfactory. I take this baseline to be the average performance of randomized simulation. But this is much too weak. Since the authors have access to the validation set during the optimization, they can simply test which of the random parameter sets performs best on the validation set and use that. This would correspond to the *best* set of parameters sampled during training. It's a valid baseline, there is no reason not to use it. It needs to be added to Table 1.\n\nAlso, 10 sets of random params seems quite low. How many sets of parameters does the RL solver sample during training? That would be the appropriate number of sets of params to test for the baseline. (And remember to take the *best* of these for the baseline.)\n\nThe last few points really boil down to setting up an honest random search baseline. I consider this to be mandatory and would like to ask that the authors do this for the rebuttal. There are also other derivative-free optimization techniques, and a more thorough evaluation would include some of these as well.\n\nMy current hypothesis is that an honest random search baseline will do as well as or better than the method presented in the submission. Then the submission boils down to \"let's automatically tune simulation parameters; we can do this using random search\". It's still a stimulating idea. Is it sufficient for an ICLR paper? Not sure. Something for the reviewers and the ACs to discuss as a group.\n",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rkgPl6OwAX",
                "reply_to": "H1e7y2eA3Q",
                "title": "Updated experiments and clarifications on rendering parameters (we do vary rendering parameters in our work)",
                "comment": "We thank the reviewer for appreciating the idea. We hope the following clarifications and experiments allow for a re-evaluation.\n \nChoice of parameters:\n(a) We believe that the paper was unclear about which parameters are learned. Specifically, in Sections 2.3 and 4.2 the reader had been lead to believe that we do not vary the rendering parameters in our work. We do vary lighting parameters in both the car counting and segmentation experiments. We define four weather types - clear noon, clear sunset, wet sunset and rainy sunset. Our policy outputs a categorical distribution over them. The illumination, color hue and light direction are varied as well as reflections from water puddles and weather particles such as rain drops. Sections 2.3 and 4.2 have been modified accordingly.\n(b) We learn not only types of cars and preponderance of cars but also the length of the road ahead, which influence the amount of cars and the structure of the scene.\n(c) We are adding a figure to the appendix to show how weather is learned over time by our method. We observe that our algorithm automatically deduces that giving higher probability to scenes without rain or puddles improves the performance of the main task model.\n(d) We have added text in the paper to highlight the variations described in (a) and (b).\n \nState-of-the-art KITTI segmentation experiment:\n(a) The submission uses ResNet-18 since it is faster for experimentation. We now use a ResNet-50 to achieve a state-of-the-art implementation.\n(b) With ResNet-50, we let our policy learn for 600 iterations and sample random parameters for 600 iterations. Our best policy iteration achieves 0.579 IoU, which is 20% better than the best dataset generated with random parameters (0.480 IoU). Thus, we show a clear improvement over a strong baseline.\n(c) We also introduce another baseline, specifically, random search to optimize over the simulator parameters. Random search achieves 0.407 IoU on the test set. Thus, learning to simulate achieves an increase in performance of 42% over this method. We hypothesize that performance using random search is low due to the nature of the problem which presents sparse and noisy rewards.\n(d) Even though our simulated scenes are limited in their realism, we achieve 57.9% IoU for car segmentation, which is reasonable. As a state-of-the-art reference, an upper-bound of 77.8% IoU is obtained by training the same network on 982 real annotated images, which is much more than the 100 synthetic images used to train our method.\n(e) To make space for these additional experiments we move the dataset size experiments to the appendix.\n \nUse of CARLA:\n(a) Our idea of learning to simulate is independent of the choice of simulator. We choose CARLA to make our contribution concrete. But the resources needed to fully demonstrate on a rich simulator like CARLA are immense. We respectfully submit that such a bar will preclude most groups from publishing on the use of simulators. On the other hand, focusing on a small set of parameters allows more insights into the proposed idea.\n(b) While CARLA is a promising tool, we do extend it in useful ways. It required a significant development effort to turn it into a procedural generator for new traffic scenes. The CARLA plugin is not necessarily built with extensions like this in mind.\n(c) We hope the orthogonal contributions of our paper suggest a useful direction for the CARLA development team too.\n\nUse validation set for training:\n(a) Our use of train-validation-test sets is conventional. It is important to note that we use the validation set akin to hyperparameter selection, rather than using it as labeled training data.\n(b) For deployment, one may follow parameter-tuning with retraining that includes the validation set to achieve the best test performance. However, it\u2019s not common practice for benchmarking new ideas and likewise, we only wish to demonstrate the benefit of learning to simulate.\n(c) There are regimes where the size of validation set is sufficient for evaluation, but not for training. But a small number of real images can instead have a significant impact in terms of bridging the domain gap from simulations, making a fair evaluation tricky.\n(d) Some advantages of learning to simulate persist even if one considers the validation set for training. For example, if a scenario is not sufficiently represented in validation set, it is hard to train a network for it simply by including those images, while our proposed method can oversample it to maximize accuracy.\n(e) As a reference, and instead of training on the validation set, we train on a large real dataset for our main KITTI segmentation experiment.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJlEVhiYhm",
                "reply_to": "iclr_2019_HJgkx2Aqt7",
                "title": "Sound method, but lacking a proper evaluation and comparison",
                "comment": "This work makes use of policy gradients for fitting the parameters of a simulator in order to generate training data that results in maximum performance on real test data (e.g., for classification). The difficulty of the task rises from the non-differentiability of the simulator.\n\n# Quality\n\nThe method is sound, well-motivated, and presented with a set of reasonable experiments. However, and this is a critical weakness of the paper, no attempt is made to compare the proposed method with respect to any related work, beyond a short discussion in Section 3. The experiments do include some baselines, but they are all very weak. \n\n# Clarity\n\nThe paper is well-written and easy to follow. The method is illustrated with various experiments that either study some properties of the algorithm or show some good performance on real data.\n\n# Originality\n\nThe related work is missing important previous papers that have proposed very similar/identical algorithms for fitting simulator parameters in order to best reproduce observed data. For example,\n- https://arxiv.org/abs/1804.01118\n- https://arxiv.org/abs/1707.07113\nwhich both make use of policy gradients for fitting an adversary between fake and real data (which is then used a reward signal for updating the simulator parameters).\n\n# Significance\n\nThe significance of the paper is moderate given some similar previous works. However, the significance of the method itself (regardless of previous papers) is important.\n",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Byg0vTOv0m",
                "reply_to": "rJlEVhiYhm",
                "title": "Added discussion on related work and points on sufficiency of comparison",
                "comment": "We thank the reviewer for their comments. We provide additional evaluations and discussion of related works to address their concerns.\n \nSufficiency of evaluation:\nWe agree that a practical implementation would use a more extensive simulator, but we believe our choices sufficiently illustrate the idea, while keeping the effort reasonable for an ICLR paper. Please refer to the first three points in the response to Reviewer 2.\n \nComparative references:\nThank you for pointing out these papers. We briefly highlight below that our contributions are quite different from both of those works. We have added this discussion to the related work section.\n \n\u201cSynthesizing Programs for Images using Reinforced Adversarial Learning\u201d from ICML 2018 trains a policy to generate a program that creates a copy of the input image.\n(a) Simulators used in the paper are a brushstroke simulator and an object placer.\n(b) Some similarities are that they use reinforcement learning to update the parameters of their policy and generate synthetic data using a non-differentiable simulator.\n(c) But they generate plausible synthetic data identical to the input, or sample from the latent space to create a program that simulates an unconditioned sample. In contrast, we learn parameters of a simulator that maximize performance of a main task model.\n(d) Importantly, we do not wish to reproduce observed data. Indeed, the reward function can even be chosen to amplify some rare cases. For example, if an object category is rare in road scenes, but important to segment for collision avoidance, our reward can be used to reflect this.\n(e) Another difference is that they use an adversarial loss to create their reward signal, while we use the validation accuracy of the main task model.\n \n\u201cAdversarial Variational Optimization of Non-Differentiable Simulators\u201d is, to the best of our knowledge, an unpublished work. It seeks to \"match the marginal distribution of the synthetic data to the empirical distribution of observations\".\n(a) They replace the generator in a GAN by a non-differentiable simulator and solve the minimax problem by minimizing variational upper bounds of the adversarial objectives.\n(b) The main similarity is tuning parameters of a domain-based non-differentiable simulator.\n(c) But they focus on particle physics and in their main experiment, tune a single parameter. Our experiments focus on computer vision and explore a higher dimensional parameter space (11 parameters, 6 for cars, 1 for length to intersection, 4 for weather type).\n(d) Further, while they use policy gradients, they use an adversarial loss to create their reward signal while we use the validation accuracy.\n(e) The most important difference is that we do not seek to mimic the distribution of real-world. In many cases, it is not the distribution that maximizes the reward. In our toy example, we achieve higher accuracy with a learned distribution that is completely different from the ground truth distribution (we add these numbers to the appendix).\n\nSufficiency of comparison:\n(a) For comparison purposes we believe there are no direct counterparts to our work. The closest related work we have identified is \"Learning To Teach\" (Fan et al.) since they seek to improve accuracy of a model. Nevertheless, they do not create new data but select which data to train on from existing datasets.\n(b) In order to evaluate our method we present baselines on our experiments. We seek to prove that by learning to simulate we achieve higher accuracy than randomly sampling scenes which is what works such as \"Playing for Data: Ground Truth From Computer Games\" (Richter et al.) do. We demonstrate this to be the case in all of our experiments.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1eN2TuDCX",
                "reply_to": "HyxPjJwf2Q",
                "title": "Added more extensive experiments and proposed baselines",
                "comment": "We thank the reviewer for noting the novelty. We hope the following clarifications and new experiments ease their concerns.\n \nUse of policy gradients:\nWe agree that our problem is a black box optimization without temporality or discounted reward. In Section 3, second paragraph, we do discuss alternatives such as evolutionary algorithms or sampling methods. Our use of policy gradients is not due to it being fashionable. Rather, we use them to estimate gradients for a non-differentiable function with the following advantages:\n(a) Simplicity: The method is simple, easy to implement and easy to formalize (see Algorithm 1). This makes it easily reproducible. There are very few hyperparameters to tune (baseline, learning rate of policy)\n(b) Flexibility: The policy that is defined can be arbitrarily flexible. We use a Gaussian policy but the work can be extended to discrete policies or those using neural networks.\n(c) Sample efficiency: We observe in all experiments that parameters converge after less than 500 iterations. For some experiments we observe convergence in less than 200 iterations. This is due to the direct relationship between our reward and the value we want to optimize (validation accuracy). In our case, those two are the same.\n(d) Interpretability: We show curves of weather probabilities and car type probabilities in a new figure in the appendix. We can visualize how the probabilities are learned through iterations.\n\nWe believe that (a) and (b) are the most distinct advantages of policy gradients. (c) and (d) are advantages to a lesser extent and can be present in other derivative-free optimization methods. The generality afforded by the method is important since our work is designed to be applied to different applications where simulation is possible.\n \nWe note that policy gradients are also used in other works that have a similar one-shot scenario as ours, such as \u201cNeural Architecture Search\u201d, \u201cNeural Optimizer Search\u201d, as well as both the works cited by Reviewer 1.\n \nEvaluation (test set):\nWe emphasize the test set is not used in any experiment for parameter tuning. It is unseen for every problem and only used once for final evaluation. Section 2.2 and Figure 4 state this. We have modified the paper to highlight this further.\n \nEvaluation (comparison with \u201cbest\u201d random sample):\nWe already present a strong demonstration on the car counting task in Figure 5, where learning to simulate outperforms the \u201cbest\u201d random parameters. We initialize two networks and train them using datasets generated by learning to simulate policy (red curve) and random policy (grey curve). We show that the random policy is vastly outperformed by the learned policy.\n\nAdditionally we present a more extensive and fair real data segmentation experiment. We use a more powerful ResNet-50 backbone (ResNet-18 was used in the original submission for faster experimentation) and let our policy learn for 600 iterations and sample random parameters for 600 iterations. Our best policy iteration achieves 0.579 IoU, which is 20% better than the best dataset generated with random parameters (0.480 IoU). Thus, we show a clear improvement over this baseline. Our intuition is that the higher the dimensionality of the parameter space and smaller the areas of high reward, the more likely random parameters will have difficulty achieving high reward.\n \nRandom search baseline as opposed to policy gradients:\nThank you for this suggestion. We believe random search is a valid baseline, but not as sample efficient or successful as policy gradients in some scenarios. To verify this, we use a hypersphere radius of 0.1 for random search, extensively tuned using several runs of the method, for both the car counting and KITTI segmentation experiments. For car counting, which presents a less noisy reward signal, random search performs about the same as our method achieving an L1 error of 16.53 reward compared to 16.94 for our proposed method. However, for KITTI car segmentation, it achieves an IoU of 40.7% (using the same number of iterations, namely 600), yet policy gradients achieve higher IoU of 57.9%. In this scenario policy gradients demonstrates an increase in performance of 42%. This has been added to the paper.\n \nKITTI segmentation evaluation:\nPlease see response to Reviewer 2, where we demonstrate 20% improvements over the best random parameters, 42% improvements over a well-tuned random search baseline, as well as obtaining IoU with 100 synthetic images that is reasonable compared to 982 real images for training.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bkx0ZTdDCm",
                "reply_to": "rkgPl6OwAX",
                "title": "(cont)",
                "comment": "\nHyperparameters:\nWe use standard hyperparameters for both tasks and use the same ones for all main task networks within an experiment. We use a learning rate of 3e-4 using the Adam optimizer for car counting, as well as standard values for beta_1 (0.9) and beta_2 (0.999). For segmentation, the optimizer used is SGD. We use a learning rate of 6e-4, tuned by generating a balanced dataset containing all weather types, semi-crowded scenes (with cars) and all car types to maximize performance on the KITTI validation dataset. We then use the same learning rate for all synthetic segmentation experiments. To obtain the upper bound trained on 982 annotated real KITTI images, we directly optimize hyperparameters by training on that dataset and using KITTI validation set as a reference.\n \nAdversarial initialization:\nWe mean initial parameters that have been chosen to be suboptimal. Specifically, these correspond to using low probability for spawning cars in the scene and higher probability for cars or weather least represented in the test distribution. We modify the paper to be more clear on this point.\n \nNotation:\nWe have modified the use of \u201cR\u201d (stylized R for rendering). The proposed policy was named \u201clts\" which stands for \"learning to simulate\u201d, but we modified to \u201cLTS\" to avoid confusion. Moreover, we have clarified Section 2.3.\n \nOversampling unusual situations:\nYes, we need the unusual situation to be present in the validation set, which we assume is representative of test scenarios. While these scenarios are present in the validation set at a low frequency, one does need several samples of rare cases in order to train a network effective for them. That is where oversampling rare scenarios can make a difference.\n \nDomain adaptation:\nWe have included extra discussion in Section 3. Note that even if the optimal parameters are learned using our method, there is still need for sim2real domain adaptation. Often the simulator will be limited and not be able to generate images that are completely realistic. Domain adaptation is needed to bridge this gap, thus, leads to orthogonal benefits. The interplay of our method and domain adaptation to achieve stronger results will be interesting future work.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the promising idea of using RL for optimizing simulators\u2019 parameters",
                "Sentiment Expression": "promising",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "insufficient experimentation",
                "Sentiment Expression": "justified",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the amendments done during the rebuttal period",
                "Sentiment Expression": "ameliorated this issue",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "extensive comparisons",
                "Sentiment Expression": "are difficult",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the methodology followed",
                "Sentiment Expression": "has a lot of space for improvement",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "some fresh and intriguing ideas",
                "Sentiment Expression": "fresh and intriguing",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "BJx040EFvH": {
        "paper_id": "iclr_2020_BJx040EFvH",
        "paper_title": "Fast is better than free: Revisiting adversarial training",
        "paper_abstract": "Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD).  In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice.  Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost.  Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy at epsilon=8/255 in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at epsilon=2/255 in 12 hours, in comparison to past work based on ``free'' adversarial training which took 10 and 50 hours to reach the same respective thresholds. ",
        "paper_acceptance": "accept-poster",
        "meta_review": "This paper provides a surprising result: that randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc. This paper produced a fair bit of controversy among both the community and the reviewers to the point where there were suggestions of bugs, evaluation problems, and other issues leading to the results. In the end, the authors released the code (and made significant updates to the paper based on all the feedback). Multiple reviewers checked the code and were happy. There was an extensive author response, and all the reviewers indicated that their primary concerns were address, save concerns about the sensitivity of step-size and the impact of early stopping.\n\nOverall, the paper is well written and clear. The proposed approach is simple and well explained. The result is certainly interesting, and this paper will continue to generate fruitful debate. There are still things to address to improve the paper, listed above. I strongly encourage the authors to continue to improve the work and make a more concerted effort to carefully discuss the impacts of early stopping.\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "ByeiH0AoFH",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Official Blind Review #2",
                "comment": "The main claim of this paper is that a simple strategy of randomization plus fast gradient sign method (FGSM) adversarial training yields robust neural networks. This is somewhat surprising because previous works indicate that FGSM is not a powerful attack compared to iterative versions of it like projected gradient descent (PGD), and it has not been shown before that models trained on FGSM can defend against PGD attacks. Judging from the results in the paper alone, there are some issues with the experiment results that could be due to bugs or other unexplained experiment settings. \n\nThe most alarming part of the results is the catastrophic failure with larger step sizes 16/255 for CIFAR10 in Table 1. This is very strange because the method works well when using epsilon=10/255 to defend against an adversary with epsilon=8/255. \nThe authors explain this with overfitting, but this is not satisfactory. Suppose I want to use the method to defend against an adversary with power epsilon=14/255, then it is conceivable that I would use a slightly larger step size, say 16/255, as suggested by the authors.  The results in the table tells me that this method will fail completely, because it cannot defend against epsilon=8/255, let alone the target perturbation 14/255. The method is probably not failing completely, because it does have good accuracy on clean data and does learn something. So it cannot be due to the model not having enough capacity to learn against an epsilon=16/255 adversary. \n\nThe authors should check some potential issues with the experiments: \n1. Is there any label leakage in the FGSM training? \n2. The pseudo-code does not contain any projection onto the feasible set; the authors should check it. \n\nSince the claim of this paper is somewhat unexpected given previous works on defending against adversaries, the experiment results have to be very solid. With these issues with the experiments I don't believe the current paper is ready for publication yet. \n\nAfter rebuttal: \nThe authors' new experiments and response answers most of my concerns. \n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Hyxi6-9rYS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Official Blind Review #1",
                "comment": "This paper revisits Random+FGSM method to train robust models against strong PGD evasion attacks. Coupled together with tricks for accelerating natural training, such as cyclic learning rate, mixed precision, the robust models can be trained faster than previous methods. \n\n+The experimental results are impressive. The trained model is robust (at Madry\u2019s PGD level), and the total training procedure is fast (6 min for CIFAR-10 and 12 hr for ImageNet).\n\n+ The method is simple, and I guess reproducible. \n\n+The paper shows surprising facts of a well-known method.\n\n+The paper is generally well-written and easy to follow.\n\nI do have some concerns of the work\n- The paper is empirical and the techniques are combinations of previous methods. Even for the surprising fact that Random+FGSM, it has been discussed in several previous papers, for example,  ICLR 2019 Defensive Quantization: When Efficiency Meets Robustness  https://openreview.net/forum?id=ryetZ20ctX. So the main contribution of the paper is limited to show RFGSM works well when combined with optimization tricks like cyclic learning rate. \n\n-In previous methods claiming random+FGSM can train robust model, their method seems to be slightly different from Alg 3 in page 4 of this paper. The alg in this paper seems to be identical to Madry\u2019s implementation of R-FGSM, which is shown not robust to PGD attacks. See discussions in https://openreview.net/forum?id=rJzIBfZAb and https://openreview.net/forum?id=ryetZ20ctX. I would like the authors to clarify their method to resolve such conflicts and make it clear how R-FGSM can be as robust as PGD as in table 1. \n\n-The first two paragraphs of section 4.1 seem to be inaccurate. One important trick in the \u201cadversarial training for free\u201d paper is to replay each minibatch m times. It is hard to say how much nonzero initialization helps. According to \u201cuniversal adversarial training\u201d (https://arxiv.org/pdf/1811.11304.pdf). It may help, but cannot compete with Madry\u2019s PGD training when defending against PGD attacks. \n\n-I am not sure if using a larger norm 1.25 * \\epsilon is a fair comparison. A baseline of PGD training bounded by 1.25 * \\epsilon would help. \n\n-Could the authors combine table 4 and 5 for easy comparison of robust accuracy and training time? Did the authors try the optimization tricks on ImageNet for the baseline free adversarial training method?\n\n\n\n================== after rebuttal =================\nI change my rating to weak accept. I tend to accept for the following reason\n(1) there seems to be no obvious flaw in the authors implementation. I quickly skimmed their code, and looks like a few researchers have tried their code and responded in public discussion. The surprising robustness of RFGSM, though the originality is questionable and the technical difference comparing to previous methods are subtle, seems to hold true. \n(2) The authors work hard to address the comments. \nI still have some concerns, mainly regarding the fairness of experimental comparison. \n(1) As pointed out in public discussion, the success of the proposed RFGSM relies on early stopping. I am wondering if early stopping also helps other methods since it turns out to be some sort of selection procedure.\n(2) The authors did not update time in table 1 for CIFAR-10 results, which I consider almost no extra efforts. I am wondering how much more time each method needs from 45% in figure 2 to higher robust accuracy in table 1.\n(3) I cannot understand why the proposed method is a particular good fit with cyclic LR and low precision tricks comparing to other methods. ",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Byxm03EsiS",
                "reply_to": "SklIbTJcjr",
                "title": "A few more pointers",
                "comment": "(I) We apologize for the confusion. Gradient clipping is unnecessary, and the code works perfectly well without it (it is a redundant artifact of the submitted code from experimenting, and we forgot to remove it from the CIFAR repository before creating the anonymous repository). Just to be safe, since the CIFAR10 experiments are quite fast, we double checked and reran the code to confirmed that the results are still consistent without the gradient clipping (note that the submitted code for MNIST and ImageNet both correctly don't use gradient clipping). With both cyclic learning rates and automatic loss scaling from the AMP mixed-precision arithmetic (both of which help combat exploding gradients), clipping gradients is indeed redundant for avoiding exploding gradients. Thank you for the taking such a detailed look at the code and pointing this out! \n\n(III) Yes, we can certainly add this. \n\nThank you once again for your valuable, detailed feedback!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJl64i4ijH",
                "reply_to": "Hye27hy5sS",
                "title": "More answers",
                "comment": "Thanks again for following up! \n\n(IV) Yes, this is correct: one epoch of free training takes half the time of randomized FGSM since it does 1 backwards pass instead of 2. We noticed this with your initial request for 15 epochs, and so we've actually already started the corresponding experiments for 30 epochs of free training instead of 15 (in order to be more fair and have comparable compute times). We will of course update the paper with these results once they are done. \n\nTo answer your second question, we use m=4 for ImageNet specifically because in the Free paper, they found that m=4 performed the best *for ImageNet* (it achieves about 3-4% more PGD accuracy than m=8). You can see this in Tables 3 and 7 in the free adversarial training paper. The usage of m=8 is the optimal minibatch replay value for free training on CIFAR10, and not ImageNet. Throughout the paper we compare to the *best* hyperparameter for minibatch replay for each specific dataset, which is why we compare to m=8 for CIFAR10 and m=4 for ImageNet, so that we do not unfairly cripple the free adversarial training benchmark. Note that this highlights another advantage of using FGSM adversarial training: there is no need to tune a minibatch replay parameter. \n\n(V) We ran the PGD attack on our FGSM trained CIFAR10 model for 1000 iterations as requested (also with 10 random restarts, in order for the adversary to be strictly stronger than what we previously considered), and in comparison to the 50 iteration PGD adversary, we observe that the stronger adversary results in a drop of 0.21% robust accuracy (from 46.46% to 46.25% for a single CIFAR10 model). ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklIbTJcjr",
                "reply_to": "HJeOG_GmsS",
                "title": "Thanks for the pointers",
                "comment": "Thanks for the clarification.\n\n(I) I thought the authors used gradient clipping in their code, but it is probably not common approach for CIFAR benchmark? line 210 & 222 here https://github.com/anonymous-sushi-armadillo/fast_is_better_than_free_CIFAR10/blob/master/train_cifar.py\n\n(III) Could you add time to table 1?",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hye27hy5sS",
                "reply_to": "SkxcEtWPsH",
                "title": "More clarification",
                "comment": "I thank the authors for their efforts. \n\n(IV) I hope two clarification from the authors: with same number of 15 epochs, Free method is half the time of proposed RFGSM, right? Why use m=4 for Free method. The original paper often uses m=8. \n\n(V) I appreciate the authors working on it. I said it is considered a plus. The cross-entropy loss used in PGD attack can lead to strange behavior related to gradient masking. I just want one more sanity check.  You could also try black-box attack, or run PGD for 1000 steps, which I personally consider too much.  ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyeFPoWDsS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Updated paper",
                "comment": "In light of the discussion with the public and the feedback from the reviewers, we have uploaded a revised version of the paper. The main changes can be summarized as follows: \n\n+ We've added the discussion and comparison to R+FGSM from Tramer et. al (Appendix A)\n+ We've added the experiments showing the full effect of step size across a wide range on CIFAR10 (Appendix C)\n+ We've included the discussion and experiments which attempt to combine the DAWNBench optimization tricks with free adversarial training on ImageNet (Appendix E)\n+ We've added training time to Table 4 as requested\n+ The main text of the paper has been adjusted to reflect these additions\n\nIf there are any unsettled concerns or comments about the paper, we eagerly await further discussion from the reviewers. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkxcEtWPsH",
                "reply_to": "r1gL4MMzor",
                "title": "Following up on the remaining points",
                "comment": "In this post we address the two remaining points that required some additional experiments to be run. \n\n(IV) As the reviewer suggested, we ran the Free adversarial training approach with the same LR schedule and training procedure as used for FGSM adversarial training on ImageNet, for both stepsizes \\alpha=\\epsilon and \\alpha=1.25*\\epsilon, for \\epsilon=4/255 over 15 epochs. In short, this results in (against a 50 step PGD adversary with 10 restarts)\n\nFree training with \\alpha=\\epsilon: 22.18% robust accuracy\nFree training with \\alpha=\\epsilon*1.25: 22.25% robust accuracy\n\nFor reference, in comparison, our FGSM adversarial training achieves 30.18% robust accuracy in the same setting with the same number of epochs. Note that these numbers for Free adversarial training are lower than what Free adversarial training can get with many more epochs, and so we see a similar trend as in the CIFAR10 experiments from Table 3: even though DAWNBench can speed up all the methods, the simultaneous gradient updates in Free adversarial training method ends up requiring more epochs to achieve the best possible performance. \n\n\n(V) Regarding CW attacks: we've done our best to reproduce the CW attack for the L-infinity perturbation model, relying on the implementation by Nicholas Carlini here: https://github.com/carlini/nn_robust_attacks/blob/master/li_attack.py\n\nNote that we had to adjust the factors for increasing/decreasing various constants in order for the runtime to even be feasible. With the default parameters in the reference implementation, it can take up to 704k iterations to perform one single attack, which is not feasible for our hardware (and takes about 6 hours to run one attack). We adjusted the constants to use in total 2k gradient iterations per attack, which is still far beyond the 50-step PGD adversary used in the paper. \n\nOn our CIFAR10 model trained with FGSM adversarial training, the attack achieves 54.49% robust accuracy, in comparison to the 46.25% robust accuracy when evaluated with the PGD based adversary, so no drastic changes here. \n\nOne final note: while it's not generally a bad idea to run multiple attacks when applicable, it is important to understand the purpose behind running additional attacks. Throughout the literature for the L-infinity threat model, the PGD based attack has generally outperformed the CW attack while being much more efficient, and this is why we initially did not consider running it. In fact, you'll find that the open source adversarial attack libraries (e.g. Cleverhans for Tensorflow and Foolbox for PyTorch) do not implement the CW attack for L-infinity perturbations (it is only implemented for L2 perturbations). In the case of Cleverhans, the author for the CW attack recognized this, and decided to not add it to the library for this reason, which you can see in this issue here: https://github.com/tensorflow/cleverhans/issues/978",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJeOG_GmsS",
                "reply_to": "r1gL4MMzor",
                "title": "Some initial answers",
                "comment": "Thanks for following up! We're of course glad to continue the discussion. A couple of your questions can be quickly answered, so we'll begin by answering those, and follow up with the remaining questions later. \n\n(I) Gradient clipping is unnecessary most likely because the cyclic learning rate schedule from DAWNBench already scales the gradients. A cyclic learning rate schedule peaks when the training loss begins to diverge (this point can be found with the learning rate test mentioned in Section 5.2 of the paper, which comes directly from Smith & Topin 2018). As a result, the model gradients throughout the training procedure, when scaled by the learning rate, are quite stable and so gradient clipping is unnecessary (which is typically used to combat exploding gradients and loss divergence in the training procedure). \n\n(II) This was a question that also came up during the public discussion, and so we can conveniently already answer this for you. You can find a plot of runs over varying step sizes here (which, of course, we intend on adding to the paper): \n\nhttps://github.com/anonymous-sushi-armadillo/openreview/blob/master/step_size_cifar10.pdf \n\n(III) Yes, we agree that this is a good idea, and the change will be present once we've uploaded a revised version of the paper. \n\nThe remaining parts will have to wait, but we'll be running the experiments and will follow up when they are done. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1gL4MMzor",
                "reply_to": "rJlYEVtJiB",
                "title": "Thanks for clarification, more experiments help",
                "comment": "Thanks for the clarification on larger norm and fair comparison. Correct me if I was wrong, the paper claims two major contributions\n(1) Showing R-FGSM can be as robust as PGD for training\n(2) Train robust model faster with R-FGSM and optimization tricks\n\nI also read the open reviews and have a quick look of the authors' code.  The authors have convinced me on the (1) contribution. I am now on the fence.\n\nSince this paper has attracted quite some attention, I may have potentially set a higher standard for it.  I would be happy to change my score if the authors are willing to try harder to convince me on the (2) contribution. \n(I) Gradient clipping nn.utils.clip_grad_norm_(model.parameters(), 0.5) seems to be unnecessary for ResNet on CIFAR. Could the authors clarify?\n(II) It makes sense that 2*\\eps stepsize does not work quite well. But how sensitive is the stepsize between \\eps and 2*\\eps, and what is the sweet point? I hope the authors could provide more ablation study. On CIFAR is good enough. It would also help to do multiple runs and show variance. It could be done during rebuttal period since each run only takes 6 min.\n(III) I strongly encourage the authors to include training time in table 1 and table 4 for directly comparison. \n(IV) I would hope the authors clarify results on Free method, and maybe add some experiments. I will use adversarial stepsize for updating perturbation, and learning rate (LR) for updating model weights.  What is the adversarial stepsize in Free when combined with DAWNBench optimization tricks? Did you use the same LR and LR scheduler and other non-adversarial-related setting for Free and RFGSM, and natural training? I would like to see results on ImageNet for Free with DAWNBench tricks, for adversarial stepsize \\eps and 1.25*\\eps, you can cut it off at 15 epochs to compare with RFGSM. Each run should take about 7 hours and can be finished in rebuttal period. \n\nI will give it extra points if you can try sanity check with CW attacks. \n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJeWKHKJjr",
                "reply_to": "HygOpABTFS",
                "title": "Thanks for your review",
                "comment": "Thanks for your review. Indeed, we hope this this work inspires new analysis which can perhaps quantify the degree to which the inner maximization must be solved in order to perform robust optimization.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJeaa4KysB",
                "reply_to": "ByeiH0AoFH",
                "title": "Clarification of the given example and addressing the potential issues",
                "comment": "Thanks for your feedback. With regards to the catastrophic overfitting observed at larger step sizes, we first clarify a misunderstanding here: defending against an adversary with radius epsilon means that we project the perturbation onto the ball with radius epsilon. With regards to your example, indeed, a step size of 16/255, *when projected onto a ball of radius 8/255*, results in overfitting, as large step size forces the generated adversarial examples to be clustered at the boundary.\n\nHowever, if, as you describe, we instead want to defend against an adversary with radius 14/255 using a step size of 16/255, then note that we project the FGSM step on the ball of radius 14/255. This is a fundamentally different scenario from the results in the table, which project onto a ball of radius 8/255, and so the table does not imply that the method is guaranteed to fail. Indeed, since the projected radius is larger, the adversarial examples are not clustered at the boundary, and so there is no overfitting.\n\nIn short: a large step size of 16/255 fails when projected onto a radius of 8/255, but works perfectly well when projected onto a similarly large radius (e.g. 14/255). This is why we wrote alpha=1.25*epsilon.\n\nAs for the potential issues in the experiments, we note below that they are not at all issues, and hope that the reviewer can reconsider:\n\nLabel leaking:\nWe do not observe label leaking (as defined in \"Adversarial Machine Learning at Scale\" by Kurakin et al. 2017). You can see this in all of our experimental results, e.g. in Table 1, Table 2, Table 4, and Figure 2, where the standard accuracy always is above the adversarial accuracy, and this behavior can be verified in the models that we have released.\n\nProjection in pseudocode:\nThe projection is in fact present in our pseudocode. It is the line that says \\delta = max(min(\\delta,\\epsilon), -\\epsilon). It is also in our submitted code. If you are referring to clipping at the [0,255] bounds from the image, this is also done in our code (as described in the public discussion). ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJlYEVtJiB",
                "reply_to": "Hyxi6-9rYS",
                "title": "Thank you for your review",
                "comment": "Thanks for your feedback regarding the connections to other randomized FGSM methods. This topic has already occurred in the public discussion, and so our response will largely reflect that. We will first discuss the main differentiating factors between our approach and the previous R+FGSM approach by Tramer et al., and follow up by answering the remaining comments.\n\nR+FGSM:\nIndeed, there has been previous work on using randomization with FGSM (e.g. R+FGSM as done by Tramer et al., which is the one used in \"Defensive Quantization: When Efficiency Meets Robustness\"). We note, however, that the R+FGSM approach from Tramer et al. is also the same randomized FGSM approach considered by Madry et al., which considers both the vanilla, non-randomized attack (which they conclude is not robust) as well as the R+FGSM attack from Tramer et al. as mentioned in their paper in Table 5 on page 17 of the Appendix.\n\nOur approach at using randomization with FGSM is very similar to Tramer et al. but differs in two aspects which are quite critical to the consistency and effectiveness of the defense. In fact, a lot of this discussion has already occurred in the public comments below (e.g. see our discussion with Florian here, where we explicitly compare our approach with R+FGSM: https://openreview.net/forum?id=BJx040EFvH&noteId=HJe2trIsDS), but we can summarize the main points for your convenience: namely, by using 1) a different random initialization and 2) a larger step size, our version of randomized FGSM adversarial training converges to better defended models with much higher consistency. By rerunning the approaches multiple times with different random seeds, one gets a fuller picture: R+FGSM as done by Tramer et al. has high variance and worse performance with respect to random seeds, whereas our approach consistent achieves results comparable to PGD adversarial training regardless of random seed (see the table at  https://github.com/anonymous-sushi-armadillo/openreview/blob/master/README.md which we generated for the referenced public discussion). So we believe that the contribution of this paper extends beyond just incorporating DAWNBench speedups.\n\nOn non-zero initialization and the connection to Free Adversarial Training:\nThe usage of minibatch-replay in free adversarial training is indeed the second key difference between free and FGSM adversarial training. While we don't mention this at the start of section 4.1, we do discuss this difference later in the last paragraph of the same section. However, we focused primarily on the initialization, because in our experiment in Table 1, we show the effect of using various initializations for FGSM adversarial training without changing any other parameters: simply going from zero to non-zero results in large gains in robustness. Note that the Universal Adversarial Training paper also uses R+FGSM as done by Tramer et al., and so it suffers from the same drawbacks as described above.\n\nOn the \"larger norm\" and fair comparison:\nWe believe there may be a misunderstanding here: the model is trained against an adversary bounded by epsilon, but the alpha=1.25*epsilon is merely the step size for the FGSM attack. Indeed, regardless of the step size, the FGSM attack is still projected back to the epsilon boundary. As a PGD adversary is allowed to tune the step size and the number of steps it takes to find an adversarial example, it should also be fair for the FGSM adversary to also tune its singular step size, as both methods project onto the same radius ball.\n\nOther comments:\nThank you for your suggestion, yes we can certainly add the training times to Table 4 to make it easier to connect the two.\n\nWe primarily focused on optimizing the DAWNBench improvements with Free adversarial training in the CIFAR10 setting, since the problem setting allows for extensive tuning of all parameters (which is not as feasible in the ImageNet setting).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HygOpABTFS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Official Blind Review #3",
                "comment": "The authors claimed a classic adversarial training method, FGSM with random start, can indeed train a model that is robust to strong PGD attacks. Moreover, when it is combined with some fast  training methods, such as cyclic learning rate scheduling and mixed precision, the adversarial training time can be significantly decreased. The experiment verifies the authors' claim convincingly.\nOverall, the paper provides a novel finding that could significantly change the adversarial training strategy. The paper is clearly written and easy to follow. I recommend the acceptance.\n",
                "rating": 8,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BygK0geC5B",
                "reply_to": "B1x0_Ul4OB",
                "title": "Thanks for the additional efforts.",
                "comment": "I rechecked the code, and I could not find any mistakes in this version. So I would prefer to believe the results (are correct). Thanks for the additional results. These results are surprising and insightful, which reminds me to rethink the necessity of pgd adversarial training (from a theoretical perspective)\n\nThanks,\nTianhang",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxATaruuS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Why does the early-stop strategy help so much?",
                "comment": "From the public comments, it seems that early-stop strategy helps the robustness so much.\n\nWhy does the missing of the early-stop strategy  make a great difference for the robustness? Do the authors have any good insights?\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkedM-SUuB",
                "reply_to": "S1g82u51_B",
                "title": "Diversity in early training helps robustness generalization",
                "comment": "Hi Tianhang,\nWe find, in early training when the network still struggles to converge, diversity (of adversarial training examples) is crucial for robustness generalization. However, at a later stage, strong advs examples that are of high convergence (inner max) quality become necessary. The random start normally does the diversity trick, but can still be improved by a weak adversary such as (eg. FGSM).\n\nThanks,\nXingjun",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkgQ8qBkuB",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Similar results have been shown in ICML2019 \"On the Convergence and Robustness of Adversarial Training\"",
                "comment": "An interesting observation!\n\nHowever, we have shown similar results with a \"FGSM-PGD\" variant of adversarial training in our ICML2019 paper \"On the Convergence and Robustness of Adversarial Training\".\n\nWe show there is even a 2%-3% robustness improvement over Madry's PGD-training  (against PGD-20, epsilon=8/255) if simply use FGSM instead of PGD for the first 20 epochs (see Figure 3b).\n\"we replace the first 20 epochs of PGD-eps/4 training with a much weaker FGSM (1 step perturbation of size \u000f), denoted as \u201cFGSM-PGD\u201d, and show its robustness and FOSC distribution in Figure 3b and 3c respectively. We find that by simply using weaker FGSM adversarial examples at the early stage, the final robustness and the convergence quality of adversarial examples found by PGD at the later stage are both significantly improved.\" \n\nBasically, what we found is that it doesn't need strong adversary like PGD to solve the inner-maximization, especially in early training.  Although we didn't test further how many  (or may be \"all\" like in this paper) epochs one can use FGSM to get the best robustness, the main finding in this paper is somewhat similar to ours. In the paper, we have already theoretically proved that the convergence of min-max adversarial training only requires the inner maximization is solved up to a certain precision  (by PGD or FGSM).\n\nIt would be interesting to see some empirical results and discussion on this. For example, what would happen if you replace the last 10 epochs of training by PGD.\n\nIn the meantime, I would like to clarify that this does not lower the contributions of this paper, as we haven't considered to replace PGD COMPLETELY with FGSM.\n\nLink to our paper: http://proceedings.mlr.press/v97/wang19i/wang19i.pdf\n\nThanks.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1x0_Ul4OB",
                "reply_to": "BJxeANq1OH",
                "title": "Results are the same",
                "comment": "Hi Tianhang, \n\nAfter rerunning the PGD attack with the correction in clipping, the MNIST results are largely the same (the performance for both PGD and FGSM training went down by a similar fraction of a percent). \n\nNote that the ImageNet results are unchanged, since our implementation is forked from the free adversarial training repository which clipped correctly (https://github.com/mahyarnajibi/FreeAdversarialTraining). \n\nAdditionally, we've released the MNIST code as well (see https://github.com/anonymous-sushi-armadillo/fast_is_better_than_free_MNIST), where running the training script with the default parameters reproduces the results in the paper. This may help identify what caused your implementation of FGSM to fail in the past (there could be potentially other small changes which cause FGSM training to fail beyond what we ran into in the paper). ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyeT0YG-dr",
                "reply_to": "Hkba731_S",
                "title": "Regarding early stopping",
                "comment": "Hi Tianhang, \n\nFor CIFAR10 models trained for 15 or 30 epochs, as reported in the paper, checking for early stopping was not necessary. We only needed to incorporate early stopping for experiments that trained for more epochs or used a larger FGSM step size. For early stopping, we only check the PGD accuracy at the end of each epoch on one minibatch, after having trained on all minibatches for that epoch. Therefore, the early stopping check adds a fairly trivial amount of computation.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hkba731_S",
                "reply_to": "ByxMkWjJOB",
                "title": "I guess the key point in their training stage is *early stop*",
                "comment": "They mentioned a sudden drop in pgd performance in the training process, so they use a \"early stop\" test using the pgd accuracy.\n\nExtremely, if you check the pgd accuracy in every step, then the cost is almost same as (even higher than) pgd adversarial training. Then this *early stop* check becomes a little bit tricky.\n\nBut it seems like the *early stop* operation *is executed every epoch in their code*. I am not sure if it can exactly find the \"sudden drop\" point compared with checking every step.\n\nThanks,\nTianhang\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJl5nj4luB",
                "reply_to": "Hkba731_S",
                "title": "I agree, but the best results in training are still much worse than baseline",
                "comment": "I also notice the sudden drop. So I guess we should report the *best* results through the training process.\nHowever, after I check the training process, I find that the best results is around \n                        Clean Acc  |  PGD20 Acc\n(epoch30)      73.79%       |  41.54 %\nwhich is worse than PGD10 advtrain baseline in my codebase (about 86%, 43%). (I use res18)",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJxeANq1OH",
                "reply_to": "S1gIWG4kdH",
                "title": "Thanks for your reply",
                "comment": "First, thanks for your reply.\n\nYep, I concur with you that this might be an issue mainly for MNIST (especially when eps=0.3, a fair number of pixels might exceed the boundaries in the optimization process). I am still not sure about the reason why we got different results on MNIST. Is it because of the different settings in the training stage or caused by this issue? I do not have the time to figure it out now, maybe I will catch it up later.\n\nI am glad that you will verify the experiments again (on MNIST and ImageNet). Look forward to more details.\n\nAnyway, this is a very interesting work.\n\nThanks again,\nTianhang",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJe1XsZydr",
                "reply_to": "B1g1hBZJdS",
                "title": "Let's wait for the results after the authors corrected it",
                "comment": "I saw the authors already corrected it in the commit history. I guess they might rerun the experiments. Lets wait for the results. I am also not very sure.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ByxMkWjJOB",
                "reply_to": "SJxjSkR0DB",
                "title": "Probably not, after changing to cyclic I still got 0.00% robust accuracy",
                "comment": " ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1g82u51_B",
                "reply_to": "SkgQ8qBkuB",
                "title": "Interesting work",
                "comment": "Interesting and solid work!\n\nA small question: what is the reason for the 2%-3% improvement?\n\nThanks,\nTianhang",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1g1hBZJdS",
                "reply_to": "Hkl-I_T0wr",
                "title": "Tianhang may be right...",
                "comment": "I check the code and find some clip operations indeed miss as Tianhang said. I am not sure if this is a big problem.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1gIWG4kdH",
                "reply_to": "Hkl-I_T0wr",
                "title": "Thanks for your feedback",
                "comment": "That's a fair point.  While we did actually have both clip operations within each iteration of PGD, we were storing the unclipped delta and clipping it before passing it through the model (so the clip between the minimum and maximum pixel range was on L55, and the epsilon clip was on L62, you can see the previous git history here: https://github.com/anonymous-sushi-armadillo/fast_is_better_than_free_CIFAR10/blob/e6032ecb1cfe32c226c9502a38f7329caafaf585/evaluate_cifar.py#L55), which is a subtle difference from other implementations. We didn't think the clipping at prediction or in delta would make much difference, especially since e.g., clipping is uncommon for CIFAR, so mainly this is an issue for MNIST.  But you're absolutely right that the procedure you suggest is indeed more correct, since otherwise there is incorrect behavior at the boundaries when delta exceeds the allowable region and only get clamped for the model prediction.  We've corrected this and updated the code in the repository.  The resulting PGD accuracies are effectively the same on CIFAR10 (reduced by 0.04%), and we'll verify our other experiments as well.  Thanks for your enthusiasm and diligence in working through this.\n\nSee our response to Dinghuai for some of our thoughts on why you may be seeing different performance in your example (https://openreview.net/forum?id=BJx040EFvH&noteId=SJxjSkR0DB).  Are you able to run the code from our repository yet?",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hkl-I_T0wr",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "The implementation of PGD attack from Ln. 54 to 63 in \"evaluate_cifar.py\" seems to miss one clip operation in each iteration (Actually there should be two clip operations)?",
                "comment": "Hi everyone,\n\nFinally, I seem to find an issue in the code:\n\nThe delta (adv perturbation) seems to be only clipped by (-epsilon, epsilon) *in each iteration* in the code (evaluate_cifar.py in the \"fast_is_better_than_free_CIFAR10\" directory)\nHowever, delta also has to be clipped by *(min_pixel_value-X,  max_pixel_value - X) in each iteration* to ensure X+delta is in the range of (min_pixel_value, max_pixel_value).\n\nThis operation is done in MadryLab's code and Trades simply by clipping the X+delta not delta. Their code clips X+delta (X_adv) by (min_pixel_value, max_pixel_value) *in each iteration*, and then optimizes on the clipped X_adv in the next iteration.\n\n**Did I make any mistake regarding this observation?** Could anyone help me double-check it?\n\nBTW, this makes a lot of difference. For example, in one step, delta_i= 0.01, X_i = 1.0, then X_adv_i = 1.01. Without the aforementioned clip operation, the delta_i = 0.01 will be maintained. And the following step will add the perturbation on 1.01 not 1.00. Suppose the perturbation is -0.01, we will get 1.00 in the next step (no movement) instead of 0.99 (which is what we expect to see).\nThanks,\nTianhang",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1lInqgJuB",
                "reply_to": "HkeAMTiRwB",
                "title": "Thanks.",
                "comment": "Thanks for providing more details.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJl5AIlyuB",
                "reply_to": "Byxb2-CRPS",
                "title": "On evaluating adversarial robustness",
                "comment": "Sorry, I do not agree that it's a little odd to use black-box attack in this setting.\n\nWhen proposing a new adversarial defense, the goal is to produce a model that is robust against _all_ possible attacks within a threat model. For instance, increased robustness to FGSM attacks does not constitute progress if the model is vulnerable to PGD attacks. This is why the robust accuracy of a model is defined as the *minimum* accuracy achieved against the worst-case attack within the threat model.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HkeqQLlJOr",
                "reply_to": "S1ezRei0wB",
                "title": "Concur",
                "comment": "\nIt is necessary to evaluation on the black box attacks, such as Nattack[1].\n\nWhen proposing a new adversarial defense, the goal is to produce a model that is robust against _all_ possible attacks within a threat model. For instance, increased robustness to FGSM attacks does not constitute progress if the model is vulnerable to PGD attacks. This is why the robust accuracy of a model is defined as the *minimum* accuracy achieved against the worst-case attack within the threat model.\n\n\n[1] NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks. ICML 2019",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Byxb2-CRPS",
                "reply_to": "SJggGjN3DS",
                "title": "On black-box attacks",
                "comment": "Hi Anthony, \n\nWhile in general it's a good idea to use black box attacks to get around gradient obfuscation defenses, it's a little odd to use them in this setting. Afterall, we are using adversarial training on standard models with standard data preprocessing techniques, and there are no steps which would hide the actual gradient, which is why PGD adversarial training (and consequently, FGSM adversarial training) is not considered to be a defense which obfuscates gradients. Additionally, they are besides the point of the paper: which is to show that FGSM adversarial training can lead to robustness against full strength multi-step PGD adversaries. \n\nThat being said, it is fair to say that we could still run the black box attacks anyways. However, it would only be useful as a comparison to the same attacks performed against PGD adversarial trained models, both of which would likely not perform as well as the white-box attacks anyways since clean model gradients are available. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJxjSkR0DB",
                "reply_to": "B1gprRaTPr",
                "title": "Piecewise learning rate with many epochs is another potential failure mode",
                "comment": "Hi Dinghuai, \n\nRegarding running the code, we've added more detailed instructions in the README of the CIFAR10 repository (https://github.com/anonymous-sushi-armadillo/fast_is_better_than_free_CIFAR10) as mentioned in another comment. It should only require PyTorch 1.2 and Apex, and if you really don't want to install Apex for half precision, it is very straightforward to just comment out the calls to apex and replace them with normal backwards calls (the Apex amp library is extremely unobtrusive, and so reverting to full precision as straightforward as undoing the 3 line changes described here: https://nvidia.github.io/apex/amp.html). \n\nAs for your training settings, the main difference is in the learning rate schedule. Something that we noticed is that when training with piecewise learning rate for a large number of epochs (e.g. 100), there's a chance that the training process will also result in the catastrophic overfitting, resulting in the 0% PGD accuracy that you obtained (though not always, depending on the random seed). However, when this does occur, the performance notably does not degrade gracefully: the performance will go from having competitive PGD accuracy to 0% PGD accuracy within the span of a single epoch. \n\nNote that this sudden drop in pgd performance is also reflected in the training data as well, so it's easily remedied by calculating the pgd accuracy on a few training minibatches to detect overfitting and just early stopping when it's detected, if you must use a piecewise learning rate for many epochs. However this occurred very rarely for the cyclic learning rate (which is what we use and describe in the paper), and so we ended up just mentioning it as a note in the appendix, since our focus was on optimizing for the fast setting with cyclic learning rates and not the much slower setting with piecewise learning rates. In hindsight, we admit that this was probably another significant failure mode for FGSM training that past attempts have run into, and will expand upon this in a followup discussion. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HkeAMTiRwB",
                "reply_to": "Hkg1AUiTvS",
                "title": "Code requirements",
                "comment": "For those who are wishing to run the provided code, we have added more detailed installation instructions and requirements in the README of the CIFAR10 repository: https://github.com/anonymous-sushi-armadillo/fast_is_better_than_free_CIFAR10 .",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1ezRei0wB",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "black box attacks (gradient-free)",
                "comment": "This is a very intriguing paper. It could be strengthened quite a lot by including results of gradient-free (black-box) attacks.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1gprRaTPr",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Cannot get the same results, maybe I took something wrong?",
                "comment": "It seems that the codes offered by authors need particular benchmark thus I cannot run them. So I do FGSM with uniform init using my own adv code base. I cannot get consistent results. I think maybe I miss something or get something wrong? \n\nFor cifar10, I use resnet18, init lr 0.1, piece-wise decay at 70, 90, 100 epoch and get final PGD20 accuracy is **0.00%** when step size alpha = 10/255.  Other settings follow (Madry et al., 2017)'s paper. My code base is here: https://github.com/a1600012888/YOPO-You-Only-Propagate-Once\n\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hkg1AUiTvS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Pytorch version and the requirements",
                "comment": "Could you tell me the pytorch version and the requirements for the repo fast_is_better_than_free_CIFAR10 ? I failed to load the released model with pytorch 0.4.0 .\n\nThanks.\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgwhU9pPS",
                "reply_to": "r1xiW0WnwS",
                "title": "Followup comparison to R+FGSM and effect of step sizes",
                "comment": "Hi Florian,\n\nTo shed some light on why the R+FGSM adversarial training that you had tried before wasn't as successful, we conducted some basic experiments. We also show the effects of step size at the end of this post.\nThe main differences between R+FGSM and our approach are the following:\n\n(1) The R+FGSM initialization is on the surface of an alpha=epsilon/2 box (e.g. alpha*sign(Normal(0,1))), whereas ours is initialized with Uniform(-epsilon,epsilon).\n\n(2) The FGSM step is taken with step size epsilon-alpha=epsilon/2.\n\nSo we tested what happened when took R+FGSM into our training procedure and changed either (1) or (2) to see why ours succeeded. We put a table of the outcome of this experiment here (https://github.com/anonymous-sushi-armadillo/openreview/blob/master/README.md), where the mean and standard deviation are taken over 10 random seeds. We found R+FGSM to be highly dependent on the random seed: the performance ranged the entire spectrum, from occasionally succeeding to completely failing. By changing the initialization to be Uniform, the variance is greatly reduced and the performance on average is better. However, only changing the step size to be a full epsilon step instead of an alpha/2 step did not seem to help on its own. When these changes are taken in combination (which results in the method we use in the paper), we get the same consistent result with little variance. Hopefully this sheds some light on why the R+FGSM method was unsuccessful while this one was!\n\nAdditionally, as requested, we have run the CIFAR10 training with varying step sizes to show the effect on FGSM training. The plot is available here (https://github.com/anonymous-sushi-armadillo/openreview/blob/master/step_size_cifar10.pdf). In summary, we find that performance deteriorates with step sizes smaller than epsilon (likely because the adversary is effectively weakened), and that once epsilon is too large (11/255 or higher in this case) it becomes easy to fail (possibly because forcing the adversarial example to the boundary of the L-inf ball makes it easy for the model to overfit).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJggGjN3DS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Evaluation on the black-box attacks",
                "comment": "The finding of this paper is very intriguing. However, the disccusion of the robustness on the black-box attacks is missing in this paper.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1xiW0WnwS",
                "reply_to": "HJe2trIsDS",
                "title": "Thanks for your comments",
                "comment": "Hi everyone,\nGood to see this paper is already generating controversy! (We expected as much :) )\n\nFirst off, thanks Florian for pointing out the connection to the R+FGSM method you tried.  We know the paper and ensembling technique well, of course, but honestly that connection slipped our mind as it was considered a failed option in that paper, so wasn't the focus.  We'll definitely add this connection and discussion.  As we hope is clear, our goal here is definitely not to claim a new algorithm, but just that this old approach works surprisingly well when tuned properly (really surprising to us too).\n\nYou're absolutely right that the step size has some effect here, and it's a great idea to compare this more formally. FWIW, it's not that we just decided alpha=10/255 randomly, but it seemed like slightly (1.25x) larger than the epsilon ball (but notably not 2x, which would be a \"full\" FGSM step) worked best, and this was consistent across all datasets.  However, there is a reasonable range of choices here that works ok, and we'll add a figure within the next few days (at least for CIFAR) showing the whole curve.\n\nThanks for the suggestion!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJxd3S9owB",
                "reply_to": "HJe2trIsDS",
                "title": "We also failed.",
                "comment": "It's unbelievable that FGSM adversarial training with random initialization can be as effective as PGD adversarial training. \n\nWe have tried the experiments of R+FGSM adversarial training, but the robustness is not competitive with that of PGD adversarial training.  Maybe the authors can consider various attacks to evaluate the robustness, such as the black-box attacks.\n\nFrom the evaluation in Tabel 1, the performace of the proposed method seems to be very sensitive  to the step size.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJe2trIsDS",
                "reply_to": "iclr_2020_BJx040EFvH",
                "title": "Similarity to R+FGSM in Ensemble Adversarial Training",
                "comment": "Very interesting work!\n\nThe proposed FGSM with random initialization is very similar to the R+FGSM attack we had discussed in our paper \"Ensemble Adversarial Training: Attacks and Defenses\" two years ago: https://openreview.net/forum?id=rkZvSe-RZ\n\nWhat surprises me here is that we had tried doing adversarial training with the R+FGSM attack on MNIST, but did not find it to be effective. Quoting from our paper:\n\n\"We also tried adversarial training using R+FGSM on MNIST, using a similar approach as (Madry et al., 2017).\nWe adversarially train a CNN (model A in Table 5) for 100 epochs, and attain > 90.0% accuracy on R+FGSM samples. However, training on R+FGSM provides only little robustness to iterative attacks. For the PGD attack of (Madry et al., 2017) with 20 steps, the model attains 18.0% accuracy.\"\n\nThe reason our experiment failed while yours presumably succeed might be related to your discussion on step-size selection (page 5). In our experiments, we were taking a random step of size eps/2 followed by a FGSM step of size eps/2, which in hindsight was too weak to properly explore the l-norm ball.\n\nIt would be interesting to include further discussion or experiments on the effect (and brittleness) of the step-size selection in the paper. ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "randomization and FGSM can produce robust models faster than previous methods given the right mix of cyclic learning rate, mixed precision, etc.",
                "Sentiment Expression": "a surprising result",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "well written and clear",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The proposed approach",
                "Sentiment Expression": "simple and well explained",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The result",
                "Sentiment Expression": "certainly interesting",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "bJz3cFePTna": {
        "paper_id": "nips_2021_bJz3cFePTna",
        "paper_title": "Mixture Proportion Estimation and PU Learning:A Modern Approach",
        "paper_abstract": "Saurabh Garg, Yifan Wu, Alexander J. Smola, Sivaraman Balakrishnan, Zachary Lipton",
        "paper_acceptance": "accept",
        "meta_review": "\nThis paper proposes new methods for the related problems of mixture proportion estimation and positive-unlabeled learning, with theoretical support, and shows state of the art performance, especially for large scale problems. I tend to agree with one of the reviewers that the MPE method is not really that novel, sharing many conceptual similarities with previous ROC based methods. This should be clearly addressed in the final revision, as should all reviewer comments. In addition it would be desired to have some theory for the iterative scheme. Without such, the authors also need to address possible failure cases and limitations of $(TED)^n$. Nonetheless, there is still sufficiently novelty and merit to warrant publication.\n\nAdditional comments:\n\nWhile the experimental contributions are clear, I'd like to ask the authors to comment on how the MPE theory compares to prior work. Is this theory merely \"supporting\", or does it offer advances in MPE theory in any substantive way?\n\nRelevant reference: Henry Reeve and Ata Kaban. Exploiting geometric structure in mixture proportion estimation with generalised Blanchard-Lee-Scott estimators, ALT 2019",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "mgGiR6BKpyz",
                "writer": "official_reviewer",
                "reply_to": "YYdkcY2r2XZ",
                "title": "Response to authors",
                "comment": " Thank you. I very much appreciate the detailed response.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H-GCH-xbdxP",
                "writer": "official_reviewer",
                "reply_to": "u-20fguyHGY",
                "title": "Thanks for the response",
                "comment": " Dear authors: \n\nThanks for your detailed and careful reply to my review comments. I have also read the other reviews for this paper and still stand by this paper.\n\nBest,  \nReviewer 31y4\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "pwntmX1K1Cv",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_bJz3cFePTna",
                "title": "",
                "comment": "The paper proposes two methods for PU learning. First, a method for estimating the fraction of positives among unlabeled examples, named Best Bin Estimation (BBE). Secondly, an approach for PU learning called Conditional Value Under Optimism (CVuO). The BBE works on top of the decision score of a given pretrained classifier. The score is used to find a region of the instance space with the minimal ratio of the portion of unlabeled and the portion of positive examples. The ratio is an upper bound on the fraction of positives being estimated. In case there exists a region populated only by positive examples (the condition called pure positive bin property), the bound is tight. The authors provide a finite sample bound for their estimator. The CVuO turns any supervised method to PU learning algorithm. The approach assumes the fraction of positive examples to be known. The idea is to rank the unlabeled examples by a score of a pretrained classifier and remove it the fraction of positive examples. The remaining examples are taken as true negatives and then used with the true positives to re-train the classifier. The procedure is repeated until convergence. Besides, the authors propose an algorithm, called $TED^n$, combining BBE and CVuO. The methods are empirically shown to outperform several recent methods on semisynthetic problems created from CIFAR, MNIST, and IMDB datasets.   Originality. The idea of finding the \"pure positive bin\" and its usage for the estimation of the fraction of positives is to my knowledge novel. The CVuO is a straightforward heuristic to convert PU learning into supervised learning, however, I am not aware of its appearance in the literature. \n\nQuality. The paper is technically sound. \n\nClarity. I am missing a high level explanation of the core idea behind the BBE. The reader is in general left on its own to extract the ideas from a dense text. On the other hand, I understand it is not easy given the limited space. I would suggest to start with showing the equation $q_u(z)=\\alpha q_p(z) + (1-\\alpha) q_n(z)$ which is easy to understand, and one can immediately see form it that $q_u(z)/q_p(z)\\geq \\alpha$ and that the bound is tight if $q_n(z) = 0$. The function of the hyper-parameter $\\gamma$ of Algorithm 1 is unclear. I do not fully see the logic behind the name \"Conditional Value Under Optimism\". I had also an impression that the limits of the proposed method are not emphasized, namely, that it requires a prediction model nearly perfectly separating the classes for the top ranked instances.\n\nSignificance. The proposed methods are simple and on standard benchmarks outperform several recent approaches. On the other hand, the methods are tested only on data favourable for the proposed method, i.e., almost perfectly separable classes and the fraction of positives equal to 0.5. \n\nExperiments. As I already mentioned, the methods are tested in the simplest setting when the fraction of positives is just 0.5. It is known that PU methods tend to fail when the classes are imbalanced and it is important to know how the proposed method behaves. The errors are reported without any measure of uncertainty and hence it is hard to judge significance of the the observed differences. The evaluation protocol is not completely clear, but the details are perhaps in the appendix. \n\nTypos:\n- line 30: \"of of\"\n- line 114: \". in our experiments\"\n- Algo 3, line 4: the same iterator \"i\" is used in both loops\n- Algo 3, line 14: \"Lines 4-7\" -> \"Lines 4-9\"  I don't see any potentially negative societal impact of the work.",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "U-1hF0d9lUT",
                "writer": "official_reviewer",
                "reply_to": "u1Y9RSPDim-",
                "title": "Thanks for the reply",
                "comment": " The authors have replied all my concerns raised in the review in very detailed manner. I believe the paper should be accepted. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jRv-fbssv3N",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_bJz3cFePTna",
                "title": "",
                "comment": "This paper tackles the well known problem of mixture proportion estimation and Positive-Unlabelled (PU) learning. The authors propose a a (new) estimator for MPE based on the assumption of a PU classifier having a \"pure\" top bin. The authors then develop a novel  iterative outlier-removal-like algorithm for learning a classifier separating the positives and negatives from PU data assuming access to the true MPE. Finally, they combine these two approaches to create a novel alternating algorithm that estimates MPE based on a current PvN classifier, and use the MPE estimate to learn a better PvN classifier from PU data.  The CvUO algorithm and TED procedure are novel, interesting, and deserve credit.\n\nThe BBE algorithm, on the other hand, seems extremely similar to other approaches, e.g. Scott (2015) gives an approach of learning a PvU classifier, and using held-out data to estimate the ROC (with confidence intervals) and give the smallest slope of the ROC curve to (1,1) as an MPE estimate.  This is, essentially, the same as finding a threshold and taking the (confidence adjusted) ratio of q_p/q_u. There are some subtle differences, but they seem rather negligible. While Scott (2015) uses Kernel logistic regression to generate the ROC, there is nothing preventing the use of deep learning methods to generate the ROC curve. The assumptions of irreducibility, i.e. existence of a set S such that P_n(S)=0 but P_p(S)>0 is exactly the same as the existence of a pure top bin when the sets S are generated by thresholding a learned PvU classifier.\n\nThe CvUO algorithm seems to essentially assume equal class priors between positive and negative. For example, if alpha=0, CvUO should just return a standard PvN classifier, but the loss function used weights the total positive samples as equal to total negative samples regardless of the actual number of samples of each type. A similar issue exists with the proof of proposition 1 in appendix D. I am guessing this is a bug and can be fixed.\n\nSome guarantee of the sort to ensure that CvUO algorithm does not get progressively worse would be valuable. For example if in one iteration a majority of the alpha fraction of sample removed are not negative the classifier learned after that epoch might be even worse than the previous epoch and start a negative spiral. Can a guarantee be given for this to not happen? The proof in Appendix D does NOT take care of this.\n\n\n\n No known negative social impact is conceivable for this work specifically, as it is primarily algorithmic in nature for solving a well-known problem. ",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "1b-Zj3uaIYU",
                "writer": "official_reviewer",
                "reply_to": "r3ogcVYjFZ0",
                "title": "Thanks for the detailed reply",
                "comment": " Thanks for the reply. \n\nIf text on detailed comparison (including formula/algebra) of BBE with the ROC slope is added, and also a clarification on why the positive and negative priors are assumed to be equal is added, this paper would go up one level in score.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "u-20fguyHGY",
                "writer": "author",
                "reply_to": "7URS0pP3GYn",
                "title": "Response to Reviewer 31y4",
                "comment": " Thanks for your detailed review and positive assessment.\n\n**\u201cMore detailed analyses for experimental results are suggested. The method consists of some hyper-parameters needed to be determined. The analyses for them (e.g. an ablation study) should be added to discuss their influence in experiments.\u201d**\n\nTo analyze our results and proposed algorithms, we perform the following experiments: \n\n(i) Results in main paper with 5 different datasets with alpha = 0.5;   \n(ii) Ablations with varying the mixture proportion alpha in Appendix F.4 (Figure 10);   \n(iii) Ablations with varying warm start iterations W in Appendix F.4 (Figure 9);   \n(iv) Results on MNIST with simulated overlaps in Appendix F.6. We create a new dataset called MNIST Overlap, where the positive class contains digits from 0 to 7 and the negative class contains digits from 3 to 9. This creates a dataset with an overlap between positive and negative support;   \n(v) Results with UCI datasets in Appendix F.5. \n\n\n* Ablations with varying W, show that our procedure is not sensitive to warm start iterations and in many tasks with W = 0, we observe minor-to-no differences in the performance of (TED)^n (Lines 300-301).  \n\n* Ablations with varying alpha show that our method (TED)^n maintains superior performance as compared to alternate algorithms. \n\n* Experiments on Overlap MNIST and UCI datasets show that our algorithm continues to obtain superior performance as compared to alternate algorithms. \n\nWe would be happy to run additional experiments if the reviewer has any suggestions. \n\n\n**\u201dThe procedure of the proposed method is relatively simple. However, it still is comprised of some parts. Would the method bring a lot of extra computational consumption?\u201d**\n\nIn our (TED)^n procedure, we alternate between mixture proportion estimation and then using the updated estimate to train the classifier for one epoch.  \n\nBecause in each round of (TED)^n we update our mixture proportion estimate with BBE and then fine-tune our classifier under the CVuO objective for one epoch (using the new estimate), the computational cost is roughly the same as under ordinary training.  Breaking it down, the cost of BBE is primarily the cost of inference on validation data and the computational cost of updating the classifier with CVuO is roughly the same as training an ordinary classifier for one epoch. Thus, the computational costs associated with (TED)^n are similar to those of training and validating a standard positive versus negative classifier for $n$ epochs.\n \n\n**\u201dThe authors argue the proposed method only relies on milder conditions. Such a description is somewhat not intuitive. An explanation at a high level is encouraged and expected.\u201d**\n\nTo guarantee consistency, existing methods that leverage a blackbox classifier discuss Bayes optimality of the PvU classifier as a sufficient condition. However, we show that even in a simple toy setup, PvU training doesn\u2019t recover the Bayes optimal classifier  (Lines 196-202). \n\nOn the other hand, we need the classifier to satisfy the assumption that there exists a top bin that mostly contains positive examples\u2014irreducible bias grows proportional to the noise in the top bin, and 0 noise is required in order for BBE to obtain consistent estimates (Lines187-191). We provide empirical evidence highlighting that deep learning classifiers indeed produce top bins that mostly contain positive examples (Fig 2(a), Lines 192-195). \n\n\n**\u201dAlthough the proposed method works well, the motivation of this paper needs a clearer explanation. Section 2, it is still a bit hard to understand the weaknesses of the prior methods\"**\n\n* For MPE, classical methods break down in high-dimensional settings, while recent proposals either lack theoretical coherence or depend precariously on tuning hyperparameters that are, by the very problem setting, untunable (Lines 39-48). This motivated our BBE algorithm.\n\n* For PU learning, we observe that existing methods leave a substantial accuracy gap when compared to a model trained just on the positive and negative (from the unlabeled) data in App. F.1 (Lines 218-220). To decrease this gap, we propose our CVuO algorithm. \n\nAs per your suggestion, we will improve the exposition on our motivation and the issues with prior methods in the final draft. \n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "u1Y9RSPDim-",
                "writer": "author",
                "reply_to": "pwntmX1K1Cv",
                "title": "Response to Reviewer HMxc",
                "comment": " Thank you for your positive assessment and constructive feedback on our work. \n\n**\u201d...  the methods are tested only on data almost perfectly separable classes and the fraction of positives equal to 0.5.\u201d**\n\nIn the Appendix, we have included additional results. In particular, we include:  \n(i) Results with varying the mixture proportion $\\alpha$ in Appendix F.4 (Figure 10);  \n(ii) Results on MNIST with simulated overlaps in Appendix F.6. We create a new dataset called MNIST Overlap, where the positive class contains digits from 0 to 7 and the negative class contains digits from 3 to 9. This creates a dataset with an overlap between positive and negative support;  \n(iii) Results with UCI datasets in Appendix F.5\n\nAs per your suggestion, we will add a summary of the results from these ablations in the main paper in the final version. \n\n\n**\u201dI am missing a high-level explanation of the core idea behind the BBE.\u201d** \n\nWe apologize for the confusion. Based on your suggestions, we will elaborate on the core idea behind BBE to improve the exposition in the final version. \n\n\n**\u201dThe function of the hyper-parameter of Algorithm 1 is unclear\u201d** \n\nWe apologize for the confusion. We use $\\gamma$ to get tight theoretic guarantees (Appendix B.1). In Algo 1, we add the confidence bound multiplied by $(1/\\widehat{q}_p(\\widehat {c}) - 1/\\gamma)+$ to our ratio estimate $ \\widehat{q}_u(\\widehat {c}) /  \\widehat{q}_p(\\widehat {c})$ to yield an estimate with tighter guarantees, i.e.,  when $\\widehat{q}_p(\\widehat {c})$ is smaller than $1/\\gamma$, adding upper confidence bound scaled by the multiplier $(1/\\widehat{q}_p(\\widehat {c}) - 1/\\gamma )$ avoids $1/\\widehat{q}_p(\\widehat {c})$ in our upper bound in Theorem 1 (Lines 490-493). \n\nHowever, as mentioned in Appendix B.1 (Lines 494-495), since we are minimizing the upper confidence bound (in Line 3 in Algo 1), we never observe $\\widehat{q}_p(\\widehat {c})$ taking small values than $\\gamma = 0.1$ (fixed throughout all of our experiments). Hence, the gamma term has no effect in experiments. \n\nMotivated by this finding, we have recently improved the analysis to derive a lower bound on $\\widehat q_p(\\widehat c)$. We show that $\\widehat {q}_p(\\widehat{c}) = \\Omega  (\\widehat{q}_p (c^*))$ and thereby we drop the upper confidence bound term (i.e., second term in the estimate $\\widehat{\\alpha}$ in Algo 1 and the new estimate is given by $\\widehat{\\alpha} = \\widehat{q}_u(\\widehat {c}) /  \\widehat{q}_p(\\widehat {c})$). \n\n\n**\u201d... logic behind the name \"Conditional Value under Optimism is unclear\"**\n\nThe name is motivated by Conditional Value at Risk (CVaR). Similar to CVaR, we discard samples from the tail of the distribution under the optimistic assumption that samples with highest loss (i.e. in the tail) are positives. \n\n\n**\u201d... limits of the proposed method are not emphasized, namely, that it requires a prediction model nearly perfectly separating the classes for the top-ranked instances\u201d**\n\nYes, we need the classifier to satisfy the assumption that there exists a top bin that mostly contains positive examples\u2014irreducible bias grows proportional to the fraction of negatives in the top bin (Theorem 1), and 0 noise is required in order for BBE to obtain consistent estimates (Lines187-191). We provide empirical evidence highlighting that deep learning classifiers indeed produce top bins that mostly contain positive examples (Fig 2(a), Lines 192-195). \n\nWe will make the limitation of BBE explicit in the final version. \n\n**\u201dThe errors are reported without any measure of uncertainty and hence it is hard to judge the significance of the observed differences.\u201d**\n\nThanks for pointing out this oversight. We have updated the draft with measures of uncertainty over multiple runs (with 3 seeds) and present the results from the updated Tables 1 and 2 with measures of uncertainty below: \n\n| Dataset          | Model  | (TED)^n            | BBE*             | Dedpul*          | KM2   | TiCE  |\n|------------------|--------|--------------------|------------------|------------------|-------|-------|\n| Binarized CIFAR  | ResNet | 0.018 $\\pm$ 0.0025 | 0.072$\\pm$  0.0006 | 0.075 $\\pm$ 0.0023 | 0.181 | 0.251 |\n| CIFAR Dog vs Cat | ResNet | 0.074 $\\pm$ 0.014    | 0.12 $\\pm$ 0.009   | 0.113 $\\pm$ 0.012  | 0.11  | 0.203 |\n| Binarized MNIST  | FCN    | 0.021 $\\pm$ 0.003    | 0.028 $\\pm$ 0.002  | 0.027 $\\pm$ 0.001  | 0.102 | 0.247 |\n| MNIST 17         | FCN    | 0.003 $\\pm$ 0.001    | 0.008 $\\pm$ 0.004  | 0.006 $\\pm$ 0.002  | 0.065 | 0.117 |\n| IMDb             | BERT   | 0.008 $\\pm$ 0.002    | 0.011 $\\pm$ 0.003  | 0.016 $\\pm$ 0.004  | -     | -     |\n\n \n| Dataset          | Model  | (TED)^n         | CVuO          | PvU*          | Dedpul*       | nnPU          |\n|------------------|--------|-----------------|---------------|---------------|---------------|---------------|\n| Binarized CIFAR  | ResNet | 82.7 $\\pm$ 0.20 | 82.6 $\\pm$ 0.44 | 78.3 $\\pm$ 0.71 | 78.4 $\\pm$ 0.54 | 76.8 $\\pm$ 0.73 |\n| CIFAR Dog vs Cat | ResNet | 76.1 $\\pm$ 0.42 | 74.0 $\\pm$ 0.61 | 71.6 $\\pm$ 1.43 | 70.9 $\\pm$ 1.69 | 72.6 $\\pm$ 0.82 |\n| Binarized MNIST  | FCN    | 95.9 $\\pm$ 0.1  | 96.4 $\\pm$ 0.11 | 94.5 $\\pm$ 0.24 | 95.2 $\\pm$ 0.14 | 95.9 $\\pm$ 0.16 |\n| MNIST 17         | FCN    | 98.6 $\\pm$ 0.11 | 98.6 $\\pm$ 0.08 | 93.7 $\\pm$ 2.04 | 98.1 $\\pm$ 0.01 | 98.2 $\\pm$ 0.18 |\n| IMDb             | BERT   | 87.6 $\\pm$ 0.37 | 87.4 $\\pm$ 0.52 | 86.1 $\\pm$ 0.63 | 87.3 $\\pm$ 0.19 | 86.2 $\\pm$ 0.15 |\n\n\n**\u201dThe evaluation protocol is not completely clear, but the details are perhaps in the appendix.\u201d** \n \nYes, we include additional experimental details in Appendix E. As per your suggestions, we will include a more detailed summary of the evaluation protocol in the main paper in the final draft.  \n\nThanks for catching the typos. We have fixed them in our draft. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YYdkcY2r2XZ",
                "writer": "author",
                "reply_to": "E1MLBHjQJ7p",
                "title": "Response to Reviewer DmAg",
                "comment": " Thanks for your positive feedback and for championing our paper. We respond to your specific concerns below:\n\n**\u201c... \\hat{\\alpha} cause some doubts. How to intuitively get this equation? From the supplementary materials, it seems to be a derivation back from the theoretical result?\u201d**\n\nWe apologize for the confusion here. Yes, the derivation for $\\widehat{\\alpha}$ in Algo 1 is motivated by our theoretical analysis. We add the confidence bound to our ratio estimate $ \\widehat{q}_u(\\widehat {c}) /  \\widehat{q}_p(\\widehat {c})$ to yield an estimate with tighter guarantees, i.e.,  when $\\widehat{q}_p(\\widehat {c})$ is smaller than $1/\\gamma$, adding upper confidence bound scaled by the multiplier $(1/\\widehat{q}_p(\\widehat {c}) - 1/\\gamma )$ avoids $1/\\widehat{q}_p(\\widehat {c})$ (which, in principle, can be arbitrarily small) in our upper bound in Theorem 1 (Lines 490-493).  \n\nHowever, as mentioned in Sec B.1 in Appendix (Lines 494-495), since we are minimizing the upper confidence bound (in Line 3 in Algo 1), we never observe $\\widehat{q}_p(\\widehat {c})$ taking small values than $\\gamma = 0.1$ (fixed throughout our experiments). Hence, the second term with gamma has no effect in experiments. \n\nMotivated by this finding, we have recently improved the analysis to derive a lower bound on $\\widehat q_p(\\widehat c)$. We show that $\\widehat {q}_p(\\widehat{c}) = \\Omega  (\\widehat{q}_p (c^*))$ and thereby we drop the upper confidence bound term (i.e., second term in the estimate $\\widehat{\\alpha}$ in Algo 1 and the new estimate is given by $\\widehat{\\alpha} = \\widehat{q}_u(\\widehat {c}) /  \\widehat{q}_p(\\widehat {c})$). \n\n\n**\u201cIn (TED)^n, how to make a reasonable division between the training set and hold-out set? Should they be evenly split?\u201d**\n\nSince the training set is used to learn the classifier (parameters of a deep neural network) and the hold-out set is just used to learn the mixture proportion estimate $\\widehat{\\alpha}$ (scalar), we use a larger dataset for training. Throughout the experiments, we use an 80-20 split of the original set. These are important details and we will be sure to include them in the final version.\n\nAt a high level, we have an error bound on the mixture proportion estimate and we can use that to decide the split. As long as we use enough samples to make the $\\mathcal{O}(1/\\sqrt{n})$ small, we can use the rest of the samples to learn the classifier. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r3ogcVYjFZ0",
                "writer": "author",
                "reply_to": "jRv-fbssv3N",
                "title": "Response to Reviewer sRZG",
                "comment": " Thank you for your positive assessment and thoughtful review of our work. \n\n**\u201dThe BBE algorithm, on the other hand, seems extremely similar to other approaches, e.g. Scott (2015)\u201d** \n\nThanks for pointing out this connection. While there are some key similarities between our approaches, there are also some important distinctions, and our final draft will benefit from a more in-depth discussion of these points. \n\nTo summarize, all of Scott (2015)\u2019s theoretical results pertain to an estimator due to Blanchard 2010 that relies on VC bounds that are known to be loose. This method and the theoretical analysis is very different from ours. However, due to the intractability of Blanchard (2010)\u2019s  estimator, Scott (2015) implement a second **heuristic estimator** (Scott 2015, Section 5) based on identifying a point on the AUC curve such that the slope of the line segment between this point and (1,1) is minimized. While on the surface, this approach is similar to our best bin estimator, there are some striking differences:\n\n1. While they provide no theoretical guarantees for their **heuristic estimator**, we provide guarantees that our estimator will converge to the best estimate achievable over all choices of the bin size and provide consistent estimates whenever a pure top bin exists.\n2. Second, while both estimates involve thresholds, the functional form of the estimates are different: Scott 2015\u2019s estimator is the ratio of quantities obtained by binomial tail inversion (i.e. upper bound in the numerator and lower bound in the denominator) which can be biased to overestimate MPE. By contrast, the final BBE estimate is simply the ratio of empirical CDFs at the optimal threshold.\n3. We implemented the proposal of Scott (2015) and identified that the choices in BBE\ncreate substantial differences in the empirical performance as shown in the table below (setup is the same as in Table 1):  \n\n| Dataset          | Model  | (TED)^n | BBE   | Dedpul | Scott (2015)$^1$ |\n|------------------|--------|---------|-------|--------|--------------------------|\n| Binarized CIFAR  | ResNet | 0.018   | 0.072 | 0.075  | 0.091                    |\n| CIFAR Dog vs Cat | ResNet | 0.074   | 0.12  | 0.113  | 0.158                    |\n| Binarized MNIST  | FCN    | 0.021   | 0.028 | 0.027  | 0.063                    |\n| MNIST 17         | FCN    | 0.003   | 0.008 | 0.006  | 0.037                    |\n\n$^1$as mentioned in the Scott (2015) implementation (https://web.eecs.umich.edu/~cscott/code/mpe_v2.zip), we use the binomial inversion at \\delta instead of \\delta/n (rescaling using the union bound). Since we are using Binomial inversion at n discrete points simultaneously, we should use the union-bound penalty. However, using union bound penalty substantially increases the bias in their estimator. \n\nThese are important points to clarify and we will add a detailed comparison of the two methods in the camera-ready version. \n\n**\u201cThe CvUO algorithm seems to essentially assume equal class priors between positive and negative\u201d**\t\n\nThanks for bringing up this important point. Indeed, we care about distinguishing between positive versus negative examples among the unlabeled set and these points are (1) not necessarily class balanced and (2) may have a different class balance from the amounts of available positive versus unlabeled data. We thought about this concern early on and in our initial experiments, attempted to address the matter via importance-weighted risk minimization (reweighting the positive loss and negative loss terms with $\\widehat{\\alpha}$ and $1- \\widehat{\\alpha}$ respectively). However, we observed no effect of multiplying the estimated mixture proportion estimate on final classification performance. Hence, similar to earlier works (Kiryo et al. 2017, Du Plessis et al, 2015), we followed equal weighting of the positive and negative loss in the final objective. \n\nWe note that for deep neural networks (for which model misspecification is seldom a prominent concern) and when the underlying classes are separable (as with most image datasets), it is known that importance weighting has little to no effect on the final classifier (Byrd 2019). This may explain why IW-ERM does not confer benefits in our experiments. For completeness, we will add relevant discussion and experiments in the final draft. \n\n\n**\u201dSome guarantee of the sort to ensure that CvUO algorithm does not get progressively worse would be valuable\u201d** \n\nThis is a very interesting suggestion. And based on your comment we thought about it and we have one result in the population case. While it is hard to argue about the one-step loss with optimization algorithms in a finite-sample case, in the population case we show that one step of our alternating procedure cannot increase the loss. \n\nConsider the following objective function, \n$$ L(f_t, w_t) = E_{x \\sim P_p}[ l( f_t(x), 0) ] + E_{x \\sim P_u}[ w_t(x) l( f_t(x), 1) ]  $$\n$$ \\text{s.t.} E_{x \\sim P_u}[ w(x)] = 1-\\alpha \\text{ and } w(x) \\in \\{0,1\\}$$\n\nGiven $f_t$ and $w_t$, CVuO can be summarized as the following two step iterative procedure: (i) Fix $f_t$, optimize the loss to obtain w_{t+1}; and (ii) Fix $w_{t+1}$ and optimize the loss to obtain $f_{t+1}$. By construction of CVuO, we select $w_{t+1}$ such that we discard points with highest loss, and hence $L(f_t, w_{t+1}) \\le L(f_t, w_{t})$. Fixing $w_{t+1}$, we minimize the $L(f_t, w_{t+1})$ to obtain $f_{t+1}$ and hence $L(f_{t+1}, w_{t+1}) \\le L(f_t, w_{t+1})$. Combining these two steps, we get $L(f_{t+1}, w_{t+1}) \\le L(f_t, w_{t})$. \n\nWe will include this result in the final version. \n\n[1] G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. The Journal of Machine Learning Research, 2010\n\n[2] C. Scott. A rate of convergence for mixture proportion estimation, with application to learning from noisy labels. In Artificial Intelligence and Statistics, 2015.\n\n[3] M. Du Plessis, G. Niu, and M. Sugiyama. Convex formulation for learning from positive and unlabeled data. In International conference on machine learning, 2015.\n\n[4] R. Kiryo, G. Niu, M. C. Du Plessis, and M. Sugiyama. Positive-unlabeled learning with a non-negative risk estimator. In Advances in neural information processing systems, 2017\n\n[5] J. Byrd, Z. Lipton. What is the Effect of Importance Weighting in Deep Learning? In International Conference on Machine Learning, 2019\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "E1MLBHjQJ7p",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_bJz3cFePTna",
                "title": "",
                "comment": "This paper proposes a unified framework for learning from positive-unlabeled (PU) data, including BBE for mixture proportion estimation (MPE)  and CVuO for PU learning. In the literature there are important and elegant works exists that analyze the consistency of the mixture proportion without the irreducibility assumption. They formalize their analyses based on an assumption that the percentage of a blackbox classifier's output for positive samples greater than a fixed value is higher than that for negative samples. I suggest a brief introduction of the conditions that this assumption satisfies, maybe the inductive bias or something, while intuitively such an assumption is quite reasonable. They provide finite sample guarantee for BBE and also empirically verify it. In the PU learning stage, the authors use an iterative manner to progressively identify and remove all the positive points from the unlabeled data during training. This is again quite a reasonable way of thinking because of the memorization of neural networks. The combination of BBE and CVuO is called (TED)^n. The authors provide a lot of supportive experiments to verify their methods.  \nTwo main tasks have been solved in this paper. The first relaxes the strong irreducibility assumption in MPE. Since we don't have any prior information about Pn, it is hard to be verified whether the assumption is satisfied in a specific problem. If not, the existing MPE methods may suffer from estimation bias. BBE is just a clever way of solving that. The second uses a simple manner establishing the state of the art. Overall, new theoretical results provide important insights on an important topic. The paper is clear and well written. There is a good balance of theoretical findings and empirical validations.\n\nMinor suggestions:\n- In Algorithm 1, \\hat{\\alpha} cause some doubts. How to intuitively get this equation? From the supplementary materials, it seems to be a derivation back from the theoretical result? Besides, it would be better to introduce the notions that appear for the first time, e.g., ()_{+}.\n- In (TED)^n, how to make a reasonable division between the training set and hold-out set? Should they be evenly split? Yes.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "7URS0pP3GYn",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_bJz3cFePTna",
                "title": "",
                "comment": "This paper targets the problems of mixture proportion estimation and PU learning. The authors propose two simple but effective methods to address two problems. Also, the combination of two methods is proposed to train an accurate binary classifier under mild conditions. A series of experiments are conducted to verify the effectiveness of the proposed method.   **Contributions and Novelty**\n\nThis paper proposes two advanced methods to tackle the problem of mixture proportion estimation and PU learning, i.e., BBE and CVuO. BBE produces consistent estimates $\\hat{\\alpha}$ under mild assumptions and achieves a $\\mathcal{O}(1/\\sqrt{n})$ convergence rate under some assumptions. Moreover, CVuO discards a certain proportion of training examples during training, which reduces the overfitting to the unlabeled positive ones. Finally, the method TED that combines the BBE and CVuO is presented to train a binary classifier under the settings of PU learning. The proposed methods are effective and somewhat novel. \n\n**Quality**\n- Theory. Detailed theoretical analyses are provided for the method BBE, which shows that, with high probability, the estimation is close to the ground truth. \n- Experiments. The experimental results are convincing. The proposed method outperforms baselines in most cases. \n\n**Clarity**\n\n- Motivation. Although the proposed method works well, the motivation of this paper needs a clearer explanation. \n- Comparison with related works. The authors state that the issues of the prior methods, e.g., complexity or strong assumptions, and aims to address these issues. However, a clearer explanation for their issues is expected to present. At the present stage (Section 2), it is still a bit hard to understand the weaknesses of the prior methods. The descriptions of experimental settings are appreciated. The **limitations and concerns** in my view are as follows. \n- The authors argue the proposed method only relies on milder conditions. Such a description is somewhat not intuitive. A explanation at a high level is encouraged and expected. \n- More detailed analyses for experimental results are suggested. \n- The procedure of the proposed method is relatively simple. However, it still is comprised of some parts. Would the method bring a lot of extra computational consumption?\n- The method consists of some hyper-parameters needed to be determined. The analyses for them (e.g. an ablation study) should be added to discuss their influence in experiments. \n",
                "rating": 6,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "novelty and merit",
                "Sentiment Expression": "sufficiently to warrant publication",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the MPE method",
                "Sentiment Expression": "not really that novel, sharing many conceptual similarities with previous ROC based methods",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "addressing the MPE method's similarities with previous ROC based methods and reviewer comments in the final revision",
                "Sentiment Expression": "should be clearly addressed",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "theory for the iterative scheme",
                "Sentiment Expression": "would be desired",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "possible failure cases and limitations of $(TED)^n$",
                "Sentiment Expression": "need to address",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "iBBcRUlOAPR": {
        "paper_id": "nips_2022_iBBcRUlOAPR",
        "paper_title": "An empirical analysis of compute-optimal large language model training",
        "paper_abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\\times$ more data. Chinchilla uniformly and significantly outperformsGopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, a 7% improvement over Gopher. ",
        "paper_acceptance": "Accept",
        "meta_review": "Four experts reviewed this paper and they all recommended acceptance. The paper finds that current Transformer-based large language models (LLM) are significantly undertrained. This is likely to be of great interest to the AI/NLP community, as it challenges current practices and recommendations from prior work. The paper's main recommendation is that, given a increase of computation budget, model size and number of training tokens should be scaled equally. The claims of the paper are supported with an extensive amount of experimentation, including 400 language models, model sizes ranging from 70M to over 16B parameters, and amounts of data ranging from 5 to 500 billion tokens. Reviewers either listed no weaknesses or had most of their concerns addressed by the authors' responses. The main remaining limitation is that the authors couldn't release any code or data, but the work seems mostly reproducible from the paper (extensive methodological and experimental details are given in the appendix).",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "XjQk_dgnLQw",
                "writer": "official_reviewer",
                "reply_to": "C9gCwt_sYUJ",
                "title": "Reply to Authors\u2019 Response (2)",
                "comment": " Thank you for including the new results in the Appendix D.1.\n\nThe discussion above addressed my major concerns. Thus, I am glad to increase Soundness score and overall rating. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "C9gCwt_sYUJ",
                "writer": "author",
                "reply_to": "9eetMKuP4fY",
                "title": "Random Seeds",
                "comment": " We have trained 5 different 1.1B models with different random data and included results in the Appendix. We have copied the text below, for ease:\n> We trained 5 different 1.1 billion parameter models on random subsets of the data to look at the variance in final performance.\nWe found that the average loss achieved was 2.488 with a standard deviation amongst the 5 runs of 0.00257. Given how small the differences are, we are confident than any given run is very indicative of a model of that size.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9eetMKuP4fY",
                "writer": "official_reviewer",
                "reply_to": "0l5vVfF7kbc",
                "title": "Reply to Authors\u2019 Response",
                "comment": " Thank you for the reply. The authors\u2019 response addressed my concern of interpolation method and data leakage. \nRegarding the weakness part, although the authors did not study the metrics from the theoretical perspective, there are some empirical results and conclusions in this paper, which are useful for practical scenarios.\nHowever, I was wondering whether if there is any research paper studying the variance between random seeds when training a LLM. It would be more convincing if the author could cite the existing findings in the Checklist part to support their claim.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "FIJn4GjMhr",
                "writer": "author",
                "reply_to": "XKQ6U26-VNe",
                "title": "Thanks for the review",
                "comment": " Thank you for your review. With respect to your question \u201cAre all tokens created equal?\u201d we added a brief discussion at the end of Appendix Section C. We certainly think that not all tokens are equal and a better understanding of how to assess data quality will be of utmost importance in the creation of even better language models.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "GmtPin6QRjr",
                "writer": "author",
                "reply_to": "fIBUHVR_fqc",
                "title": "Thank you for the review.",
                "comment": " Thank you for the review. Is there anything more we can do to improve the paper? \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "j4eCIXN1sj1",
                "writer": "author",
                "reply_to": "PmInOr131Z4",
                "title": "Thank you for the review.",
                "comment": " Thank you for the review. Understanding the optimal scaling properties of other model types is an exciting direction of future work. With respect to the impact of data quality, in the supplement we show isoFLOP analysis on both the C4 and GitHub datasets. In both cases, we recover the same scaling exponents (~0.5) suggesting that while a model trained on higher quality data may be better, we actually expect the scaling between model size and dataset size to hold independent of data quality (though better data quality will likely lead to better model performance).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0l5vVfF7kbc",
                "writer": "author",
                "reply_to": "r6SbeNf2lvt",
                "title": "Thank you for your review",
                "comment": " Thank you for the thorough and careful review.  We have updated the paper to correct the typos you pointed out. With respect to the questions:\n\n* Question 1: Thank you for pointing this out. We used scipy interp1d which uses a linear interpolant between data points. We have updated the text to make this clear.\n* Question 2: For Curation Corpus, Wikitext103, and LAMBADA we did perform test set filtering. However for the Pile, MMLU, and BIG-Bench we did not perform test set filtering as they are were collected after MassiveText was collected. There is a discussion on the test set filtering in \u201cScaling Language Models: Methods, Analysis & Insights from Training Gopher\u201d [2022]. In general, a better understanding of test set filtering and the impact it has in the extreme data regime is very important. However as Gopher and Chinchilla were trained on the same data we suspect the impact to be small, even though Chinchilla did see much more data. Additionally, the degree to which Chinchilla outperformed Gopher across many tasks (including BIG-Bench tasks) which are unlikely to be present in the training data suggests that the performance gains are not due to leakage.\n* Question 3 + Weakness 1: Better metrics to quantify how a model is undertrained is a very important research question, however one that we have not yet systematically approached. In the supplement, we show Figure A4 which attempts to provide some early quantification of this question. Specifically we show a model trained based on the approach from Kaplan et al compared to that which our analysis suggests. We find that a smaller model trained on more data is more performant.\n* With respect to the random seeds: for training these language models (smaller and larger), there is minimal variance between random seeds and therefore there is no benefit in running multiple seeds. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r6SbeNf2lvt",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_iBBcRUlOAPR",
                "title": "",
                "comment": " This paper presents an empirical study to explore the scaling law for training Transformer-based large language models (LLM). In this paper, the scaling law is the trade-off between model size and training tokens, and it is explored by different fitting methods. To obtain the empirical data for fitting the scaling curves, the authors train various language models by varying the model size and the training FLOP counts. The estimated power-law relationship indicates that the optimal scaling way is to increase the size of training data equally while enlarging the model size. To verify the correctness of this optimal scaling law, the authors train the *Chinchilla* model with larger training tokens but decrease the model size to 70B compared to *Gopher* (280B). According to the evaluation results shown in Appendix H, *Chinchilla* outperforms other *Gopher* on most benchmarks while sharing the same FLOPs cost. \n\nThe contributions of this paper are as follows: 1) refining the existing scaling law explored by *Gopher*; 2) revealing an optimal scaling law regarding the training tokens and model size for training LLMs.\n **Strength**:\n1. ***Significance***: The scaling law explored in this empirical study is useful to train the LLM. The authors also claim that the current LLMs are under-trained, the conclusion in this paper may show an interesting direction for this community to keep optimizing the LLMs: we need to pay attention to efficiently learning the data instead of enlarging the model size.\n2. ***Originality***: Although this paper follows the research methodology of a previous study [1], i.e., an empirical study, this paper eventually shows a new scaling law. \n\n**Weakness**:\n1. ***Soundness***: Since this paper uses the empirical method to explore the optimal scaling law, the theoretical foundation is not very solid. How can we judge whether a model is under-trained in a more sound way? Besides, given that the three modeling approaches in this paper rely on empirical training records, one problem is the random factors of training. As the authors claimed in Checklist 3.(c), the costs of training these models are expensive, they did not perform training with different random seeds.\n2. ***Clarity***: This paper is easy to follow and well-written. But there are some typos in the paper:\n    - Line 1: a transformer -> a Transformer\n    - Line 37: budget, Instead, -> budget, instead\n    - Line 71: [24] first -> Kaplan et al. [24] first \n    - Line 74: differs from [24] -> differs from Kaplan et al. [24]\n    - The paper title is different from the information shown in this forum \n\n**Reference**:\n\n[1] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. https://arxiv.org/abs/2001.08361\n\n***Update***: The authors addressed most of concerns mentioned in Weaknesses part. 1. In Section 3.1 (Line 130), you mentioned that smoothing and interpolation methods were applied to each training loss curve. I found the details of the smoothing method in Appendix D.1. But which interpolation method did you use in Approach 1?\n2. Have you examined the overlap between pre-training data and downstream fine-tuning data? (especially for the evaluation of language modeling tasks)\n3. You mentioned that current LLMs are under-trained. Is there any metric to evaluate the bottleneck of model capacity instead of empirical methods?\n\n***Update***: The authors have replied to Q1, Q2 with further clarification. - Societal Impact: The societal impact is positive. For those researchers who work on developing large-scale language models, the authors provide a useful scaling law to save the computational budget. \n- Limitation: The major limitation still lies in the research methodology, the empirical study may not always be solid as I stated in the Weaknesses part.\n",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "PmInOr131Z4",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_iBBcRUlOAPR",
                "title": "",
                "comment": " This paper explores what is the best model size and number or tokens for training Transformer models. They trained over 400 language models with model sizes ranging from 70M to over 16B parameters on 5 to 500 billion tokens, and it suggests the model size and training data should be scaled equally. Based on this finding, they train a 70B parameters model, a predicted compute-optimal model, called Chinchilla, on more training data which outperforms the Gopher 280B model with same compute budget.  Strengths:\n\nThe paper is very interesting and inspiring. \n\nIt gives a good guideline when people want to scale up their LMs. \n\nStrong results. \n\n\nWeaknesses:\n\nIt is only tested on autoregressive models. I\u2019m wondering whether this observation is held on BERT-like models.   \n I'm wondering how the data quality affects this scale-law.  None",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "fIBUHVR_fqc",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_iBBcRUlOAPR",
                "title": "",
                "comment": " Kaplan et al. (2020) suggested that large models should not be trained to their lowest possible loss to be compute-optimal: instead, model size should grow faster than the size of the training set, given a fixed computational budget increase. \nThis paper is a more thorough investigation of the question: what are the optimal model size and number of training tokens for a given training budget, tuning additional hyperparameters ignored by prior work? The authors find that model size and training budget should, in fact and in contrast to prior findings, be scaled equally -- since this isn't typically done, many state-of-the-art models are undertrained. They then use this insight to train a new model (which they call Chinchilla), which uses more data but the same compute as a state-of-the-art model and significantly outperforms the latter on multiple downstream tasks.  Strengths:\n- This paper provides experimental results that contradict previous findings and might be quite consequential for the future of large language models. \n- The experiments are quite extensive. (Except for the fact that there are limited complete pretraining runs, but this makes sense given the computational cost.) \n- The paper is clearly written.\n\nWeaknesses:\n- No obvious ones. None. The authors do explicitly address the limitations of their work in Section 5, which is great. I don't see any direct negative societal impact of the word besides, maybe, the deployment of biased language models, but the authors mention this risk. ",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "XKQ6U26-VNe",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_iBBcRUlOAPR",
                "title": "",
                "comment": " Current NLP is centered around language modeling. We've observed that their performance can significantly increase when their parameter count is increased and therefore a lot of effort has been focused on scaling that property. \nThe authors here note that the community hasn't really explored the importance of the amount of training data, and so that is the focus here. The authors, through many experiments, discover that when the number of parameters is doubled, so should the amount of training data. \nThe experiments are thorough and the marquee result is incredibly interesting and impactful- with a 70B param model they are able to outperform GPT-3 and other 170B+ parameter models on many challenging downstream tasks including MMLU and some Big Bench tasks.  Strengths:\n1) Very thorough experiments.\n2) Good writing\n3) An easy-to-understand conclusion that is very straightforward to implement. \n4) The bottom line here (how much data is needed to train big models) is an incredibly important result which will be useful for years to come.\n5) This paper explores a very important but very under-explored topic.\n\nWeaknesses:\n1) No code/models are released, which is really bad for reproducibility.  Are all tokens created equal? I didn't really see a discussion of this in the paper, but when you talk about doubling the training data, is any data going to be OK? Are there specific domains that are more beneficial for downstream tasks? I'm sure 20 papers could be written about this question, I don't think you need to provide all the answers, but it could be beneficial to have a brief discussion of this in the paper.\n\n The authors adequately addressed the limitations and potential negative societal impact of their work.",
                "rating": 8,
                "confidence": 5
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "finds that current Transformer-based large language models (LLM) are significantly undertrained",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "This",
                "Sentiment Expression": "likely to be of great interest to the AI/NLP community, as it challenges current practices and recommendations from prior work",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The paper's main recommendation",
                "Sentiment Expression": "is that, given a increase of computation budget, model size and number of training tokens should be scaled equally",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The claims of the paper",
                "Sentiment Expression": "are supported with an extensive amount of experimentation, including 400 language models, model sizes ranging from 70M to over 16B parameters, and amounts of data ranging from 5 to 500 billion tokens",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Reviewers",
                "Sentiment Expression": "either listed no weaknesses or had most of their concerns addressed by the authors' responses",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The main remaining limitation",
                "Sentiment Expression": "is that the authors couldn't release any code or data, but the work seems mostly reproducible from the paper (extensive methodological and experimental details are given in the appendix)",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "rRFIni1CYmy": {
        "paper_id": "iclr_2021_rRFIni1CYmy",
        "paper_title": "End-to-End Egospheric Spatial Memory",
        "paper_abstract": "Spatial memory, or the ability to remember and recall specific locations and objects, is central to autonomous agents' ability to carry out tasks in real environments. However, most existing artificial memory modules are not very adept at storing spatial information. We propose a parameter-free module, Egospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere around the agent, enabling expressive 3D representations. ESM can be trained end-to-end via either imitation or reinforcement learning, and improves both training efficiency and final performance against other memory baselines on both drone and manipulator visuomotor control tasks. The explicit egocentric geometry also enables us to seamlessly combine the learned controller with other non-learned modalities, such as local obstacle avoidance. We further show applications to semantic segmentation on the ScanNet dataset, where ESM naturally combines image-level and map-level inference modalities. Through our broad set of experiments, we show that ESM provides a general computation graph for embodied spatial reasoning, and the module forms a bridge between real-time mapping systems and differentiable memory architectures. Implementation at: https://github.com/ivy-dl/memory.",
        "paper_acceptance": "poster-presentations",
        "meta_review": "In this paper, the authors combine ideas from SLAM (using an Extended Kalman Filter and a state with nonlinear transitions and warping) and differentiable memory networks that store a spherical representation of the state (from the ego-centric point of view of an RL agent moving in an environment) with depth and visual features stored at each pixel and dynamics transitions corresponding to warping.\n\nThe main idea in the paper is very simple and elegant, but I will concur with the reviewers that the writing of the first version of the paper was extremely hard to understand and that the experimental section was too dense. Two subsequent revisions of the paper have dramatically improved the paper.\n\nGiven the spread of scores (R1: 6, R2: 7 and R3: 4) and the fact that only R1 and R2 have acknowledged the revisions, I will veer towards acceptance.\n",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "m5Q_yRkj82",
                "reply_to": "iclr_2021_rRFIni1CYmy",
                "title": "Interesting approach to egocentric memory based on forward warping; experiments cover a wide range of tasks, but are very hard to understand as there is little / no explanation of the evaluated methods; some claims seem too strong",
                "comment": "The paper considers the problem of creating spatial memory representations, which play important roles in robotics and are crucial for real-world applications of intelligent agents. The paper proposes an ego-centric representation that stores depth values and features at each pixel in a panorama. Given the relative pose between frames, the representation from the previous frame is transformed via forward warping (using known depth values) to the viewpoint of the current frame. The proposed approach has no learnable parameters. Experiments on a wide range of tasks show that the proposed approach outperforms baselines such as LSTM and NTM.\n\nOn the positive side, the approach is positively simple, in the sense that it relies on known techniques (EKF, forward warping, etc.) that ensure that it is easy to implement while achieving good results in the experiments. Up to Sec. 4, I found the paper easy to follow, although some design choices could be better motivated (e.g., I assume that diagonal covariances are assumed for simplicity).\nThe paper evaluates the proposed approach on multiple tasks and in various configurations, which is another strength of the paper.\n\nWhile I like the proposed approach, I also see multiple significant weaknesses:\n1) I found the experimental evaluation nearly impossible to understand. My main problem is that I don't understand what the different method that are evaluated are:\n * Given the abbreviation ESMN introduced in the abstract, I assume that ESMN is the proposed approach. ESM seems to be a variant of ESMN, but I am not sure how ESM and ESMN differ as the difference is never clearly described (or if it is, I seem to have missed it). Sec. 4.1.1 mentions training with a convolutional encoder in the context of ESMN, Sec. 4.1.3 and Sec. 4.1.4 only evaluate ESM but not ESMN, while Sec. 4.2 states that \"ESM represents map-only inference, while ESMN includes convolutions for both the image-level and map-level inference\". Unfortunately, the term \"map-level\" inference is not well-defined. Overall, I don't understand the difference between ESM and ESMN. As a result, it is unclear to me why ESM performs worse than ESMN in Tab. 1 for DR-Ego-S but comparable for all other tasks in the table, or why ESM and not ESMN is used for some of the experiments. Similarly, what is the \"ESM-DepthAvoid\" baseline? Does it only use depth and no features?\n * Tab. 1 contains a baseline called \"PO\" and I don't understand how it works. The abstract introduces PO as an abbreviation for partial observability, but that does not seem to be an explanation for a baseline. The PO baseline performs similarly well as ESM and ESMN in Tab. 1, which makes me wonder whether Tab. 1 really shows the superiority of the proposed approach.\n * I am confused by the statement \"In contrast, ESM by design stores features in the memory with meaningful indexing. The inclusion of relative cartesian co-ordinates in the memory image also effectively aligns each pixel with an associated relative translation.\" since the inclusion of such coordinates is never mentioned before. I assume ESM uses some form of handcrafted features?\n2) Some of the statements made in the paper seem too strong:\n * I don't see how the claim \"Our memory is much more expressive than these 2D examples, with the ability to represent detailed 3D geometry in all directions around the agent.\" (Sec. 2.2)  holds. I agree that being able to store information in 2.5D (panorama and depth) is more powerful than storing only top-down 2D maps. However, the allocentric maps of the references can store larger and more complicated scene parts. E.g., if an agent would turn around a corner, ESMN would essentially be forced to forget about everything that is not directly visible anymore as it is occluded and thus not included in the memory structure anymore. As such, a point can be made that ESMN is much less expressive than for example Henriques & Vedaldi. Given that the latter evaluate on significantly more complex scenes compared to the ones used in this paper (which do not have strong occlusions) strengthens this impression. \n * Regarding the statement \"Although the most recent depth frame is also a strong signal for local obstacle avoidance, we show that the avoidance based on the full ESM geometry results in fewer collisions when tested on a variant of the drone task with the inclusion of 25 obstacles.\": Looking at Tab. 2, it seems to me that the standard deviation is so large for both ESM and ESM-DepthAvoid that it is unclear to me whether one is consistently better than the other. \n3) The CodeSLAM approach from Bloesch et al. also provides a form of memory representation and I don't understand why the paper, and its follow-up (Zhi et al., SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded\nScene Representations, CVPR 2019), is not discussed in the related work section.\n\nOverall, I believe that the paper has potential. My main criticism is that I do not feel able to properly understand the experimental evaluation. As such, it is hard to recommend acceptance. However, I am willing to increase my score if the information necessary to understand this part of the paper is provided.\n\n### After rebuttal phase ###\nThe answers provided by the authors and the revised version of the paper sufficiently address my concerns. As such, I recommend to accept the paper. I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines. Still, I feel that in the current form, the paper can be accepted.",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "47cA43iaQcp",
                "reply_to": "iclr_2021_rRFIni1CYmy",
                "title": "Author Comments on the 3rd Paper Revision",
                "comment": "To further address some of the reviewer comments raised, we have uploaded a 3rd and final revision to the paper with regards to the discussion phase. We would like to again thank all reviewers for taking the time to provide very helpful feedback. We hope our answers to your questions and our revisions adequately address the concerns raised.\n\nWe provide a brief summary of the main changes in the latest revision below:\n\n1. The RL experiments are extended to include all 7 baseline methods, the same as used in the imitation learning experiments. This means seven methods are now evaluated on the sequential reacher task trained via RL, as opposed to only two in the previous revision. We hope this addresses concerns raised by Reviewer 3 regarding lack of RL baselines.\n\n2. A discussion on the LSTM auxiliary basline has been added to the appendices, with the losses on the validation set during imitation learning included in new plots. We outline overfitting as a failure mode of the auxiliary baselines, and expand discussions on the difficulty in tuning auxiliary losses for different tasks. We hope these plots and discussion further address concerns raised by Reviewer 3 regarding the need for additional baselines.\n\n3. Principal component visualizations of the implicit features stored in memory by the pre-ESM encoder (from the ESMN architecture) have been added to the appendices, as well as discussions on the meaning of these. This provides further insights into the manner in which ESMN succeeds at the different reacher tasks where ESMN-RGB fails. The encoded features visibly represent concepts such as foreground, background, and object edges, which are more expressive than color values alone.\n\n4. A runtime analysis has been included in the appendices. We compare across deep learning framework, computing device, and monocular and memory image dimensions. ESM with medium sized images, for example 480x640 monocular images and 360x720 memory images, is able to run at ~60fps. These results help to reinforce the statement in the abstract that: \"the module forms a bridge between real-time mapping systems and differentiable memory architectures\".",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Z3duNLAYpzH",
                "reply_to": "RuazhfNqnKj",
                "title": "Reviewer 2 Author Response",
                "comment": "Thank you for responding to our comments; we appreciate you taking the time to give further feedback, and we are very happy to hear that our answers and revisions have addressed most of your concerns.\n\nWe take on board the comment about the \"keyframe removal strategies\" being slightly unfair to CodeSLAM, and we also acknowledge your general thoughts on allocentric vs egocentric formulations, and the density of the experiment section. We respond to these points below.\n\nFirstly, we acknowledge that many simple and effective heuristics exist for keyframe selection, which combine time and space constraints, and we agree that preventing map growth should not be the only motivation for the egocentric formulation over allocentric approaches.\n\nIn the latest revision, we have removed the sentence \u201cTaking CodeSLAM as an example, keyframe removal strategies are needed to prevent the map size from growing indefinitely.\u201d. We have added a sentence elaborating on the benefits of the structured memory and egocentric indexing for downstream tasks. Quoting the latest revision: \u201c... Unlike allocentric formulations, the memory indexing is fully coupled to the agent pose, resulting in an ordered representation particularly well suited for downstream egocentric tasks, such as action selection.\u201d\n\nElaborating further on this general point, it is not clear how keyframe-based methods (like CodeSLAM) can represent the memory in a compact and ordered fashion suitable for conditioning a downstream reactive policy network. The keyframes could be stacked channelwise for example, but frame-stacking is already well explored in image-to-action learning, and even with sensible frame-skipping to maximise scene visibility, this is generally inferior to the use of dedicated memory such as LSTMs. Finding a better way of conditioning a policy on the keyframes is non-trivial. ESM instead represents the memory compactly as a single egocentric image, which can be input to CNNs directly, and outperforms existing memory baselines.\n\nRegarding the general egocentric vs. allocentric debate, we agree that egocentric mapping is by no means a replacement for allocentric mapping, and indeed, allocentric is inherently more expressive than egocentric. As you pointed out, our work explores contexts where local spatial memory is sufficient, and specifically how the egocentric formulation enables direct integration into neural networks for learning tasks in an end-to-end manner.\n\nFinally, regarding the experiment section, we acknowledge that there is a lot of content, and that the descriptions are quite compact as a result. We will explore moving some experiments to the Appendix and expanding the explanations in time for the camera-ready paper if accepted.\n\nAgain, we thank the reviewer for their very helpful comments, which have greatly helped to shape the paper. We look forward to Jan 12th.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RuazhfNqnKj",
                "reply_to": "n0fy07vabDl",
                "title": "The revised version addresses my concerns",
                "comment": "Thank you very much for the detailed answers to my questions and the revised version of the paper.\nYour answers and the revision address my concerns and I willing to change my rating to recommend acceptance.\n\nI think that the statement \u201cTaking CodeSLAM as an example, keyframe removal strategies are needed to prevent the map size from growing indefinitely.\u201d is a bit unfair towards CodeSLAM. ESM can only remember parts of the scene that are still visible. While CodeSLAM has the potential to \"remember\" larger areas, one could simply choose to forget old observations by dropping all but the last k keyframes. This strategy is commonly employed by visual odometry approaches (which in contrast to SLAM only keep local maps). As such, one does not need to develop keyframe removal strategies as suggested by the text.\nOverall, I am not convinced that egocentric representations are better than allocentric maps. In my opinion, the latter can easily encompass the former (by simply forgetting older observations) while the former cannot model the latter. On the downside, allocentric maps are harder to construct and maintain. As such, I see the paper as an interesting step towards comparing both types of representations. I see ESMs as an alternative to more complex maps and determining in which scenarios they are sufficient is an interesting question.\n\nIn my personal opinion, I still think that the experimental part of the paper is a bit hard to read. There are a lot of experiments and many baselines, but few of them are really described in detail. Many of these details are provided in the appendices, but this makes the paper hard to read as one needs to jump back and forth. Using a subset of the experiments, with a more detailed description of the experiments and the baselines, and providing the rest in the appendices would, in my opinion, improve readability. But this is my personal opinion and definitively not a reason for rejection.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2Ab6gHwGAYH",
                "reply_to": "iclr_2021_rRFIni1CYmy",
                "title": "Author Comments on the 2nd Paper Revision",
                "comment": "To further address some of the reviewer comments initially raised, we have uploaded a 2nd revision to the paper.\nWe provide a brief summary of the main changes below:\n\n1. We extend the Appendix to include the training and validation curves for each of the imitation learning networks, as well as a discussion on these curves in light of the policy performances. We also more thoroughly outline the finer details for each of the experiments throughout the paper.\n\n2. Following on from the request for additional baselines from Reviewer 3, we added a new baseline (LSTM-Aux) which combines auxiliary losses adapted from two papers which propose LSTM-based image-to-action learning pipelines. We include further details of this auxiliary baseline in the Appendix.\n\n3. Other smaller fixes/improvements to some figures and explanations throughout the text.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "n0fy07vabDl",
                "reply_to": "WoTDSD1K3pS",
                "title": "Reviewer 2 Author Response (2/2)",
                "comment": "**Looking at Tab. 2, it seems to me that the standard deviation is so large for both ESM and EMM-DepthAvoid that it is unclear to me whether one is consistently better than the other.**\n\nIn the original results, we ran the avoidance controller at the same frequency as the ESM updates, which resulted in a large number of collisions even when the obstacles were well detected. We now run the avoidance algorithm at 10x the ESM rate, and observe much clearer distinctions between the baselines. \u201cESM Depth Map Avoidance\u201d now outperforms \u201cSingle Depth Frame Avoidance\u201d by more than one standard deviation in both columns 4 and 6 of Table 2.\n\n**The CodeSLAM approach from Bloesch et al. also provides a form of memory representation and I don't understand why the paper, and its follow-up (Zhi et al., SceneCode: Monocular Dense Semantic Reconstruction using Learned Encoded Scene Representations, CVPR 2019), is not discussed in the related work section.**\n\nThe paper is mentioned in the related work section on the original upload, but was not significantly discussed. Our reasoning for this was that CodeSLAM does not use a fixed-size memory to encode the surrounding geometry. Rather, multiple keyframes are used, each of which optimizes a keyframe-specific code. The scene geometry is then composed of the individual keyframes projected into world space. \n\nHowever, we do acknowledge that this distinction could be made more clearly, and so in the revision we have extended our discussion of CodeSLAM in the related work section. The new sentence reads: \u201cTaking CodeSLAM as an example, keyframe removal strategies are needed to prevent the map size from growing indefinitely.\u201d\n\nWe also agree that SceneCode is a clear omission, particularly in light of our object segmentation experiments. We have added a short discussion of this in the object segmentation experiment section. Quoting the latest revision: \u201cOne approach is to perform image-level segmentation only in individual monocular frames, and then perform probabilistic fusion when projecting into the map\u2026 Another approach is to first construct an RGB map, and then pass this map as input to a network\u2026 SceneCode takes a different approach, and combines monocular predictions with multi-view optimization to gain the benefits of wider surrounding context.\u201d\n\n**Some design choices could be better motivated (e.g., I assume that diagonal covariances are assumed for simplicity).**\n\nWe thank the reviewer for pointing this out. We agree this could be made more clear, and we will include a sentence in the method section for the next paper revision stating: \"Diagonal covariances are assumed due to the large state size of the images, for which the full covariance matrices cannot be efficiently stored in memory.\"\n\nElaborating on this, the egosphere state in our experiments is 90x180. The covariance is then represented by an image of 16200 variance values using the diagonal assumption. The full covariance would include ~262 million values, which would dominate GPU memory.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "o5XbrJQ94D0",
                "reply_to": "BGlQS3nCO-V",
                "title": "Reviewer 1 Author Response",
                "comment": "We thank the reviewer for their helpful comments and feedback.\nWe address the main points of concern below.\n\n**I think it'll be worthwhile to elaborate on the \"Mono\" method, including the network architectures as well as training details and such. It will also help understand which part of ESMN improves over this baseline method.**\n\nWe agree, and we have added a high-level schematic of the architectural differences between Mono, LSTM/NTM, ESMN-RGB , and ESMN in Figure 2. We have also provided the full network architectures for the imitation learning and object segmentation experiments in the Appendices.\n\n**In the related work section, one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory. While the proposed approach ESMN is reasonable, I wonder if it is possible to have a concrete experiment comparing those two different designs.**\n\nWe do compare ESMN against NTM in our imitation learning experiments, the results are presented in Table. 1. For improved performance, we implement the same changes proposed by (Wayne et al., Unsupervised Predictive Memory in a Goal-Directed Agent, 2018). These changes are fully explained in the Appendices.\n\n**Could the authors elaborate on Sec 4.1.1 Fig 5? What's the meaning of the colorings in this figure, and how to interpret that?**\n\nWe thank the reviewer for pointing this out. We have improved the explanations of this figure both in the main text and in the figure caption. To answer the question directly, these images show the 2D representation of the features stored in the spherical ESM memory (in the same way that an atlas shows a 2D representation of a spherical globe), for adjacent timesteps in different reacher tasks, going chronologically from left to right. The rows correspond to separate chronological image sequences.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kiP3VcjE19",
                "reply_to": "_20jTGak8bl",
                "title": "Reviewer 3 Author Response (2/2)",
                "comment": "**The choice of baselines is not well motivated. Why were LSTM and NTM chosen in the first experiment? \nThere is very little rationale for the choice in the submission. Do these approaches show the state-of-the-art performance on the examined tasks?**\n\nWe have added a description at the beginning of section 4.1 clearly outlining the rationale behind our choice of baselines. Quoting this section: \u201cWhile ego-centric cameras are typically used when learning to navigate planar scenes from images(Mnih et al., 2016; Jaderberg et al., 2016; Zhu et al., 2017; Gupta et al., 2017; Parisotto & Salakhut-dinov, 2017), static scene-centric cameras are the de facto when learning multi-DOF controllers for robot manipulators (Levine et al., 2016; James et al., 2017; Matas et al., 2018; James et al.,2019b). We consider the more challenging and less explored setup of learning multi-DOF visuomotor controllers from ego-centric cameras, and also from moving scene-centric cameras. LSTMs are the de facto memory architecture in the RL literature (Mnih et al., 2016; Jaderberg et al., 2016; Espeholtet al., 2018; Kapturowski et al., 2018), making this a suitable baseline.  NTMs represent another suitable baseline, which have outperformed LSTMs on visual navigation tasks (Wayne et al., 2018). Many other works exist which outperform LSTMs for planar navigation in 2D maze-like environments(Gupta et al., 2017; Parisotto & Salakhutdinov, 2017; Henriques & Vedaldi, 2018), but the top-down representation means these methods are not readily applicable to our multi-DOF control tasks. LSTM and NTM are therefore selected as competitive baselines for comparison.\u201d\n\n**In the second experiment, the authors only use the LSTM network to train the RL agent. Various methods tackle the navigation problem in the RL domain, and hence the authors should choose better baselines for comparison.**\n\nReferring to the beginning of section 4.1, and also to our earlier response on the choice of baselines, we articulate why we find the LSTM and NTM methods to be strong baselines for the tasks we test on. Quoting section 4.1: \"LSTMs are the de facto memory architecture in the RL literature (Mnih et al., 2016; Jaderberg et al., 2016; Espeholtet al., 2018; Kapturowski et al., 2018), making this a suitable baseline.  NTMs represent another suitable baseline, which have outperformed LSTMs on visual navigation tasks (Wayne et al., 2018).\" \n\nWe do however agree that some other baselines could further strengthen our results. Auxiliary losses are a common addition to improve LSTM learning (Jaderberg et al., Reinforcement learning with unsupervised auxiliary tasks, 2016,  Sadeghi et al., Sim2Real View Invariant Visual Servoing by Recurrent Control, 2017,  Mirowski et al., Learning to Navigate in Cities Without a Map, 2019), and so we are running experiments which combine the LSTM baseline with the auxiliary losses as used in (Sadeghi et al., 2017, Mirowski et al., 2019). We feel this baseline plus the existing baselines represent a comprehensive set of competitive baselines for the tasks we evaluate on. We don\u2019t believe this additional baseline will change the results or conclusions of the paper. If the reviewer has specific examples of additional baselines they feel are missing, we would be very happy to run additional experiments to further validate our results.\n\n**I suppose that the PO task is some form of navigation task, although the authors never explain what the PO task is.**\n\nIn the original submission, PO in this context referred to a baseline which we called Partial-Omni. It did not stand for partial observability as introduced in the abstract. To avoid this clash of terminology, we now call this baseline Partial-Omni-Oracle (PO2). We have also moved the explanation of PO2 from the appendices into the main text, in section 4.1.\n\nTo further explain, PO2 is a baseline which uses a ground-truth omni-directional camera of the same dimensions as ESM (90x180), but with ESM re-projections used to mask the observed pixels throughout the image sequence. Quoting the revised paper: \u201cPO2 cannot see regions where the monocular camera has not looked, but it maintains a pixel-perfect memory of anywhere it has looked.\u201d\n\n**The simple approach to directly quantize pixel projections leads to artifacts in the map. It is a bit unclear why this option was chosen, especially now that progress on differentiable renderers has been made.**\n\nThis was chosen for simplicity. Our module is implemented using core tensor operations alone, with scatter_nd responsible for the projections and depth buffer. Scatter functions are available in all major deep learning frameworks, whereas differentiable rendering functions are not. We see benefit in the implementational simplicity, particularly when our results indicate that this simpler method is sufficient. Having said this, the ESM module could easily be modified to use rasterization and mesh rendering, without change to other aspects of the method.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fVugSAqhLax",
                "reply_to": "iclr_2021_rRFIni1CYmy",
                "title": "Author Comments on the Paper Revision",
                "comment": "We thank all reviewers for their helpful comments and feedback.\n\nWe have uploaded a new revision of the paper, and responded directly to each reviewer, outlining where our revised paper addresses the reviewer\u2019s comments.\n\nWe provide a brief summary of the major changes as a result of the reviewers\u2019 comments below:\n\n1. Clearer explanations and diagrams outlining the method, in particular Figure 1.\n\n2. Diagram of the different network architectures, clearly outlining the difference between ESMN and ESMN-RGB, as well as the Mono and LSTM/NTM baselines in Figure 2.\n\n3. Improved explanation for the choice of experiments and motivation, improved explanations of the experiments, and better justification for the choice of baselines in section 4.\n\n4. Improved clarity for Figure 4, which presents sequences of the full RGB memory through time.\n\nOther changes have been made which are not in direct response to reviewer comments. We list these additional changes here for the reference of all reviewers:\n\n1. Paper title changed from \u201cEgo-Centric Spatial Memory Networks\u201d to \u201cEnd-to-End Egospheric Spatial Memory\u201d, in order to better capture the novelty of the method in the spherical representation.\n\n2. Added new experimental results on reacher tasks conditioned on shape ID, further showing the benefits of learning features through the memory, using the ESMN architecture.\n\n3. Added new experimental results for policies generalizing between ego-centric and scene-centric image acquisition, and added a diagram showing example image trajectories for both modes of acquisition.\n\n4. Added new experimental results for the obstacle avoidance task, following improvements to the local avoidance algorithm. The benefits of the ESM geometry over single depth frame avoidance is now more evident in the results.\n\n5. Added new experimental results for the object segmentation task, which explore the effects of changing monocular image resolution, memory resolution, and image sequence length on segmentation accuracy. Also added an improved diagram showing the object segmentation predictions, and failure modes of the mono method which ESMN is able to overcome.\n\nIn addition to these changes to the paper, we have also added a 5th video to the shared site showing object segmentation results on a test scene from the ScanNet dataset. The shared site can be found at:\nhttps://sites.google.com/view/egocentric-spatial-memory-nets\n\nWe hope these changes make the paper easier to follow, and the inclusion of new results create a stronger impression of the utility of our ESM module in different learning contexts involving spatial reasoning.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_20jTGak8bl",
                "reply_to": "E74-ljiAYm4",
                "title": "Reviewer 3 Author Response (1/2)",
                "comment": "We thank the reviewer for their helpful comments and feedback.\nWe address the main points of concern below.\n\n**The authors only briefly outline the overview of the method and then jump into specific details. The method section would benefit from restructuring, especially giving a clear overview of the method would help the reader to understand the method better.**\n\nWe thank the reviewer for pointing this out. We have modified the method section to introduce the module more clearly, and also extended the section to explain the integration of our ESM module in the network architectures used for the experiments. We hope this clarifies confusion, otherwise we are very happy to make further changes to benefit understanding.\n\n**The text mostly focuses on improvement, and hence it is hard to judge which parts are novel and what the contribution is.**\n\nWe thank the reviewer for pointing this out. We have added a sentence in the introduction clearly outlining the key contribution: \u201cTo the best of our knowledge, ESM is the first end-to-end trainable egocentric memory with full panoramic representation, enabling direct encoding of the surrounding scene in a 2.5D image.\u201d\n\nWe hope this helps to clarify the central novelty of the method, but we are happy to make further changes to the method section if this remains unclear.\n\n**u_t is first defined as incremental pose measurements and then as a control vector.**\n\nWe thank the reviewer for pointing this out. Our reasoning behind this was that control vectors in state estimation are usually measurements of some kind, for example, velocity or odometry measurements. Therefore, these two descriptions of u_t are not contradictory. However, to improve clarity, we now only use the term: \u201cincremental pose measurement\u201d.\n\n**Kamera intrinsic matrix K_1 is never introduced.**\n\nWe thank the reviewer for pointing this out. The term K_1^(-1) was introduced as the inverse camera intrinsic matrix in the text just before Eq. 1. We assume the reviewer would prefer K_1 be introduced before K_1^(-1), and so we have now added a more explicit introduction to the K_1 parameter earlier in the same sentence.\n\n**Experiments are not well explained. In all experiments, the experimental set-up is not well defined.**\n\nWe agree that the experiments were not clear in the original submission. The experiment descriptions have now been improved, and some important content has been moved from the appendices to the main body. We hope these changes help the reviewer to better understand the experiments - we are very happy to make further changes to benefit understanding.\n\n**The experimental set-up needs to be clearly outlined, and the goal of the experiment needs to be provided.**\n\nWe have added a new paragraph at the beginning of the experiments section (Sec 4), which outlines the overall goal of the experiments. We have also improved the clarity of the experiments section. Quoting the new introductory paragraph at the beginning of section 4: \n\n\u201cThe goal of our experiments is to show the wide applicability of ESM to different embodied 3D learning tasks, where ESM outperforms existing memory baselines. We test two different applications:\n\n1.Image-to-action learning for multi-DOF control (Sec 4.1).  Here we consider drone and robot manipulator target reacher tasks using either ego-centric or scene-centric cameras.We then assess the ability for ESMN policies to generalize between these different camera modalities, and assess the utility of the ESM geometry for obstacle avoidance.  We train policies both using imitation learning (IL) and reinforcement learning (RL).\n\n2.Object segmentation (Sec 4.2). Here we explore the task of constructing a semantic map, and the effect of changing the ESM module location in the computation graph on performance.\u201d\n\n**In the first experiment, we find out that the authors use imitation learning only from the appendix.**\n\nWe thank the reviewer for pointing this out, this has now been moved to the main body.\n\n**In the second experiment, the environment used for the evaluation is not introduced.**\n\nWe thank the reviewer for pointing this out. The description of the reinforcement learning experiment has now been extended, to properly introduce the environment. Quoting the revision: \u201cWe therefore train both ESMN-RGB and an LSTM baseline on a simpler variant of the MR task via DQN. We refer to this variant as MR-Seq, where the manipulator must reach red, blue and then yellow spherical targets from egocentric observations, after which the episode terminates. The only other difference to MR is that MR-Seq uses 128x128 images as opposed to 32x32.\u201d\n\nWe emphasize that the original MR task is modified from the RLBench reacher task, and further details about the task setup can be found in the code and paper for this benchmark.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WoTDSD1K3pS",
                "reply_to": "m5Q_yRkj82",
                "title": "Reviewer 2 Author Response (1/2)",
                "comment": "We thank the reviewer for their helpful comments and feedback.\nWe address the main points of concern below.\n\n**I am not sure how ESM and ESMN differ as the difference is never clearly described**\n\nWe acknowledge that this was unclear in the original submission, and have now added network diagrams clearly outlining the differences in Figure 2. We have also slightly changed terminology to avoid such confusion. Before, we used ESM interchangeably to refer to both the standalone module and to a network using the module without convolutions beforehand, with RGB values projected into the module.\n\nWe now refer to ESM purely as the standalone parameter-free memory module, and refer to the two network variants which make use of this module as ESMN-RGB and ESMN, to keep the distinctions clear.\n\nIn terms of the difference between ESMN and ESMN-RGB, ESMN projects convolutional features into the ESM module, whereas ESMN-RGB directly projects RGB features from the acquired images into the ESM module. Again, Figure 2 outlines this architectural difference more clearly.\n\n**What is the \"ESM-DepthAvoid\" baseline? Does it only use depth and no features?**\n\nWe have renamed this baseline as \u201cSingle Depth Frame Avoidance\u201d for better clarity, and added an explanation in the obstacle avoidance section.\n\nTo answer your question directly, this baseline performs obstacle avoidance using only the geometry available from the most recent depth frame. In contrast, \u201cESM Depth Map Avoidance\u201d uses the depth information stored in the full ESM memory.\n\n**Tab. 1 contains a baseline called \"PO\" and I don't understand how it works.**\n\nIn the original submission, PO in this context referred to a baseline which we called Partial-Omni. It did not stand for partial observability as introduced in the abstract. To avoid this clash of terminology, we now call this baseline Partial-Omni-Oracle (PO2). We have also moved the explanation of PO2 from the appendices into the main text, in section 4.1.1.\n\nPO2 is a baseline which uses a ground-truth omni-directional camera of the same dimensions as ESM (90x180), but with ESM re-projections used to mask the observed pixels throughout the image sequence. Quoting the revised paper: \u201cPO2 cannot see regions where the monocular camera has not looked, but it maintains a pixel-perfect memory of anywhere it has looked.\u201d\n\n**I am confused by the statement \"In contrast, ESM by design stores features in the memory with meaningful indexing. The inclusion of relative cartesian coordinates in the memory image also effectively aligns each pixel with an associated relative translation.\" since the inclusion of such coordinates is never mentioned before. I assume ESM uses some form of handcrafted features?**\n\nThis statement in the original submission was actually intended to say \u201cthe inclusion of polar coordinates in memory\u201d. We apologise for this mistake.\n\nWe have modified the method section to bring earlier attention to these coordinates. We now consider the polar coordinates as part of the egosphere state. Quoting the beginning of our revised section 3: \u201cThe egosphere image consists of 2 channels for the polar and azimuthal angles, 1 for radial depth, and n for encoded features. The angles are not included in the covariance, as their values are implicit in the egosphere image pixel indices. The covariance only represents the uncertainty in depth and features at these fixed equidistant indices.\u201d\n\nWe then refer to these again in the revised section 3.3: \u201cThe inclusion of polar angles, azimuthal angles and depth means the full relative polar coordinates are explicitly represented for each pixel in memory\u201d\n\nFinally, quoting our revision of the sentence which caused confusion: \u201cESM by design stores the encoded features in memory with meaningful indexing. The ESM structure ensures that the encoded features for each pixel are aligned with the associated relative polar translation, represented as an additional feature in memory.\u201d\n\nWe hope that these revisions clarify the original confusion.\n\n**I don't see how the claim \"Our memory is much more expressive than these 2D examples, with the ability to represent detailed 3D geometry in all directions around the agent.\" (Sec. 2.2) holds.**\n\nWe agree that this statement was too simplistic and strong, and we have modified the statement with a more balanced comparison. Indeed, the benefits of our approach (detailed immediate unoccluded local geometry with orientation aware indexing) are complementary to 2D methods (occlusion aware planar understanding for navigation).\n\n\nThe sentence now reads: \u201cOur memory instead focuses on local perception, with the ability to represent detailed 3D geometry in all directions around the agent. The benefits of our module are complementary to existing 2D methods, which instead focus on occlusion aware planar understanding suitable for navigation.\u201d",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "E74-ljiAYm4",
                "reply_to": "iclr_2021_rRFIni1CYmy",
                "title": "Ego-centric Spatial Memory Networks - Review",
                "comment": "# Summary\n-------\nThis paper presents a method to build an ego-centric spatial memory map from an agent's viewpoint. This map module is differentiable and can be used for a variety of tasks, such as object segmentation, or image-to-action learning in different control tasks.\n\n# Pros\n----\n+ The idea of combining the ego-centric representation seems interesting and novel. \n+ Evaluation in Image-to-Action learning shows good performance compared to baselines. \n\n# Cons\n----\n   *  _[Major]_ The Method section is very confusing. The method is based on the EKF pipeline and modifies the particular parts of the pipeline. The authors only briefly outline the overview of the method and then jump into specific details.  The text mostly focuses on improvement, and hence it is hard to judge which parts are novel and what the contribution is. Furthermore, there are errors in notation and variables that are not properly explained. u_t is first defined as incremental pose measurements and then as a control vector. Kamera intrinsic matrix K_1 is never introduced. The method section would benefit from restructuring, especially giving a clear overview of the method would help the reader to understand the method better.\n\n   * _[Major]_ Experiments are not well explained, and more baselines are necessary for proper evaluation. In all experiments, the experimental set-up is not well defined. For example, in the first experiment, we find out that the authors use imitation learning only from the appendix. In the second experiment, the environment used for the evaluation is not introduced. The experimental set-up needs to be clearly outlined, and the goal of the experiment needs to be provided. The choice of baselines is not well motivated. Why were LSTM and NTM chosen in the first experiment? There is very little rationale for the choice in the submission. Do these approaches show the state-of-the-art performance on the examined tasks? In the second experiment, the authors only use the LSTM network to train the RL agent. Various methods tackle the navigation problem in the RL domain, and hence the authors should choose better baselines for comparison. I suppose that the PO task is some form of navigation task, although the authors never explain what the PO task is. \n\n   * _[Minor]_ The simple approach to directly quantize pixel projections leads to artifacts in the map. It is a bit unclear why this option was chosen, especially now that progress on differentiable renderers has been made.\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BGlQS3nCO-V",
                "reply_to": "iclr_2021_rRFIni1CYmy",
                "title": "Reivew",
                "comment": "The paper introduced Egocentric Spatial Memory Networks (ESMN), a novel learning paradigm and architecture for encoding spatial memory in a sphere representation. The representation can be used for several downstream applications ranging from semantic segmentation as well as action learning for robotics applications.\n\nThe formulation of the proposed approach builds on a Kalman Filter setting and encodes memory in a spherical structure. Both the formulation and design makes sense, and the downstream applications show that the proposed approach is indeed plausible.\n\nWhile I think the value of this paper is well justified, I do have a few comments on the paper:\n- I think it'll be worthwhile to elaborate on the \"Mono\" method, including the network architectures as well as training details and such. It will also help understand which part of ESMN improves over this baseline method.\n- In the related work section, one argument made by authors is that ESMN and MemNN/NTM are two different paradigms for learning spatial memory. While the proposed approach ESMN is reasonable, I wonder if it is possible to have a concrete experiment comparing those two different designs.\n- Could the authors elaborate on Sec 4.1.1 Fig 5? What's the meaning of the colorings in this figure, and how to interpret that?",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The writing of the first version of the paper",
                "Sentiment Expression": "was extremely hard to understand",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the experimental section",
                "Sentiment Expression": "was too dense",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "two subsequent revisions of the paper",
                "Sentiment Expression": "have dramatically improved the paper",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "rylDzTEKwr": {
        "paper_id": "iclr_2020_rylDzTEKwr",
        "paper_title": "Variational Hashing-based Collaborative Filtering with Self-Masking",
        "paper_abstract": "Hashing-based collaborative filtering learns binary vector representations (hash codes) of users and items, such that recommendations can be computed very efficiently using the Hamming distance, which is simply the sum of differing bits between two hash codes. A problem with hashing-based collaborative filtering using the Hamming distance, is that each bit is equally weighted in the distance computation, but in practice some bits might encode more important properties than other bits, where the importance depends on the user. \n      To this end, we propose an end-to-end trainable variational hashing-based collaborative filtering approach that uses the novel concept of self-masking: the user hash code acts as a mask on the items (using the Boolean AND operation), such that it learns to encode which bits are important to the user, rather than the user's preference towards the underlying item property that the bits represent. This allows a binary user-level importance weighting of each item without the need to store additional weights for each user. We experimentally evaluate our approach against state-of-the-art baselines on 4 datasets, and obtain significant gains of up to 12% in NDCG. We also make available an efficient implementation of self-masking, which experimentally yields <4% runtime overhead compared to the standard Hamming distance.",
        "paper_acceptance": "reject",
        "meta_review": "There was a clear consensus amongst reviewers that the paper should not be accepted. This view was not changed by the rebuttal. Thus the paper is rejected. ",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "Sye7HBCqsS",
                "reply_to": "HyevKp7TFr",
                "title": "Author response part 2",
                "comment": "Q: \u201cTo realize the self-masking role, the paper proposed to use the function f(z_u, z_i)=g(Hamming_self-mask(z_u, z_i)), Equ. 12, and demonstrate the effectiveness of using this function by experiments. Since the necessity of using self-masking is not very convincing, I doubt whether this self-masking function f(z_u, z_i) is indispensable. Maybe, if some other functions that takes z_u and z_i as input are used, better results may be observed. \u201c\nA: The necessity of self-masking is due to the inability to *efficiently* compute a *weighted* Hamming distance, as it is only based on a hardware supported sum (popcount) and a Boolean XOR. Motivated by the hypothesis that not all bit dimensions are equally important for each user, we derive self-masking as a way to do binary weighting. The binary weighting is implemented by masking the item by the user hash code (i.e, using an AND operation), such that the user hash code determines which dimensions are important for the user. After the masking, the Hamming distance is now effectively only computed on the bit dimensions set to 1 in the user hash code (since all items have a value of -1 in dimensions that are -1 in the user hash code).\n    While in principle other functions taking z_u and z_i as input might exist, they would have to be described using only Boolean operations and without using other inputs (otherwise, it would no longer be fast to compute). Thus, this set of possible functions is extremely limited, and we have not been able to find any other meaningful functions than our self-masking. \n    Without self-masking, we are left with our VaH-CF model as seen in Table 2, where the experimental results clearly show the large performance improvement obtained by self-masking.\n\nWe hope the above comments clarify the uncertainties, and that the reviewer would reconsider their recommendation.\n\n[1] Chenghao Liu, Tao Lu, Xin Wang, Zhiyong Cheng, Jianling Sun, and Steven C.H. Hoi. 2019. Compositional Coding for Collaborative Filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). ACM, New York, NY, USA, 145-154.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bke64EA9oS",
                "reply_to": "SJgFWI0B9r",
                "title": "Author response",
                "comment": "We thank the reviewer for their insightful review, which we have taken into consideration for improving our submission. Below we detail each comment individually.\n\nQ: \u201cIn Table 2, the reported results of MF and its variants are lower than those of DCF. These results are very unreasonable. I have also conducted many similar experiments and the obtained results are not consistent with the results this paper reported.\u201d\nA:  We used the code as provided in the CCCF [1] GitHub repository [2]. However, after your comment, we found that a wrong distance measure had been used for MF, and this has now been correctly changed to the dot product. We have updated the MF baseline results in the revised paper, and the performance is now notably higher, as expected (thus reflecting the performance scores observed in previous work). This, however, does not change the performance comparison against the state-of-the-art hashing based approaches, which is the focus of the paper and where our approach still leads to a very substantial improvement.\n\nQ: \u201cIn addition, in the original paper, CCCF is reported to achieve the superior performance over DCF. However, in this paper, the results are also inconsistent.\u201d\nA: CCCF learns floating-point weights that are used for the weighted Hamming distance computation. In their original paper, the authors (mistakenly, we believe) do not count these floating-point weights when counting the total number of bits for each hash code, thus leading to an unfair experimental comparison. For example, they use 8 blocks for all hash code lengths, which corresponds to 128-512 (depending on the floating-point precision) additional unaccounted bits compared to the baselines (this is a very significant difference when using hash codes of length 32-64). \n    When we account for the floating-point weights, CCCF obtains similar performance to DCF, which is to be expected, since single-block CCCF has an almost identical formulation as DCF. As described in our Section 4.3, we experiment with block sizes of {8,16,32,64} and count each floating-point as only 16 bits \u2014 we try all combinations within the bit budget (32 and 64) and report the best performance (if a single block is chosen, we do not count the floating-point weight). We focus on hash code lengths of 32 and 64 as these correspond to the common machine word lengths, and thus the case where the Hamming distance is fastest to compute (compared to cases where the hash code spans multiple machine words).\n\nQ. \u201cTo sum up, the reported results in the paper are not convincing to me. I would doubt that there is no much accuracy improvement compared to the state-of-the-art methods.\u201d\nA: We have revised our paper in response to the reviewer\u2019s comments regarding the two questions above. Our results (see Table 2) show our approach does in fact provide a large improvement compared against the state-of-the-art hashing based methods. We hope our comments and paper changes related to the previous questions above clarify this in a satisfactory manner. \n\nQ: \u201cFinally, as I understand, using variational hashing to maximize the likelihood of all observed items and users will inevitably increase the training time. The presented experiments, however, include no results on the efficiency comparison.\u201d\nA: In Section 4.5 we analyze the convergence rate. Our self-mask method enables the model to converge extremely fast (often 5-20 epochs)  compared to the no self-masking case (up to 1000 epochs), which means it is fast to train.\n\nQ: \u201cIf there are no advantages on the recommendation accuracy and efficiency, the motivations of this designed hashing method are vague.\u201d\nA: We hope our changes and explanations above provide evidence of the advantages of our approach with regards to both effectiveness and (training/runtime) efficiency, and that the reviewer will revise their recommendation.\n\n[1] Chenghao Liu, Tao Lu, Xin Wang, Zhiyong Cheng, Jianling Sun, and Steven C.H. Hoi. 2019. Compositional Coding for Collaborative Filtering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). ACM, New York, NY, USA, 145-154.\n[2] https://github.com/3140102441/CCCF\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1gVPH09jr",
                "reply_to": "HyevKp7TFr",
                "title": "Author response part 1",
                "comment": "We thank the reviewer for their insightful review, which we have taken into consideration for improving our submission. Below, we detail each comment individually.\n\nQ: \u201cI have concerns over using the VAE to model the ratings of each user and item pairs here. Essentially, you are building a VAE for every rating value R_u, i.  Although the parameters are shared, the VAE\u2019s have their own prior. For generative models like VAE, they need to learn from a lot of data, not just one data point. From the perspective of generating data looking similar to the training data, I don\u2019t think your model have learned anything. Only the reconstruction part is important to your model. \u201c\nA: It is correct that only the reconstruction part is important in our case, since we do not need the generative properties of the model for its use-case of collaborative filtering. As described in Section 4.1, and similarly done in related work, we require each user to have at least 10 ratings, and each item to be rated at least 10 times, such that the model does have data to learn the dynamics between users and items. While this amount of data may not be sufficient to utilize the generative properties, we find that it is sufficient for learning hash codes that enable highly efficient item recommendations. We have extended our revised paper with this information in Section 3.4.\n\nQ: \u201cThe idea of using discrete VAE for hashing tasks has been explored before, see \u201cNASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing\u201d.  It employed a very similar idea, although it is not used for CF, but for hashing directly. The novelties of the model is limited.\u201d\nA: Methods based on discrete VAE have indeed previously been used in the task of semantic hashing for both text and images, where text/images are transformed to short hash codes similarly to our work. In contrast to these, we are the first to design a discrete VAE for hashing-based collaborative filtering, whereas existing approaches are based on traditional non-neural setups. However, the most important contribution of our work is the introduction of self-masking in the Hamming distance. While this may appear to be minor, we are the first to consider changes to the Hamming distance which we experimentally show to significantly improve performance.\n\nQ: \u201cSince ratings are essentially ordinal data, using Gaussian distribution to model the rating data may be not appropriate.\u201d\nA: We use a Gaussian distribution to model the difference between the observed and estimated ratings. While one option would be to use different variances depending on the rating, this would effectively enforce different weights in the reconstruction. We make the choice of weighting all ratings equally and thus fix the variance to be the same across all ratings. As such, we do not agree that it is inappropriate, and in fact this leads to an MSE objective identical to our baselines (which implicitly make the same Gaussian assumption). We have updated Section 3.4 in our revised paper to reflect this.\n\nQ: \u201cThe whole paper, especially the model, is not presented well. The model is not presented in a rigorous way, and some sentences in this paper are difficult to follow.\u201d\nA: We have updated the paper and made small changes to improve the presentation.\n\nQ: \u201cThe runtime analysis is not sufficient. In addition to comparing with the methods using hamming distance, we also want to see the advantages of the hamming based method over the real-value based method on speed acceleration.\u201d\nA: We have now added the floating-point case using the dot product (see Section 4.6).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BylyTNC9oH",
                "reply_to": "H1lGdIJEKH",
                "title": "Author response",
                "comment": "We thank the reviewer for their insightful review that we have taken into consideration for improving our submission. Below we detail each comment individually.\n\nQ: \u201c1 The studied problem (i.e., rating prediction) is well studied in the community of recommender systems, and there are many more accurate algorithms than the basic MF algorithm used in the empirical studies. I thus suggest the authors to include more such algorithms though the focus of this paper is for efficiency (but the authors also claim the accuracy of the proposed model).\u201d\nA: While this could in principle be included, the focus of our work is on efficient hashing based approaches, and similar to all related work in this and similar domains (described in Section 2, but also that of \u201csemantic hashing\u201d not part of our related work), this is outside the scope of the paper. Thus, we keep an experimental setup similar to related work, where the focus is on the performance among efficient hashing based approaches.\n\nQ: \u201c2 The authors are suggested to give a brief explanation on choosing those baseline methods in the context of other hashing-based CF methods.\u201d\nA: There has been fairly scant research done on hashing based approaches for collaborative filtering with explicit feedback without side information (e.g., content information such as item descriptions or user reviews). As such, DCF and CCCF represent the state-of-the-art in this domain. We have updated parts of section 4.2 to provide a better explanation for our choices.\n    An older two-stage rounding approach named BCCF has been used in related work, however, all current methods have been shown to outperform this approach, which is why we have not included it as part of our experimental setup. \n\nQ: \u201c3 Some details are missing, e.g., the number of latent dimensions in MF. And some presentation in the parameter setting are not clear, e.g., 'is chosen consistently across all data sets', 'was consistently chosen'. Does 'consistently' means 'exactly the same'?\u201d\nA: The latent dimension in MF is the same as the number of bits used, e.g., for hash codes of 32 bits the competing MF would use a latent dimension of 32 as well. \nYes, \u201cconsistently chosen\u201d means exactly the same. \nWe have updated the paper to reflect both points.\n\nWe hope the above comments clarify the uncertainties, and that the reviewer will reconsider their recommendation.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1lGdIJEKH",
                "reply_to": "iclr_2020_rylDzTEKwr",
                "title": "Official Blind Review #3",
                "comment": "In this paper, the authors study a classical (and well-studied) problem called rating prediction from a new perspective, i.e., learning binary vector representations of the users' and items' latent representations for efficiency. In particular, the authors introduce a personalized self-masking shown in Eq.(2) and in Figure 1 (the 'AND' operation) in order to improve the previous hashing-based collaborative filtering methods without increasing the time cost much.\n\nEmpirical studies on four public datasets show the effectiveness of the proposed model, i.e., variational hashing-based collaborative filtering with self-masking (VaHSM-CF).\n\nSome comments/suggestions:\n\n1 The studied problem (i.e., rating prediction) is well studied in the community of recommender systems, and there are many more accurate algorithms than the basic MF algorithm used in the empirical studies. I thus suggest the authors to include more such algorithms though the focus of this paper is for efficiency (but the authors also claim the accuracy of the proposed model).\n\n2 The authors are suggested to give a brief explanation on choosing those baseline methods in the context of other hashing-based CF methods.\n\n3 Some details are missing, e.g., the number of latent dimensions in MF. And some presentation in the parameter setting are not clear, e.g., 'is chosen consistently across all data sets', 'was consistently chosen'. Does 'consistently' means 'exactly the same'?\n\nMinors:\nThere are some typos: 'a user, u, and', 'of e.g., restaurant and shopping malls', etc.\n\n",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HyevKp7TFr",
                "reply_to": "iclr_2020_rylDzTEKwr",
                "title": "Official Blind Review #1",
                "comment": "The author proposes a variational encoder (VAE) based approach to perform hashing-based collaborative filtering, which focuses on the weighting problem of each hash-code bit by applying the \"self-masking\" technique. This proposed technique modifies the encoded information in hash codes and avoids the additional storage requirements and floating point computation, while preserving the efficiency of bit manipulations thanks to hardware-based acceleration. An end-to-end training is achieved by resorting the well-known discrete gradient estimator, straight-through (ST) estimator. \n\nStrength:\nThe idea of employing the discrete VAE framework to perform hashing-based collaborative filtering is interesting. From the perspective of applications, I think it is somewhat novel.\n\nThe experimental results demonstrate the performance superiorities of the self-masking hashing-based collaborative filtering method. \n\n\n\nWeakness:\n\nI have concerns over using the VAE to model the ratings of each user and item pairs here. Essentially, you are building a VAE for every rating value R_u, i.  Although the parameters are shared, the VAE\u2019s have their own prior. For generative models like VAE, they need to learn from a lot of data, not just one data point. From the perspective of generating data looking similar to the training data, I don\u2019t think your model have learned anything. Only the reconstruction part is important to your model. \n\nThe idea of using discrete VAE for hashing tasks has been explored before, see \u201cNASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing\u201d.  It employed a very similar idea, although it is not used for CF, but for hashing directly. The novelties of the model is limited.\n\nSince ratings are essentially ordinal data, using Gaussian distribution to model the rating data may be not appropriate.\n\nThe whole paper, especially the model, is not presented well. The model is not presented in a rigorous way, and some sentences in this paper are difficult to follow.\n\nThe runtime analysis is not sufficient. In addition to comparing with the methods using hamming distance, we also want to see the advantages of the hamming based method over the real-value based method on speed acceleration.\n\n\nOther question:\n\nTo realize the self-masking role, the paper proposed to use the function f(z_u, z_i)=g(Hamming_self-mask(z_u, z_i)), Equ. 12, and demonstrate the effectiveness of using this function by experiments. Since the necessity of using self-masking is not very convincing, I doubt whether this self-masking function f(z_u, z_i) is indispensable. Maybe, if some other functions that takes z_u and z_i as input are used, better results may be observed.  \n",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SJgFWI0B9r",
                "reply_to": "iclr_2020_rylDzTEKwr",
                "title": "Official Blind Review #2",
                "comment": "In this paper, the authors proposed an end-to-end variational hashing-based collaborative filtering scheme with self-masking to solve the efficiency of large-scale recommender systems. It mainly targets at addressing a problem in existing hashing-based collaborative filtering methods, where each bit is equally weighted in the Hamming distance computing process. To this end, the presented hashing scheme develops a self-masking technique to encode which bits are important to the user. The comparative experiments on several datasets demonstrate the superior performance of the presented hashing method.\n\nThe idea is interesting in the sense that an efficient self-masking technique is proposed to generate user-adaptive hash codes, by differentiating the importance of binary bits, for a fast recommendation system. However, the issues with the experimental results make me inclined to reject this paper.\n\nIn Table 2, the reported results of MF and its variants are lower than those of DCF. These results are very unreasonable. I have also conducted many similar experiments and the obtained results are not consistent with the results this paper reported.\n\nIn addition, in the original paper, CCCF is reported to achieve the superior performance over DCF. However, in this paper, the results are also inconsistent.\n\nTo sum up, the reported results in the paper are not convincing to me. I would doubt that there is no much accuracy improvement compared against the state-of-the-art methods.\n\nFinally, as I understand, using variational hashing to maximize the likelihood of all observed items and users will inevitably increase the training time. The presented experiments, however, include no results on the efficiency comparison.\n\nIf there are no advantages on the recommendation accuracy and efficiency, the motivations of this designed hashing method are vague.\n\nBased on the above reasons, I tend to reject this paper.",
                "rating": 1,
                "confidence": 5,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "should not be accepted",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "BkSDMA36Z": {
        "paper_id": "iclr_2018_BkSDMA36Z",
        "paper_title": "A New Method of Region Embedding for Text Classification",
        "paper_abstract": "To represent a text as a bag of properly identified \u201cphrases\u201d and use the representation for processing the text is proved to be useful. The key question here is how to identify the phrases and represent them. The traditional method of utilizing n-grams can be regarded as an approximation of the approach. Such a method can suffer from data sparsity, however, particularly when the length of n-gram is large. In this paper, we propose a new method of learning and utilizing task-specific distributed representations of n-grams, referred to as \u201cregion embeddings\u201d. Without loss of generality we address text classification. We specifically propose two models for region embeddings. In our models, the representation of a word has two parts, the embedding of the word itself, and a weighting matrix to interact with the local context, referred to as local context unit. The region embeddings are learned and used in the classification task, as parameters of the neural network classifier. Experimental results show that our proposed method outperforms existing methods in text classification on several benchmark datasets. The results also indicate that our method can indeed capture the salient phrasal expressions in the texts.",
        "paper_acceptance": "accepted-poster-papers",
        "meta_review": "despite not amazing scores, this is a solid paper.\nit created a lot of discussion and was found to be reproducible.\nwe should accept it to let the iclr community partake in the discussion and learn about this method of n-gram embeddings\n",
        "meta_review_title": "ICLR 2018 Conference Acceptance Decision",
        "reviews": [
            {
                "review_id": "ryjxrEwlM",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Review of \"Bag of region embeddings ...\"",
                "comment": "The authors propose a mechanism for learning task-specific region embeddings for use in text classification. Specifically, this comprises a standard word embedding an accompanying local context embedding. \n\nThe key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size. Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings. The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways. Region embeddings are then composed (summed) and fed through a standard model. \n\nStrong points\n---\n+ The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization. Further, the work is clearly presented.\n\n+ At the very least the model does seem comparable in performance to various recent methods (as per Table 2), however as noted below the gains are marginal and I have some questions on the setup.\n\n+ The authors perform ablation experiments, which are always nice to see. \n\nWeak points\n---\n- I have a critical question for clarification in the experiments. The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach? \n\n- The gains here appear to be consistent, but they seem marginal. The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range). Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see \"A sensitivity analysis of (and practitioners guide to) CNNs...\" Zhang and Wallace, 2015.)\n\n- The related work section seems light. For instance, there is no discussion at all of LSTMs and their application to text classificatio (e.g., Tang et al., EMNLP 2015) -- although it is noted that the authors do compare against D-LSTM,  or char-level CNNs for the same (see Zhang et al., NIPs 2015). Other relevant work not discussed includes Iyyer et al. (ACL 2015). In their respective ways, these papers address some of the same issues the authors consider here. \n\n- The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc. I would have appreciated more intuition behind these approaches. \n\nSmall comments\n---\nThere is a typo in Figure 4 -- \"Howerver\" should be \"However\"\n\n*** Update after author response ***\n\nThanks to the authors for their responses. My score is unchanged.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1sXToOgf",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "New model for text classification",
                "comment": "The authors present a model for text classification. The parameters of the model are an embedding for each word and a local context unit. The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word). After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task. The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task. The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings. Here the embedding of word i is combined with the elements of the context units of words in the context. To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units). \n\nThe method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy. the choice of baselines is convincing. What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?\n\nThe introduction was fine. Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.\nThe related work section only makes sense *after* there is at least a minimal explanation of what the local context units do. A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear. Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.\n\nThe authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}.  \n\nThe included baselines are extensive and the proposed method outperforms existing methods on most datasets. In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper. Figure 2 and 3 could be next to each other to save space. \nI found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?\n\nQualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level. It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position. Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? \n\nPros:\n + simple model\n + strong quantitative results\n\nCons:\n - notation (i.e. precise definition of r_{i,c})\n - qualitative analysis could be extended\n - writing could be improved  ",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Sy6ClHqef",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "review",
                "comment": "() Summary\nIn this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark. The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words. In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a \"context unit\" of size d x K, where d is the embedding size and K the region size. Thus, the model also have a vector representation for pair of word and position in the region. Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the \"context unit\" of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and \"context\" vectors are exchanged). The max-pooling operation is then used to obtain a vector representation of size d. Then a linear classifier is applied on top of the sum of the region embeddings. The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015). They obtain state of the art results on most of the datasets. They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the \"context units\" vector by a scalar. The authors also provide some visualisation of the parameters of their model.\n\n() Discussion\nOverall, I think that the proposed method is sound and well justified. The empirical evaluations, analysis and comparisons to existing methods are well executed. I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art. I suspect that the model is also computationally efficient: can the authors report training time for different datasets? I think that it would make the paper stronger. One of the main limitations of the model, as stated by the authors, is its number of parameters. Could the authors also report these?\n\nWhile the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed. Indeed, the papers contains many typos (I have listed a few), as well as ungrammatical sentences. I also think that a discussion of the \"attention is all you need\" paper by Vaswani et al. is needed, as both articles seem strongly related.\n\nAs a minor comment, I advise the authors to use a different letter for \"word embeddings\" and the \"projected word embeddings\" (equation at the bottom of page 3). It would also make the paper more clear.\n\n() Pros / Cons:\n+ simple yet powerful method for text classification\n+ strong experimental results\n+ ablation study / analysis of influence of parameters\n- writing of the paper\n- missing discussion to the \"attention is all you need paper\", which seems highly relevant\n\n() Typos:\nPage 1\n\"a support vectors machineS\" -> \"a support vector machine\"\n\"performs good\" -> \"performs well\"\n\"the n-grams was widely\" -> \"n-grams were widely\"\n\"to apply large region size\" -> \"to apply to large region size\"\n\"are trained separately\" -> \"do not share parameters\"\n\nPage 2\n\"convolutional neural networks(CNN)\" -> \"convolutional neural networks (CNN)\"\n\"related works\" -> \"related work\"\n\"effective in Wang and Manning\" -> \"effective by Wang and Manning\"\n\"applied on text classification\" -> \"applied to text classification\"\n\"shard(word independent)\" -> \"shard (word independent)\"\n\nPage 3\n\"can be treat\" -> \"can be treated\"\n\"fixed length continues subsequence\" -> \"fixed length contiguous subsequence\"\n\"w_i stands for the\" -> \"w_i standing for the\"\n\"which both the unit\" -> \"where both the unit\"\n\"in vocabulary\" -> \"in the vocabulary\"\n\netc...",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "B1b7od97z",
                "reply_to": "r1sXToOgf",
                "title": "Response to ICLR 2018 Conference Paper65 AnonReviewer3",
                "comment": "Thank for your suggestions. We will explain your concerns point by point.\n\n1) \"What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?\"\nWe evaluated the experiments with pretrained word embeddings on Yelp.F and Yelp.P dataset:\n\n*Datasets*        *Method*        Best epoch(start from 0)*        *Accuracy*\nYelp F.        Random        1        0.649500\nYelp F.        Finetune        1        0.638580\nYelp F.        Frozen        2        0.633060\nYelp P.        Random        2        0.963895\nYelp P.        Finetune        1        0.962500\nYelp P.        Frozen        2        0.960842\n\nThe word embeddings are pre-trained by Wikipedia+Gigaword 5 glove with 200 dimensions, region size is 7.  The result of a) is similar with randomly initialized word embeddings, and result of b) is slightly worse.\nIntuitively, pre-trained word embeddings should have a role, but maybe should not be applied directly. In fact, we will explore the way to apply local context unit to semi-supervised and unsupervised learning in our future work.\n\n2) \"Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.\" && \"the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.\"\n\nWe have deeply rewrited the introduction section and added citations related to ours.\n\n3)\"The authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}. \"\n\nThanks for your suggestions, equation numbers have been added, and expressions in 3.2 and 3.3 have been updated. We also add explanations and discussions in 3.2 to make this paper clearer.\n \n4)\"I found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?\"\n\nDetailed information about multi region sizes has been added at section 4.5.1. The gains of the multi region sizes method are not so large. As a natural extend for our core idea, we prefer to discuss it in the exploratory experiments sections. Performances for each data set with different region size have been reported in Appendix.A now.\n\n5)\"Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? \"\n\nWe have discussed this issue at section 4.5.4 and added words whose positions are less relevant in how they affect nearby words, which is consistent with our previous hypothesis. As for the second question, we have evaluated the entire vocabulary from the perspective of statistics and no obvious differential distribution built on different columns. In fact, it seems size of half of region characters expression patterns more, instead of strong distance distinction.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1_2OO9QG",
                "reply_to": "Sy6ClHqef",
                "title": "Response to ICLR 2018 Conference Paper65 AnonReviewer2",
                "comment": "Thank you very much for suggestions and meticulous corrections for this paper. Your description of our work is accurate. We have addressed each of your comments:\n\n1) Didn't report training time.\nWe have reported the training time for each dataset with different region sizes in appendix A.\n\n2) Didn't report number of parameters.\nParameters have been discussed in 4.5.1, and reported in appendix A for different settings.\n\n3) Typos and ungrammatical sentences in the paper.\nWe have greatly improved the writing of this paper, including all the typos you pointed out and others textual errors. We will continue to improve the writing quality before the camera ready version.\n\n4) The lack of discussion of the \"attention is all you need\" paper.\nThis work has been discussed now in the related work section.\n\n5) Should use different letters for \"word embeddings\" and the \"projected word embeddings\"\nWe have improved the notations to make the paper clearer.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJGB_OqXz",
                "reply_to": "ryjxrEwlM",
                "title": "Response to ICLR 2018 Conference Paper65 AnonReviewer1",
                "comment": "Thanks for your valuable comments. We will explain your concerns point by point:\n\n1)\"The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach? \"\n\nThis is true  only for the proposed approach. Results of the previous methods are their best results reported in corresponding previous papers, in which the hyperparameters are not identical for each datasets. (reference section 3.3 in D-LSTM, Table 5 in VDCNN, Table 1 and second paragraph of section 3.1 in FastText, section 4.2 in char-CRNN).\n\n\n2) \"The gains here appear to be consistent, but they seem marginal. The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range). Moreover, it is not clear if these results are averaged over multiple runs of SGD or not.\"\n\nThe result are averaged over multiple runs. We have experimented the performance variance in independent tries on yelp datasets, the results are reported in Appendix. A.\n\nIn this paper, we want to show that, with the ability of word-specific contextualization given by our proposed local context unit, our simple model can consistently beats or achieves the state-of-the-art results on almost all text classification tasks against to previous methods (traditional and deep models). This gives us an insight to represent and understand natural language by word specific context units in our future work. Therefore, we didn't use any trival tricks (e.g. multi-region-size which have been proved can improve the performance) and extra regularization methods. In fact, the gains are not so marginal since the best previous method's gains are similar and even less than ours on some datasets.\n\n3)The related work section seems light.\n\nWe have improved and completed related work section now.\n\n4)The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc. I would have appreciated more intuition behind these approaches. \n\nThe main intuition we compose the region embeddings in two approaches is the flowing: We consider the semantic of a given region is derived from the mutual influences of the words in this region. Since the regions can be regarded as snapshots of a window sliding on a document, whose middle words are contiguous,  we can just focus on the middle word influences on the context words, or the context words' influences on the middle word. According to the property of local context unit introduced in section 3.1, embeddings and units are used in two ways to address these influences, respectively. Finally, to extract the most predictive information and then produce a fixed length vector representation, a max pooling operation is used.\n\nWe also revised sections 3.2 and 3.3 of this paper.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Sy29H_5Qz",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Submission Update 2017-01-03: Summary of Changes",
                "comment": "Thank you very much for reviewing our submission and making so many valuable comments. We also thank people for their attention to our work and their experiments in reproducing our results.\n\nWe have addressed all the issues pointed out by you and now the submission is of quite high quality.  Could you please review our submission again?  We hope that our submission will be accepted.\n\nWith the help from our colleagues, we have significantly improved the writing of the paper.  The title has been modified to better describe the main contribution of the work. The abstract and body have also been significantly revised accordingly. \n\nWe further plan to have a native speaker to conduct proof reading on our paper, if it is accepted. \n\nBelow is a summary of the major changes.\n\n1. Title has been changed to \u201cA new method of region embedding for text classification\u201d.\n\n2. Abstract and introduction have been deeply revised. The expression has been improved, and a more clear explanation about the local context unit has been added in the introduction.\n\n3. In the related work section, discussion about \"Attention is all you need\" paper and citations including xx have been added. \n\n4. In method, we have improved the notations. including the notations of projected embedding (e_{w_i}^j - > p_w{w_i}^j), region size(c -> 2 * c +1).  We also added equation numbers and refine the equations in 3.2 and 3.3.  More explanations and discussions about the intuition behind the approaches we produce the region embeddings have been added in 3.2.\n\n5. We have added information about the datasets(average document lengths), implement details(multi-region-sizes mode), more cases about context units visualization, experimental results(training time and parameters numbers, best region sizes) in experiments section and appendix A. Figure 2 and Figure 3 are put next to each other to save space.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyE1mxEGf",
                "reply_to": "B1QjRyMGG",
                "title": "Reply",
                "comment": "Thank you very much for the reproducing experiments and suggestions about this paper. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nPlease see our latest comments to get more detailed information.\n\nThank you for pointing out, we have fix the stop condition issue in train.py and refined the code.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJphme4Mz",
                "reply_to": "Sy-OR3-fM",
                "title": "Reply",
                "comment": "Thank you very much for the reproducing experiments and suggestions about this paper. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\nThe significant different reproducing results on DBPedia and AG News can be explained by the preprocess bug in our published code, and we have fixed it.\nPlease see our latest comments to get more detailed information.\n\nWe have add scalar mode context unit in our code, which should not take 2 days long to train the model until convergence, could you refer our implementation or share yours so we can find out the problem?\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkQabg4MM",
                "reply_to": "Hk992WfGz",
                "title": "Reply",
                "comment": "Thank you very much for the reproducing experiments and suggestions about this paper. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\nThe significant different reproducing results on DBPedia and AG News can be explained by the preprocess bug in our published code, and we have fixed it.\nPlease see our latest comments to get more detailed information.\n\nCould you please share which datasets you applied on FastText Uni. & Bigram with different embedding sizes? Since some datasets like DBPedia were preprocessed incorrectly, experiments on these datasets may lead different conclusion. \n\nInterestingly\uff0cwe found there is a hidden part in the .tex file (%experiment notes part)of the original FastText paper(https://arxiv.org/abs/1607.01759, click other formats link). Embedding size 10 is better than 100 for both unigram&bigram in Fasttext.\n\nModel && AG & Sogou & DBP & Yelp P. & Yelp F. & Yah. A. & Amz. F. & Amz. P. \\\\\n%Ours, $h=100$                         && 91.0 & 92.6 & 98.2 & 92.9 & 59.6 & 70.7 & 55.3 & 90.9 \\\\\n%Ours, $h=100$, bigram                 && 92.4 & 96.4 & 98.5 & 95.7 & 63.7 & 71.9 & 59.2 & 94.5 \\\\\n\\texttt{fastText}, $h=10$             && 91.5 & 93.9 & 98.1 & 93.8 & 60.4 & 72.0 & 55.8 & 91.2 \\\\\n\\texttt{fastText}, $h=10$, bigram     && 92.5 & 96.8 & 98.6 & 95.7 & 63.9 & 72.3 & 60.2 & 94.6 \\\\\n\nFrom this result we can make the consistent conclusion with our paper.\nWe have add multi-region size mode in our code and we will add more implement details in our paper, thank you for your suggestion! \nIs there a typo of the results you reported on Yahoo! Answers and Yelp Review? the numbers seems not similar with the results reported in this paper.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJvnFyVMG",
                "reply_to": "SyUEJzzfM",
                "title": "Reply",
                "comment": "Thank you very much for the reproducing experiments. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\nPlease see our latest comments to get more detailed information.\nThank you for your suggestions, we have refined the code with more guild-lines and comments.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryfTd1Nfz",
                "reply_to": "SklnG7Mff",
                "title": "Reply",
                "comment": "Thank you very much for the reproducing experiments. We have updated our code and discussed the common issues about the reproducibility. \n\nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \n\nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\n\nPlease see our latest comments to get more detailed information.\n\nThank you for your suggestions about participating in some competitions, we will consider it!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkA08yEGG",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Reproducibility official explanation",
                "comment": "Thank you very much for the effort on reproducing experiments and suggestions for this paper. \n\nAlthough results on most datasets were reported reproducible, we have updated our code to reproduce more consistent experimental results straightly(including the exploratory experiments). Due to the time limit, the previous version of the shared code is not complete clear enough, we have update the code: 1)fixed a bug in the preprocess code which leads significant difference on DBPedia and AG News; 2)added exploratory experiments module, 3)published training configures for each dataset 4)added guild-lines and comments for the code. The latest code can be pulled from the same repository. We will also update this paper in a few days.\n \nWe reply issues about reproducibility here together:\n1. Significant difference of reproducing results on DBPedia and AG News:\nWe find a bug in the public version of prepare.py that we treat the raw csv input files as two-columns for all datasets, while some of them are not. \nThis bug is caused by our negligence during migrate the code from internal version(which worked as expected) to public version. Unfortunately, we only verified the public version code on Yelp datasets which are two-columns files. This bug may lead the significant different reproducing results on DBPedia and AG News. We are very sorry for this bug in the shared code and now it has been fixed and verified reproducible.\n\n2. 1% variance on most reproducing result:\nAlthough similar results(variance within 1% on accuracy) have been reported on most datasets, the training data were defaultly set to 90% training data to tune the hyperparameters  in the example configure. Since we only tuned the hyperparameters on Yelp F and applied these hyperparameters on all datasets,  the results reported in this paper are trained by 100% training data, this can explain the variance on accuracy in reproducing results. And we have set 100% training data as default value in the new version of config.\n\n3. Training time:\nWe have listed the training time and best epoch with different region sizes of each dataset in our paper(will be upload in a few days). In our experiments, it usually converges at 2 or 3  epoch with learning rate 1e-4 instead of more than 20 epochs, we are not sure whether it misled people that the initial learning rate in the example configure was 1e-5 which declared 1e-4 in the paper. Ignored the extra look up operation, the computational complexities of the proposed methods are basically the same magnitude with CNN. However, the shared code was not well optimized which may lead somehow slower in practice.\n\n4. Hyperparameters:\nWe have tuned hyperparameters on Yelp. F. and applied them on all datasets, so there may be better hyperparameters for a given dataset. We chose the region size as 7 since we found that the performances with region size 7 and 9 were almost the same and 7 needed less model parameters, and similar with the embedding size.\n\n5. Reproduction for baseline methods:\nWe have implemented some baseline models and achieved similar results with small fluctuations, considering the consistency of the comparison and lack of implementation details of some models, we reported the best results from previous works instead of reproducing all the baseline models.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SklnG7Mff",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Reproducibility review",
                "comment": "This paper proposes two novel text classification models, Word-Context and Context-Word Region Embeddings, where every word is represented by a word embedding vector and a local context unit matrix. The local context unit is designed to capture the semantic and syntactic information of a word in a given context. The columns of the local context unit are used to interact with the embeddings of the words of the same region, creating projected word embeddings. Max pooling is applied to the projected word embeddings to create region embeddings which are context dependent. The Word-Context method is based on the interaction between the local context unit of a given word with the surrounding word embeddings, while the Context-Word is based on the interaction between the embedding of a given word with the surrounding local context units. A document is represented by a weighted summation of all its region embeddings, which is fed to an upper Fully-Connected layer for text classification. The proposed methods are designed to outperform some of the most commonly used methods in the literature such as bag of words, ngrams, bigram-FastText, D-LSTM among others. \n\nThe authors provide code to facilitate the reproduction of their results. Although mostly functional, it required some tweaking before it was ready to go. The code was used to reproduce the results found on the Yelp Full Review dataset, since the model's configuration parameters seem to have already been specified in the code. The results were found to be similar to those published by the authors (within 1\\% variance). However, training the model can be computationally expensive if the required hardware is not available. Using a machine with 24 vCPUs and no powerful GPUs, the model required a training time of 1.2 hours per epoch. This presented serious constraints for tweaking the model's parameters. \n\nCode for baseline implementations can be found in projects associated with the papers cited by the authors, but some of them have specific hardware requirements, and a modification of the input data format. The hyperparameters for implementing these baseline linear classifiers are in some cases left vague (such as the logistic regression of the n-gram TFIDF method). Thus, many baseline methods had to be inferred and implemented independently, and implementing the new methods required certain hardware resources to take advantage of the torch-driven parallelized implementation. It was thus challenging to reproduce the baseline results presented in this paper.\n\nFinally, we introduce a new dataset on sentiment analysis of movie reviews from Kaggle to evaluate if this method can generalize well to other tasks. The dataset was particularly interesting since it had an imbalanced class distribution, and since it is associated to an online competition. The best published accuracy for this competition is 76.5\\%. Although upon inspection this dataset had many noisy samples, word-context model resulted in a 54.02\\% accuracy after twenty epochs of training, putting it slightly above the majority class prediction baseline. However, the test accuracy was constantly increasing until the last epoch, so if left longer the model might have achieved better classification results, or the convergence would have been faster if the parameters were tweaked. The main limitation here was again, computational complexity. We encourage the authors to participate in such competitions.\n\nStrengths:\n+ Intuitive concept, clear paper supported by figures and graphs\n+ Open source code\nWeaknesses:\n- Code completeness and clarity\n- Computational Complexity\nSmall Comments:\n~ typos\n~ Data statistics for the amazon polarity and full datasets in table 1 are interchanged",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyUEJzzfM",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Reproducibility review of \"Bag og region embeddings via...\"",
                "comment": "The authors behind the paper \"Bag of region embeddings via local context unit for text classification\" have come up with a new method to help classify text by looking at words' contextual effect on surrounding regions and how that changes with their relative positions. This is in contrast to just looking at the distribution of words and/or sequences of words. The authors were able to come up with some very good and state-of-the-art results. We have worked with the paper and the code made public by the team, in order to reproduce some of their results.\n\nThe report is very well structured, and it coherently develops the model contained within. Everything follows naturally with respect to these initial ideas. They show a lot of good examples of the power of their model, for example how some words contribute to the sentiment of the sentence, in different ways. The report does however have some grammatical errors, and some sections that did not read well.\n\nWe found that the results obtained by the authors were indeed reproducible, since we, using the same method and code, got very similar results. The authors made their code publicly available for all reviewers to see,  in the interests of reproducibility, which is definitely an asset for their credibility. Furthermore, the authors used publicly available data that was very easy to find. They were also able to compare their results to other more established models, which have all classified and obtained results on the same data. The authors tried two different approaches using the same model, and found that one was much more effective than the other, by comparing the results. One of their models got state-of-the-art results on a range of the datasets.\n\nThe Github code given by the authors had some good general instructions on how to run the pre-training and the actual training. Using Python 2.7.1 with Tensorflow and standard libraries such as Scipy and Numpy, we think that it is very accessible to reproduce their results, hardware considerations included.\n\nThe whole idea makes a lot of sense intuitively and mathematically, and we give the authors a lot of credit for this. They make it simple to understand. Even though the authors explained the feature selection model with good detail, and strong mathematical reasoning, we do not think the detail on the actual implementation is appropriate. We consider the lack of detail on the implementation a shortcoming of the paper, and the reproducibility of the results. The code given by the authors is very good, especially the hyper-parameter configuration file, which made it possible for us to reproduce the results. We did not find that we could run the code provided straight from Github, due to a very simple problem that was easily fixed. To make the code more easily understandable it would have been beneficial to add some more comments, since it is a non-trivial pre-training model, particularly given the lack of implementation detail in the paper. We do not think it would have been possible for us to reproduce the results without having been given the code.\n\nOur results:\n\nWe tried to reproduce the results of the Word-context model which is the better one of the two models made by the authors. We tried only to reproduce the results on the two datasets, Yelp Full-Review and Yelp Polarity, and we obtained accuracies which were very comparable to the results published in the paper (within 0.5-0.8% of that published). We consider this result successful in our effort to reproduce the results. The key limitation of our work was the hardware, because the authors used very powerful hardware that was not available to us. With the hyperparameters given by the authors, and limited to one epoch with the hardware accessible to us, we found training to take around 12 hours. This forced us to slightly modify some of the hyperparameters, in order to run the code in a reasonable amount of time.  ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hk992WfGz",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Reproducibility review",
                "comment": "New Proposed method of Text Classification\nSummary\nWithin this paper, the author proposed two fresh text classification methods that learn task specific region embeddings without hand crafted features. In the model, there are two attributes for every word, which are generally word embeddings representing regions and local context unit which interacts with the word's context. After learning the bag of region embeddings which is either word-context embedding or context-word embedding, a linear classifier is used for classification. The authors implemented the context unit model and compared them with other 8 baseline methods using 8 datasets and showed the beating results over all previous models on all datasets except VDCNN on Amazon datasets. Besides, they explored the effects of a hyper-parameter of selecting region size since small region size loses patterns of long distances and large region size gets more noises. Additionally, they experimented with the effect of context window size and the embedding size. Finally, they visualized the contribution of each word and selected phrase of classification.\nDiscussion:\nThrough repetitive validations on baseline methods as well as proposed methods by the authors, we proved some baseline results are reproducible, however, some are not. To be more specific, we derived comparable results to the ones referenced by the paper using FastText Bigram or Unigram method. Nevertheless, when we tried to fit Dbpedia dataset with Bag of Words or Ngrams-TFIDF model, the prediction accuracy was low, achieving only around 70% where the authors claimed it would reach 96.6%. Thus, we suggest the authors to manually implement several baselines to make sure the reliability of the source data. After the reproduction of different embedding sizes on FastText Uni. & Bigram, the prediction accuracy is not declining as the embedding size increases. The advantage of using proposed method instead of FastText to avoid over-fitting is not obvious.\nFor the experiments using local context unit models, we reproduced the baseline run with optimal parameters on four datasets and studied the effects of embedding sizes and window sizes. We obtained similar results on Yahoo! Answers and Yelp Review Full datasets, with 64.6% and 68.9% respectively. When training with Word Context model, the best results we obtained on DBPedia within 20 epochs is 71.7%, differing significantly from the claimed results 98.9%. We carefully checked the source code and performed multiple trainings with different random initialization, however the accuracy didn\u2019t improve. We doubt whether this problem is cause by implementation issue or the discrepancies in terms of training parameters. Moreover, we obtained 87% accuracy on AG\u2019s News within 39 epochs, 5% less than the claimed results.\nDue to the limited the computing power, we validate the effect of window sizes we obtained the results with only 8 ~ 12 epochs, except for windows size 1 and 7 of more than 20 epochs. However, the accuracy increase rate drops between adjacent epochs by the end of training on each dataset. Also, a clear distribution that is similar to the one illustrated in the paper is observed: 61.01%, 63.00%, 63.92%, 64.6%, 63.15%. We can thus conclude that the variation across window sizes is reproducible. However, the mixed region sizes are not reproduced because the implementation of this is not specified by neither the paper nor the source code.\nWe failed to perform FastText (Win-pool) and W.C. region embedding with one dimensional context unit because of some implementation issues which was hard to overcome. We suggest authors releasing the implementation for the variation or more verbose guild-lines for customizing the code.\nStrength:\n+ Proposed model is very intuitive, the representation of the architecture of C.M. & M.C. region embedding is clear and easy to follow.\n+ Exploratory experiments on embedding size effect and region effect as well as the visualization make it more vivid\nWeakness:\n- The authors didn\u2019t manually implement the baseline methods through 8 datasets.\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1QjRyMGG",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Review",
                "comment": "This paper proposed a new method on document classification task. In addition to word embedding, it assigns a context unit, which is a matrix, to each word. Given a region, the method uses these matrix\u2019s columns and the word embedding vectors within the region to compute element-wise multiplication. The results are some projected vectors. Then they apply element-wise max-pooling for these projected vectors. In the end, the model extracts a feature vector on each N consecutive words and then it computes the average of these vectors (or computes the weighted sum). This final vector can be regarded as the feature vector of the entire document. Finally, the model feeds the vector into a fully connected layer and gets the classification results. This feature construction process is similar to that of the N-gram model since they all extract a feature vector from each region of fixed length, except that, in N-gram, each consecutive N words is used as a feature directly. \nThe authors justified the effectiveness of their model by comparing the test set accuracy to several well-known baseline models on 8 different data sets. Based on their result, the model outperformed all previous methods on all data sets except for VDCNN on two Amazon data sets. In general, the model achieved an overall outstanding performance in terms of accuracy. The paper also carried out several exploratory experiments to study the effect of each hyper-parameter on the model. The authors compared the effectiveness of the model under various region sizes and embedding sizes. They concluded from the results that their model is robust to the over-fitting problem. The paper also investigated the effects of context unit by successively removing each independent method and comparing the performance of each resulting model. It showed that accuracy increased every time when they added one more component to the model. Furthermore, a significant improvement occurred when the context unit was fully utilized.   \n\nOur result:\nFor Yelp Review Polarity data set, our test set accuracy does not match the reported accuracy, yet they are consistent to some extent. Despite the subtle disagreement, our result still exceeds the all test accuracy form previous work by at least 0.6\\%. When comparing the results on Yelp Review Full data set, however, we observe large discrepancy. The authors claimed that their model achieved a test set accuracy of 64.9% and as such beat all previous models whereas our experiment yields an accuracy of 64.6% which is a bit far from so-claimed 64.9%. Moreover, based on our result, the proposed model also fails to outperform VDCNN. \nWe notice that, for all region sizes, we obtained a test set accuracy lower than the one reported by the authors. Based on our result, Best performance is achieved when region size is set to be 9 as opposed to the paper's result that the performance continues to increase with the growth of region size up to 7. Nevertheless, as the region size increases, model performance from two results firstly get dramatically promoted and begin to decline after some point.\n\nPros: \n-The paper is easy to understand. Moreover, the authors used a graph to illustrate their model structure which is helpful.\n-The model and the idea are simple, which makes the model easy to interpret.\n-Code is provided which makes it easy to reproduce the result. \n-The paper provided results and comparisons on several benchmarks with several state-of-the-art models. \nCons: \n-Computational time for each task was not showed in this paper. If the authors provided the running time with different choice of hyper-parameters, it will be helpful for the readers to choose the combination of hyperparameters so as to best fit their computation environment when time is limited.  \n-The hyper-parameter settings used for producing results in the paper were not clearly stated. For example, in table 2 of this paper the epoch size was not given. There is ambiguity of all the numerical results. The accuracy can be either interpreted as the average accuracy of several repeated runs of this task or interpreted as the highest accuracy obtained from running the same task repeatedly for several times.  \n-The open source code has one major issue where the stop condition in trainer.py/function train() will result the whole training process to be forcefully terminated with error before computing the test and dev data accuracy of last epoch of the training process.  \n-The naming of variables in this paper is inconsistent and does not align with the open source code variable name, such as, hyper-parameter \"region size\" are renamed as \"window size\" in section 4.5.2.\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Sy-OR3-fM",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Reproducibility  review",
                "comment": "This paper proposed a new technique to learn a vector representation of local context information, where the model learns a separate target embedding and context embedding for each word in a sequence. Under the proposed framework, there are two models: Context-Word and Word-Context. Word-Context uses the same context embedding for the target word on each context word, and Context-Word uses the same target embedding for each context word. Max pool is applied to get the final word representation. The authors evaluate the quality of the learned embeddings on text classification task where the word and context embeddings are directly feed into a linear classifier as input. The embedding for target word and context words and the neural net parameters are trained jointly on the supervised classification task. \n\nWe conduct experiments using both Word-Context and Context-Word on 4 out of the 8 datasets: Yelp Full Review, Yelp Review Polarity, AG news and DBP dataset. After running the experiments given the details provided, we compare our result against the paper\u2019s claim, and conclude that the proposed model  is  mostly as effective as  the  paper  claimed (variance within 1% on accuracy), while in some cases we get results worse than the reported and we are not able to reproduce. \n\nFirst, we reproduced the text classification tasks on the 4 datasets mentioned above. We find that there are small fluctuations (around 0.5% to 1%) on each result, but the results are in general consistent with the paper on all datasets except DBP and AG news. Surprisingly, our results on DBP dataset and AG News are not consistent with the authors\u2019: our result is around 75% while the paper reports over 98% on DBP, and 88% while the paper reports 92% on AG news. We find no obvious reason for the model to fail, and thus the experiments on those dataset is not reproducible. \n\nNext, we test several reported baselines on the full dataset. Due to time and computation limit, we reproduce 4 out of 8 baselines: Bag of Word (BoW), Bag of Ngrams, Bag of Ngrams with TFIDF and FastText.\nThe BoW, Ngram and Ngrams with TFIDF baselines are easy to implement with Scikit-learn library, but we find our results significantly lower than the reported results on an average of 5-7% in classification accuracy. The code for FastText is public online, and our reproduced results are in line with the claimed results for all datasets.\n\nIn addition, we conduct the experiments for analytical purpose as described in the paper. In the paper, the effect of region size and embedding size are studied separately, showing that a region size of 7 (3 words to the left of the target word and 3 words to the right) and an embedding of dimension 128 produces the best result. Here we also reproduce similar results on the same dataset following the instructions given in the paper. We find that the optimal region size is around 3-4 words on each side, and slight numerical difference (within 0.5% variation) might exist depending on the dataset and other hyperparameters such as embedding size. For example, on Word-Context model with embedding size 200 on Yelp Full Review, a region size of 9 turns out to yield the best results. \n\nFurthermore, we conduct the experiments on the settings of different embedding size up until our time and computation power limit, and we find that the accuracy score is about 0.5% worse than the claim on large embedding sizes but about 1% better than the score reported on the paper.\n\nLastly, we examine the effect of context unit by comparing the performance of the proposed model with FastText baseline. As described in the paper, we train the model to learn a scalar representation of the word and its context, and compare it with the normal vector version and baselines. However, in practice it takes more than 2 days to train (on GPU) the model until convergence. Therefore we are not yet able to draw conclusion about the effectiveness of the model with scalar representation model compared to the one with vector form.\n\nWe also record the average cost for each model on each dataset. On large dataset such as Yelp Review, both proposed models take about 7-10 hours to converge with the reported hyperparameters, while on small datasets such as AG News, the models converge in 2-3 hours. The model learns relatively slowly compared to some baseline models such as FastText, which converges within half an hour. This is expected, given the number of parameters the proposed models have.\n\nOverall, we conclude that the proposed methods are well justified and the empirical evaluations and analysis are well executed. Most results are reproducible within certain time and computation limit, but there also exist certain experiments that are not reproducible.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1hPEvKlM",
                "reply_to": "H1cW4Kulf",
                "title": "Embeddings in competing methods",
                "comment": "Thank you for asking.  \n\nIn this paper, previous works' experimental results in Table 2 are reported from Joulin et al.(2016). From our knowledge, we believe FastText and the character based methods did not use pre-trained word embeddings, and we are not very sure whether the embeddings are pre-trained  in Discriminative LSTM from now on.  Performances on these eight datasets of word based CNN can be found in Zhang & LeCun (2015)(with and without pre-trained word embeddings), which show the effect of whether the embeddings are pre-trained. \n\nMore details about those experiments under different conditions can be found in the original papers for each method. ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1cW4Kulf",
                "reply_to": "HJl5GqvlG",
                "title": "What about the embeddings in competing methods?",
                "comment": "Thank you so much for the clarifications! What about the embeddings of the competing methods, e.g. Fasttext? Are they pretrained? Are they retrained for the classification task at hand?",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJl5GqvlG",
                "reply_to": "ryecqWvxG",
                "title": "They are jointly trained as classification model parameters with cross entropy loss",
                "comment": "Thanks for your question.\n\nIn this paper, we are going to produce the task related region embeddings which can be used to improve the performance of classification tasks, so the word embedding matrix E and local context unit matrix U are randomly initialized and jointly trained as classification model parameters, here we use the cross entropy loss as the classification loss.\n\nNoticed that the power of the local context unit on learning task related region embeddings, we are\ninterested to explore its ability to semi-supervised or unsupervised learning in our future work, but in this paper, we only focus on developing a new mechanism to extract the predictive features on small text regions.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryecqWvxG",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Please clarify loss",
                "comment": "How are you training the local context units? Which loss are you optimizing to get the embeddings E and context parameters U?",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1AXIZSeG",
                "reply_to": "ryjWc4Jgz",
                "title": "Further information of the datasets",
                "comment": "Thank you for your comments and suggestions. We generally agree with your understanding of this paper\uff0c if there are any problems with the implementation, please let us know.\n\nAbout your questions with datasets:\n\n1)The average document length for the different datasets are following, we will add them in the next revision:\n\n    *Dataset*       *Average Document Length*\n    Yelp P.         156.153767857\n    Yelp F.         157.641587692\n    Yah.A.          111.638106429\n    Sogou           578.654484444\n    DBP             55.3340017857\n    Amz.P.          90.8814119444\n    Amz.F.          92.7758653333\n    AG              43.6560583333\n\nIt is worth to mention that the motivation of our method is not to capture the long distance dependence, but to capture the local features of the text from a new perspective(Although more complex upper layers like RNNs can be applied to capture the long term dependencies in the document, we simply used a bag of region embeddings upper layer).\n\n\n2)The datasets are built from real world and are widely used to evaluate the model performance on text classification tasks(see details in the references of this paper). On some datasets such as AG and Sogou, the Bow and Bag-of-Ngrams can achieve 90%+ accuracy which indeed shows that these datasets are naturally easy to be separated, but we can see that our method achieves the state of the art results against the baselines and along with other deep models(LSTMs, CNNs), on some other more difficult datasets, such as multi-level sentiment analysis(Yelp.F., Amz.F), QA matching(Yah.A), our method yields a significant performance gain compared to the BoW and ngrams baselines (5%-8%), which shows that our method can capture local text features better.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryjWc4Jgz",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Query about the datasets",
                "comment": "Hey authors\n\nThank you for sharing the implementation of the paper - it goes a long way towards ensuring reproducibility.\n\nMy understanding of the paper is the following: for each word in the vocabulary, along with learning a word vector, learn a context matrix. This context matrix would introduce a soft-attention kind of effect and is expected to be more powerful than the use of just a context vector for capturing the context. Please correct me if there is something wrong in my understanding :)\n\nIn the experiments section, the paper uses 8 different datasets. It would be helpful if the paper also mentions the average document length for the different datasets. That could be a crude proxy to understand how important is it to capture the long term dependencies in the document. Further, the simple baselines of BoW and ngrams give decent performance (around 90% for 5 datasets). Could it be the case that the datasets are not very \"difficult\"? I have not used these datasets and would be glad to know what do the authors feel about the same.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkKu25KJM",
                "reply_to": "iclr_2018_BkSDMA36Z",
                "title": "Code of the proposed models",
                "comment": "In order to facilitate reviewers to reproduce our results, we share the implementation of our method at here(https://github.com/text-representation/local-context-unit).  We will formally open-source our code upon publishing the paper.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "scores",
                "Sentiment Expression": "not amazing",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "it",
                "Sentiment Expression": "reproducible",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "vuFJO_W85VU": {
        "paper_id": "nips_2021_vuFJO_W85VU",
        "paper_title": "Iterative Amortized Policy Optimization",
        "paper_abstract": "Policy networks are a central feature of deep reinforcement learning (RL) algorithms for continuous control, enabling the estimation and sampling of high-value actions. From the variational inference perspective on RL, policy networks, when used with entropy or KL regularization, are a form of amortized optimization, optimizing network parameters rather than the policy distributions directly. However, direct amortized mappings can yield suboptimal policy estimates and restricted distributions, limiting performance and exploration. Given this perspective, we consider the more flexible class of iterative amortized optimizers. We demonstrate that the resulting technique, iterative amortized policy optimization, yields performance improvements over direct amortization on benchmark continuous control tasks.\n",
        "paper_acceptance": "accept",
        "meta_review": "All the reviewers think introduce the advanced variational inference for making the policy flexible by exploiting the connection between RL and variational inference is interesting. \n\nAlthough I recommend the acceptance for this submission based on the reviewers feedback, there are several issues should be extensively discussed:\n\n- EBM closed-form for policy parametrization: in fact, with entropy-regularization, the policy will lie in energy-based model. Therefore, a natural competitor is advanced sampling algorithms, e.g., Langevin, HMC, SVGD [1], Wasserstein flow [2] etc, based on learned EBM. \n\n- Performance tradeoff in computation, sample complexity, vs flexibility:  as the complicated parametrization introduced, the computation cost and sample complexity will increase. This should be carefully discussed, as almost all reviewers pointed out. From the empirical study, it seems the flexible policy family does not provide significant benefits, which diminishes the significance of this paper. \n\nIn sum, I think this paper is interesting and could be better if the authors carefully address these questions. \n \n[1] Haarnoja, Tuomas, Haoran Tang, Pieter Abbeel, and Sergey Levine. \"Reinforcement learning with deep energy-based policies.\" In International Conference on Machine Learning, pp. 1352-1361. PMLR, 2017.\n[2] Zhang, Ruiyi, Changyou Chen, Chunyuan Li, and Lawrence Carin. \"Policy optimization as wasserstein gradient flows.\" In International Conference on Machine Learning, pp. 5737-5746. PMLR, 2018.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "RAicCjP08tG",
                "writer": "official_reviewer",
                "reply_to": "KlMS7DR8ZI-",
                "title": "Post-rebuttal response",
                "comment": " I want to thank the authors for addressing my concerns. I am satisfied with the authors' response and I am keeping my original score. Good work!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "X0NXaq2af8I",
                "writer": "author",
                "reply_to": "5faDgw3kUQ",
                "title": "Response to Reviewer CT92",
                "comment": " Thank you for your review. We are glad that you found the paper novel, thorough, and technically sound. To address your questions:\n\nWe will include an additional discussion on the computational/performance trade-offs of iterative amortization. Briefly, computational cost scales linearly with the number of iterations per environment step, whereas there are diminishing performance improvements for additional iterations (see Figure B.9). However, even with a single iteration per step, one can obtain the other benefits of iterative amortization: with a single iteration, iterative amortization outperforms direct amortization on Walker2d.\n\nIt is not entirely clear what properties of the environment and Q-value estimator are necessary for iterative amortization to yield significant improvements, and we agree that this would be a useful direction for further investigation. We suspect that there are multiple factors at play, including the complexity of the environment as well as the flexibility and accuracy of the Q-value estimator. We are glad that you appreciate our analysis on Walker2d as a meaningful first step in addressing this point, and we are hopeful that introducing the terminology of amortization to RL will inspire future works in this direction.\n\nYou are correct on Eqs. 1 and 2. We will fix these in an updated version of the paper. Thank you for pointing this out.\n\nRegarding Figure 7b, the main takeaway is to note the multi-modal policy optimization surface, resulting in the multi-modal policy in Figure 7c. We will clarify this point in the main text.\n\nIn the final version of the paper, we will provide more specific links to the supplementary material throughout the paper.\n\nAs noted in the paper, the relationship between the amortization gap and performance varies across environments. However, we empirically demonstrate that improvements in the amortization gap are consistently accompanied by similar or improved performance. That is, there may be limits beyond which improvements in optimization do not yield improvements in performance. Regarding Figure 8 specifically, the cumulative reward on HalfCheetah is significantly larger than those on most other environments and tends to increase more throughout the course of training. We suspect that the increasing amortization gap (which is measured in absolute terms) may reflect the increasing magnitude of the Q-value estimates. Note that the amortization gap generally remains constant or decreases on the other environments, where the cumulative reward magnitude does not increase as much throughout training.\n\nFinally, we would like to thank you for noting the novelty of this perspective and the potential for future work afforded by our paper. We appreciate that you found these aspects appealing.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eKc-Saz_0t0",
                "writer": "author",
                "reply_to": "4Vfdb53YyZD",
                "title": "Response to Reviewer Embw",
                "comment": " Response:\n\nThank you for your review. We are glad you found the paper novel, technically sound, and thorough. Regarding your questions:\n\nWe agree that generalizing across environments is an interesting direction for investigation. We note that this still would require training a value estimator for this new environment. Alternatively, with a differentiable model or simulator, one could potentially directly adjust these parameters. We intend to investigate this setting and include the analysis in the final version of the paper. Thank you for the suggestion.\n\nRegarding lines 112-116, yes, you are correct. Estimation in this context refers to estimating the action distribution, i.e., policy, for a given state. We will clear up this ambiguity (and others) in the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KlMS7DR8ZI-",
                "writer": "author",
                "reply_to": "65v7oyiXRbt",
                "title": "Response to Reviewer Cxw1",
                "comment": " Thank you for your review. We are glad you found the paper insightful and easy to read. To answer your questions:\n\n1. Value overestimation results from bootstrapping the target Q-value estimate, which will have some regions with positive bias errors. Because actor-critic methods use a policy to maximize the target Q-value (i.e., find higher values), such methods will tend to exploit regions with positive bias. During the course of training, this results in a build-up of positive bias, leading to overestimated values overall. That is, due to the value maximization during policy optimization, any small (positive) errors in value estimates will become magnified. Because iterative amortization is a more powerful optimizer, it is more capable of exploiting positively biased target Q-value estimates. In other words, if you are able to find higher-value policy estimates (even if they are inaccurate), you are more prone to overestimate values overall. We see this as an issue to be addressed in value estimation, e.g., through better uncertainty estimates, rather than handicapping the policy optimizer. Accordingly, we use a simple uncertainty down-weighting scheme to mitigate value overestimation.\n\n2. We agree that the performance differences are relatively modest on some environments. However, considering that we are only swapping out the policy optimizer, we believe that it is still surprising that we can consistently match or outperform direct policies. Direct policy networks are a central aspect of modern actor-critic RL algorithms, yet, we show that it is possible to still improve beyond this scheme. Regarding performance on the more challenging MuJoCo environments, we speculate that the Q-value networks may be a limiting factor here. The quality of the Q-value estimates sets an upper limit on performance, even with a perfect policy optimizer. Thus, direct and iterative policies may perform similarly for a relatively limited Q-value estimator (although we do see a slight performance boost on HumanoidStandup). Our paper introduces iterative amortized policy optimization, identifying several issues with current direct policies. Applying and analyzing this technique in the context of state-of-the-art value estimators, e.g., combinations of model-based and mode-free [Amos et al., 2020], would be an interesting direction for follow-up work.\n\nBrandon Amos, Samuel Stanton, Denis Yarats, and Andrew Gordon Wilson. On the model-based stochastic value gradient for continuous reinforcement learning. arXiv preprint arXiv:2008.12775, 2020.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "F1CiEOGA7-S",
                "writer": "author",
                "reply_to": "qX4uPo4ByVV",
                "title": "Response to Reviewer fdJ1",
                "comment": " Thank you for your review. We are glad you found the paper novel, well-written, and comprehensive. We hope to answer your remaining questions here:\n\n1. Regarding the computational complexity of iterative amortization, it scales linearly with the number of policy optimization iterations per-step. Each iteration requires a backward gradient pass through the Q-network or model, as well as a forward pass through the amortized policy network. However, by using a learned policy optimizer, even a single iteration can perform reasonably well. As shown in our experiments, there are diminishing benefits for additional iterations, and, in principle, one could adaptively tune the number of iterations. Regarding implementation complexity, we apply iterative amortized optimizers out-of-the-box, using the exact same policy network architectures as SAC. The only added challenge is increased value overestimation, which we address with a simple solution in the paper.\n\n2. Across environments, iterative amortization does not appear to be significantly more or less stable than direct amortization. With the specific example of HalfCheetah, it is unclear why iterative amortization experiences some brief dips in performance. We speculate that this may be due to the more exploratory nature of iterative amortization, which is capable of jumping between policies. With an inaccurate or uncertain value estimate, this may result in more dips in performance. However, it is worth noting that iterative amortization results in higher performance and a lower variance across seeds. This suggests that, although direct amortization may be slightly more stable, it is getting stuck in various sub-optimal solutions.\n\n3. We found that a decrease in the amortization gap consistently corresponds to an increase in performance (see Figure B.10 in the supplementary), however, as you note, the relationship between these changes is not the same across environments. This could be for a number of factors, including the accuracy of the Q-value estimate or intrinsic aspects of the environment dynamics and reward function. In our paper, we have brought the terminology of the amortization gap to the reinforcement learning community, along with substantial analysis. We agree, though, that a more in-depth investigation will be a useful direction for future work.\n\n4. Our analysis has provided clear qualitative evidence for improvements in exploration. We agree that experiments on sparse-reward environments would be a useful addition, and we intend to include these in the final version of the paper. Thank you for this suggestion. We note, however, that improving policy optimization is only one aspect of exploration, and additionally improving Q-value uncertainty estimates or state-entropy maximization would provide a more comprehensive approach.\n\nAgain, thank you for your review, and we hope that you will consider updating your score to reflect our response.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qX4uPo4ByVV",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_vuFJO_W85VU",
                "title": "",
                "comment": "This paper proposes iterative amortized policy optimization, a class of methods that use the gradients online to optimize the policy iteratively.  This paper proposes iterative amortized policy optimization, a class of methods that use the gradients online to optimize the policy iteratively. To the best of my knowledge, the idea is novel and original. The paper is generally well-written and easy to follow and in particular, I appreciate the authors making a detailed, aligned, and symmetric comparison of direct amortization and iterative amortization (the one-to-one pros and cons, as well as the graphical demonstration, and the algorithm, etc...). The empirical experiments are quite comprehensive, which includes quite some qualitative analysis of the claimed properties. And indeed, the proposed method is validated by the learning curves in MuJoCo continuous control tasks benchmark, as the proposed method outperforms and matches direct amortization in most continuous control tasks. Some questions and/or suggestions: 1. could you talk more about what's the added complexity of the proposed method? 2. In Fig.5 (c), while direct amortization seems to be very stable, iterative amortization seems to have two moments with sudden performance collapse, is there any reason why? 3. the increase on performance gains on MuJoCo tasks (Fig.5) and the corresponding decrease in amortization gap (Fig.6) do not seem to be be consistent, e.g. there is a significant decrease in amortization gap on Ant, while the expected reward is almost the same. So maybe it's good to investigate what brings the performance gains? 4. for exploration, it's also helpful to test the proposed algorithm in MuJoCo tasks with sparse rewards.\n\n\n See the main review.",
                "rating": 6,
                "confidence": 2
            },
            {
                "review_id": "65v7oyiXRbt",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_vuFJO_W85VU",
                "title": "",
                "comment": "This work analyzes policy optimization from an inference perspective. It first shows that policy optimization with entropy or KL regularization in DRL with a policy network is a form of amortized optimization where instead of directly optimizing the action distribution parameters, we optimize a network which outputs the action distribution parameters. This results in an amortization gap which negatively impacts performance and exploration. Next the authors propose an alternative iterative amortization method for training the policy network. This new approach is shown to have various benefits which the authors demonstrated through a series of experiments on the MuJoCo environments.  Overall, I found this paper to be very insightful. It brings forward many interesting ideas which helps to bridge the gap between inference and RL. The paper was very easy to read. The authors did a good job of explaining the issues of direct amortization and presenting their novel technique. The diagrams were also very helpful! The discussion on amortization gap is particularly interesting and to the best of my knowledge not something that has previously been explored by the RL community. The experiments in this paper are very well designed in explaining the iterative amortization approach. I have two questions for the authors which I hope they can clarify in their response:\n\n- In the section on value overestimation (Section 3.3), you mentioned that \u201cIf the policy can exploit regions of high uncertainty, the resulting target values will introduce positive bias into the estimate. More flexible policy optimizers exacerbate the problem, exploiting this uncertainty to a greater degree.\u201d Could you elaborate on this claim a little further? Why are more powerful policies more prone to overestimation of the Q function? This seems to be a fairly central point to Section 3.3.\n- Empirical performance of the iterative amortization approach does not seem to be that strong. While there are some improvements on some of the simpler environments. Performance on the most challenging MuJoCo environments (Ant, Humanoid, HumanoidStandup) appear to be on par with the direct amortization method. Any sense why the iterative approach does not result in any improvement on these environments?\n Yes",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "4Vfdb53YyZD",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_vuFJO_W85VU",
                "title": "",
                "comment": "The authors, via a standard reinterpretation of reinforcement learning as an inference problem, discusses the resulting \u201camortization gap\u201d that results from using policy networks to optimize the variational lower bound. In order to close this gap, the authors propose to train an iterative procedure parameterized by $\\phi$, which uses estimated gradients of the return to iteratively refine $\\pi(\\cdot | s_t)$ for the current state $s_t$. They then provide an extensive experimental evaluation demonstrating their proposed method and several desirable properties, including adaptability to gradients estimated by different methods.  **Originality:**  \nThe authors leverage a known connection between variational inference and reinforcement learning to motivate their problem (which is known in the VI literature). Although the techniques are not novel, they are novel in their application to the RL setting.\n\n**Quality:**  \nThe submission is technically sound, with authors leveraging known techniques from the generative modeling literature. Furthermore, the authors present a very thorough experimental evaluation of their claims, with very interesting results. Although the performance gains in some of the tasks are not substantial, the demonstrated reduction in the amortization gap, inherent multi-modal behavior, and transferability between different value estimation methods are worthy of note.\n\nAs an additional experiment, it would be interesting to see whether the learned policy is robust to slight changes in the environment (e.g. changing weight/friction parameters of the MuJoCo simulation) as an extension of the generalization experiment.\n\n**Clarity:**  \nThe paper is mostly clear, although as someone who is not as familiar with the generative modeling literature, a few parts of the paper were a bit confusing from my perspective.\nFor example, in the context of RL, what is estimation referring to in lines 112-116 (I assume in this case that it is the process of choosing an action distribution)? I think it would be really helpful to have these connections explicitly mentioned.\n\n**Significance:**  \nThe paper introduces really interesting ideas, including having a trained policy which includes a trained optimizer that improves upon an initial action based on predicted returns. The work suggests several venues for future work, including across-task generalization, better policies for existing algorithms, among others.\n\n The authors have adequately addressed the limitations of their approach. I particularly appreciated that the authors spent time addressing the value overestimation problem that is inevitably exacerbated by having a more expressive policy class.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "5faDgw3kUQ",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_vuFJO_W85VU",
                "title": "",
                "comment": "This paper introduces a novel approach to policy optimization which leverages the connection between amortized variational inference and policy-based RL with function approximation. Specifically, rather than rely on a \"direct\" parameterization of the state-conditioned distribution over actions, in which an input state is mapped directly to distribution parameters (e.g., mean and variance), the approach adds an additional optimization procedure over the distribution parameters themselves, rather than those of the policy network. Experimental results are then used to demonstrate the benefits of this approach for performance, accuracy, and flexibility in the optimization process.   Originality: The application of iterative amortization to policy optimization for RL is interesting and novel to the best of my knowledge. The connections between variational inference and RL with function approximation are deep yet not fully explored, and this is an excellent example of the interesting insights that can be gained through the lens of this relationship. The relationship to previous work and associated citations appear thorough. \n\nQuality: The submission appears technically sound, and the claims seem to be supported. However, it would be helpful if the authors discussed the tradeoffs inherent in iterative amortization (e.g., how is performance improvement balanced by increased computational expense?). Additionally, iterative amortization appears to help on some tasks and not at all on others. What properties of an environment make iterative amortization more or less useful? How do these properties vary among the tasks used in the paper? These are important questions from both a practical and theoretical standpoint. The analysis of the contribution of multimodality to performance is likely a good first step in this direction. There are also a number of supporting results in the appendix. Minor: For eq. 2, should there be a $\\rho(s_1)$ prefixed to the righthand side of the equation? And shouldn't the upper limit of the products in both eqs. 1 and 2 be $T-1$? \n\nClarity: The paper is mostly clear and well-organized. The background section and discussion of value overestimation were particularly enjoyable to read. Minor: Figure 7b is difficult to parse--I'm unclear as to what takeaway is intended. It also would be helpful to mark the exact section of the appendix in which supplementary results appear. \n\nSignificance: The magnitude of the performance gain on the majority of Mujoco tasks is not significant. It's difficult to gauge the significance of reducing the amortization gap--why this gap appear to grow over the course of training (Figure 8, right)?  While the raw performance improvement due to the proposed approach is not generally notable, I think the main significance of this work is captured by results like the ability to flexibly adapt to new objectives, such as to model-based value estimation. Namely, its significance lies in its novelty and the potential for follow-up work. Some theoretical analysis would be helpful in developing a deeper understanding. \n\n\n\nGiven the above factors, I recommend acceptance.  See the main review for a discussion of limitations and suggestions for improvement. I do not believe there are obvious societal impacts for this work. ",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "introduce the advanced variational inference for making the policy flexible by exploiting the connection between RL and variational inference",
                "Sentiment Expression": "interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "this submission",
                "Sentiment Expression": "recommend the acceptance",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "This",
                "Sentiment Expression": "should be carefully discussed",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the flexible policy family",
                "Sentiment Expression": "does not provide significant benefits",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "is interesting and could be better",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "XQu7UFSbzd2": {
        "paper_id": "nips_2022_XQu7UFSbzd2",
        "paper_title": "Towards Out-of-Distribution Sequential Event Prediction: A Causal Treatment",
        "paper_abstract": "The goal of sequential event prediction is to estimate the next event based on a sequence of historical events, with applications to sequential recommendation, user behavior analysis and clinical treatment. In practice, the next-event prediction models are trained with sequential data collected at one time and need to generalize to newly arrived sequences in remote future, which requires models to handle temporal distribution shift from training to testing. In this paper, we first take a data-generating perspective to reveal a negative result that existing approaches with maximum likelihood estimation would fail for distribution shift due to the latent context confounder, i.e., the common cause for the historical events and the next event. Then we devise a new learning objective based on backdoor adjustment and further harness variational inference to make it tractable for sequence learning problems. On top of that, we propose a framework with hierarchical branching structures for learning context-specific representations. Comprehensive experiments on diverse tasks (e.g., sequential recommendation) demonstrate the effectiveness, applicability and scalability of our method with various off-the-shelf models as backbones. ",
        "paper_acceptance": "Accept",
        "meta_review": "This paper proposes a method for predicting the next event given sequential data. For the prediction under distribution shift, the proposed method uses a backdoor adjustment, variational inference of latent context, and hierarchical branching structure. The proposed method that combines techniques from different fields is interesting. In particular, the use of causal methods for the distribution shift problem in temporal event prediction is novel. The experimental results demonstrate the effectiveness of the proposed method well quantitatively and qualitatively. The paper should be improved according to the reviewers\u2019 comments, e.g., clarifying the motivation in real-world applications.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "ODY472ud7bE",
                "writer": "official_reviewer",
                "reply_to": "1w9yet0P6_I",
                "title": "Thank you",
                "comment": " I thank the authors for the detailed replies to reviewer comments. The clarification on the role of context was helpful for me to better understand the contribution.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dL9B1py0XVo",
                "writer": "official_reviewer",
                "reply_to": "vWo-irvTnw8",
                "title": "Thanks for the reply",
                "comment": " I have read the reply to my questions as well as other reviews. I will keep the same score (6).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aJ2_jTMTH0n",
                "writer": "author",
                "reply_to": "nips_2022_XQu7UFSbzd2",
                "title": "New Draft Uploaded",
                "comment": " Dear Reviewers,\n\nThanks again for your detailed review and constructive suggestions for improvement. A new version of our paper has been uploaded with modified parts colored blue, based on our rebuttal. We will also further refine these parts and implement your suggestions in the final version.\n\nSincerely, \nThe Authors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "vWo-irvTnw8",
                "writer": "author",
                "reply_to": "ocJpNSIVloP",
                "title": "A kind reminder before the discussion phase ends",
                "comment": " Dear reviewer k7dp,\n\nThanks again for your review. Since the discussion period is approaching its end, we would be glad to hear from you if we have addressed your questions/concerns.\n\nKind regards, The Authors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cLWmYz95_P",
                "writer": "author",
                "reply_to": "jVBvA33OOv",
                "title": "A kind reminder before the discussion phase ends",
                "comment": " Dear reviewer HB3W,\n\nThanks again for your review. Since the discussion period is approaching its end, we would be glad to hear from you if we have addressed your questions/concerns.\n\nKind regards, The Authors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oNh4wONHQf",
                "writer": "author",
                "reply_to": "F7dl77V9uEf",
                "title": "A kind reminder before the discussion phase ends",
                "comment": " Dear reviewer t8MQ,\n\nThanks again for your review. Since the discussion period is approaching its end, we would be glad to hear from you if we have addressed your questions/concerns.\n\nKind regards, The Authors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "MclOV6jfnq2",
                "writer": "author",
                "reply_to": "vVPIloiWVgM",
                "title": "A kind reminder before the discussion phase ends",
                "comment": " Dear reviewer XNzH, \n\nThanks again for your review. We have provided informative responses to clarify your potential misunderstanding of our motivation for the problem formulation. We have also summarized ablation studies with newly added experimental results for justification. Since the discussion period is approaching its end, we would be glad to hear from you if we have addressed your concerns.\n\nKind regards,\nThe Authors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "P_ONh-cxGd",
                "writer": "author",
                "reply_to": "nbtKOn7cXb_",
                "title": "Response to Reviewer k7dp (Part 2 of 2)",
                "comment": " >***Q4: \"It is not clear how sensitive the model is to the number of contexts when dealing with datasets of differing scales and changes in distributions.\"***\n\nWe have studied how the context number (determined by choices of $K$ and $D$) affect model's ability in dealing with temporal distribution shift in section 4.5 with fig.7. By comparing model's performance in different gap size, we observe that in general larger context number within certain range indicates lower performance drop.\n\n>***Q5: \"% improvement is used as the primary metric of comparison. A better approach would be to provide absolute results and note statistical significance.\"***\n\nThanks for the nice suggestion. We use % performance drop mainly because it is more intuitive and will try the method suggested by the reviewer in the future.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nbtKOn7cXb_",
                "writer": "author",
                "reply_to": "ocJpNSIVloP",
                "title": "Response to Reviewer k7dp (Part 1 of 2)",
                "comment": " Thank you for your time and thorough review. We are glad that you appreciated our strong motivation, significance, novel method, and extensive experiments. In response to your questions and weakness points mentioned, we provide detailed answers to increase your confidence. \n\n> ***Q1: \"How does model deal with novel contexts?\"***\n\nThis is indeed an intriguing question, and we consider two cases that differ in the definition of \u201cnovel context\u201d. In a nutshell, our model could deal with novel contexts if they are in the context set supported by both intuitive and theoretical justifications provided in the following. We also claim the situation where novel context is not \"in\" the context set technically does not exist and discuss the real implication of \"novel context\" in our framework.\n\n\n**(Case 1)**. Novel context refers to a minority of unseen context in the context set, i.e., $c_{test} \\in \\mathcal C$.\n\nWe have already provided some justifications for this case in Appendix C, and updated this part with more details and a new illustrative fig.8 for better understanding. \n\nIn short, our hierarchical branching structure combined with the definition of context posterior is able to build connections between majority context and minority (or unseen) context. Therefore, our model is more likely being able to generalize to those novel contexts even if they are unseen in training. An intuitive but unrigorous example would be: Context 1 (seen in training) represents \u201csummer and sunny\u201d, context 2 (seen in training) represents \u201cwinter and rainy\u201d, and we expect our model can generalize to context 3 (unseen in training) that represents \u201csummer and rainy\u201d.\n\n\n**(Case 2)**. Novel context refers to a context that is not in the context, i.e., $c_{test} \\notin \\mathcal C$.\n\nIn practice, there probably exist infinitely many specific contexts and the objective is intractable in this case, which is why we **stratify context into discrete pieces** to create $\\mathcal C$ (which is a conventional practice) as an approximation. In other words, each context now actually means \"a class of specific contexts\", and all contexts in $\\mathcal C$ are sufficient to represent all existing specific contexts. In this sense, the size of $\\mathcal C$ means **\"to what extent of granularity we seperate context\" or \"the number of classes of context\"** rather than \"the number of specific context\". \n\nTherefore, we agree in that **\u201cusing backdoor adjustment to effectively deconfound context to events causal relationship may not in many real-world cases\u201d** due to the aforementioned approximation. This remains a common issue for all works that are dealing with infinitely many discrete confounders, and our framework has already made a progress in this direction by increasing the size to $K^D$.\n\nHandling novel (specific) context subsequently boils down to accurately predicting which class of contexts it belongs to. Since the prediction results of novel context presumably is less confident, it is indeed useful to consider uncertainty estimation in this case. \n\n\n> **Q2: \u201cWill the generative model adapt to uncertain situations or provide good uncertainty bounds?\u201d**\n\nWe are not certain as we are unfamiliar with this field. But one possible solution would be using Q(C|S) produced by a well-trained model to compute a confidence score (e.g., directly based on softmax output or by measuring the difference with training data) for uncertainty estimation or out-of-distribution detection. The result might be useful for letting models to \"know when they don't know\" in the face of novel context (case 2), which could be an interesting future research direction.\n\n\n\n> ***Q3: \u201cEvaluation on a benchmark where changes in context is known to test if the learned contexts map to domain knowledge would be useful.\u201d***\n\nConsidering the pressing time, we conducted an additional experiment based on a synthetic dataset generated by four different models trained on different parts of Yelp dataset for a quick (but not rigorous) verification. We manipulate the temporal distribution shift by treating two of these models as majority for generating training sequences and minority for testing sequences. We then re-train a model on the synthetic dataset with $K=4$. We compare the true and estimated frequency of context (where their correspondences are manually chosen) in the testing set, and the results are as follows.\n\n|  | Context 1 | Context 2 |Context 3 | Context 4 |\n| -------- | -------- | -------- |-------- | -------- |\n| Ground-Truth     | 0.4     | 0.4     | 0.1 | 0.1 |\n| Estimated     | 0.33     | 0.39     | 0.15 | 0.13|\n\nWe will consider rigorous evaluation in other benchmarks in the future.\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1w9yet0P6_I",
                "writer": "author",
                "reply_to": "jVBvA33OOv",
                "title": "Response to Reviewer HB3W",
                "comment": " Thank you for the thorough review and nice suggestions. We are glad that you like our approach. We next answer your questions to increase your confidence.\n\n>***Q1: \"It would be helpful to describe in the first paragraph of Section 3 which components are novel?\u201d***\n\nIndeed, the inference unit could be specified as standard building blocks in sequence models such as GRU, LSTM and self-attention, which gives our model a lot of freedom to be combined with existing sequence models. Besides, we also use the standard event embedding layer in Eq.(5), output layer for producing prediction results in Eq.(14), and standard prediction loss in the first term of Eq.(16). All the rest components in section could be considered as novel parts that are specifically designed based on our variational context adjustment framework in Section 2. They are summarized as follows:\n\n1. We use each inference unit (instantiation of P(Y|S,C), Eq.(6) and Eq.(7)) as a context-specific encoder to learn sequence representations conditioned on a specific context type, where the context is sampled in inference unit (instantiation of Q(C|S), Eq.(9) and Eq.(10)) by computing its distribution based on similarity scores.\n2. We further use Gumbel-Softmax trick (Eq.(11)) to make the sampling process in Eq.(3) differentiable, extend the model to a hierarchical branching structure (Eq.(12) and Eq.(13)) to incorporate more context types (i.e., K^D) under limited sources, and propose a new way (Eq.(15)) to compute variational posterior that adapts to the hierarchical structure. The final implementation of Eq.(3) is given by the loss function in Eq.(16).\n\n>***Q2: \"What happens if there are distribution shifts for other reasons, e.g. changes in user preferences over time that may not be related to a context?\"***\n\nThis an interesting question and we would like to add more details to answer it. \n\n**Definition and concept of context.**\n\nWe mentioned in the introduction that the concept of context refers to external factors that may impact the generation of events. Here, \u201cexternal\u201d here means any random variables that is not sequence $S$ and event $Y$ themselves. In other words, the definition of \u201ccontext\u201d is not restricted to the conventional meaning of \u201ccontext\u201d such as season and fashion trends that we use only as examples. It could also be interpreted as abstract \u201cuser preference\u201d or other properties relating to events and user themselves if they are indeed affecting the generation process of events (i.e., match the data generation process in fig.2).\n\n**How our framework address distribution shift caused by user preference or other factors.**\n\nTheoretically, our framework is general enough to address distribution shift caused by user preference shift or any other factors as long as 1) they are affecting or causing the data generation and  2) the event sequence is informative enough for our model to explore the shift. Therefore, **the question really is if we can truly discover the user preference shift or shifts of other latent confounding factors hidden in the data**, which the challenging part. This challenge is tackled in our work by integrating variational inference into the causal interventional model. If our model is not able to address distribution shift caused by these factors, it could be due to improper implementation/training, the intrinsic limitation of context stratification (see limitation in appendix), the bottleneck of dataset, or that they are not affecting the data generation.\n\n> ***Q3: \"Several of the figures are quite small (e.g. Figure 4) because the authors are trying to present a lot of results. I would suggest moving some of the results to the supplementary material.\"***\n\nThanks for the nice suggestion. We will modify them accordingly in the final version to further improve readability.\n\n\n>**Other minior issues (typos)**\n\nAll fixed in the new version. Thanks for pointing them out.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7OocOMGru9l",
                "writer": "author",
                "reply_to": "no7wm4zIwf1",
                "title": "Response to Reviewer t8MQ (Part 2 of 2)",
                "comment": " > ***Q4: \"the reviewer would recommend the authors to address potential limits of this work, per the check list\"***\n\nFor clarity, we list and summarize the limitations and potential societal impacts in the last section of Appendix in our new version.\n\n> ***Q5: \"Mainly, given the potentially large number of event types for real-world problems, it seems like the branching factor of K^D is quite limiting\"***\n\nIndeed, this is a limitation of our work and remains a major impediment for all works that are dealing with infinitely many discrete confounders. We also want to remark that ours is already an improvement over existing causal frameworks by increasing $K*D$ to $K^D$. In practice, we also found (in section 4.5) that increasing $K$ and $D$ within a certain range is useful, but will harm the performance if we keep increasing $K^D$ possibly because of the bottleneck of the dataset.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "no7wm4zIwf1",
                "writer": "author",
                "reply_to": "F7dl77V9uEf",
                "title": "Response to Reviewer t8MQ (Part 1 of 2)",
                "comment": " Thank you for the positive comments on our clear presentation, novel ideas, and solid experiments. In response to your raised questions, weaknesses, and limitations, we provide detailed answers below and supplement new experiments.\n\n\n> ***Q1: \u201cCould the authors comment on the model's performance where the context shifted to a point where previously common events now become rare and previously rare events now become common\u201d***\n\nWe answer this question respectively from theoretical and implementation perspectives.\n\n**(Theoretical Perspective)** In the following we will show despite we did not explicitly consider event frequency, our framework is general enough to incorporate and mitigate event frequency shift (i.e., event frequency change due to context shift). \n\n- Fact 1: As is known, there are normally two type of distribution shift [1], namely covariate shift where $P_{train}(S)\\neq P_{test}(S), P_{train}(Y|S)= P_{test}(Y|S)$ and concept shift where $P_{train}(Y|S)\\neq P_{test}(Y|S), P_{train}(S)= P_{test}(S)$. The problem formulation we consider in section 2 is a mixture of both, since both covariate shift and concept shift can cause $P_{train}(Y, S) \\neq P_{test}(Y, S)$. \n- Fact 2: Formally, the problem mentioned by the reviewer (i.e., event frequency shift) can be formulated as $P_{train}(X)\\neq P_{test}(X)$, where $X$ denotes random variable of event. It is actually a sufficient condition for covariate shift $P_{train}(S)\\neq P_{test}(S)$ in the sense that it is impossible $P_{train}(X)\\neq P_{test}(X)$ and $P_{train}(S) = P_{test}(S)$ hold at the same time. It is also intuitively reasonable since rare events in the sequence tend to make the sequence itself rarer. \n\nTherefore, since our framework aims to address temporal distribution shift $P_{train}(Y, S) \\neq P_{test}(Y, S)$, theoretically it is also able to address event frequency shift (i.e., \u201ccontext shifted to a point where previously common events now become rare and previously rare events now become common\u201d). \n\n[1] A unifying view on dataset shift in classification, Pattern Recognition 2012\n\n**(Implementation Perspective)** In terms of implementation (i.e., formulation of learning objective), if we compare the objective with and without causal intervention (i.e., do operation), ours has an additional scaling factor $P(C)/P(C|S)$, meaning the model tends to \u201cup-weight\u201d minority context in training. Since context shift is the cause of the difference in event frequency, it also means the model tends to \u201cup-weight\u201d those samples with rare events, so that all events tend to be equally trained.\n\n>***Q2: \"Experiment with a synthetic evaluation where ground-truth contexts/confounders are known and are manually manipulated.\"***\n\nWe conducted an additional experiment based on a synthetic dataset generated by four different models trained on different parts of Yelp dataset for a quick (but not rigorous) verification. We manipulate the temporal distribution shift by treating two of these models as majority for generating training sequences and minority for testing sequences. We then re-train a model on the sythetic dataset with $K=4$. We compare the true and estimated frequency of context (where their correspondences are manually chosen) in the testing set, and the results are as follows, which show our model can to some extent recover the latent context in this particular setting.\n\n|  | Context 1 | Context 2 |Context 3 | Context 4 |\n| -------- | -------- | -------- |-------- | -------- |\n| Ground-Truth     | 0.4     | 0.4     | 0.1 | 0.1 |\n| Estimated     | 0.33     | 0.39     | 0.15 | 0.13|\n\n\n> ***Q3: \u201cIs there a way to relate how the changing context causes certain events to happen more/less?\u201d and \u201ccan we draw any practical insights from these shifts w.r.t the real-world datasets like movies reviews / preferences\u201d***\n\nYes, one can statistically analyze the relation between changing context and event frequency by, e.g., using a certain branch of our model to generate event sequence, and even measure this relation by quantifying the context shift and event frequency shift. \n\nBut unfortunately, due to the nature of the problem setting where the context is defined as a latent and abstract factor (which is a major challenge we aim to tackle and serves as our contribution), it would be hard or even impossible to identify its meaning in the real world. Hence, we can hardly attribute event frequency shift to certain meaningful context or other explainability purposes that involve the practical meaning of the context. However, we think it is actually an advantage if the goal is to handle temporal distribution shift since it allows flexibility and capability for exploring latent context and distribution shift in a data-driven manner. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7uOnweEMH5O",
                "writer": "author",
                "reply_to": "zICLK59ku1",
                "title": "Response to Reviewer XNzH (Part 3 of 3)",
                "comment": " ### 2. Ablation Study and More Discussions\n\n> ***Comment 4. \"To justify this complicated formulation, it would have been valuable to include comparisons to simplifications of the proposed framework.\"***\n\n**Ablation studies in the main paper. (Ablation on Structure)**\n\nIn fact, we have already conducted some ablation studies (though not explicitly pointed out) in the experiment section to justify modules of the framework. We summarize them as follows:\n\n1. In section 4.1, the backbone models we use for recommendation task are indeed simplifications of our CaseQ by **removing branching modules in the architecture** (the KL-divergence term in loss function is naturally inapplicable). We can see from table 1 there is a remarkable decrease in the percentage of performance drop as we enlarge the time gap between training and testing data, compared with these backbone models.\n2. In section 4.2, baselines DMoE and multi-head Transformer could be considered as simplifications of our CaseQ **in terms of branching unit design** (since they have the same inference unit, width, and depth). The difference is that they are not able to dynamically learn context. Our method is still more robust as shown in figure 3.\n3. In section 4.4, when $K=1$, our model degrades to **a basic model where there is only one context**, and as we can see from figure 7, the performance drop of this basic model is more significant compared with models with larger $K$. The effectiveness of our model with increasing depth $D$ also justifies our hierarchical branching structure design.\n\n**More results. (Ablation on Loss Function)**\n\nTo see the effect of the additional KL-divergence term in the loss function in Eq.(16), we conduct more experiments to see the performance variation of CaseQ. Results are shown in the appendix of our updated version with a new plot. We found that our approach is less robust when removing this term ($\\alpha=0$). Moreover, by gradually increasing $\\alpha$ in a certain range, our approach becomes more robust w.r.t. distribution shift (i.e., less performance drop with larger gap size). This shows **the KL-divergence term in the objective (that is derived from the causal framework and integrated with our structure) is crucial** for the effectiveness of handling distribution shift.\n\n**More discussion on the complexity of model**\n\nThough our model may seem complex in appearance, it introduces marginal extra parameters compared with similar-structured base models (i.e., Multi-head Transformer, Ensemble, DMoE), and could also be efficiently **trained in an end-to-end manner**. In our experiments, we control the **complexity of our model to be the same as baselines\u2019** to ensure fair comparison (see section 4.2). Also, we conduct scalability test in section 4.4 and found that both **training and inference time scale at a sub-linear rate with more contexts**. While our training time cost is indeed larger than same-sized baselines because of different formulation of the loss function, **the inference time is very close** while ours is considerably more robust to distribution shift.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zICLK59ku1",
                "writer": "author",
                "reply_to": "4iBSt71IcBr",
                "title": "Response to Reviewer XNzH (Part 2 of 3)",
                "comment": " > ***Comment 3. \"... a continuous time model that uses timestamp information would be more appropriate to properly model how the gap between training and prediction ...\" and \"... If you train a model that does not account for latent confounders in a scenario where those confounders have changed, it is clearly not going to work well ...\"***\n\n(***The role of timestamp and \u201cexplicit context\u201d information***)\nWe appreciate the reviewer hinting on that explicitly considering time-stamp information (e.g., explicitly incorporate seasonality) may help to mitigate the distribution shift. More generally, there might exist other explicit contexts that can not be directly extracted from timestamp but is helpful for mitigating the shift such as user\u2019s preference, fashion trend, etc. \n\nBut the key question is **\"what if we don't have any such information about time-stamp or other explicit contexts, or such context is abstract and infeasible to describe?\".** This is the major challenge we actually face in event prediction (no explicit context including time-stamp), and is tackled by our variational context adjustment approach. We also remark that our method **can be combined with existing methods (e.g., some continuous-time models) that consider explicit contexts including time-stamp**, but this is beyond the scope of this work.\n\n\n> ***Question. \"Can the authors identify a scenario in which this approach is more appropriate than a continuous-time model that leverages timestamps to model the gap between training and deployment and also models the context's temporal dynamics?\"***\n\nTo conclude, we can't say our model is more or less appropriate than a continuous-time model since they are orthogonal used in different settings and tackling different technical challenges:\n1. Our model is used for \u201c(next) event prediction\u201d task where time-stamp is not necessarily available, while continuous-time model assumes its availability.\n2. Our model aims to deal with \"distribution shift\" in definition B with no assumption on the time gap between sequence and prediction, and continuous-time model seems to a reasonable choice in definition A.\n3. Our model tackles the challenge of implicit context, and continuous-time model deals with explicit context that is related with time-stamp.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4iBSt71IcBr",
                "writer": "author",
                "reply_to": "vVPIloiWVgM",
                "title": "Response to Reviewer XNzH (Part 1 of 3)",
                "comment": " Thank you for the time, thorough review, and constructive suggestions. Also, thanks for appreciating our novel methods, significance, and extensive experiments. We noticed that your main concerns lie in our motivation for the problem formulation and more experiments for verifying the necessity of proposed components. In the response below, we first provide a detailed and organized clarification for our problem motivation (including all related questions and weaknesses), then we supplement new ablation study results. We hope these answers could be valuable for your re-assessment of our work. \n\n### 1. Clarification on Motivation and Beyond\n\n> ***Comment 1. \"I am not convinced that it is well motivated for real APPLICATIONS.\" and \"There may be specific APPLICATIONS where the proposed method is needed, but the paper has not provided one.\"***\n\n\n\n(***Target application scenario***) \nPredicting the next type of event based on historical event sequence (a.k.a. event prediction) is actually a well-established problem with a variety of applications (e.g., epidemic control, clinical treatment). Perhaps the most prominent application under active research is (sequential) recommender system [1-4], where **the task is EXACTLY to use a fixed length of most recent interaction sequence (i.e., event sequence) to predict a new item the user is likely to interact (i.e., the next event)** for recommendation. Crucially, in these applications and recommendation datatsets used in our experiment, **the timestamp information is in default NOT available** or converted into a form that is easier to handle (e.g., season, age) as part of event feature. \n\n\n[1] Sequential Recommender Systems: Challenges, Progress and Prospects\n[2] Self-Attentive Sequential Recommendation, ICDM'18\n[3] BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer, CIKM'19\n[4] Deep Interest Evolution Network for Click-Through Rate Prediction, AAAI'19\n\n\n\n> ***Comment 2. \u201cif the goal is to predict events in the REMOTE FUTURE, ... , the event to be predicted will most likely not actually be the next event\" and \u201cfix a model which is not appropriate for its task ... for predicting events in the REMOTE FUTURE\u201d***\n\n(***Clarifying the definition of \u201cremote future\u201d***) \nBased on the above comments, we suspect there might be a misunderstanding regarding the definition of \u201cremote future\u201d and our problem setting. For problem setting, in this work, **predicting the next event is the \u201ctask\u201d** and **temporal distribution shift is the \u201cchallenge\u201d** (which will be discussed in detail next), NOT \"trying to use next event prediction for the task of solving temporal distribution shift\". The next clarify the definition of \"remote future\":\n\n\n[Definition A (a different problem)]: The time-stamp of $\\hat y$ is significantly greater the time-stamp of last event $x_t$ in the input sequence $S$. Example: using a user\u2019s behavior in 2020 as input $S$ to predict his/her behavior $\\hat y$ in 2022.\n\n[Definition B (ours)]: The time-stamp of testing data $(S_{test},\\hat y)$ is significantly greater the time-stamp of $(S_{train}, y_{train})$. Example: training the model by using data collected in 2020, and using user's behavior in 2022 (i.e., $S_{test}$) to predict next behavior in 2022 (i.e., $\\hat y$). \n\nWe focus on definition B that is essentially different with definition A in the sense that **it does NOT make any assumption on the time gap between last event and prediction**. This is indeed a widely-recognized challenge ([41, 36, 22] in the main paper) in both research and industrial fields reflected by the widely observed discrepancy between offline and online performance, which unfortunately is neither well-understood nor properly solved. \n\nWe also have given a formal formulation of definition B in appendix G with a new illustrative figure.9 for readers to better understand the problem formulation.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "l3sB-fG7dcY",
                "writer": "author",
                "reply_to": "nips_2022_XQu7UFSbzd2",
                "title": "Message to area chairs and all the reviewers",
                "comment": " Dear Area Chairs and Reviewers,\n\nWe thank the reviewers for their time, valuable comments, and constructive feedback. We are encouraged to see that generally, the reviewers found our paper well written (XNzH, t8MQ, HB3W), the problem significant (XNzH, k7dp), our models reasonable (XNzH, k7dp) and novel (XNzH, t8MQ, HB3W, k7dp), the evaluation solid and extensive (XNzH, k7dp), and the results promising (t8MQ, HB3W, k7dp).\n\nIn light of the nice suggestions from the reviewers, we modify our paper accordingly with a newly uploaded version. In the response below, we address all the questions point-by-point and add extra experiment results. We also welcome further comments or requests.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "vVPIloiWVgM",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_XQu7UFSbzd2",
                "title": "",
                "comment": " This paper aims to address the task of event sequence modeling, where the aim is to predict the next event in sequential data, focusing on the scenario where there is a substantial gap between the time the data is collected and when the prediction is made.  In this setting, the data distribution may have shifted over time, and the events' context may also have changed.  The authors propose a strategy that combines several ideas: a causal \"backdoor adjustment\" to correct for the confounding nature of the latent context, variational inference to infer the distribution over the latent context, and a neural network with a \"hierarchical branching structure\" for the inference. The proposed methodology combines a diverse range of ideas from different subfields of machine learning (temporal modeling, causality, variational inference, neural networks) into a sophisticated, cohesive approach.  The proposed methodology seems reasonable (though with this many moving parts, I could easily have missed something if there was an issue).\n\nWhile the problem formulation is interesting, I am not convinced that it is well motivated for real applications.  Arguably, if the goal is to predict events in the remote future, a model which predicts the next event in a sequence is not appropriate, since the event to be predicted will most likely not actually be the \"next\" event.  In this scenario, a continuous time model that uses timestamp information would be more appropriate to properly model how the gap between training and prediction would impact the prediction.  E.g., in the example where the context is the season, one can easily incorporate seasonality into a model that takes the timestamp into account.  There may be specific applications where the proposed method is needed, but the paper has not provided one.\n\nI am also not convinced that the method needs to be this complicated, and the experimental results have not confirmed this.  To justify this complicated formulation, it would have been valuable to include comparisons to simplifications of the proposed framework.\n\nThe analysis regarding the limitations of maximum likelihood estimation (Section 2.1) was very nice, though not really surprising.  If you train a model that does not account for latent confounders in a scenario where those confounders have changed, it is clearly not going to work well.  The issue is not really with maximum likelihood estimation, but with the misspecification of the model in not modeling the confounder, as well as using a non-causal method in a scenario where causality is needed.\n\nThe paper is reasonably well written.  If it is hard to follow in parts, this is mostly because of the complicated combination of disparate ideas.\n\nStrengths:\n\n  - Innovative approach combines ideas from different subfields in a unique way.\n  \n  - The use of causal methods for this problem is worth pursuing.\n  \n  - Experimental results include both quantitative and qualitative analyses, and investigate scalability and hyper-parameter settings.\n\nWeaknesses: \n\n  - The problem formulation is not well motivated (see above).  It is a lot of effort to try to fix a model which is not appropriate for its task (event sequence modeling for predicting events in the remote future) instead of just using an appropriate model (continuous time models, particularly those which explicitly model the context and distribution shift).  Those types of more traditional models should also be compared to as baselines.\n  \n  - The approach, although interesting, is complicated, and there are no experimental results that help to justify this complexity. Can the authors identify a scenario in which this approach is more appropriate than a continuous-time model that leverages timestamps to model the gap between training and deployment and also models the context's temporal dynamics? Since there is little focus on the potential application of the work, there is not much to say about its societal impacts.\n",
                "rating": 4,
                "confidence": 4
            },
            {
                "review_id": "F7dl77V9uEf",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_XQu7UFSbzd2",
                "title": "",
                "comment": " The paper investigates the the temporal distribution shift problem or sequential event prediction and propose a new variational context adjustment approach. The main idea of the approve leveraged variational inference to estimate and adjust for the casual contexts, treating them as latent variables to be learned by the model. Lastly, a hierarchical branching structure was proposed to accommodate large number of events  .  Pros:\n1) Clarity of writing. The paper is very well-written and easy to follow, which should help with reproducing the results of this work.\n2) Originality. Although the idea of leveraging variational inference for casual effect inference is not new [1], the application of it to event prediction is worthy of investigation, in addition to the novelty from the proposed hierarchical structure that can scale up to large number of events and the mixture of posteriors as prior.\n3) Evaluation results looks promising, especially the visualization of context probabilities over time.\n\nCons:\n1) Lack of synthetic evaluations. For works with casual inference, it's necessary that the authors should experiment with a synthetic evaluation where ground-truth contexts/confounders are known and are manually manipulated. This allow one to verify exactly how good are the estimated contexts compared to ground-truths and how the model adjust for the context to arrive at the next-step prediction. I don't recall seeing it in the paper.\n2) Lack of insights for the real-world datasets in the evaluation section. Is there a way to relate how the changing context causes certain events  to happen more/less? Although the proposed approach did well at detecting shifts and adjusting for these shifts to improve event predictions, can we draw any practical insights from these shifts w.r.t the real-world datasets like movies reviews / preferences.\n\n[1] Louizos, C., Shalit, U., Mooij, J. M., Sontag, D., Zemel, R., & Welling, M. (2017). Causal effect inference with deep latent-variable models. Advances in neural information processing systems, 30 Could the authors comment on the model's performance where the context shifted to a point where previously common events now become are and previously rare events now become common. The latter case would suggest that the model will have trouble learning good embeddings for the rare/possibly unseen events during training. The reviewer agrees with the authors that there are no imminent societal impact with the current work.\n\nHowever, the reviewer would recommend the authors to address potential limits of this work, per the check list. It was marked as yes but no reference section. Mainly, given the potentially large number of event types for real-world problems, it seems like the branching factor of K^D is quite limiting.\n\n",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "jVBvA33OOv",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_XQu7UFSbzd2",
                "title": "",
                "comment": " The authors propose the CaseQ framework for temporal event prediction under distribution shift. They identify a clear problem in typical temporal event prediction approaches: learning spurious correlations between items in the training set that do not generalize to the test set, which consists of events at later times. They hypothesize that these spurious correlations are due to latent contexts that may change from the training to the test set. They consider structural causal models for the data generating process that incorporate context and propose to use a backdoor adjustment to deconfound the context. They then propose a variational approximation that is tractable and treat the evidence lower bound (ELBO) as the learning objective. They demonstrate that their CaseQ framework can be used to mitigate accuracy drops when the gap between the training and test sets grows over time.\n Strengths:\n- Addresses the distribution shift problem for temporal event prediction in an innovative manner (to the best of my knowledge) using a causal inference approach combined with variational inference.\n- Strong experimental results showing that the proposed CaseQ framework can mitigate drops in recommendation accuracy by about 50% as the gap between training and test set grows to about 30 events.\n- Paper is mostly well written and explains the reasoning behind the proposed generative perspective on temporal distribution shift in a friendly manner.\n\nWeaknesses:\n- Novelty of the specific model instantiations are unclear. It appears to me that the authors are taking standard sequence models (e.g. GRU) as the inference unit, but then then hierarchical branching structure is novel. It would be helpful to describe in the first paragraph of Section 3 which components are novel.\n- Several of the figures are quite small (e.g. Figure 4) because the authors are trying to present a lot of results. I would suggest moving some of the results to the supplementary material, e.g. the hyperparameter analysis in Section 4.5. I'm not sure what I'm supposed to understand from Figure 4 aside from seeing some changes over time.\n- Code is not provided with the submission.\n\nOther minor issues:\n- \"Prior art\" and \"prior work\" are already plural, not \"prior arts\" and \"prior works\"\n- Time unit is missing in Figure 6. Train time of 0.12 in what unit of time?\n 1. Which components of the model instantiations section are novel?\n2. The proposed framework considers a very specific reason for distribution shift--dependence on a hidden context. What happens if there are distribution shifts for other reasons, e.g. changes in user preferences over time that may not be related to a context?\n Limitations are not really discussed aside from some effects of hyperparameters in Section 4.5. There could be many different reasons for temporal distribution shift, and the proposed work considers one specific reason: dependence on a latent context. The discussion of limitations could be improved to discuss other potential reasons for context shift and how they could be incorporated into the model, or how their current model may try to capture these other reasons.\n",
                "rating": 7,
                "confidence": 2
            },
            {
                "review_id": "ocJpNSIVloP",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_XQu7UFSbzd2",
                "title": "",
                "comment": " The paper tackles the problem of event sequence modelling and forecasting by allievating problem of adapting to OOD data. The authors propose a variational learning algorithm that encodes the context embeddings which model changing data distribution of past event sequences and motivate it as a Structural Causal model where the causal relation between context and future even sequence is deconfounded so that the causal chain only goes through past event sequence. Strengths\n1. Paper is strongly motivated and tackles an important problem in deploying real-world predictive systems with rapidly changing data distributions.\n2. The method is a novel application of SCM for event sequence prediction and modeling choices are well justified both rationally and empirically.\n3. The empirical results show improvement in some standard benchmarks. Ablation and secondary results show that the model indeed automatically learns to detect changes in context and switches across them resulting in increase in performance.\nWeaknesses\n1. % improvement is used as the primary metric of comparison. A better approach would be to provide absolute results and note statistical significance.\n2. Model doesn't adapt to novel context in test set and there is no evidence of good uncertainty estimation in unknown scenarios when facing novel contexts/data distributions. 1. How does model deal with novel contexts?\n2. Will the generative model adapt to uncertain situations or provide good uncertainty bounds?\n3. EValuation on a benchmark where changes in context is known to test if the learned contexts map to domain knowledge would be useful. 1. .It is not clear how sensitive the model is to the number of contexts when dealing with datasets of differing scales and changes in distributions.\n2. It is not clear how the model deals with novel or derivative contexts.\n3. The underlying assumption of using backdoor adjustment to effectively deconfound context to events causal relationship may not in many real-world cases.",
                "rating": 6,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The proposed method that combines techniques from different fields",
                "Sentiment Expression": "is interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the use of causal methods for the distribution shift problem in temporal event prediction",
                "Sentiment Expression": "is novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The experimental results",
                "Sentiment Expression": "demonstrate the effectiveness of the proposed method well quantitatively and qualitatively",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "should be improved according to the reviewers\u2019 comments",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "yqPnIRhHtZv": {
        "paper_id": "iclr_2021_yqPnIRhHtZv",
        "paper_title": "Learning Hyperbolic Representations of Topological Features",
        "paper_abstract": "Learning task-specific representations of persistence diagrams is an important problem in topological data analysis and machine learning. However, current state of the art methods are restricted in terms of their expressivity as they are focused on Euclidean representations. Persistence diagrams often contain features of infinite persistence (i.e., essential features) and Euclidean spaces shrink their importance relative to non-essential features because they cannot assign infinite distance to finite points. To deal with this issue, we propose a method to learn representations of persistence diagrams on hyperbolic spaces, more specifically on the Poincare ball. By representing features of infinite persistence infinitesimally close to the boundary of the ball, their distance to non-essential features approaches infinity, thereby their relative importance is preserved. This is achieved without utilizing extremely high values for the learnable parameters, thus the representation can be fed into downstream optimization methods and trained efficiently in an end-to-end fashion. We present experimental results on graph and image classification tasks and show that the performance of our method is on par with or exceeds the performance of other state of the art methods.\n      ",
        "paper_acceptance": "poster-presentations",
        "meta_review": "The paper proposes a novel method for representing persistence diagrams by embedding them to a Poincare ball.  The representation is learnable, and unifies essential and non-essential features.   The experimental comparisons with existing representation methods show significant improvements in performance.   \nThe flexible data-driven embedding to a suitable geometric space is a novel idea, which will certainly advance the usefulness of TDA.  The  experimental resutls demonstrate well the advantage of the proposed repesentation.  The authors have also addressed the review comments appropriately, with some extra experiments.  This is also a good addition.   ",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "NTpqIcJOSoT",
                "reply_to": "iclr_2021_yqPnIRhHtZv",
                "title": "Insufficient Experiments",
                "comment": "In this paper, the authors proposed a new representation of persistence diagrams that can include `''essential features''. Essential features correspond to the intrinsic topology of the underlying space that will not die during the filtration. To include the fact that the essential features are infinitely far from other normal features in the diagram, the authors proposed to use a Poincare ball representation, which maps the diagram into a disk whose boundary is infinitely far from inside. \n\nThe authors further proposed a classifier that learns the parameterization of the embedding of a diagram in the Poincare ball. The presentation learning procedure seems to be similar to (Hofer et al. 17), except for using a Poincare ball representation instead of the Euclidean square/triangle representation. Experiments are carried on graph classification tasks and image classification task.\n\nOn the positive side, I think the proposed representation well unified essential and non-essential topological structures. It is elegant and well-thought. A stability theorem is proven (in a similar manner as other known representations). The references are also reasonably complete.\n\nHowever, I am having doubts on the practical motivation of the representation in the learning context. To me, essential features can be considered by simply adding the histogram of their birth times as additional features (to the neural networks at an appropriate layer). I think this approach is a natural and necessary baseline to be compared with.\n\nGenerally, the empirical results are not particularly strong. On graph datasets, the proposed method is only winning 2 out of 5 times. Many important baselines are also missing. For graph classification methods, state-of-the-art classifiers such as GIN and GraphSAGE should be at least compared with. For topological classifiers, many kernel methods could be compared with: PWGK, PI (both of which were used in the other experiment), also Sliced-Wasserstein Kernel. I honestly think an ensemble of these methods and the histogram of birth times of essential diagrams can be easy to tune and perform better. \n\nOther questions/comments:\n\nThe references are not standard and need to be fixed. \n\nFor graph with Rips filtration, why are there 1-dim essential homology? Wouldn't all 1D homology features be killed eventually?\n\nDetails of the classifier (how the representation is learned) is still not clear even after reading the section in the supplemental material. \n\nExperimental details are missing. I understand this is 80% training 20% testing. But how many folds were used to evaluate? Some baseline numbers are very similar to the numbers in the GIN paper. However, the GIN paper was using a 10-fold cross-validation. So there is some discrepancy in the experiments. I would appreciate if the authors could kindly elaborate.\n\nOverall I think this is a nice mathematical formulation to incorporate essential features into the representation of persistent homology. But the practical usage in learning is not very convincing. Fundamentally, the essential features are completely different from non-essential ones. There is nothing in between them. Thus the benefit of a unified representation does not bring much more information than simply treating them separately.\n\n\n** After rebuttal: \nI am increasing the score to 6. I appreciate the authors' response to the reviews. They did the additional experiments I asked for. \n\nAs I stated in the original review, I really liked the unified approach. It is elegant and is nicely presented.\n\nAfter reading the authors' response to R4, it is clarified that the 1D essential homology is because the computation over all threshold is too expensive. I think there might be an opportunity to better justify this paper: we often have to stop the filtration early due to computational concern. This unified representation could potentially be a good solution for this: without computing the actually death time, the unified solution can still 'learn' the real death time of the 1D essential classes. The authors might want to discuss or ideally empirically verify this in the final version. For example, can you show that using the new approach, and stopping earlier during the filtration, the unified classifier can be as good as when we run the whole filtration and compute the real death  time for all 1D classes. Moreover, it will be ideal if the authors can manage to show that the unified approach can actually learn the real death time for these fake essential classes (I do not know how). This way the paper can potentially have a bigger impact. \n\n\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "LNuETgQ_0qh",
                "reply_to": "iclr_2021_yqPnIRhHtZv",
                "title": "Interesting direction but needs to be motivated and discussed more",
                "comment": "The authors propose to learn representations of topological persistence diagrams in hyperbolic spaces (Poincare balls). They provide a step-by-step methodology, and an algorithm for creating the representation. They also compare their approach with various graph classification and other ways of representing persistence diagrams.\n\nStrengths:\n=========\n1. This is the first work that learns hyperbolic representations of persistence diagrams.\n2. The methodology is sufficiently clear and the paper is reasonably well-written.\n3. Applications to graph data as well as image data show some promise.\n\nThings to improve:\n=================\n1. My biggest question in this paper is around the assumptions the authors have made and the motivation. They have mentioned that essential features have infinite persistence and truncating the persistence is not the best thing to do. I agree, however, there is really only one essential feature - the 0th order persistent homology group that becomes a single connected component at a scale that is sufficiently large. Since this \"essential\" group is one and the same for all point clouds that we compare,  they always match to one another, and it is safe to say that this truly essential group can be ignored in ideal cases. All other \"essential\" groups are only artifacts of our construction, which happen because we do not use a large enough scale for computational or some other reason.\n\n2. Even in an ideal setting (with large enough scales), it would be good to discuss why hyperbolic coordinates make sense. Why do we expect the distance between the persistence homology module to have some hyperbolic trend (see fig. 2 in https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/). Some thought and discussion around this would be helpful to understand the motivation of the approach.\n\n3. Based on fig. 2 I was initially under the impression that individual points will be mappable between persistence diagrams and hyperbolic representations, but it took some more reading to understand that there is one single representation for the entire persistence diagram in the hyperbolic space. Perhaps the authors can make it clear somewhere (even in Fig. 2).\n\n4. How is x0 chosen in (11). Why is summing in tangent space meaningful? Why not some  other operation?\n\n5. What is the absolute state-of-the-art  for Table 1 and Table 2? I am not holding this as a negative, but it may be good for the readers to know where TDA methods stand in these applications.\n\n6. Along similar lines, are there any applications that are especially suited to TDA that the authors can demonstrate? \n\n7. The authors seem to have used multiple runs for results in Table 1, but the text in Sec. 4.1 does not indicate how it is done. Please elaborate.\n\n8. Why are some values missing for some methods in Table 1.\n\n9. Why does P-Eucl make sense as a baseline? Is this like a so-called ablation study for this procedure? How are the representations created for P-Eucl? Please feel free to add more details on this in supplement.\n\n10. In Sec. 4.2, do the  compared methods also use the same simplicial construction (cubical complexes constructed in a  specific way)? If so, would the compared baselines work better with some other construction?\n\nIn summary, this is an interesting direction, but clarity is needed regarding the motivation,  and presenting extra details along the way will also be appreciated.  Finally, it could make sense to think about  an appropriate application.\n\nUpdate post-rebuttal\n==================\n- I am happy with the thorough engagement by the authors and their clarifications. So  I am bumping the up the score a notch.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "bO61maWLZ3",
                "reply_to": "B3J7GqNd9Of",
                "title": "Thank you!",
                "comment": "We are glad that you are satisfied with our revision and the added baselines!\n\nWe have updated the reference for the GFL paper. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iuWx8HlKICW",
                "reply_to": "NTpqIcJOSoT",
                "title": "Response to Reviewer Feedback",
                "comment": "We would like to thank the reviewer for this feedback and accept his criticism regarding the motivation and the lack of sufficient experiments to demonstrate our motivation. We have revised the paper following your suggestions and hope that the updated version addresses your concerns.\n\nWe have added the baseline that you recommended. We created a paragraph in the experimental section and consider a histogram-based feature separation baseline. While including essential features via the histogram of their birth times is natural and simple to implement, unfortunately, it does not yield the best results. We agree that there may be a combination of methods (e.g., PWGK + histogram of birth times for essential features) that performs better than our method. An important benefit of our approach is, as you noticed, the unified representation. Therefore, we do not need to hand-tune \u201ccomponents\u201d of other methods to obtain good performance. Also, we added an ablation study which demonstrates that the performance of our method is driven by the hyperbolic embedding. \n\nWe have also added more state of the art results in the graph case study. Admittedly, our method does not achieve the best performance on all datasets. Our comparisons with other methods act as a proof-of-concept rather than an attempt to find the \u201cuniversal best\u201d. Please note that despite comparing several state of the art methods, it is hard to pick one that performs \u201cbest\u201d on all benchmarks. We hope that you will agree that the facts that \n\n1) our representation is unified.\n2) the benefit of hyperbolic representation has been demonstrated via the ablation study,\n3) the simplified feature separation baseline performs poorly irrespectively of the inclusion of essential features,\n4) our method performs better or on par with several state of the art methods \n\nsuffice to lend merit to our approach. \n\nThe number of folds is 10 and we added that to the experimental section. The representation is learned using standard gradient methods. We highlighted that at the beginning of the experimental section and the full details are given at the end of the appendix. \n\nRegarding your comment on the essential features of the Rips filtration, please look at the follow-up response to Reviewer 4.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KEvM0qWVXP",
                "reply_to": "ddq_OUNzykQ",
                "title": "Follow-up on reviewer's comments",
                "comment": "Thank you for your second response. \n\nWe now understand better what you meant by your initial comment.  We kindly ask you to have a quick look at this: (https://ripser.scikit-tda.org/notebooks/Basic%20Usage.html) practical computation of PDs under the VR filtration for a simple point cloud and focus on cells 9-12. As you can see, by varying the maximum radius of the VR filtration we obtain 0-th and (multiple) 1-th dimensional essential features. While we may be able to increase the scale sufficiently high and end up having only the trivial essential feature that you mention, the resulting persistence diagrams may not be the ones that best summarize the data in terms of performance on the underlying learning task. We have added this comment in the second paragraph on page 2. \n\nThe process of obtaining the PD is essentially a feature extraction process and the scale can be considered as a hyper-parameter (among others).  In our experimental setup, we set all parameters related to PD extraction to pre-defined constants. Extending this to account for learnable parameters in the PD extraction process (such as parameters that appear in the filtration function or the scale itself) is an interesting direction for future work. We mention the paper Graph Filtration Learning by Hofer et al. (also mentioned by Reviewer 2) which is along these lines. \n\nExtended persistence is indeed a different way to construct persistence diagrams and our novelty is in the interpretation of ordinary persistence diagrams.  Other than our intuitive motivation in our previous response (2 in the list), unfortunately, we do not have a formal justification why we should treat points in a PD this way. Whether or not there exists some hyperbolic trend in the distance between persistence homology modules, as you commented in your previous response, is a very interesting question and a good direction for future search. \n\nWe have added these two directions for future work in the conclusion. \n\nRegarding the modification on Fig.2, we had added a small label to indicate that the representation of the persistence diagram belongs on the Poincare ball (blue color, above the arrow). We newly added a sentence in the caption to emphasize that.\n\nRegarding your comment on a niche application, we have thoroughly thought of that and tried to identify a case where our method would be particularly good. We observed that the hyperbolic embedding outperforms all other baseline methods for the IMDB datasets, which are known to be smaller graphs compared to the REDDIT ones. Therefore, we hypothesize based on this that it could be better suited for smaller networks. We have added this at the end of Sec. 4.1.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B3J7GqNd9Of",
                "reply_to": "S04Rl_bKU6",
                "title": "Thanks!",
                "comment": "Thanks a lot for your detailed response and the large number of changes you implemented, specifically those that pertain to an improved 'disentanglement' of the individual constituents of your method. The baseline values look encouraging to me, and I appreciate the time you took to rewrite and extend the paper!\n\nI have one tiny suggestion left to make: the reference for GFL appears to be outdated; please see the [PMLR page](http://proceedings.mlr.press/v119/hofer20b.html) for the most recent one (the paper was presented at this year's ICML conference, AFAIK).",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "n1gb6xK5Ci",
                "reply_to": "NwhSGlxMnvD",
                "title": "Thanks for the clarification",
                "comment": "Thank you for your clarification. \nI am happy to increase my rating for the submission (5-->6).\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zCqrv0z_kLH",
                "reply_to": "iclr_2021_yqPnIRhHtZv",
                "title": "Learning Hyperbolic Representations of Topological Features",
                "comment": "The authors propose to learn a representation for the persistence diagram (PD) in the hyperbolic space to incorporate the essential features (i.e., infinite persistence). The authors show that the hyperbolic representation has stability. Empirically, the authors illustrate that the hyperbolic representation for PD compares favorably with other baselines on graph and image classification.\n\nThe motivation to use hyperbolic representation to include essential features for PD is interesting. The authors give some details about the background (e.g., persistence diagrams, Poincare ball). However, the main part of the framework and how to learn the parameters of the embedding are missing.\n\nAs I understand, the authors propose: \n\n(1) to use some auxiliary transformation to lift points of $R^2$ into $R^m$ (however, there are no properties or information about this auxiliary transformation, and how one can do it)\n\n(2) projects points in $R^m$ into the Poincare ball, parameterized by $\\theta$. Why ones need this parameterization, and why this parameterization is important in applications? (It seems the authors combine projection with some transformation here?). It is better in case the authors give the projection (w.r.t. what distance?) and then transform the projected points. \n\n+ There is no description of the space of $\\Theta$ for the parameterization?\n\n(3) combine the representations of each point in PD. It seems that the authors use the sum of all points w.r.t. hyperbolic manifold to represent PD.\n\n+ I also concern about the usage of exp and log map at point $x$ for this combination. Typically, the tangent space is just simply a \"flattened\" space of the hyperbolic space at point $x$, it can only preserve the geometry for some close neighbor points of $x$ (this \"flat\" approach has a large distortion for those points which are far to $x$. It is better in case the authors give more discussion about how to choose $x$, and how it affects the geometry of the hyperbolic?\n\nThe main framework to learn those parameters is not presented in the main manuscript, and it is also unclear how one uses the learned hyperbolic representation for the downstream task?\n+ It seems that the authors incorporate the procedure to learn those parameters inside the networks for classification and learn end-to-end?\n\nSomehow, it seems that the authors propose to use a hyperbolic embedding within a neural network as a classifier for PDs. (in my opinion, the novelty may be incremental in case the authors simply replace the Gaussian-like embedding of Hofer et al. into a hyperbolic embedding inside a neural network. However, the hyperbolic representation for PD may be still interesting by itself. )\n\nOverall, I think this work seems interesting and has good potential. The authors may need to describe more details about the neural framework use to learn the parameters of the hyperbolic representation.\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "S04Rl_bKU6",
                "reply_to": "q4Pm8b_u4DP",
                "title": "Response to Reviewer Feedback",
                "comment": "We would like to thank the reviewer for his/her comprehensive feedback and we are glad that s/he liked our paper. We found your suggestions very insightful and they helped us bring the paper in much better shape. \n\n# Summary of the review - Response\n\n1) That is correct, we can use any of the representations developed by Hofer. In fact, there is not one single choice for $\\rho$. To guarantee stability, there are two conditions that $\\rho$ needs to satisfy and we have re-written them more clearly in Theorem 1. To ensure that the representation is not only stable but also expressive, we require injectivity (please also see response to your comment about an \u201cunderpowered\u201d $\\rho$). We have added a relevant discussion below Theorem 1.  The exact formula for $\\rho$ is of little interest and we have added it in the implementation details in the appendix. \n\n2) Thank you for providing the references. Indeed the GFL and the WKPI methods achieve better performance on some datasets that the state of the art we had included and we added them. The P-WL focuses on labeled graphs, which our method cannot currently handle, so direct comparison is difficult. Additionally, we adopted your suggestion about the ablation study and added a paragraph in the experimental section. The P-Eucl and P-Poinc variants are essentially an ablation study and we added a hybrid method (H-Hybrid). Please have a look at that part and the newly added experimental results. We believe that you will find them interesting. \n\n3) We moved that part from the appendix to the experimental section of the paper and elaborated on how we select the manifold dimension and the number ($K$) of the so-called projection bases (i.e., the replicas which we concatenate). Both of these are treated as hyper-parameters and the empirical rule for picking them is as follows: We initially attempt small values for the manifold dimension (e.g, $m=3$) and pick the number of projection bases depending on the type of data (i.e, in the range 5-10 for images and 200-500 for graphs). As we increase m, we decrease K to maintain a similar model capacity and avoid overfitting. We cannot say that a certain manifold dimension (e.g., $m=32$) would suffice for all practical cases, as it could differ from one dataset to another. In practice, we choose the optimal combination via the validation dataset. There is a discussion about how increasing m affects the validation performance at the end of Sec. 4.2.  \n\n# Detailed comments - Response\n\n>In the abstract, I would say that 'Existing methods are restricted in terms of their expressivity'. I would use 'bottleneck' only to refer to the eponymous distance, in order to prevent confusion.\n\nThank you for this suggestion. We like  the term \u201cexpressivity\u201d and we adopted it throughout the paper whenever appropriate.  \n\n> The definition of 'filtration' in the introduction is incorrect; a filtration is a sequence of subspaces.\n\nWe corrected this. \n\n> I would not refer to diagrams being injective. I understand what is mean here, viz. the fact that under some assumptions (!), one can reconstruct the input space from a diagram (this is known as solving the inverse problem), but I think this might be slightly confusion here. Why not focus on the expressivity properties of diagrams and the fact that one can approximate their inputs under some conditions?\n\nYou are right, \u201cinjective\u201d is misleading. We are referring to the inverse problem, as you noted. We re-phrased to avoid confusion. \n\n>The comment on 'extended persistence' on p. 2 is slightly imprecise; 'extended persistence' is well studied now, but PersLay is indeed the first 'deep learning' method incorporating it. This should be clarified.\n\nWe re-phrased this to emphasize that PersLay is the first deep learning method to utilize extending persistence. \n\n> When discussing filtrations and homology groups, I would stick with \u2286 instead of \u2282. The former is more generic and would make the write-up more consistent, as currently both forms are being mixed.\n\nWe corrected this. \n\n>As outlined above, I would suggest to be more verbose when it comes to the description of the method in terms of the individual functions, i.e. \u03c8 and \u03c1.\n\nWe tried to be more verbose on what $\\rho$ is and the conditions that it needs to satisfy. Regarding $\\psi$, we chose not to add any details in the main text as they are of little interest for our method and we prefer to limit the number of equations that the reader is exposed to. The analytical expression for it is still given in the appendix.  \n\n(continued)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "l423gf4nNOh",
                "reply_to": "1kcA4mxrlzQ",
                "title": "Thanks",
                "comment": "Thanks; this is an excellent clarification of my original points! I'll add another comment to your other response soon.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ddq_OUNzykQ",
                "reply_to": "n5gJ-aRgIZe",
                "title": "Response clarifies some questions",
                "comment": "Thanks to the authors for their response.\n\nI now understand that filtration plays a pivotal role even though in an absolute sense there is really only one essential feature. Like the authors pointed out Carriere et al. (ref. 19 in the paper) provides some hints around this. The authors should attempt to  clarify this in the  text, since Reviewer 1 also seems to have a similar question (For graph with Rips filtration, why are there 1-dim essential homology? Wouldn't all 1D homology features be killed eventually?). I am still not  completely convinced that we  should treat the points with apparently infinite persistence this way (the  extended persistence seems to introduce a novelty in the construction of the diagrams rather than just in the interpretation like what the authors do here). However, I  am willing to accept that this is one possible way of doing this.\n\nAlso, I do not see any changes in Fig. 2 compared to the initial revision that clarifies my original comment 3. Am I missing something?\n\nLooking at table 1, and the paragraph just above sec. 4.2, while you claim that P-Poinc is on par or exceeds state of the art, I see this only for 2  out of 5 datasets shown. I do not really expect any one method to outperform other TDA and non-TDA methods in all datasets.  This is why I was asking if the authors think there is  some niche application where  treating essential  features this way provides a good advantage (my original comment 6). It would be great if the authors consider this and provide their thoughts. This is also a good way to make this method more impactful in the crowded space of various distances between persistence diagrams.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1kcA4mxrlzQ",
                "reply_to": "q4Pm8b_u4DP",
                "title": "Response to Reviewer Feedback ",
                "comment": "(continued)\n\n>Am I mistaken or could \u03c1  also be any universal approximator, such as a set function or, more generically, a deep neural network? Would it be possible to use the method by Hofer et al. (2017) to obtain this map and then subsequently train a better hyperbolic embedding?\n\nIn practice, this auxiliary $\\rho$ could be any generic neural network approximator or the representation method by Hofer et al. (2017) or the PersLay by Carriere et al. (2020). Note that such a $\\rho$ would not satisfy, in general, the conditions of stability given by Theorem 1. Nonetheless, in practice, even unstable representations seemed to perform equally good in the classification tasks. In fact, we even tried picking $\\rho$ to be a simple zero-padding embedding, which does not satisfy the second condition of Theorem 1, i.e, it is not zero on the diagonal. The classification performance was equally good. This is explainable because the condition requiring $\\rho$ to be zero on the diagonal exists due to theoretical artifacts. Please see the discussion following Theorem 1. \n\n>It is my understanding that the Euclidean method P-Eucl is only driven by \u03c1. Is this correct? How critical is this choice in practice? Moreover, how was it chosen for this paper? I am asking because it is clear that the hyperbolic embedding helps improve predictive performance, but I wonder what would happen if one uses an 'underpowered' \u03c1 function.\n\nIn all cases (P-Eucl, the newly added P-Hybrid and the P-Poinc), \\rho is the same (as described in the newly added paragraph in the Appendix C). To obtain the P-Hybrid, we need to replace the Poincare ball with the Euclidean space. This implies that the exponential and logarithmic maps reduce to identities. Additionally, we obtain P-Eucl  from P-Hybrid by replacing the learnable parameterization given by Eq. 8 by simple addition of the learnable parameters. Please see the ablation study paragraph in the beginning of the experimental section. \n\nYour comment about an \u201cunderpowered\u201d $\\rho$ function highlights an important subtlety: The stability theorem (Theorem 1) does not prohibit us from choosing a degenerate $\\rho$. For example, $\\rho=0$ satisfies the conditions of Theorem 1 and therefore leads to a stable representation. However, such representation is obviously not useful for learning tasks. An implicit requirement for $\\rho$ is that it is injective (everywhere except the diagonal), which, given that $\\rho$ is a higher dimensional embedding this is a mild condition, fairly easy to satisfy. Please see the discussion following Theorem 1. \n\n# Style and clarity - Response\nWe correct typos and/or adopted your suggestions. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "n5gJ-aRgIZe",
                "reply_to": "LNuETgQ_0qh",
                "title": "Response to Reviewer Feedback ",
                "comment": "We would like to thank the reviewer for the feedback. We give our responses to your comments bellow:\n\n1) The scale is indeed very important for determining when features appear and when/if they disappear. But the filtration plays a pivotal role as well. Consider for example a filtration over graphs where a certain loop appears, unaltered, at the very beginning and the very end of the filtration. The persistence of this loop (i.e., 1-dim homology group) is infinite as it never \u201cdies\u201d in the filtration. We kindly ask you to look at the section \u201cExtended Persistence Diagrams\u201d of the paper by Carriere et al. (ref. 19 in our paper) where that same example is mentioned and used for motivating the introduction of extended persistence in the deep learning context. \n2) The hyperbolic representation is meaningful as it allows us to treat essential and non-essential features via a unified representation. The Euclidean representation would shrink the relative importance of essential features and the Euclidean metric cannot assign infinite distance to finite points.  \n3) Yes you are right, that figure might give that impression. We added a label to emphasize that the PD is mapped on a single point in the Poincare ball.\n4) The choice of $x_0$ is highlighted in Theorem 1. We choose $x_0=0$ to guarantee the stability of the representation. \n5) It is hard to pick an absolute state of the art as different methods perform differently across different datasets. We have included more state of the art results in Table 1 to show the comparison with an even wider range of methods. \n6) The method is not necessarily best suited for any special application. It is a generic method for learning representations of persistence diagrams extracted from any type of data to which TDA can be applied (e.g., graphs, images, time-series, high-dimensional point clouds etc). \n7) The multiple runs are the different cross-validation folds. We have highlighted that. \n8) Some values are not available for the corresponding dataset in the respective papers.\n9) In fact, the P-Eucl does act as an ablation study for this procedure. We have added a paragraph in the experimental section elaborating on P-Eucl and a  newly added P-Hybrid method. \n10) For the baselines, we used the same PDs as the ones we used for our method. We obtained the baseline results using the publicly available code from the respective paper. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "NwhSGlxMnvD",
                "reply_to": "zCqrv0z_kLH",
                "title": "Response to Reviewer Feedback",
                "comment": "We would like to thank the reviewer for the feedback and we are glad that s/he found hyperbolic representations of persistence diagrams an interesting research topic. Based on the feedback, it seems that the reviewer has fully understood our method. We provide clarifications on details that may have been unclear in our paper. \n\n1) Regarding the properties of the auxiliary transformation, we kindly point the reviewer to Theorem 1. We have re-written the theorem to highlight the properties. The auxiliary transformation \\rho is assumed to be Lipschitz continuous and zero on the diagonal, i.e., \\rho(x) = 0 for all x \\in \\mathbb{R}_\\Delta. Both assumptions are needed to ensure stability as per Theorem 1. There is a short explanation of why we need \\rho(x) = 0 on the diagonal and the full proof is provided in Appendix B. Please also note that we have added (following comments from Reviewer 2) a paragraph after Theorem 1 where we explain another property of \\rho. \n\n2) The \u201cprojects\u201d word is a misleading choice. We use it interchangeably with \u201ctransform\u201d. What we are actually doing is transforming the points from \\mathbb{R}^m to the Poincare ball via the learnable parameterization given by Eq. 8. We rephrase the text replacing the word \u201cproject\u201d with \u201ctransform\u201d to avoid any confusion. The parameter space \\Theta is essentially the R^m space, we corrected that. The parameters \\theta are a pivotal part of our method because they allow us to learn the representation of each point in the PD (and consequently, to learn the representation of the PD itself) on the Poincare ball. Our method is somehow similar to the one by Hofer et al. in the sense that both learn the representations of PD and feed them to neural networks for classification tasks. However, we believe that this is not an incremental contribution; as the reviewer points out, the hyperbolic representation of PDs is the main novelty of our work. \n\n3) We kindly point the reviewer to Theorem 1 where we state the choice of x (or x_0 as denoted in that theorem). We choose x_0=0 which guarantees that the representation is stable. As before, this is discussed in the paragraph following Theorem 1. The choice of x_0 does not have any major impact on our method. Please note that the desired property of the Poincare space (i.e., the ability to assign infinite distances to finite points) holds irrespectively of the chosen x_0. The exponential and logarithmic maps are utilized so that we can combine the representations of individual points of the PD into a single point in the Poincare space. Regarding your comment about the tangent space. The tangent space is a vector space and if we equip it with the Euclidean metric it does indeed cause distortion on geometric quantities. However, we typically equip the tangent space with the induced metric.  The transformations given by these maps are norm-preserving, i.e., for example, the geodesic distance from $x$ to the transformed point $\\exp_{x}(v)$ coincides with the metric norm $||v||_g$ induced by the metric tensor $g^\\mathcal{B}_x$.\n\n>It seems that the authors incorporate the procedure to learn those parameters inside the networks for classification and learn end-to-end?\n\nThis is in fact exactly what we are doing. The learnable representation acts as an input layer in a DNN and the parameters are learned end-to-end. The reviewer is right, we are not explicitly mentioning that in the main text, we only implicitly mention it in Appendix C. We added a short clarification in the beginning of the experiments section (Sec. 4). For reason for mentioning this in the experiment section rather that Sec. 3 is the following: Even though our method is validated on a neural network classifier using the representation as an input layer, the hyperbolic representation itself may be of independent interest, as the reviewer points out, not necessarily tied to the fact that is used as input to a neural network or to the classification task. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "q4Pm8b_u4DP",
                "reply_to": "iclr_2021_yqPnIRhHtZv",
                "title": "A novel embedding method for persistence diagrams",
                "comment": "# Synopsis of the paper\n\nThis paper proposes a novel embedding or representation algorithm for\npersistence diagrams, i.e. topological descriptors. Existing methods are\nrestricted because they do not feature *learnable* or *trainable*\nparameters for their representations (with the exception of methods such\nas the one by Hofer et al. or PersLay). This method, by contrast,\npresents a trainable embedding to the Poincar\u00e9 ball, thus representing\npersistence diagrams in a hyperbolic space.\n\nThis has the advantage of being more appropriate for representing\n*essential features*, i.e. features of infinite persistence. Thus,\nit is possible to perform end-to-end training of neural networks\nthat use persistence diagrams as their input.\n\nExperiments with graph classification and image classification tasks\ndemonstrate the utility of the proposed method.\n\n# Summary of the review\n\nThis is a very well-written paper, with a strong contribution to\ntopological data analysis and machine learning. The paper is technically\nsound, apart from some minor inconsistencies, which I shall discuss\nbelow. It is exciting for me to see how to obtain fully-trainable\nembeddings here, and I am happy to endorse this paper.\n\nThere are a few issue that I would like to see rectified in a revision\nof the paper, though:\n\n1. Some details on the method are missing. This concerns primarily the\n   learnable auxiliary transformation from Eq. 7. While the paper\n   subsequently discusses potential choices for this function, this is\n   not specified. In fact, the method tying everything together even\n   cites this equation again, without providing a definition for it.\n\n   This needs to be rectified---I am assuming that one could, for\n   example, use a transformation such as the one provided by Hofer et\n   al.; is this correct?\n\n2. The experiments on graph classification are lacking some comparison\n   partners. Specifically when discussing topology-based approaches, it\n   is useful to compare to other topology-based approaches. Here are\n   some suggestions:\n\n      - Hofer et al.: *Graph Filtration Learning*\n      - Rieck et al.: *A Persistent Weisfeiler\u2013Lehman Procedure for Graph Classification* \n      - Zhao & Wang: *Learning metrics for persistence-based summaries and applications for graph classification*\n\n    There is an overlap of the data sets assessed using these methods,\n    so the appropriate results could be cited and used. I wanted to\n    raise this concern primarily because I am aware of other methods\n    obtaining somewhat better classification performance in some cases.\n    \n    It would strengthen the paper immensely if it could perform some\n    form of 'ablation study', i.e. highlighting to what extent the\n    improved results are driven by the embedding in hyperbolic space, or\n    the choice of filtration, etc.\n\n3. Adding to this, some details of the method need to be described\n   better; only the supplements briefly mention how $m$, the embedding\n   parameter, is chosen in the end, but I would prefer a more in-depth\n   analysis of the choice of this parameter. Is it sufficient to pick,\n   say, $m = 32$ for all practical purposes? Or does it make sense to\n   concatenate the representations afterwards, as discussed in the\n   supplements? Adding more details will aid in understanding the paper\n   and, ultimately, promote further adoption of the method.\n\n# Detailed comments\n\n- In the abstract, I would say that 'Existing methods are restricted in\n  terms of their expressivity'. I would use 'bottleneck' only to refer\n  to the eponymous distance, in order to prevent confusion.\n\n- The definition of 'filtration' in the introduction is incorrect;\n  a filtration is a *sequence* of subspaces. \n\n- I would not refer to diagrams being *injective*. I understand what is\n  mean here, viz. the fact that under some assumptions (!), one can\n  reconstruct the input space from a diagram (this is known as solving\n  the inverse problem), but I think this might be slightly confusion\n  here. Why not focus on the expressivity properties of diagrams and the\n  fact that one can *approximate* their inputs under some conditions?\n\n- The comment on 'extended persistence' on p. 2 is slightly imprecise;\n  'extended persistence' is well studied now, but PersLay is indeed the\n  first 'deep learning' method incorporating it. This should be\n  clarified.\n\n- When discussing filtrations and homology groups, I would stick with\n  $\\subseteq$ instead of $\\subset$. The former is more generic and would\n  make the write-up more consistent, as currently both forms are being\n  mixed.\n\n- As outlined above, I would suggest to be more verbose when it comes to\n  the description of the method in terms of the individual functions,\n  i.e. $\\psi$ and $\\rho$.\n\n- Am I mistaken or could $\\rho$ also be *any* universal approximator,\n  such as a set function or, more generically, a deep neural network?\n  Would it be possible to use the method by Hofer et al. (2017) to\n  obtain this map and *then* subsequently train a better hyperbolic\n  embedding?\n\n- When running the experiments, is $m$ fixed or is the best $m$ selected?\n  I see that $m \\in \\{2, \\dots, 12\\}$, but I do not understand how this\n  is actually used in the network. Figure 4 is somewhat helpful here,\n  but it would be interesting to see what happens for a range of\n  parameters.\n\n- It is my understanding that the Euclidean method P-Eucl is only driven\n  by $\\rho$. Is this correct? How critical is this choice in practice?\n  Moreover, how was it chosen for this paper? I am asking because it is\n  clear that the hyperbolic embedding helps improve predictive\n  performance, but I wonder what would happen if one uses an\n  'underpowered' $\\rho$ function.\n\nAll in all, I feel that this could make a very strong addition to the\ntopological machine learning literature!\n\n# Style & clarity\n\nThe paper is very well-written, the authors are to be commended for\nthat. I have a few minor suggestions, some of which are more personal\npet peeves, but which might help make this paper shine even more!\n\n- 'root of a complex polynomial' --> 'roots of a complex polynomial' (?)\n\n- I would prefer not to use citations as nouns, i.e. I would prefer\n  writing 'Kusano et al. (5)' instead of 'In (5), Kusano et al.'; the\n  former strikes me as more readable and also translates well to\n  different citation styles.\n\n- The image of $f_n^{i,j}$ is *the* $n$th persistent homology group\n\n- Whenever possible, I would use $\\operatorname{text}$ for operators or\n  functions, instead of 'raw' $text$. This concerns for example the rank\n  function but also the degree function.\n\n- I would write 'Poincar\u00e9' everywhere\n\n- 'persistent diagram' --> 'persistence diagram'\n\n- 'persistent image' --> 'persistence image'\n\n- 'Vietories' --> 'Vietoris'\n\n- 'gray-scale' --> 'grey-scale' (or vice versa, if American English is\n  the preferred spelling)",
                "rating": 7,
                "confidence": 5,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "method for representing persistence diagrams",
                "Sentiment Expression": "novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The representation",
                "Sentiment Expression": "is learnable, and unifies essential and non-essential features",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "experimental comparisons",
                "Sentiment Expression": "show significant improvements in performance",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "data-driven embedding",
                "Sentiment Expression": "is a novel idea",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "experimental results",
                "Sentiment Expression": "demonstrate well the advantage of the proposed repesentation",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "addressing the review comments",
                "Sentiment Expression": "have addressed appropriately",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "This",
                "Sentiment Expression": "is also a good addition",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "eNB4WXnNczJ": {
        "paper_id": "nips_2021_eNB4WXnNczJ",
        "paper_title": "CANITA: Faster Rates for Distributed Convex Optimization with Communication Compression",
        "paper_abstract": "Zhize Li, Peter Richtarik",
        "paper_acceptance": "accept",
        "meta_review": "The review committee agreed that the paper is an interesting contribution and hence should be accepted for presentation at Neurips. However, there were some concerns regarding the practical relevance of the work. The authors mentioned in their response that they have run experiments on \"logistic regression tasks\". I strongly encourage the authors to include those simulations in the final version of the paper.\n\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "3zYJTrWGyBZ",
                "writer": "official_reviewer",
                "reply_to": "BHMTLIW2md1",
                "title": "Reply to the rebuttal",
                "comment": " Thanks for the rebuttal. I keep my score unchanged.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4TMuQnUzNwH",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_eNB4WXnNczJ",
                "title": "",
                "comment": "The paper propose a compressed and accelerated gradient method called CANITA for distributed optimization. \nIt improves the the state-of-the-art result which is achieved by DIANA in smooth and convex problems.\nThe main contribution is the (near) optimal algorithm and its theoretical analysis.  Originality:\n\nThe paper considers a unexplored topic, how to fuse acceleration technique and unbiased compression operator organically. \nThough the algorithm (CANITA) is a combination accelerated ANITA method and compressed DIANA method, the analysis of the resulting algorithm has its own difficulty, i.e., how to analyze the error introduced by the compression operator while not ruining the delicate structure of acceleration for convex problems.\nI think the analysis is new.\n\nQuality:\n\nThe theoretical analysis is valid.\nThe author also provides a proof sketch to illustrate how to construct a potential function for convergence analysis.\n\nClarity:\n\nThe paper is well written and clear.\n\nSignificance:\n\nThe paper improves the state-of-the-art result of non-accelerated compressed algorithms.\nI think the work might give insights on how to combine compression and acceleration optimally.\n\n\n======================================================\n\n\nI have read the author's response.\nI think the author well answers my questions.\nI keep my score.\n Some questions:\n1. Though CANITA has a theoretical superiority, it is still unclear how it performs empirically. I suggest the author could add some numerical experiments to investigate the empirical performance and its dependence on the hyper parameters. \n2. I am curious about why not provide the convergence rate of CANITA for strongly convex cases? ANITA could handle convex and strongly convex cases simultaneously. So I naturally expect CANITA could achieve it also. Comparing ANITA and CANITA, I find that the key is how to perform update rule (line 9). So why not use $x_{t+1}=\\frac{1}{1+\\mu \\eta_{t}}\\left(x_{t}+\\mu \\eta_{t} \\underline{x}_{t}\\right)-\\frac{\\eta_{t}}{\\theta_{t}} \\widetilde{\\nabla}_{t}$ in the line 9 of CANITA? Does there exist some fundamental difficulties\uff1f\n3. A typo: In line 148, I guess it should be $\\theta_t = \\frac{a_3}{t+a_4}$.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "9uP4odV37xq",
                "writer": "author",
                "reply_to": "nips_2021_eNB4WXnNczJ",
                "title": "To All Reviewers",
                "comment": " Dear Reviewers,\n\nThanks for the effort you put into reading and commenting on our work, and for the positive evaluation of our paper. You have given our work scores: 6, 6, 7 and 7, and have described our paper positively in the following way:\n\n- This paper is clearly written and well organized. \n- The proven convergence with the proposed algorithm is faster than a few earlier works.\n- This method achieves big O(1/T^2) convergence rate benefit from the acceleration technique.\n- The paper considers a unexplored topic, how to fuse acceleration technique and unbiased compression operator organically. \n- Though the algorithm (CANITA) is a combination accelerated ANITA method and compressed DIANA method, the analysis of the resulting algorithm has its own difficulty, i.e., how to analyze the error introduced by the compression operator while not ruining the delicate structure of acceleration for convex problems. I think the analysis is new.\n- The theoretical analysis is valid. The author also provides a proof sketch to illustrate how to construct a potential function for convergence analysis.\n- The paper is well written and clear.\n- The paper improves the state-of-the-art result of non-accelerated compressed algorithms. I think the work might give insights on how to combine compression and acceleration optimally.\n- This paper combines the accelerated ANITA in [16] and the compressed DIANA in [23]. I think it is a novel combination of two well-known techniques. It also extends the accelerated and compressed gradient method in [19] from strongly convex problems to nonstrongly convex ones. The convergence rate improvement over previous results is compared clearly.\n- The theory is technically solid. State-of-the-art convergence rate for nonstrongly convex problems is proved.\n- This paper is written well. The proof is organized very clearly.\n- This paper studies acceleration and compression in federated learning. This is a significant topic. \n- State-of-the-art convergence rate for nonstrongly convex problems is proved. The previous work on acceleration and compression all focus on strongly convex problems. I think the result is significant.\n\nYou have also raised some questions/concerns, to which we have replied in our detailed rebuttal. **Please can you let us know how well we did? Did we resolve your questions/concerns? If yes, we expect that you consider raising your scores appropriately. If not, what concerns remained unaddressed? We will be glad to get a chance to explain in more detail.**\n\nThanks for your time and support!\n\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LYblYpL3HJe",
                "writer": "author",
                "reply_to": "fmTBHAabs5g",
                "title": "Major concern: gradients vs stochastic gradients ",
                "comment": " Issue: \"My major concern is the usefulness of the studied setting. Requiring each node to compute its gradient (instead of stochastic/sampled gradient) in a distributed optimization does not seem practical. The setting is much less challenging than the conventional distributed (compressed) SGD setting as considered in QSGD in [1]. So it is unfair to say the results in the current paper improve [1] as in table 1. The order-wise faster convergence (compared to [1]) is not quite surprising to me under this distributed GD setting.\"\n\nReply:\n- The setting studied in our paper *is* useful in practice. Indeed, relying on exact/full gradient computation is both possible and desirable in the regime when the bottleneck of any algorithm/system is high cost of communication of messages between the workers and the master and not anything else, such as the computation cost on each node/machine/worker. In particular, if the data on each node is not very large (a typical scenario in federated learning), and/or if each node has large enough computational power (a typical scenario in commodity clusters), then the nodes are able to compute their gradients at a cost that can be far smaller than the cost of communication. This is the key scenario we focus in; and in this scenario CANITA works best. \n- We appreciate that you point out the possibility to study the stochastic setting, and we agree that this is an interesting subject of further study. However, it is also not trivial, and we made a conscious decision to leave this to future research. Indeed, note that we explicitly mention the stochastic setting (stochastic gradient, variance-reduced stochastic gradient [6,18]) as possible future work in our conclusion (Section 6).\n- Please note that no method works best in all scenarios. For example, if communication cost is not an issue, then communication compression is not necessary, and no method in this vast subarea of distributed optimization is relevant. This does not invalidate work in the area. It just means that the sweet spot/applicability domain for such method is elsewhere: in regime when the cost of communication is sufficiently high compared to other costs.\n- We disagree with the claim that the QSGD [1] setting is much more challenging if you are trying to say that the analysis of a method (such as QSGD) that uses i) communication compression and ii) subsampling is more challenging than analyzing a method (such as our method CANITA) that uses i) communication compression and iii) momentum/acceleration. In fact, we believe that the analysis of CANITA is much more involved, and the fact that there is no accelerated theoretical result in this area is an empirical evidence for this claim. In general, convergence/complexity proofs of accelerated/momentum methods are much more difficult than proofs convergence/complexity proofs of non-accelerated methods.\n- We agree with the claim that the QSGD [1] setting is more challenging if by this you mean a setting where, in addition to what we consider, one is unable to compute full gradients on each node, as we do. This is clearly true, almost tautologically, since the former imparts one more constraint on algorithms. However, this setting is not *much* more difficult in the analysis. Indeed, we believe that one could analyze CANITA with stochastic gradients as well. This can be considered as future work as we discussed above.\n- On inclusion of QSGD in Table 1: We include this in the table for several reasons. First, virtually everyone familiar with methods supporting compressed communication is familiar with QSGD just like everyone familiar with stochastic approximation is familiar with SGD. So, including theoretical result for QSGD is helpful didactically, and in order to put a flag at a traditional and well-known baseline. Further, QSGD *can* be applied as \"QGD\", that is, without stochastic gradients, and to distributed convex problems. Naturally, its rate will be slightly better (but still not accelerated) in this regime than in stochastic regime. So, it is a method that can solve the problem we propose, and hence is a completely valid method to compare to. Indeed, it is not surprising that other newer methods beat its rate - it would be surprising if they did not, as that would mean that progress in the area stalled a long time ago! This is not supposed to be surprising - it is supposed to be educational, and also scholarly, in that we give tribute to an early method in the area. Indeed, we are thankful to QSGD authors as they inspired our work (not only in this paper!).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Q39GCeIqfRZ",
                "writer": "author",
                "reply_to": "mLg-dtQyH0X",
                "title": "Issue: suitability for traditional distributed/federated optimization ",
                "comment": " Issue: \"I do not think this method is suitable for traditional distributed optimization problems or federated optimization problems. In traditional distributed optimization problems, the data size and dimension are large. Calculating $\\nabla f_i(w)$ is slow and usually is worse than calculating a stochastic gradient, even when the model is convex. Furthermore, additional parameters $h_t$, $\\bar{x}_t$ and $w_t$ will cost huge memory. In federated learning, usually only few machines participate training in each communication round.\"\n\nReply: \n\n- Calculation of gradients: We already replied to \"Reviewer Yf7V\" about the same issue in detail. Please can we refer you to that reply?\n- Parameters: We point out that in all methods, nodes always require the memory for at least one parameter: the model $x$ we are learning. In CANITA, the additional parameters are *reused* in memory across all iterations, i.e., the memory space does not accumulate, we only need one space for each parameter. Since we have three parameters, CANITA requires at most three times the standard memory cost of the least memory expensive methods. Moreover, it turns out that no accelerated methods or variance-reduced (shifted) compression methods can avoid introducing additional parameters, and CANITA is no exception. Thus, we do not think this additional memory is an issue; this is to be expected. Our improved theoretical rates are worth such a slim memory overhead!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5MwYHxppaqV",
                "writer": "author",
                "reply_to": "JucAPL7wQm",
                "title": "Minor comment: strongly convex case",
                "comment": " Issue: \"This paper studies the nonstrongly convex problems while [19] studied strongly convex ones. I am interested how fast CANITA converges for strongly convex problems by adoptting the proof techniques used in this paper. Is it faster than the one in [19]? The two rates in the last two lines of Table 1 looks different by letting $\\kappa=\\frac{L}{\\epsilon}$ in the first one. I suggest to include the proof for strongly convex problems in the supplementary material such that other researchers are likely to use the proof techniques in their work. For example, the authors may organize the proof in a unified framework for both strongly convex and nonstrongly convex problems.\"\n\nReply: \n- Thanks for this thoughtful comment! \n- We would like to refer the reviewer to our response to \"Reviewer CD9C\" related to \"Question: strongly convex case\". \n- Currently, we do not know whether our CANITA method is faster than the one in [19] in the strongly convex setting since we did not analyze CANITA in that setting.\n- Your observation of letting $\\kappa=L/\\epsilon$ in the first one makes some sense. Although the two rates look different, the order seems to be similar (if you move $\\omega$ into the square root in the first one, you will get the same term $\\sqrt{\\omega^3/n}$. As you suggested, we will try to include the proof for the strongly convex case, but this may be difficult. If we succeed, we will also try to organize a unified proof for both convex and strongly convex problems in the final version, such that other researchers are more likely to use our proof techniques.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-EXWw0dDLPf",
                "writer": "author",
                "reply_to": "4TMuQnUzNwH",
                "title": "Question: experiments",
                "comment": " Issue: \"Though CANITA has a theoretical superiority, it is still unclear how it performs empirically. I suggest the author could add some numerical experiments to investigate the empirical performance and its dependence on the hyper parameters.\"\n\nReply:\n- Thanks for this constructive suggestion. \n- We have already run some initial experiments on logistic regression problems, in which we compared our CANITA method with QSGD and DIANA. The numerical results indeed show that our CANITA method converges faster and with less communication cost. We will add the experiments in the final camera-ready version of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "OXCIJM_5GHG",
                "writer": "author",
                "reply_to": "4TMuQnUzNwH",
                "title": "Question: strongly convex case",
                "comment": " Issue: \"I am curious about why not provide the convergence rate of CANITA for strongly convex cases? ANITA could handle convex and strongly convex cases simultaneously. So I naturally expect CANITA could achieve it also. Comparing ANITA and CANITA, I find that the key is how to perform update rule (line 9). So why not use $x_{t+1}=\\frac{1}{1+\\mu \\eta_{t}}\\left(x_{t}+\\mu \\eta_{t} \\underline{x_t}\\right)-\\frac{\\eta_{t}}{\\theta_{t}} \\widetilde{\\nabla}_{t}$ in the line 9 of CANITA? Does there exist some fundamental difficulties\"\n\nReply:\n- Our initial thinking was as follows: the ADIANA method authors [19] already analyzed the strongly convex case and obtained acceleration, as we show in Table 1. However, there is no work on the general non-strongly convex case. So, in order to focus on what is not known, we omitted the more restrictive strongly convex case and tried to resolve the open problem: whether the benefits of compression and acceleration can be combined in the more general convex case as well.\n- Besides, your observation is correct. The key point is the update rule in line 9 for dealing with both convex and strongly convex cases simultaneously. If we use the similar update in ANITA for $x^{t+1}$ in line 9 of CANITA ,as you pointed out, we believe that CANITA can also deal with the strongly convex case as well. We do not think there exist some fundamental difficulties here.\n- Thanks for pointing out this point and letting us rethink it. We will think about this carefully and if we succeed, we will add the strongly convex case of CANITA later.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "P5_TWQ2k4JD",
                "writer": "author",
                "reply_to": "4TMuQnUzNwH",
                "title": "Question: typo on line 148",
                "comment": " Issue: \"A typo: In line 148, I guess it should be $\\theta_t = \\frac{a_3}{t+a_4}.$\"\n\nReply:\n- You are right. Thanks for pointing this out! We will correct.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4ZZGT_v3q8k",
                "writer": "author",
                "reply_to": "mLg-dtQyH0X",
                "title": "Thanks!",
                "comment": " Thanks for your positive and helpful comments and for valuing our paper for:\n- proposing a novel distributed method with acceleration and communication compression,\n- achieving the accelerated convergence rate which is faster than previous theoretical SOTA.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "TDxZfLwqUvK",
                "writer": "author",
                "reply_to": "4TMuQnUzNwH",
                "title": "Thanks!",
                "comment": " Thanks for the positive evaluation of our paper and for valuing its originality, quality, clarity, and significance in detail!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KPW6JgbPZnC",
                "writer": "author",
                "reply_to": "JucAPL7wQm",
                "title": "Thanks!",
                "comment": " Thanks for the positive evaluation of our paper and for valuing its originality, quality, clarity, and significance in detail!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uOHV-m78k9",
                "writer": "author",
                "reply_to": "mLg-dtQyH0X",
                "title": "Issue: partial participation",
                "comment": " Issue: \"In federated learning, usually only few machines participate training in each communication round.\"\n\nReply: \n- Yes, we know that in cross-device federated learning (FL) partial participation is crucial. However, in cross-silo FL, partial participation is not needed, and this is the focus of our work (note we work with a finite number of nodes, and not with a population of nodes, as is often done in cross-device FL). \n- Moreover, many federated learning works only theoretically analyze the full participation setting, like we do. This is often done because the main difficulty of such works lies elsewhere. For example, many Local SGD papers ignore to deal with partial participation because the combination of local steps and SGD alone is already theoretically difficult and not understood, especially in the heterogeneous data regime. Likewise, in our regime, the difficult part is to combine acceleration and communication compression in the convex regime as there are no results in this realm. We argue that one first needs to obtain such results in such an idealized scenario before further extensions are to be investigated. Partial participation is such an extension, and should be dealt with in subsequent work(s). Replacing full gradients by inexact/stochastic gradients is another possible future extension that needs to be carefully investigated. We can think of many possible future avenues for research, but note that none of them would be possible without the groundwork that our paper provides.  ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wHAyCWtxdhj",
                "writer": "author",
                "reply_to": "fmTBHAabs5g",
                "title": "Thanks!",
                "comment": " Thanks for your positive and thoughtful comments and for valuing our paper for:\n- being clearly written and well organized,\n- proving faster convergence rates (using a new method) than previous theoretical SOTA.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JTzG6LIy4oY",
                "writer": "author",
                "reply_to": "mLg-dtQyH0X",
                "title": "Issue: experiments",
                "comment": " Issue: \"Authors should add experiments to compare CANITA with other distributed communication efficient optimization methods. I think a convex optimization experiment can be easily conduct on pytorch and tensorflow.\"\n\nReply:\n- Thanks for this constructive suggestion. And thanks for not penalizing our work for lack of experiments which means you value theory!\n- We have already run some initial experiments on logistic regression tasks, in which we compared our CANITA method with QSGD and DIANA. The numerical results indeed show that our CANITA method converges faster and with less communication cost. We will add these and other experiments in the final version of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "J7BmsZk7Mvs",
                "writer": "author",
                "reply_to": "JucAPL7wQm",
                "title": "Minor comment: reorganization of some results",
                "comment": " Issue: \"I suggest to remove the proof of Lemma 6 to the supplementary material and include the numerical experiment in page 8. The proof of Theorem 1 in page 16 is redundant, because it is proved in page 9.\"\n\nReply: \n\n- Following your suggestion, we will move the proof of Lemma 6 to the appendix. \n- We will add the experiments where you suggest.\n- We will remove the redundant proof of Theorem 1 in the final version of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BHMTLIW2md1",
                "writer": "author",
                "reply_to": "JucAPL7wQm",
                "title": "Main issue: Experiments",
                "comment": " Issue: \"The experiment is missing. I suggest to include the numerical experiment in the final version. This paper proposes a new method, rather than reanalyzing a widely used method. So I think the numerical experiment is necessary.\"\n\nReply: \n- Thanks for this constructive suggestion! \n- We have already run some initial experiments on logistic regression problems, in which we compared our CANITA method with QSGD and DIANA. The numerical results indeed show that CANITA converges faster with less communication cost. We will add these and other experiments in the final version of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fmTBHAabs5g",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_eNB4WXnNczJ",
                "title": "",
                "comment": "This paper considers a distributed GD setting where multiple nodes need to jointly minimize the average of individual convex objectives by computing and communicating its compressed local gradients.  The proposed algorithm is shown to achieves an accelerated convergence rate with compressed gradient passing.  This paper is clearly written and well organized. The proven convergence with the proposed algorithm is faster than a few earlier works.\n\nMy major concern is the usefulness of the studied setting. Requiring each node to compute its gradient (instead of stochastic/sampled gradient) in a distributed optimization does not seem practical. The setting is much less challenging than the conventional distributed (compressed) SGD setting as considered in QSGD in [1].  So it is unfair to say the results in the current paper improve [1] as in table 1.  The order-wise faster convergence (compared to [1]) is not quite surprising to me under this distributed GD setting. Yes",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "mLg-dtQyH0X",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_eNB4WXnNczJ",
                "title": "",
                "comment": "This paper proposes a novel distributed methods with acceleration and communication compression. Authors proof the convergence and show that it is faster than existing SOTA methods.  Positive: \nThis method achieves bigO(1/T^2) convergence rate benefit from the acceleration technique.\n\nNegative:\n1. I do not think this method is suitable for traditional distributed optimization problems or federated optimization problems. In traditional distributed optimization problems, the data size and dimension are large. Calculating \\nabla f_i(w) is slow and usually is worse than calculating a stochastic gradient, even when the model is convex. Furthermore, additional parameters h_t, \\bar{x}_t and w_t will cost huge memory. In federated  learning, usually only few machines participate training in each communication round.\n\n2. Authors should add experiments to compare CANITA with other distributed communication efficient optimization methods. I think a convex optimization experiment can be easily conduct on pytorch and tensorflow.  See above.",
                "rating": 6,
                "confidence": 2
            },
            {
                "review_id": "JucAPL7wQm",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_eNB4WXnNczJ",
                "title": "",
                "comment": "This paper studies the compressed and accelerated gradient method for nonstrongly convex federated learning. State-of-the-art accelerated convergence rate is established.  Originality: This paper combines the accelerated ANITA in [16] and the compressed DIANA in [23]. I think it is a novel combination of two well-known techniques. It also extends the accelerated and compressed gradient method in [19] from strongly convex problems to nonstrongly convex ones. The convergence rate improvement over previous results is compared clearly.\n\nQuality: The theory is technically solid. State-of-the-art convergence rate for nonstrongly convex problems is proved.\n\nClarity: This paper is written well. The proof is organized very clearly.\n\nSignificance: This paper studies acceleration and compression in federated learning. This is a significant topic. State-of-the-art convergence rate for nonstrongly convex problems is proved. The previous work on acceleration and compression all focus on strongly convex problems. I think the result is significant.\n\nMajor comment:\n\nThe experiment is missing. I suggest to include the numerical experiment in the final version. This paper proposes a new method, rather than reanalyzing a widely used method. So I think the numerical experiment is necessary.\n\nMinor comment:\n\n1. This paper studies the nonstrongly convex problems while [19] studied strongly convex ones. I am interested how fast CANITA converges for strongly convex problems by adoptting the proof techniques used in this paper. Is it faster than the one in [19]? The two rates in the last two lines of Table 1 looks different by letting $\\kappa=\\frac{L}{\\epsilon}$ in the first one. I suggest to include the proof for strongly convex problmes in the supplementary material such that other researchers are likely to use the proof techniques in their work. For example,  the authors may organize the proof in a unified framework for both strongly convex and nonstrongly convex problems. \n\n2. I suggest to remove the proof of Lemma 6 to the supplementary material and inlcude the numurical experiment in page 8.\n\n3. The proof of Theorem 1 in page 16 is redundant, because it is proved in page 9.\n No limitation is discussed. I suggest to include the numerical expriment in the final version.",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "is an interesting contribution and hence should be accepted for presentation at Neurips",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the practical relevance of the work",
                "Sentiment Expression": "there were some concerns",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "those simulations",
                "Sentiment Expression": "strongly encourage the authors to include",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "-geBFMKGlkq": {
        "paper_id": "iclr_2022_-geBFMKGlkq",
        "paper_title": "Density-based Clustering with Kernel Diffusion",
        "paper_abstract": "Finding a suitable density function is essential for density-based clustering algorithms such as DBSCAN and DPC. A naive density corresponding to the indicator function of a unit $d$-dimensional Euclidean ball is commonly used in these algorithms. Such density suffers from capturing local features in complex datasets. To tackle this issue, we propose a new kernel diffusion density function, which is adaptive to data of varying local distributional characteristics and smoothness. Furthermore, we develop a surrogate that can be efficiently computed in linear time and space and prove that it is asymptotically equivalent to the kernel diffusion density function. Extensive empirical experiments on benchmark and large-scale face image datasets show that the proposed approach not only achieves a significant improvement over classic density-based clustering algorithms but also outperforms the state-of-the-art face clustering methods by a large margin.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper proposes a kernel diffusion method to improve upon density-based clustering methods. The reviewers found the empirical results quite promising and there is consensus that there are some good ideas in this work. However, their criticisms are strikingly consistent that the technical details are lacking and some of the claims are not fully supported, and these criticisms were not found to be fully addressed in the author responses. I agree with the assessment that this is promising in a major revision toward a future submission but it is currently not complete, especially in the theoretical and technical details.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "-hZF1thsrif",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_-geBFMKGlkq",
                "title": "",
                "comment": "The authors propose a new density function for density clustering models such as DBSCAN and Density Peaks Clustering (DPC). Given a kernel $K$ and the corresponding normalized random walk transition matrix $P = diag(K\\mathbf{1})^{-1}K$, the authors propose to use the density function which corresponds to the stationary distribution attained in the limit. Since this function is expensive to compute, the authors propose a surrogate density function which is easier to compute.\n\nThe main experiments compare various density functions for DPC on UCI classification datasets and the DPC on density functions to face detection methods on two face identification datasets.  In some regards, the motivation of the proposed density function is clear. The performance of density-based clustering methods depends to a large degree on the employed density measure and the traditional measures have obvious flaws. The proposed idea is sound and the experiments indicate that the proposed measures are able to improve the performance of clustering methods. \n\nI am missing in this paper though the connection between the employed clustering methods (DBSCAN and DPC) and the density measure. Likewise, the connection of the proposed method to spectral clustering using the random walk Laplacian is not discussed at all. Shouldn't spectral clustering with the random walk Laplacian return a clustering which maximizes the stationary distribution function within each cluster as well[1]? The relation between spectral clustering and DBSCAN has been established [2,3]. Likewise, spectral clustering itself has an interpretation as a density-based clustering method[4]. As a result, since there is already some literature about kernel diffusion maps and spectral clustering (as cited by the authors), the relationship between these clustering methods and what is particularly novel in this work should be made clear.\n\nFor example, Theorem 1 states that clusters which are disconnected in the graph indicated by the epsilon neighborhood kernel have the same density in each cluster. From best practice in spectral clustering we know that disconnected clusters are very sensitive to noise and other perturbations. So, the case which is assumed in Thm 1 to motivate the proposed density measure, should actually be avoided. In this regard, I would ask for a more complete picture of density-based clusterings (including spectral clustering), their theoretical background, and how the proposed density measure can improve the clustering performance (also in dependence of the employed clustering method). At the same time, there exist methods to learn the kernel function such that the data is well clusterable. How is the proposed density measure positioned with respect to these methods? \n\nThe experimental evaluation should be made stronger. The authors state that the required kernel parameters are tuned, which indicates for me that the experimental evaluation makes use of the class information which is not available in real-life clustering applications. I think that this approach could be interesting for experiments such as the ones in Tbl1, where the potential of the density function is evaluated. However, there should be more experiments which simulate the actual clustering process. I would propose to use synthetic data to illustrate differences in the clustering behavior, where the clusters are generated according to given density measures. Such an analysis could also provide insight into the cases where the proposed measures do not work well. Another interesting analysis would be an experiment on synthetic data where the noise is increased. Then, also other clustering procedures should be compared, e.g. spectral clustering and newer nonconvex clustering methods.  How the parameters are defined in the face clustering application is not explicitly described, so I assume that again the parameters have been tuned, which should be avoided.\n\n[1] Von Luxburg, Ulrike. \"A tutorial on spectral clustering.\" Statistics and computing 17.4 (2007): 395-416.\n\n[2] Y. Chen, \"DBSCAN Is Semi-Spectral Clustering,\" 2020 6th International Conference on Big Data and Information Analytics (BigDIA), 2020, pp. 257-264, doi: 10.1109/BigDIA51454.2020.00048.\n\n[3] Schubert, Erich, Sibylle Hess, and Katharina Morik. \"The relationship of DBSCAN to matrix factorization and spectral clustering.\" LWDA. 2018.\n\n[4] Hess, Sibylle, et al. \"The SpectACl of nonconvex clustering: a spectral approach to density-based clustering.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019. The authors propose a potentially interesting density measure for density-based clustering methods. Yet, the proposed measure is not sufficiently discussed in the scope of similar works for spectral clustering and the experimental evaluation is using class-label information for hyperparameter tuning, which is not eligible for clustering analyses. Hence, I reject the paper as is and suggest that the authors extend the experimental evaluation and provide a comparison to the related spectral clustering approaches for the next submission.",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "cRQi0LaPMOg",
                "writer": "official_reviewer",
                "reply_to": "722_TQXHAA2",
                "title": "Final Feedback ",
                "comment": " I find the idea of the paper quite cool, and as far as I see it's also correct. However, the experiments are just weak because of the hyperparameter tuning and there is no synthetic data used, to demonstrate emperically what the method is claimed to provide (generating the data according to the motivating case). In particular, I would like to respond to the following comment in the rebuttal:\n\n> A less effective method might be given the wrong credit due to a better parameter tuning.\n\nThis is why at least some kind of heuristic should be provided to set the parameters. Follow-up researchers can then use these heuristics to compare to the new method. Ideally, you try to find good parameters for competitors but the heuristic is used to set your parameters. If you can then show an improvement over your competitors (not necessarily on all data but on the data which fits the motivation for your work at least), then this makes for a strong argument.\n\nThere have been some improvements, such as the addition of SC to the experiments and also the parameter setting is clarified with regard to the face data. The connection to spectral clustering and the added novelty is still largely unclear. I take from the authors' response that their method is at least quite different to the cited reference (Nadir et al). I did not get a real response about my concerns of the Theorem assumptions. All in all, I would really like to see this paper in a polished and more round version. \n\nI'll increase my score to weak reject since I don't think anymore that there are no severe no gos in the paper after the rebuttal (the face data hyperparameters are at least not tuned).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8v6rRLSnnpa",
                "writer": "official_reviewer",
                "reply_to": "D5YK4LDJVep",
                "title": "Towards the end of the discussion phase, my recommendation does not change",
                "comment": " I think that the authors have not adequately addressed my concerns. \n-It has been admitted that the method lacks theoretical justification and provides superior performance in the case of dense datasets with 'many complex local features'. However, only two datasets (related to faces) of this type have been considered in the experiments. Moreover, how one should know about the existence of 'many complex local features' in order to prefer the method over alternatives?\n-In order to achieve improvement over typical density-based methods, additional hyperparameters are used which are tuned in a supervised way taking into account the ground truth. Tuning a hyperparameter without ground truth knowledge is not an easy task, therefore hyperparameters should be kept to a minimum number.\n- I don't see any reason why \\rho_{FKD} should perform better than \\rho_{KD} as happens in several experiments. The performance difference is significant in some cases. Moreover, there is no consistent trend: in some cases  \\rho_{FKD} performs better, while in other cases \\rho_{KD} performs better. I don't like this inconcistency.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HUojLsMH9RJ",
                "writer": "author",
                "reply_to": "UfY8nsp3AG",
                "title": "Response to optimal parameter tuning",
                "comment": " We agree with the reviewer that hyperparameter tuning in k-means is much easier compared to many state-of-the-art clustering methods. Actually, this was reflected in our numerical results in the revised paper (e.g., see Table 2 and 3), where $k$-means always pick the correct number of clusters. We also would like to mention that it is a common practice in the unsupervised learning community (not just [5]) that optimal tuning was carried out for complex data experiments (see all the recent competitor methods we cited in this paper). Although we agree this is not the same case as in real applications where no label information is available, thus is problematic to some extend. It is a good way to see how well different unsupervised learning methods can achieve in those datasets. Otherwise, it will be very difficult to fairly compare the performance of methods. A less effective method might be given the wrong credit due to a better parameter tuning. Also, people fore sure would argue for different results on the same method applied to the same dataset, but with different implementations of parameter tuning schemes. Therefore, it leaves us with no choice if we want to compare the proposed method with state-of-the-art works on face image clustering. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Mf4OFO66ER1",
                "writer": "author",
                "reply_to": "XStdNEfFNDA",
                "title": "Difference to Nadlre et al. 2005",
                "comment": " \nThank you for your further comments. We would like to take this opportunity to clarify the fundamental difference between the proposed method and spectral clustering through diffusion maps, such as Nadlre et al.(2005).\n\n\nDiffusion maps leverages the relationship between heat diffusion and a random walk (Markov Chain) on a weighted graph whose nodes are sampled from the data. It maps coordinates between data and diffusion space, aims to reorganize data according to a new metric (diffusion distance). For example, if we look into diffusion maps, time $t$ is a crucial hyperparameter (which is equivalent to the bandwidth in KDE) in the diffusion density $\\rho(x,t)$. Small $t$ represents local random walk, where diffusion distances reflect local geometric structure. Large $t$ represents global random walk, where diffusion distances reflect large-scale connected components. After a carefully tuning of $t$, a spectral clustering can be performed through diffusion maps, based on the intuition that the diffusion distance describes a \u201cperceptual similarity\u201d of points, e.g., points within the same cluster have small diffusion distances while in different clusters have large diffusion distances. Meanwhile, in the proposed method, we are constructing a diffusion process with  dissimilar properties and using its limit density as a function for density-based clustering.  If we consider the limiting density $\\rho(x)$ in diffusion maps, it trivially groups all data points into one large cluster. Moreover, the local adaptive kernels that have been proposed in this paper do even satisfy the definition of kernels in classic settings, which is quite different from the heat kernel used in diffusion maps.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1eAay9rKfza",
                "writer": "official_reviewer",
                "reply_to": "PiaGw1aj-MM",
                "title": "Towards the end of the discussion phase, only a marginal improvement in my assessment",
                "comment": " I remain somewhat ambivalent about the paper: On the one hand, there are some interesting points to the method and there is some evidence of potential for practical relevance in the experimental results. The authors have also done a reasonable job of handling some of my initial concerns. However, some concerns/questions seem to have been side-stepped to some extent. For example, my question about boundary bias and why that is not an issue for the proposed method was handled by suggesting a modification of their implementation which they admit is very similar to a reflection method for kde. If the problem can be corrected in both kde and their method, why should I conclude that theirs actually doesn't suffer from the same issue in practice? Most importantly, my concerns about the manner in which the experimental results were conducted remain. The \"oracle tuning + sensitivity study\" approach is not completely devoid of merit, but it doesn't give a clear indication of actual practical performance. The authors insist that tuning their method IS indeed easier than the other density based methods, but if this is the case then I don't understand why their experiments weren't conducted under a practicable scenario (i.e., by using whatever method they use for tuning their own method in practice). Since other reviewers have raised concerns about the tuning of parameters, it seems an obvious change to make. If the authors feel very strongly about the results they have presented, then they could provide both sets.\n\nMy assessment of the paper is slightly more positive than it was initially, however not enough to tip it over the line. I feel there is still too much uncertainty about the the actual practical performance of the method.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nARO9Vy1XIv",
                "writer": "official_reviewer",
                "reply_to": "PoilDoOMX-m",
                "title": "Thanks",
                "comment": " Thank you! I realise now my mistake, I missed the appendix in the main pdf and was looking at the appendix given as supplement.\nBest",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PoilDoOMX-m",
                "writer": "author",
                "reply_to": "pLYIWhXPyby",
                "title": "Table 6 in the Appendix",
                "comment": " Dear Reviewer Nbci,\n\nAs we checked Table 6 was included in the Apendix of the latest revised version. The results for spectral clustering were presented in the last column of Table 6.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "pLYIWhXPyby",
                "writer": "official_reviewer",
                "reply_to": "cZdYqtLhOXt",
                "title": "Spectral clustering results not present",
                "comment": " The authors say that the results for spectral clustering are given in Table 6 in the appendix, but I cannot find a Table 6. Is this just me?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UfY8nsp3AG",
                "writer": "official_reviewer",
                "reply_to": "OP3sO7amzPQ",
                "title": "Experimental Evaluation",
                "comment": " Dear authors, \nI have seen that you already discussed the tuning parameters issue with Rev Nbci and that for example the parameters in the face clustering application are not tuned. However, also the other experiments should not involve hyperparameter tuning. There is a difference between choosing an integer in k-means - for which also a lot of literature exists to set this parameter automatically- and tuning a combination of continuous parameters. What are then the experiments worth for practical applications? If [5] tunes the parameters in unsupervised applications then that is just bad practice and should certainly not be replicated.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "XStdNEfFNDA",
                "writer": "official_reviewer",
                "reply_to": "QS-hgKn1ooV",
                "title": "Spectral Clustering Connection",
                "comment": " Dear authors, \nthank you for addressing my concerns, but it seems like we are talking past each other. You mention in your paper a reference (Nadler et al 2005) where diffusion maps are discussed with relation to spectral clustering. I was saying that you should explain your approach also with respect to possible applications in spectral clustering, because spectral clustering is also a density-based clustering method with strong ties to DBSCAN. In particular, I wonder how your approach connects with the existing work you cite?  \nThe relationship of Spectral clustering with DBSCAN is simply established over the objective to find points that are densely connected (this is discussed more in detail in the references I pointed out). You propose a new way to define the density, yielding, in the end, a kernel or similarity matrix.  Density in \"density-based\" clustering may address the probability distribution of the underlying data distribution, but in the end, you calculate with a matrix giving you distances or inversely similarities - the matrix $W$. \n\nDo you agree with this description and if so, could you please elaborate on the connections and differences to (Nadlre et al. 2005)?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "D5YK4LDJVep",
                "writer": "author",
                "reply_to": "6f1qLEz5Pgx",
                "title": "Thank you very much for your prompt follow up comments!",
                "comment": " Please see our response below. We also revised the paper to include additional information on number of clusters as you suggested.\n\n**As far as I can see, you have not responded to the following question: 'It is important to provide information on the number of clusters returned by the density-based methods for all datasets. Do they recover the ground truth number of clusters?'**\n\nThank you for your very helpful suggestion. In the revised paper (see Tables 2 and 3), we now provide the number of clusters returned by the density-based methods for the face image datasets.  The proposed methods returned significantly better estimates of the number of clusters,  comparing to classic density-based methods such as $\\rho_{naive}$ and $\\rho_{LC}$. The estimates are quite close the ground truth. As we are carrying out experiments based on  optimal parameter tuning, methods such as $k$-means and HAC, which directly include the number of clusters as a tuning parameter, will not superisingly recover the ground truth number of clusters exactly. However, their clusetering performance are not as good as the proposed methods. \n\n\nWe also provide the number of clusters returned by the density-based methods for the benchmark datasets in the revised Appendix (see Table 7). We can observe a similar improvement in recovering the ground truth across most datasets.\n\n**As it can be observed in Table 6, k-means provides better NMI results in 5 out 13 datasets which is a significant percentage.**\n\nYes, we agree 5 out 13 datasets is a significant percentage. However, the benchmark datasets are mostly small-scale, low dimensional, with only a few clusters and not too many complex local features. See the meta information summarized in Table 4. As a result, the  advantage of local adapativity in the proposed methods is not that significant in these datasets. But still we can observe a very stable good performance across all datasets, and an uniform improvement comparing to classic density-based competitors.\n\nIn more complex applications such as  clustering the face image datasets, the advantage of the proposed methods becomes more obvious.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6f1qLEz5Pgx",
                "writer": "official_reviewer",
                "reply_to": "jqVDo6JRfDM",
                "title": "Response to response",
                "comment": " Thank you for the response and the revision.\n\nAs far as I can see, you have not responded to the  following question:\n'It is important to provide information on the number of clusters returned by the density-based methods for all datasets. Do they recover the ground truth number of clusters?'\n\nAs it can be observed in Table 6, k-means provides better NMI results in 5 out 13 datasets which is a significant percentage.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cZdYqtLhOXt",
                "writer": "author",
                "reply_to": "M2Bgk9Vrzn6",
                "title": "Thank you for the quick feedback, we hope the following points provide a satisfying answer to your concerns",
                "comment": " **1. I was curious about whether or not a standard KDE with the locally adaptive kernels and DBSCAN or DPC was done. If not, then it isn't clear if any improvements in performance are due to the different density estimator, or simply the different kernel being used.**\n\nThank you for clarifying your point. Note that KDE is used in DBSCAN or DPC as an estimate of the undelying pdf. Even if we use other classic KDE kernels or adaptive ones such as variable-bandwidth KDE [1], put aside the problem of tuning varying multivariate bandwidths, what we can get is a hopefully better estimate of the underlying pdf. The existence of local features/structures still chanllenges density-based clustering methods (recall that in DBSCAN we need to decide cluster center, core points, and noises based on the height of density function). For example, small or less concentrated clusters are still difficult to discover, especially if they are next to a large and compact cluster, as shown in Figure 1.  We want to justify that the fail of DBSCAN and DPC is not just because they are not using other standard kernesl or adaptive kernel bandwidths. In complex datasets such as emore_2000k and MS1M, there are thousands of clusters with a large number of local features/structures. Using a different kernel or improve the parameter tuning scheme will not solve the problem, or at least will not contribute to a significant improvement like our methods.\n\nThe proposed $\\rho_{KD}$ is not an estimate of the underlying pdf, but a finer version which overcomes abovementioned difficulties and is especially suitable for density-based clustering tasks. It resembles local contrast function, but the latter is too artificial and is mainly focus on magnifying the signal of small clusters.    \n\n**2. \"choosing hyperparameters in the proposed methods is much easier\": This needs to be justified if true.**\n\nThank you for pointing this out. In our sensitivity analysis (Figure 3), the clustering results are very stable on a wide range of values in terms of hyperparameters. These parameters in standard KDE-related methods are known to be very sensitive. Since we are using a diffusion density but not KDE, we do not have the problem of choosing optimal bandwidth to balance the bias-variance trade-off in estimation. We agree a rigorous theoretical justification is helpful to understand this phenomenon better, but it is beyond the scope of this paper.\n\nAs in our answers to your point 1, and to your last question in the other response, we want to emphasis that our promising numerical results are not due to a finer parameter-tuning. In the face image clustering, we simply fixed the hyper-parameters at reasonable values without worrying about tuning them.  \n\n**3. Spectral clustering vs proposed diffusion method**\n\nWe agree with the reviewer that spectral clustering and our approach are  relevant in the sense that both running a random walk on the graph for considerably periods before exiting. However, in the proposed approach we do not seek to find subsets during the diffusion process, but to reveal the local features/structures in the density gradualy as the process moving forward. We added the performance of spectral clustering  on the benchmark datasets as a comparison in the Appendix, Table 6. The proposed approach uniformly outforms spectral clustering. \n\n\n\n**4. \"The object D always denotes the dataset\": I was under the impression that the proposed method seeks to define a Markov process over the entire space and cluster based on the evaluation of the stationary distribution at the sample points themselves. Although practically these are essentially equivalent, is it in fact the case that the Markov process is defined only on the data set? If I am wrong, then should not all instances of \"density\" be replaced with \"mass\" when speaking about the limiting distribution used for clustering?**\n\nThe Markov process is on the data set $D$. In relevant literature, $\\rho(x,t)$ is known as \"density\" of the graph diffusion process at time $t$, see [2]. Moreover, although KDE is an estimator over the entire space,  as you said, clustering is only based on its value at $n$ data points. The proposed $\\rho_{KD}$ serves the same purpose as KDE (restrict to $D$) in density-based clustering.  Therefore, to avoid confusions we keep the term \"density function\" in this paper.   \n\n**5. In relation to Theorem 2, it may be that sure convergence rather than almost sure convergence is achieved.**\n\nYes it is sure/pointwise convergence. As this concept is not used very often in convergence of random variables and has no actual payoff compared to almost sure convergence, we feel the latter is preferable.\n\n[1]  *Variable kernel density estimation.* Terrell, D. G. and Scott, D. W.   Annals of Statistics, 1992. \n\n\n[2]  *Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators.*  Nadle, B., et al., NIPS, 2005. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jqVDo6JRfDM",
                "writer": "author",
                "reply_to": "fKNdh_hSOW",
                "title": "We look forward to your further feedback!",
                "comment": " Dear Reviewer SSTf,\n\nThanks again for your constructive suggestions and comments. As the deadline of discussion is approaching, we would like to know if there is any additional clarification or explanation that you may need, and we will be happy to provide them accordingly.\n\nIn our previous response, we have studied your comments and addressed them carefully. We summarized the main points below: \n1. We provided  explanation on the possibility of theoretical justifications for density-based clustering.  \n2. We conducted the experiments on benchmark datasets, using NMI as you suggested \n3. We provided additional explanation on the hyperparameter tuning issues. You may also want to have a look at our discussoins with Reviewer Nbci regarding similar issues.\n\nPlease do not hesitate to let us know if there is additional response we can offer.\n\nThank you for your time and effort spent in reviewing our paper!\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "722_TQXHAA2",
                "writer": "author",
                "reply_to": "QS-hgKn1ooV",
                "title": "We look forward to your further feedback!",
                "comment": " Dear Reviewer BUbF,\n\nThanks again for your constructive suggestions and comments. As the deadline of discussion is approaching, we would like to know if there is any additional clarification or explanations that you may need, and we will be happy to provide them accordingly.\n\nIn our previous response, we have studied your comments and addressed them carefully. We summarized the main points below: \n1. We provided  explanation on the differences between the proposed method and spectral clusting.  \n2. We added a numerical study on comparison to spectral clustering, based on the bench datasets.\n3. We provided explanation on the hyperparameter issues. You may also want to have a look at our discussoins with Reviewer Nbci regarding similar issues.\n\nPlease do not hesitate to let us know if there is additional response we can offer.\n\nThank you for your time and effort spent in reviewing our paper!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kofXn8G-54U",
                "writer": "author",
                "reply_to": "wOpXzF8UOo",
                "title": "We look forward to your further feedback!",
                "comment": " Dear Reviewer apj8,\n\nThanks again for your very helpful suggestions and comments. As the deadline of discussion is approaching, we would like to know if there is any additional clarification or explanation that you may need, and we will be happy to provide them accordingly.\n\nIn our previous response, we have studied your comments and addressed them carefully. We summarized the main points below: \n1. We provided explanation on the graph diffusion process, and the associated terminologies and notations. \n2. We rephrased the intuition of the diffusion density in the revised paper.\n3. We made it clear that the numerical studies are real data analysis and provided the computational cost of the proposed approach.\n\nPlease do not hesitate to let us know if there is additional response we can offer.\n\nThank you for your time and effort spent in reviewing our paper!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_nnZ_ZO3wLZ",
                "writer": "author",
                "reply_to": "378A3Ib4MRe",
                "title": "Thank you for the quick feedback, we hope the following points provide a satisfying answer to your concerns",
                "comment": " **It isn't clear to me exactly why this would be the case. Boundary bias in kde is associated, as far as I know, with discontinuities in the density which lead to over-smoothing at these discontinuities when using kde. This does not seem as though it would have substantial bearing on the clustering result. Furthermore, even if it does, I do not see any reason why the diffusion density overcomes this bias. Perhaps the authors can explain?**\n\nThe reviewer's understanding about boundary bias is correct. This over-smoothing will generally result in unreliable estimates at the discontinuities/boundaries, for example, peaks at the density boundaries, which might lead to spurious clusters.   We agree that a formal analysis would be very interesting. Actually the proposed diffusion density overcomes the boundary bias problem. If we know the domain of the data, say data is on a one-dimensional close interval $[0,1]$,  we just need to solve our density in Equation (4) with the initial conditions, and with the following Neumann boundary condition. This is similar to carrying out a boundary correction using the reflection method.\n$$\n\\dfrac{\\partial}{\\partial x}\\rho(x,t)\\bigg\\vert_{x=0}=\\dfrac{\\partial}{\\partial x}\\rho(x,t)\\bigg\\vert_{x=1}=0.\n$$\n We refer to [1] for a similar argument.\n\n\n\n**I agree that the three \"true\" modes are visible, but, as the authors note, we do not know there are only three \"true\" clusters, and so the presence of additional modes may be problematic. Furthermore, the plots in Figure 1 for KDE are clearly dependent on the bandwidth. Is it not the case that there is a bandwidth for which the three \"true\" modes are picked up? Since the authors do not mind about additional modes, the same concession should be made for the KDE, where a smaller bandwidth which, presumably, will pick up on these modes, will also lead to additional modes from the sparse cluster just as the diffusion density has.**\n\n\nIn Figure 1, it is the local features (a smaller and another less concentrated cluster next to a large and compact cluster) that makes classic density-based clustering methods problematic. Here, three modes are not significant for clusteirng purpose even in the true underlying density. The standard KDE is not suitable in this scenario not just because of the choice of bandwidth. To see this, we plot the naive density in 3D on a range of smaller bandwidths in Appendeix, Figure 5. \n\n\nJust like our answer to your point 1 in the reponse to reponse (major comments), standard KDE with variable (data adaptive) kernel bandwidths will recover the true density better, but is not as helpful as our diffusion density in terms of clustering. \n\n\n\n\n[1] *Kernel Density Estimation Visa Diffusion.* Botev, Z., et al. Annals of Statistics, 2010.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "M2Bgk9Vrzn6",
                "writer": "official_reviewer",
                "reply_to": "8qIYydY6Sb-",
                "title": "Resonse to response",
                "comment": " 1. It seems the authors may have misunderstood my first point. I fully understand what has been done with THEIR method, I was curious about whether or not a standard KDE with the locally adaptive kernels and DBSCAN or DPC was done. If not, then it isn't clear if any improvements in performance are due to the different density estimator, or simply the different kernel being used.\n2. \"choosing hyperparameters in the proposed methods is much easier\": This needs to be justified if true.\n3. Spectral clustering vs proposed: I fully understand how spectral clustering works, and I believe I understand how the proposed method works. However, similarities between two clustering models depend not on the algorithms, but on the formulation of the models. As far as I can tell, both spectral clustering and the proposed approach seek to find subsets of the sample in which a random walk will spend considerably periods before exiting. Whether one uses eigenvectors of the transition matrix and the other the stationary distribution is rather irrelevant if the way clusters are defined is so similar.\n4. \"The object D always denotes the dataset\": I was under the impression that the proposed method seeks to define a Markov process over the entire space and cluster based on the evaluation of the stationary distribution at the sample points themselves. Although practically these are essentially equivalent, is it in fact the case that the Markov process is defined only on the data set? If I am wrong, then should not all instances of \"density\" be replaced with \"mass\" when speaking about the limiting distribution used for clustering?\n5. In relation to Theorem 2, it may be that sure convergence rather than almost sure convergence is achieved.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "378A3Ib4MRe",
                "writer": "official_reviewer",
                "reply_to": "x7Ci-EqN0Yd",
                "title": "Response to response",
                "comment": " \"Yes, boundary bias will lead to problems in density-based clustering, as the data near the finite endpoints of the support will be not well-clustered\":\n- It isn't clear to me exactly why this would be the case. Boundary bias in kde is associated, as far as I know, with discontinuities in the density which lead to over-smoothing at these discontinuities when using kde. This does not seem as though it would have substantial bearing on the clustering result. Furthermore, even if it does, I do not see any reason why the diffusion density overcomes this bias. Perhaps the authors can explain?\n\n\"Since we don't know the true number of clusters, some boundary points may be viewed as outliers for density-based clustering methods. In Figure 1, the density centers of the three main clusters are very clear. The small density modes are at the boundary of the distribution mixtures, which is reasonable and has little effect on the clustering results.\":\n- I agree that the three \"true\" modes are visible, but, as the authors note, we do not know there are only three \"true\" clusters, and so the presence of additional modes may be problematic. Furthermore, the plots in Figure 1 for KDE are clearly dependent on the bandwidth. Is it not the case that there is a bandwidth for which the three \"true\" modes are picked up? Since the authors do not mind about additional modes, the same concession should be made for the KDE, where a smaller bandwidth which, presumably, will pick up on these modes, will also lead to additional modes from the sparse cluster just as the diffusion density has.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "OpwL_LgDUgz",
                "writer": "author",
                "reply_to": "CRIVEpKD3cQ",
                "title": "Response to Review  apj8 (other comments)",
                "comment": " **There needs to be a more concrete description/explanation of the density-based clustering algorithms, i.e. DBSCAN and DPC, using the notations from this paper.**\n\nThank you for your suggestion. In the revised appendix, we added descriptions of the DBSCAN and DPC algorithms through pseudo code . \n\n**Section 4.3 is a little hard to follow. For example on page 6, \"This shows that $\\rho_{FKD}$ elevates the density of small clusters\": what does this sentence mean, and how does it follow from Theorem 1? Are there any divide-by-zero problems that could happen with $\\rho$?**\n\nIn Theorem 1 we showed that \"the average $\\rho_{FKD}$ in each cluster are the same regardless of cluster sizes and other local features\". This means $\\rho_{FKD}$ has the effect of magnifying density values in small clusters, similar to the local contrasting. Thus when applying $\\rho_{FKD}$ to density-based clustering, it becomes easier to identify small clusters.\n\n\nThere is no divide-by-zero problem. The normalized graph Laplacian construction is well-defined.\n\n\n\n\n\n\n\n**Sections 5.1 and 5.2 are missing sufficient reproducibility information. How many times were the experiments conducted? Should there be error bars? What is the \"suitable range in the parameter space\"? These are information that should have been included in the appendix if page limit was the issue.**\n\nAll the numerical studies in Sections 5.1 and 5.2 are clearly conducted on real-world datasets. They are not Monte Carlo simulations. As a result, we run each experiment once and there are no error bars to show. This is quite standard in most unsupervised learning works, where real-world data with complex features are much more challenging than synthesized data. We do not quite understand why the reviewer had concerns about reproducibility.\n\n\n\nWe have now included the following information in the Appendix regarding the choice of parameters: The parameter $\\varepsilon$ (radius of the ball, used in $\\rho_{\\text{naive}}$, $\\rho_{\\text{LC}}$, $\\rho_{\\text{KD}}^{\\text{sym}}$ and $\\rho_{\\text{FKD}}^{\\text{sym}}$) is tuned by searching within the range between 0.1 and 1 with am increment of 0.1, parameter $k$ (number of nearest neighbors, used in $\\rho_{\\text{LC}}$, $\\rho_{\\text{KD}}^{\\text{asym}}$ and $\\rho_{\\text{FKD}}^{\\text{asym}}$) is tuned by searching within the range between $0.1n$ and $0.5n$, with an increment of $0.1n$, where $n$ is the sample size.\n\n\n**In Sections 5.4, how does the computational cost of $\\rho_{FKD}$ compare to the other density functions such as $\\rho_{naive}$ and $\\rho_{LC}$? Is there a computation-accuracy tradeoff?**\n\n$\\rho_{FKD}$ has the same linear computational cost just as linear KDE methods such as $\\rho_{naive}$ and $\\rho_{LC}$. In small sample datasets, it is possible that $\\rho_{FKD}$ and $\\rho_{KD}$ have different performances, but there is no strict computation-accuracy tradeoff phenomenon.\n\n\n**Smaller problems that can be fixed with editing...**\n\nThank you very much for pointing out these smaller problems. We have revised them accordingly.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "OP3sO7amzPQ",
                "writer": "author",
                "reply_to": "-hZF1thsrif",
                "title": "Response to Reviewer BUbF (Regarding Theorem 1 and Numerical Experiments)",
                "comment": " ## Theorem 1\nThe na\u00efve density is defined as:\n$$\\rho_{\\text{naive}}(x) = \\dfrac{1}{n \\varepsilon^d}\\sum_{y\\in D} \\dfrac{\\textbf{1}_{B(x, \\varepsilon) }(y)}{V_d},$$\nwhich is proportional to the number of data points covered by the $\\varepsilon$-neighborhood around $x$. Since $\\varepsilon$ is identical for every data point, the na\u00efve density in large and compact clusters will become much larger than that in small and spread clusters, which makes it hard to discover small cluster centers. See our Figure 1 as a simple illustration. Theorem 1 shows that no matter how the points are distributed, the proposed densities of different points are now always comparable.\n\n## Experimental evaluation\n**About the benchmark datasets**\n\nWe agree that it is good to also evaluate the numerical performance on synthetic data. However, only using real-world data with complex features is quite standard in unsupervised learning papers. \n\nThe benchmark datasets were also used in the paper of local contrast density [5] and many other relevant works. We consider it is better to apply our method to the same collection of the datasets.\n\n**About the tuning parameters**\n\nJust like k-means and spectral clustering that requires the  number of clusters as input, there are tuning parameters in density-based clustering methods such as DBSCAN and DPC. Tuning these hyperparameters properly is a necessary step to evaluate the clustering performance of all these methods. \n\nIn $\\rho_{na\u00efve}$, $\\rho_{LC}$, $\\rho_{KD}$ and $\\rho_{FKD}$, all hyperparameters were tuned in the same manner, just as in the  paper of local contrast density [5], which is one of our main competitors. Furthermore, in face image clustering, deep learning based methods such as CDP, L-GCN, LTC and GCN(V+E) are all tuned based on supervised measures. This is a common practice in existing works and we carried out experiments in the same way for a fair comparison.\n\n\nAdditionally, as our methods do not rely on estimating the density via KDE, where bandwidth selection is very sensitive and  crucial. Sensitivity analysis is presented in Figure 3, where We can see the hyperparameters used in $\\rho_{KD}$ and $\\rho_{FKD}$ are less sensitive compared to other methods. And in this way, the selection of hyperparameters in our methods is much easier than other density-based methods.\n\n\n\n\n\n[5] Bo Chen, Kai Ming Ting, Takashi Washio, and Ye Zhu. Local contrast as an effective means to robust clustering against varying densities.Machine Learning, 107(8):1621\u20131645, 2018",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QS-hgKn1ooV",
                "writer": "author",
                "reply_to": "-hZF1thsrif",
                "title": "Response to Reviewer BUbF (Regarding the connection to spectral clustering)",
                "comment": " The procedure of a density-based clustering algorithm is to first generate/estimate a density function and calculate its value at each point, then find the data points that can be viewed as cluster centers or core sets, and finally assign remaining points to the clusters by connecting with the higher density points nearby. It seems there is a serious misunderstanding of this procedure and the associated terminologies that have been discussed throughout the paper.\nWe hope you would re-evaluate your score as all your main concerns are regarding this point. We will address your questions below in detail.\n\n**Shouldn't spectral clustering with the random walk Laplacian return a clustering which maximizes the stationary distribution function within each cluster as well?**\n\nSpectral clustering with random walk Laplacian first computes the first $k$ eigenvectors, then clusters the eigenvectors by k-means into $k$ groups. Our approach derives the limit distribution of the graph diffusion as the density. It shares the similar spitir to the probability density function of the underlying data generating mechanism. What we did is to apply this density to the density-based clustering methods. \n\n**The relation between spectral clustering and DBSCAN has been established [2,3]**\n\nThe established relationship between spectral clustering and DBSCAN in [2,3] is based on the reachability between data points. As exhibited in [3], the reachability of OPTICS is defined as\n$$\\text{reachability}(x_{i}, x_{j}):=\\max\\{\\text{dist}(x_{i},x_{j}), \\text{minPts-dist}(x_{i})\\}.$$\n\n\nOur approach aims to provide a better alternative to the underlying \"density function\" of DBSCAN, DPC, and any other density-based clustering methods. As we can see there is no obvious connection to the spectral clustering methods. \n\n**Likewise, spectral clustering itself has an interpretation as a density-based clustering method[4]**\n\nAs exhibited in [4], spectral clustering can be viewed as maximizing the average cluster density. For a cluster/subgraph $S$, this cluster density can be defined as\n$$\\delta(S, W)=\\frac{\\sum_{i,j\\in S}W_{ij}}{|S|},$$\nwhere $W$ is the adjacency matrix. The cluster density can be viewed as the average intra-connection. \n\n\nHowever, the term \"density\" used in density-based clustering is the probability density function of the underlying data distribution. This is quite different from the cluster density defined in [4]. We believe this is the major point that caused the reviewer's misunderstanding.\n\n[1] Von Luxburg, Ulrike. \"A tutorial on spectral clustering.\" Statistics and computing 17.4 (2007): 395-416.\n\n[2] Y. Chen, \"DBSCAN Is Semi-Spectral Clustering,\" 2020 6th International Conference on Big Data and Information Analytics (BigDIA), 2020, pp. 257-264, doi: 10.1109/BigDIA51454.2020.00048.\n\n[3] Schubert, Erich, Sibylle Hess, and Katharina Morik. \"The relationship of DBSCAN to matrix factorization and spectral clustering.\" LWDA. 2018.\n\n[4] Hess, Sibylle, et al. \"The SpectACl of nonconvex clustering: a spectral approach to density-based clustering.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. No. 01. 2019.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "x7Ci-EqN0Yd",
                "writer": "author",
                "reply_to": "PiaGw1aj-MM",
                "title": "Response to Reviewer Nbci (Minor Comments)",
                "comment": " **Why is a standard kernel density estimate referred to as na\u00efve? Perhaps standard is more appropriate?**\n The term \"na\u00efve\" referred to using the simplest $\\epsilon$-ball in the kernel density estimate. This is to distinguish from standard KDE using Gaussian or other common kernels.\n\n **The sentence \"Such density suffers from capturing local features in complex datasets.\" in the abstract seems to be incorrect. Should this not read \"Such a density suffers from an inability to capture local features in complex datasets.\", or similar?**\n \nThanks for noting this. We have corrected it accordingly in this version.\n\n**You mention the issue of boundary bias in kernel density estimation. Does this have any relevance in density clustering?**\n\nYes, boundary bias will lead to problems in density-based clustering, as the data near the finite endpoints of the support will be not well-clustered. \n\n**In Figure 1 I would argue that it isn't clear the diffusion density shows three clusters \"clearly\". Arguably the most sparse of the clusters actually manifests as multiple modes and that over-clustering might result.**\n\nSince we don't know the true number of clusters, some boundary points may be viewed as outliers for density-based clustering methods. In Figure 1, the density centers of the three main clusters are very clear. The small density modes are at the boundary of the distribution mixtures, which is reasonable and has little effect on the clustering results.\n\n**The operator T_F seems to take x as an argument, so why is this suppressed in the definition?**\n\nIn the revised version, we add the argument of $x$ in the operator $T_F$.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8qIYydY6Sb-",
                "writer": "author",
                "reply_to": "PiaGw1aj-MM",
                "title": "Response to Reviewer Nbci (Major Comments)",
                "comment": " Thanks for your feedback. We hope the following point-by-point response addresses your concerns.\n\n**It isn't clear to me if the locally adaptive kernels were only used within the proposed approach, or also within DBSCAN and DPC.**\n\nSimilar to linear KDE and local contrasting, the proposed kernel-diffusion approach focuses on developing a new density function, which shares the same spirit as the probability density function of the underlying data generating mechanism. The density can be applied to any density-based clustering methods, such as DBSCAN and DPC. The locally adaptive kernels were used to generate our kernel diffusion density. It can be seen as a pre-step of DBSCAN and DPC.\n\n**Without a data-driven means for selecting the tuning parameters for the model, it is not clear whether the results presented are realizable in practice.**\n\nIn all the experiments, each hyperparameter in $\\rho_{na\u00efve}$, $\\rho_{LC}$, $\\rho_{KD}$ and $\\rho_{FKD}$ was tuned in the same manner, just as in the paper of local contrast density [1], which is one of our main competitors. Furthermore, in face image clustering, deep learning-based methods such as CDP, L-GCN, LTC, and GCN(V+E) all include tuning parameters based on supervised measures. This is common in almost all the existing works and we carry out the same setting for fair numerical comparison. Additionally, as our methods do not rely on estimating the density via KDE, where bandwidth selection is very sensitive and crucial, choosing hyperparameters in the proposed methods is much easier compared to classic density-based clustering. We agree that in real-world applications, we have to select the tuning parameters in a data-driven manner.\n\n\n\n\n**The method is arguably more similar to normalized spectral clustering than to density clustering. Including results from spectral clustering seems an important change to make. In particular, if you use the same locally adaptive kernels within spectral clustering, does your method outperform?**\n\nWe respectfully disagree with the reviewer regarding this point. Although both the proposed approach and spectral clustering require the construction of random walk Laplacian, their motivations and associated methodologies are completely different.\n\nSpectral clustering with random walk Laplacian first computes the first $k$ eigenvectors, then clusters the eigenvectors by k-means into $k$ groups. Our approach derives the limit distribution of the graph diffusion as the density. It serves a similar role to the probability density function of the underlying data generating mechanism. What we did is to apply it to the density-based clustering methods. \n\nWe do not see a strong connection between these two methods. From our perspective, there is no reason to use the proposed kernels in Equations (4) and (5) in spectral clustering, and we can not see why improvement is expected, as the local adaptivity of the proposed kernels is coming from applying them to the diffusion process. \n\n\n\n**The object $D$ is unclear. At first, this is the data set, and then later becomes the entire input space, as far as I can tell.**\n\nThe object $D$ always denotes the dataset and never becomes the entire input space. Note that in the paper, if an integral is on $D$, it is always with respect to an empirical (discrete) measure $F_n(\\cdot)$.\n\n**In the theoretical discussion, the objects $\\rho_{KD}$ and $\\rho_{FKD}$ are treated as constant, yet these are random variables. Is the convergence described in Theorem 2 sure convergence? As far as I can tell from the proof, it is, but the proof is not very clearly presented.**\n\n**The way Theorem 2 is stated, seems to describe pointwise convergence rather than uniform convergence, yet uniform convergence is used in the text. Can you clarify?**\n\nThank you for your suggestions. Yes, in Theorem 2 it is almost sure convergence. We have revised the theorem and its proof to make this clear. We also replaced the word \"uniformly approximation\" in the revised version to avoid any confusion.\n\n\n\n**In Assumption 1 the value of $c$ needs to be independent of $n$**\n\nYes, the value of $c$ is independent of $n$. We emphasize this in the revised version. \n\n**You state that $\\rho_{FKD}$ can be determined in linear time. However, doesn't construction of $P$ require at least log-linear time (or even worse?)**\n\nThe construction of $P$ is needed in both $\\rho_{\\text{KD}}$ and $\\rho_{\\text{FKD}}$. This construction can be efficiently established [2], i.e. only takes a few seconds for more than a million data points. The more time-consuming step in practice is the computation of $\\rho$. As a result, we always refer to the running time after the construction of $P$.\n\n[1] *Local contrast as an effective means to robust clustering against varying densities.Machine Learning*.  Chen, B., etc.  Machine Learning, 2018\n[2] Johnson, Jeff, Matthijs Douze, and Herv\u00e9 J\u00e9gou. *\"Billion-scale similarity search with gpus.\"* IEEE Transactions on Big Data (2019).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wOpXzF8UOo",
                "writer": "author",
                "reply_to": "CRIVEpKD3cQ",
                "title": "Response to Reviewer apj8 (on Section 4)",
                "comment": " We understand the reviewer's concerns and hope the following points provide a satisfying response.\n\n1. **I don't think random walk is the correct term here, as random walks can be paths of any length.**\n2. **...a graph is a set of nodes and edges. Right now, $D$ is only a set of points/samples. What are the edges?** \n3. **If $L$ is the normalized graph Laplacian, is it a $n\\times n$ matrix? If so, what is $L\\rho(x,t)$ in equation (4) with the output of $\\rho$ being a scalar?** \n4. **Aren't equations (4) and (5) first-order differential equations?**\n5. **The approach should be better motivated, i.e. include the answer to what is the point of all this discussion about diffusion, in the context of clustering? The paragraph on the top of page 5 is probably the most important part of Section 4, but it's not easy to understand. Can the authors explain the second half of that paragraph in more detail, perhaps with a figure or a toy example?**\n\nWe respectfully disagree with the reviewer regarding the above concerns. The construction of random walk/diffusion on graphs is quite standard in all areas of spectral graph theory. The exact same set of terms such as \"random walk\", \"second-order differential equation\" have been widely used in many popular existing works on different applications, to name a few, see [1,2,3]. We include as many explanations as any of these works. It is slightly unexpected for us to receive strong criticism with a low mark in terms of correctness.  We believe the presentation in Section 4 is mathematically rigorous and concise. \n\nTo answer the reviewer's questions in more detail: \n\n1. A random walk on a graph is a process that begins at some vertex, and at each time step moves to another vertex, e.g., the vertex that the random walk moves to is chosen with probability proportional to the weight of the corresponding edge among the neighbors of the present vertex. If we consider finer time steps in which smaller fractions of the probability leave the vertices. In the limit, this results in continuous random walks that can be modeled by the matrix exponential, as described in the paragraph before Equation (4). If we understand your question correctly, the \"path of any length\" in the classic random walk theory is just equivalent to \"moves to any vertex on the graph\" here.  This is an elegant analog of classic random walk theory in Euclidean space, where we can develop counterparts for concepts such as lazy walker, filtration, Markov property, and local limit theorems, etc. We refer to Section 10 in [4] for a basic-level introduction. This analog was adopted in numerous existing works. We do not see any problem with using the term \"random walk\" here.\n\n2. Note that the transition probability matrix $P$ induces a weighted graph with vertices in $D$. Right after \"view $D$ as a graph\", we also mentioned that \"(view) $L = I - P$ as the normalized graph Laplacian\". \n\n3. Yes, the graph Laplacian operator $L$ is a $n\\times n$ matrix. As described in the paper, $\\rho(x,t) \\in D\\times \\mathbb{R}^+$ is the associated probability density, where the first argument takes different values in $\\{x_1, x_2,\\dots, x_n\\}$. As a result, if $x=x_i$, $L\\rho(x,t)$ is just the inner product of the $i$-th row in $L$ and the vector of $(\\rho(x_1,t), \\dots,\\rho(x_n,t))^T$. This is a  standard notation in the literature, for example, see [3].\n\n4. Equations (4) and (5) are second-order differential equations, as graph Laplacian corresponds to the second-order derivative. Note that we were not referring to the partial derivatives of $\\rho(x,t)$ with respect to $t$ on the LHS of Equations (4) and (5). Please see [3, 5] for similar usage of terms.\n\n5.  Above explanations might be helpful for the reviewer to understand better the discussion of the diffusion process on a graph. This is the core of our methodology. Using the limiting distribution of this diffusion, we developed a completely new framework of density with desired properties such as local adaptivity. This density can be applied to density-based clustering algorithms and improve their performance by a large margin. We have rephrased the paragraph on the top of page 5. \n\nWe hope that this reply convinces you that our Section 4 is accurate and hope you would re-evaluate your score as these graph diffusion terminologies and notations seem to be your main technical concern. \n\n1. *Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps.* Coifman, R. R., etc. PNAS, 2005.\n2. *Graph Laplacians and their Convergence on Random Neighborhood Graphs.* Hein, M., etc. JMLR, 2007. \n3. *Towards a theoretical foundation for Laplacian-based manifold methods.* Belkina, M. and Niyogib, P. Journal of Computer and System Sciences, 8(74), 1289-1308, 2008.\n4. *Spectral and Algebraic Graph Theory.* Spielman, D. A. 2019.\n5.  *Diffusion maps, spectral clustering and eigenfunctions of fokker-planck operators.*  Nadle, B., etc. NIPS, 2005.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fKNdh_hSOW",
                "writer": "author",
                "reply_to": "zxf31K8zoYP",
                "title": "Response to Reviewer SSTf",
                "comment": " We understand the reviewer's concerns, and hope the following answers provide a satisfying response.\n\n**There is no theoretical justification that the proposed method is able to capture the local characteristics of the dataset**\n\nAlthough we are very keen to establish rigorous theoretical results to support the proposed method in terms of clustering, this is not a quite feasible task. Note that the theoretical consistency theory for DBSCAN is developed in [1] and follow-up works, on a simplified version of the naive-DBSCAN. For naive-DPC, even it has been widely used in enormous applications, analysis of its theoretical properties is still not available to the best of our knowledge.  Fairly speaking, we believe this requirement is far beyond the scope of this paper intended for ICLR.\n\n\nThe purpose of Theorem 1 is actually to provide a simple justification. For the na\u00efve approach, density is calculated as the number of data points covered in the $\\varepsilon$-nerghborhood. Since the $\\varepsilon$ is the same for all data points, the density of points in large and compact clusters will be much larger than that of points in small and spread clusters, which makes it difficult to discover these small and loose clusters. In the special scenario introduced in Theorem 1, no matter the varying distribution of different clusters, the average densities of all clusters are always comparable using our kernel diffusion approach. This lead to better clustering results as we can now easily identify small clusters.  \n\n**Are there cases where $\\rho_{\\text{FKD}}$ does not perform well? What is the complexity of computing $\\rho_{\\text{KD}}$?**\n\nAs shown in Theorem 2, $\\rho_{\\text{FKD}}$ and $\\rho_{\\text{KD}}$ attain the same accuracy when the sample size $n$ is sufficient large. In large datasets such as face image clustering data,  $\\rho_{\\text{FKD}}$ has comparable performance as $\\rho_{\\text{KD}}$ . The difference of $\\rho_{\\text{FKD}}$ and $\\rho_{\\text{FKD}}$ becomes more significant in small datasets. The complexity of computing $\\rho_{\\text{KD}}$ is $O(n^2)$.\n\n\n**The method includes additional hyperparameters compared to the na\u00efve approach. In the presented experimental results (Table 1) the value of hyperparameter $h=0.5$. Therefore the empirical conclusions are conditioned on this value.**\n\nThe additional hyperparameter is not a particular problem, as our methods do not rely on estimating the density via KDE, where bandwidth selection is very sensitive and crucial. For example, see the sensitivity analysis of $h$ in Figure 3, the clustering results are very stable for a wide range of $h$.\n\n**Clustering is an unsupervised problem. It is not acceptable to tune the hyperparameters based on supervised measures**\n\nJust like k-means and spectral clustering that requires the number of clusters as input, there are tuning parameters in density-based clustering methods such as DBSCAN and DPC. Tuning these hyperparameters properly is a necessary step to evaluate the clustering performance of all these methods. \n\nIn $\\rho_{na\u00efve}$, $\\rho_{LC}$, $\\rho_{KD}$ and $\\rho_{FKD}$, all hyperparameters were tuned in the same manner, just as in the paper of local contrast density [2], which is one of our main competitors. Furthermore, in face image clustering, deep learning-based methods such as CDP, L-GCN, LTC, and GCN(V+E) are all tuned based on supervised measures. This is a common practice in existing works and we carried out experiments in the same way for a fair comparison.\n\nAdditionally,  sensitivity analysis is presented in Figure 3, where We can see the hyperparameters used in $\\rho_{KD}$ and $\\rho_{FKD}$ are less sensitive compared to other methods. And in this way, the selection of hyperparameters in our method is much easier than other density-based methods.\n\n**I suggest that the authors should provide NMI values for the datasets of Table 1. This is the typical measure. For the more complex face datasets, the use of F-measures is acceptable. Moreover, for the datasets of Table 1, it is suggested to present results using typical clustering methods (eg. k-means) as happens with the face datasets**\n\nThanks for your suggestion. We now provide NMI values in the appendix. \n\n\n**What is the dimensionality of the face datasets? As a general remark, density-based methods are not considered effective in the case of small datasets of relatively high dimensionality**\n\nThe dimensionality of face datasets is 256. Since the sample sizes of face image datasets are usually large, density-based clustering methods are suitable.\n\n\n[1] *Consistency and rates for clustering with dbscan.\" Artificial Intelligence and Statistics*. Sriperumbudur, B., & Steinwart, I, PMLR, 2012.\n\n\n[2] *Local contrast as an effective means to robust clustering against varying densities.Machine Learning*.  Chen, B., etc.  Machine Learning, 2018",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CRIVEpKD3cQ",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_-geBFMKGlkq",
                "title": "",
                "comment": "This paper proposes a different density function for popular density-based clustering algorithms, i.e. truncated symmetric and asymmetric Gaussian kernels for DBSCAN and DPC. It offers some diffusion-based argument for why that is a better density function for clustering than naive function, and provides a computationally more efficient surrogate function as well. \nPerformance of proposed method on many clustering experiments are provided. The suggested kernels are easy to understand, and many experimental results are provided, including sensitivity analysis to hyperparameters and computational analysis. \n\nHowever, the authors need to more clearly explain and motivate their approach.\n\nFirst of all, Section 4.1 needs to be re-written much more carefully and accurately. For example:\n-  \"$p(x,y)$ can be viewed as a probability for a random walk on the dataset from point x to point y\": I don't think random walk is the correct term here, as random walks can be paths of any lengths.\n-  \"view D as a graph\": a graph is a set of nodes and edges. Right now, D is only a set of points/samples. What are the edges?\n- If L is the normalized graph Laplacian, is it a $n \\times n$ matrix? If so, what is $L \\rho(x, t)$ in equation (4) with the output of $\\rho$ being a scalar?\n- Aren't equations (4) and (5) *first*-order differential equations?\n- The approach should be better motivated, i.e. include the answer to what is the point of all this discussion about diffusion, in the context of clustering? The paragraph on the top of page 5 is probably the most important part of Section 4, but it's not easy to understand. Can the authors explain the second half of that paragraph in more detail, perhaps with a figure or a toy example? \n\nClarifications for other parts:\n- There needs to be a more concrete description/explanation of the density-based clustering algorithms, i.e. DBSCAN and DPC, using the notations from this paper. The introduction, related work or preliminaries would be a good place.\n- Section 4.3 is a little hard to follow. For example in page 6, \"This shows that $\\rho_{FKD}$ elevates the density of small clusters\": what does this sentence mean, and how does it follow from Theorem 1? Are there any divide-by-zero problems that could happen with $\\rho$?\n\nFor the experiments:\n- Sections 5.1 and 5.2 are missing sufficient reproducibility information. How many times were the experiments conducted? Should there be error bars? What are the \"suitable range in the parameter space\"? These are information that should have been included in the appendix if page limit was the issue.\n- In Sections 5.4, how doest the computational cost of $\\rho_{FKD}$ compare to the other density functions such as $\\rho_{naive}, \\rho_{LC}$? Is there a computation-accuracy tradeoff?\n\nSmaller problems that can be fixed with editing:\n- The sentence \"Letting $h \\to 0$, the random walk...\" doesn't make sense. It may have too many verbs?\n- In page 5, \"some intuitions that why...\"\n- In page 5, \"process grouping of all the data...\"\n- $\\hat{g}$ and $\\hat{\\rho}$ in the appendix are not defined.\n This paper has promising experimental results, but is not ready for publication at this stage. It could become a stronger paper after major revision by motivating and explaning their approach more carefully.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "PiaGw1aj-MM",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_-geBFMKGlkq",
                "title": "",
                "comment": "The paper introduces a new approach to performing clustering, based on constructing a Markov process on the input space, in which transitions are determined according to normalised kernel similarity scores, as in Spectral Clustering (SC). Unlike SC, however, the proposal utilises the stationary distribution of the process, or an approximation thereof, evaluated on the data set, as a density function to be used within existing density based clustering algorithms, such as DBSCAN and Density Peaks Clustering (DPC). Experimental results are given to illustrate the methods potential improvement of standard kernel density estimates, and also its potential improvement over purpose-built models for face clustering. The proposed method is interesting, and the presentation in the paper is reasonably clear, with some exceptions which I will note below. The empirical results certainly exhibit that potential for strong performance may be there, but I have reservations...\n\nMy primary concerns relate to the experimental set up and some uncertainties relating to the presentation, and also the clarity in relation to some of the technical points.\n\nExperiments: -It isn't clear to me if the locally adaptive kernels were only used within the proposed approach, or also within DBSCAN and DPC. If only in the proposed approach, then it is not clear if the improvements in performance are actually a result of the new method, or simply down to the choice of kernel. I appreciate that the asymmetric kernel may not be usable in these frameworks, but simple adjustments can be used to symmetrise if necessary. \n -Without a data-driven means for selecting the tuning parameters for the model, it is not clear whether the results presented are realisable in practice. While \"best case\" plus \"sensitivity study\" do provide some evidence of practical relevance, it still isn't entirely clear how well we can expect the method to perform without access to an oracle tuner.\n -In relation to the face clustering problem, were the purpose-driven methods also given access to oracle tuning, or were they tuned using the data? If the latter, then these performance comparisons are not indicative of the actual comparative performance.\n- The method is arguably more similar to normalised spectral clustering than to density clustering. Including results from spectral clustering seems an important change to make. In particular, if you use the same locally adaptive kernels within spectral clustering, does your method outperform?\n\nClarity: - The object D is unclear. At first this is the data set, and then later becomes the entire input space, as far as I can tell. This is especially important when included in technical statements like Assumption 1 and Theorem 2, where the interpretation might differ considerably.\n- In the theoretical discussion, the objects \\rho_{KD} and \\rho_{FKD} are treated as constant, yet these are random variables. Is the convergence described in Theorem 2 sure convergence? As far as I can tell from the proof, it is, but the proof is not very clearly presented.\n- The way Theorem 2 is stated, it seems to describe pointwise convergence rather than uniform convergence, yet uniform convergence is used in the text. Can you clarify?\n- In Assumption 1 the value of c needs to be independent of n, presumably, otherwise a sequence tending to one as n tends to infinity might present a problem.\n- You state that \\rho_{FKD} can be determined in linear time. However, doesn't construction of P require at least log-linear time (or even worse?)\n\nIf these concerns can be suitably addressed, I would be willing to adjust my score upwards.\n\n* Note that my score on \"correctness\" below is motivated by the fact that, without further evidence, the claims about practical performance are, in my opinion, not adequately supported. Otherwise the paper appears to be correct.\n\nSome minor comments/questions/corrections follow:\n- Why is a standard kernel density estimate referred to as ``naive''? Perhaps ``standard'' is more appropriate?\n- The sentence \"Such density suffers from capturing local features in complex datasets.\" in the abstract seems to be incorrect. Should this not read \"Such a density suffers from an inability to capture local features in complex datasets.\", or similar?\n- You mention the issue of boundary bias in kernel density estimation. Does this have any relevance in density clustering?\n- In Figure 1 I would argue that it isn't clear the diffusion density shows three clusters \"clearly\". Arguably the most sparse of the clusters actually manifests as multiple modes and that overclustering might result.\n- The operator T_F seems to take x as an argument, so why is this suppressed in the definition?\n\nFinally, although the grammar does not lead to difficulty in comprehension, I would recommend that the authors have their submission proof-read by a native English speaker as there are numerous minor grammatical imprecisions. An interesting idea with a fairly thorough discussion. However, some issues about clarity in relation to the technical parts of the paper.\n\nExperimental results illustrate some promise, but it is very unclear if the reported performance is practicable as they are based on oracle tuning of hyper-parameters.",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "zxf31K8zoYP",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_-geBFMKGlkq",
                "title": "",
                "comment": "The paper presents a methodology for defining the density reference function required in density-based clustering methods such as DBSCAN and DPC. The density function is obtained as the limiting probability density of a diffusion process and it takes into account the local characteristics of the dataset. A surrogate density is also proposed that is fast to compute. The method is compared to typical density functions used in DBSCAN and DPC.   The proposed idea is interesting and novel. However, there are several concerns to be addressed:\n-There is no theoretical justification that the proposed method is able to capture the local characteristics of the dataset.  \n-The fast surrogate (FKD) density is very simple to compute. It seems strange that it provides comparable performance to KD. Are there cases where FKD does not perform well?\n-What is the complexity of computing \\rho_{KD}?\n- A major drawback of the method is that it includes additional hyperparameters compared to the na\u00efve approach. In the presented experimental results (Table 1) the value of hyperparameter h=0.5. Therefore the empirical conclusions are conditioned on this value.\n- Clustering is an unsupervised problem. It is not acceptable to tune the hyperparameters based on supervised measures.\n- I suggest that the authors should provide NMI values for the datasets of Table 1. This is the typical measure. For the face datasets that are more complex, the use of F-measures is acceptable. Moreover, for the datasets of Table 1, it is suggested to present results using typical clustering methods (eg. k-means) as happens with the face datasets.\n-  It is important to provide information on the number of clusters returned by the density-based methods for all datasets. Do they recover the ground truth number of clusters?\n- What is the dimensionality of the face datasets?\n- As a general remark, density-based methods are not considered effective in the case of small datasets of relatively high dimensionality. \n The proposed idea is interesting and suggests an alternative way for specifying density functions for density-based clustering. However, there exist several unresolved issues regarding theoretical justification and, mainly, experimental validation.",
                "rating": 5,
                "confidence": 5
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "technical details",
                "Sentiment Expression": "are lacking",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "some of the claims",
                "Sentiment Expression": "are not fully supported",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the empirical results",
                "Sentiment Expression": "quite promising",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "some good ideas",
                "Sentiment Expression": "in this work",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "Ehhk6jyas6v": {
        "paper_id": "iclr_2022_Ehhk6jyas6v",
        "paper_title": "On The Quality Assurance Of Concept-Based Representations",
        "paper_abstract": "Recent work on Explainable AI has focused on concept-based explanations, where deep learning models are explained in terms of high-level units of information, referred to as concepts. In parallel, the field of disentanglement learning has explored the related notion of finding underlying factors of variation in the data that have interpretability properties. Despite their overlapping purpose, the metrics to evaluate the quality of concepts and factors of variation in the two fields are not aligned, hindering a systematic comparison. In this paper we consider factors of variation as concepts and thus unify the notations in concept and disentanglement learning. Next, we propose metrics for evaluating the quality of concept representations in both approaches, in the presence and in the absence of ground truth concept labels. Via our proposed metrics, we benchmark state-of-the-art methods from both families, and propose a set of guidelines to determine the impact that supervision may have on the quality of learnt concept representations.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper considers the question of whether recent concept-based learning algorithms, as well disentangled representation learning algorithms, result in high-quality representations. In particular, the authors consider what high-quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts. While the premise of this paper has been appreciated by the reviewers, some concerns about the details of the metrics proposed and experimental results which have been raised by the reviewers remain post rebuttal. Given this, we are unable to recommend the acceptance of the paper at this time. We hope the authors find the reviewer feedback useful.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "J8RDzsktD5J",
                "writer": "official_reviewer",
                "reply_to": "qF1QJn4ulE",
                "title": "Thanks for response",
                "comment": " Thanks to the authors for their response. A couple thoughts on what they've discussed:\n\n**About the choice of AUC.** If the main motivation for using AUC is to bound the metric in [0, 1], that's not a very good reason. The mutual information $I(X; Y)$ is bounded by $0 \\leq I(X; Y) \\leq H(Y)$, and if $Y$ is discrete then $H(Y)$ is easy to estimate. So you could achieve a similarly simple range of scores for mutual information rather than AUC. Not that AUC is a terrible choice, my complaint here was that it's a bit arbitrary and somewhat difficult to generalize to non-binary concepts.\n\n**About the takeaway from experiments.** Thanks for clarifying the more nuanced interpretation of the various methods' performance. Indeed, it's not as simple as concept supervision yielding good representations and everything else failing. A new takeaway seems to be that semi-supervised DGL may not outperform unsupervised DGL (in the sense of having good concept-based representations), which casts some doubt on this line of work.\n\n**Other clarifications.** Thanks for clarifying a couple other points. Describing how you chose $\\beta$ will be helpful for readers, although what you've described sounds a bit subjective. Adding the additional methods is definitely an improvement. The various limitations of your metrics is helpful to point out, though I mostly agree that there are relatively simple ways to generalize them to real-valued or multi-class concepts. \n\nOverall though, I remain only moderately excited about the execution and impact of this paper, so I'm leaving my score as is.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Wlfsql39mfg",
                "writer": "author",
                "reply_to": "5RjLl7ahkyZ",
                "title": "Replies to iQK1's Rebuttal Comments - Part 2/2",
                "comment": " ### Non-oracle Impurity \u201cMisleading\u201d\n\nAs mentioned, there is certainly a difficulty disentangling the possible causes of the discrepancy shown in Figure 2 between oracle and non-oracle impurity. For the sake of an example, however, consider our results shown in Figure 3 for CBM: we see that our impurity metric remains relatively stable as the number of dependencies changes in the data while the non-oracle impurity increases as the amount of dependencies is introduced. Nevertheless, a closer look at Figures 2, 9, and 10 shows that CBMs learn concept representations that have equally as good predictive power (almost 100%) both when $\\lambda = 0$ (i.e., no dependencies) and $\\lambda = 5$ (maximum number of dependencies) in 3dshapes$(\\lambda)$. In fact, when looking at how the predictive performance of CBM\u2019s representations change as we vary $\\lambda$, we see that they remain very stable, with both being almost 100% (see here:\u00a0https://i.imgur.com/ZlIaJ5B.png). This indicates that the cause of the increase in the non-oracle impurity is very likely linked to the incapability of the non-oracle impurity metric to correctly generalize to scenarios where concepts are correlated. A similar effect is observed in other models and datasets. To clarify this point, we will include a brief description highlighting the discrepancy between what we see in the non-oracle impurity in Figure 2 and the predictive performance stability we see in Figures 2, 9, and 10.\n\n\nMany thanks for going through our updated manuscript. Your comments have helped improve the quality of our work. We are happy to answer any additional questions or provide other details that would help support our paper\u2019s acceptance. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5RjLl7ahkyZ",
                "writer": "author",
                "reply_to": "a0ud9FzaHA",
                "title": "Replies to iQK1's Rebuttal Comments - Part 1/2",
                "comment": " We thank the reviewer for going over the new version of our manuscript. Below are our replies to the concerns raised in the comment above:\n\n### Invertibility of $g$\n\nThank you for further clarifying this concern. In the case of CCD, whose design and rationale we describe in that paragraph, we intended to say that if concept representations $g(\\phi(\\mathbf{x}))$ capture a complete set of statistics for the task in hand, then one should be able to recover all of the **task-specific crucial information** in $\\phi(\\mathbf{x})$ from $g(\\phi(\\mathbf{x}))$ using some function $\\psi(\\cdot)$. This would enable one to approximate $f(\\phi(\\mathbf{x}))$ as $f(\\psi(g(\\phi(\\mathbf{x}))))$. This same logic is outlined in [4] (from references below) page 3 above Definition 3.1. While learning $\\psi = g^{-1}$ would be ideal, we understand that $\\psi$ may instead need to recover only certain aspects of $\\phi(\\mathbf{x})$ for it to perform equally as well in the task (so there is no strict need to learn an inverse mapping). In fact, due to $g$\u2019s thresholding mechanism, it is not an invertible function by construction so $\\psi$ will very likely not be equal to $g$'s inverse. We now understand how our statement in that paragraph can lead to confusion regarding the invertibility of $g(\\cdot)$ in CCD, and will update that line accordingly.\n\n### Language Around Concepts\n\nFor the sake of simplicity, throughout our paper we refer to both concept representations and latent codes as \u201cconcepts\u201d. Based on your previous feedback, we updated our introduction to state that \u201cWe unify the language and notation across CL and DGL by framing factors of variation and latent codes in DGL as ground truth concepts and concept representations in CL, respectively.\u201d We then mathematically represent this use of language when describing our mathematical notation in the 2nd paragraph of our background section. To add further clarity, we will **reiterate** our use of \u201cconcepts\u201d for latent codes in the background section.\n\n### Alignment\n\nWe completely agree that an alignment is not a property of the representation but rather a semantical assignment to each axis of the representation. At its most formal, and as described in Appendix 6.4, an alignment $\\mathcal{A}:$ {$1, \\cdots, k$} $\\mapsto$ {$1, \\cdots, k^\\prime$} between a concept representation $\\mathbf{\\hat{c}}\u00a0\\in \\mathbb{R}^{d \\times k^\\prime}$ and a ground truth concept vector $\\mathbf{c} \\in \\mathbb{R}^{k}$ is an **injective** function such that $\\mathcal{A}(i) = j$ indicates that concept representation $\\mathbf{\\hat{c}}_{(:, j)}$ is *semantically* aligned with ground truth concept $c_i$. In other words, $\\mathbf{\\hat{c}}$ encodes information that is specific to ground truth concept\u00a0 $c_i$. Notice that when we generate as many concept representations as ground-truth concepts (e.g., as in CBM and CW) we will have $k^\\prime = k$. For the sake of simplicity, and without loss of generality, in our definitions we simply assume that representations are *aligned element-wise* and describe it informally as \u201cfor all $l \\in$ {$1, \\cdots, k$} , the $l$-th concept representation of $\\mathbf{\\hat{c}}^{(i)}$ encodes for the same concept as the $l$-th concept label in $\\mathbf{c}^{(i)}$\u201d (in Definition 1). This can be formally thought of as assuming that $\\mathcal{A}(l) = l$ for all $l \\in$ {$1, \\cdots, k$}. We opted for the informal definition rather than the formal one to simplify and make our definitions more clear. Nevertheless, we will point out in our paper\u2019s main body that a formal definition of an alignment can be found in Appendix 6.4 and extend that appendix to formally clarify the terminology we use.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4YRC-uJSng5",
                "writer": "author",
                "reply_to": "DHMbCif1GMv",
                "title": "Reply to Reviewer yfkD - Part 3/3",
                "comment": " ## Replies to Experiment Concerns\n\n### Take away from experiments\n\nShowing that supervision results in better quality of concepts as opposed to no supervision may be expected, but there are definitely exceptions to this: while this intuition is true for CBMs, our results actually show that it does not always hold. CW is surprisingly worse than unsupervised CL methods such as CCD. \nIn addition, the fact that mere label supervision can generate concept representations that are as high quality as those benefiting from concept supervision is indeed novel and impactful given the high cost of proving concept annotations. The same observation holds for the weak supervision provided in DGL: although such weak supervision was introduced in response to the difficulty of learning representations that capture the underlying factors of variations in the data in an unsupervised manner, empirically it lags far behind supervision from task or concepts. Interestingly enough, the new experiments we have run on unsupervised DGL (thanks for suggesting that), namely vanilla VAE [1] and $\\beta$-VAE [2], show that our metrics record that the representations learnt by these models can be at least as good, if not better, than those learnt by weakly supervised DGLs. This results in the representations learnt by unsupervised DGLs being significantly better than the weakly supervised ones at predicting both concepts and task labels (see the following two figures at https://i.imgur.com/jRwZCiL.png and https://i.imgur.com/LFq4D5m.png, respectively). \nWe agree that the result sections could be more clear on what results are in line with intuition, with our empirical studies confirming those with rigour,  and what results are new and can serve as the basis of making informed choices when choosing a method for finding an intermediate representation of the data. We will accommodate this in the revised version of the manuscript.\n\n## References\n\n[1] Amirata Ghorbani, James Wexler, James Y. Zou, and Been Kim. Towards automatic concept-based explanations. In Neural Information Processing Systems (NeurIPS), pp. 9273\u20139282, 2019.\n\n[2] Chih-Kuan Yeh, Been Kim, Sercan \u0308Omer Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Raviku-mar. On completeness-aware concept-based explanations in deep neural networks. In NeuralInformation Processing Systems (NeurIPS), 2020.\n\n[3] Diederik P. Kingma and Max Welling.   Auto-encoding variational bayes.   In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations (ICLR), 2014.\n\n[4] Irina Higgins,  Lo\u0131c Matthey,  Arka Pal,  Christopher Burgess,  Xavier Glorot,  Matthew Botvinick, Shakir  Mohamed,  and  Alexander  Lerchner.   beta-vae:  Learning  basic  visual  concepts  with  a constrained  variational  framework. In International  Conference  on  Learning  Representations (ICLR). OpenReview.net, 2017.\n\n[5] David Alvarez-Melis and Tommi S. Jaakkola.  Towards robust interpretability with self-explaining neural networks.  In Advances in Neural Information Processing Systems (NeurIPS), pp. 7786\u20137795, 2018.\n\n[6] Koh, Pang Wei, et al. \"Concept bottleneck models.\" International Conference on Machine Learning. PMLR, 2020.\n[7] Chen, Zhi, Yijie Bei, and Cynthia Rudin. \"Concept whitening for interpretable image recognition.\" Nature Machine Intelligence 2.12 (2020): 772-782.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qF1QJn4ulE",
                "writer": "author",
                "reply_to": "8_NK8OUhp21",
                "title": "Reply to Reviewer yfkD - Part 1/3",
                "comment": " We thank the reviewer for their constructive and insightful comments. The clarity of the definitions will be improved upon and succinctness will be ensured in the revised version of the paper. In the meantime, below we address the main concerns raised in the review:\n\n## Replies to Oracle Impurity Score Concerns\n\nThank you for your questions and comments regarding the OIS. We agree with the reviewer that our description of the metric could be more clear and succinct. We will appropriately address this in our updated manuscript.\n\n### AUC as Proxy for Mutual Information\n\nWe first agree that, for OIS to be bounded in [0,1], it requires that AUC is used as a proxy to measure the mutual information between learnt concepts and ground truth concepts. In our work, we focus on this instantiation and mention in the paragraph preceding definition 2 that, while one may choose to measure this in different ways, for the purposes of our evaluation we use AUC as a proxy for mutual information. In our updated manuscript, we aim to motivate this decision based on the following two facts: (1) AUC provides a tractable way of computing a proxy for the mutual information between the two variables of interest and (2) AUC trivially allows our score to be bounded in [0, 1]. While we believe that using something such as cross entropy loss may be interesting and similarly aligned with our goals, we opted away from such approaches mostly due to the difficulty that would come from bounding the resulting metric. We believe, however, that such an approach may be interesting to try as an unbounded version of this metric and is worth exploring as future work.\n\nSimilarly, we agree that other similarity metrics could be used to quantify the discrepancy between the oracle matrix and the representation\u2019s purity matrix. We primarily chose the Frobenius norm due to its efficiency and simplicity and will update our manuscript to make this decision\u2019s rationale explicit. Furthermore, notice that the use of the Frobenius norm, together with AUC scores to compute the entries in the matrices, is what enables easy normalization of our norm. This is very useful when attempting to interpret the output score.\n\nFinally, as correctly pointed out by the reviewer, the current instantiation depends on concepts being binary. Nevertheless, notice that, as it is the case for most of the literature in CL ([6], [7], [2]), in this work we are concerned with tasks in which ground truth concepts are binary. This, however, does not imply that the OIS could not be applied to multivariate concepts. One could, for example, compute the mean one-vs-all AUC across all possible labels a concept may take or, if one has a choice in the training and design of the model which will be evaluated, binarize all concepts so that each label a concept may take is assigned its own binary concept representation. Nevertheless, notice that this does imply that our metrics are not applicable for real-valued concepts. We believe that this can be justified by noticing that there has not been any substantial work on learning concept representations where ground truth concepts are real-valued. To clarify this, we are adding a line in our updated manuscript where we make this limitation explicit and state that our work is primarily focused on binary concepts and multivariate tasks (for niching scores).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "a0ud9FzaHA",
                "writer": "official_reviewer",
                "reply_to": "SpYrYWWtAQ",
                "title": "Response",
                "comment": " Thanks for your rebuttal - a couple points to clarify:\n\n- invertibility of g: on p3, you say \"if this is the case, then there must exist a mapping \\psi that recovers \\phi(x) from g(\\phi(x))\". Without any further qualifier, this looks like it is saying that \\psi = g^-1 must exist - am I missing something?\n\n- language around concepts: in the 2nd paragraph of background, I don't see any language stating that you use the word \"concept\" to refer to both CL and DGL; rather there is discussion of the common mathematical notation which will be used. I'm referring specifically to the terminology (in natural language) which needs a little more care.\n\n- alignment: since it is used in a proof, this concept could use a little more formalization - in my view it's not really a property of the representation so much as its a property of how we interpret it, but I could be misinterpreting the concept.\n\n- non-oracle impurity \"misleading\": I think it's a reasonable demonstration that the metrics are different in this setting, but I think it's important to state there are two reasons why the impurity score could rise in the high \\lambda setting: 1. the metric's assumption about leakage, 2. the model might actually be doing a worse job of modelling that data (even accounting for leakage - for instance, high leakage data may be harder to model). It's not clear how we disentangle 1 from 2. This may be difficult to show/beyond the scope of the paper but some more care in discussion would be helpful.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0iqsQ5_B5bB",
                "writer": "author",
                "reply_to": "iclr_2022_Ehhk6jyas6v",
                "title": "Summary of Changes in Revised Manuscript",
                "comment": " We would like to thank reviewers for all their feedback, suggestions, and comments. We have updated our manuscript to address the concerns and suggestions you brought forth in your reviews and include a summary of all the changes here.\n\n### Summary\nOur updated manuscript contains the following updates from our original submission:\n1. We have updated several of our definitions, as well significant parts of our text, to make sure that our ideas are communicated in a more succinct and clear manner.\n1. As per reviewers 7LHy and yfkD request, we have updated our paper to include three new baselines: VAE, $\\beta$-VAE, and Self-supervised Neural Neworks (SENN). We are delighted to say that this has led to our metrics highlighting new surprising insights and we discuss such insights with more detail in our experimentation and discussion sections. In summary, we found that unsupervised VAEs were able to extract purer, and higher-quality, representations that those learnt with weak-supervision. Similarly, we found that SENN, as was the case for CCD, can potentially learn concept representations whose quality matches that of representations learnt with full concept supervision (e.g., Concept Whitening).\n1. We have slightly updated our abstract to include these new findings in its conclusion. \n1. We have updated our experiments and discussion sections to make the distinction between expected and novel results clearer.\n1. We have updated our OIS definitions to justify why we make use of AUC as a proxy to mutual information and why we opted to use the Frobenius norm of the difference between the oracle and purity matrices as a similarity metric.\n1. We provide a new empirical justification for our niching metrics based on a real-world task (Appendix 6.6).\n1. We include new experiments showing the effect that capacity has in the encoder and decoder models independently of each other (Appendix 6.7).\n1. We include further details on how to select a value of $\\beta$ for evaluating niching metrics in Appendix 6.5.\n1. We have added a new Appendix (6.3) clarifying the details of how purity matrices are computed.\n1. We have added a new Appendix (6.4) clarifying the details of how concept alignments are computed when evaluating OIS if a method was not provided with explicit concept supervision.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SpYrYWWtAQ",
                "writer": "author",
                "reply_to": "fPJ2mn9yJh",
                "title": "Reply to Reviewer iQK1 - Part 1/3",
                "comment": " Response: We thank the reviewer for their constructive feedback. There are several comments about simplifying definitions, clarification, and fleshing out more details, which we are factoring in while preparing the revised version of the manuscript. Below we provide replies to more substantial comments.\n\n#### **CL and DGL**\nThe paper considers two broad paradigms of CL and DGL that aim at generating intermediate representation of the data. It first (i) unifies their formal languages and then (ii) proposes metrics to contrast the quality of intermediate representation within and between the two paradigms. \n\n**$g$ and $\\phi$**  \nConcepts are often extracted from a hidden layer of the neural network instead of raw input. $\\phi$ represents the function that  transforms the raw input into such a hidden representation. If $g$ is applied to the raw input directly, $\\phi$ is an identity function.\n\n#### **The role of tasks**  \nThe intermediate representation of the data we are focusing on in this paper can be extracted from an unsupervised setting (e.g., autoencoders) or a supervised one where essentially a classification problem with task labels (i.e., classification labels) is the starting point. \n\n#### **$g$ must be invertible**  \nThere may have been a misunderstanding related to this. There is no claim in the paper regarding invertibility of $g$. Can you please explain the question further? \n\n#### **Assumption for CL**  \nThere is not really an equivalent assumption to that of DGL in CL. However, one may say that CL assumes that the task in hand can be fully explained using units of information that are more abstract and simple than the raw input features. \n\n#### **Weakly supervised CL**  \nWe are not aware of any weakly supervised CL given that the field is relatively recent. Developing such a method may be a fruitful direction for future work. \n\n#### **quality of their learnt concepts**  \nIn the background section, second paragraph, we state that for simplicity we refer to the representation found in both DGL and CL  as concepts. We agree with the reviewer that this point needs to be stated clearly earlier in the Introduction and will reflect on that in the revised version of the manuscript.\n\n#### **No-leakage assumption**  \nLeakage refers to cases where learnt concepts encode information beyond the ground truth ones they are aligned with. If two ground truth concepts are correlated, then their learnt versions may indeed encode some information about one another. However, such overlap is not referred to as leakage as it is the reflection of the real-world conditions, as pointed out by the reviewer. The OIS metric proposed indeed caters for such cases by factoring in the correlation between ground truth concepts. Encoding information beyond the correlation seen in the ground truth ones counts as leakage and not otherwise. \n\n#### **Assumption implicit in niching**  \nNiching metrics work even if all tasks depend on all concepts. Task-separability of concepts is an empirical observation rather than an assumption. We motivate this by showing this phenomenon in CUB, as a representative of real-world datasets. The figure here (https://i.imgur.com/GmJhj61.png) shows the absolute values of concepts-to-tasks linear correlation coefficients in CUB. As evident, tasks (i.e., individual class labels) indeed often rely on a non-overlapping and relatively small set of concepts. We will highlight this motivation in the revised version of the manuscript. \n\n#### **Datasets compatible with both CL + DGL**  \nDGL is an unsupervised paradigm for which the notation of a downstream task is non-existent, whereas CL is a paradigm that deals with concepts in a classification task, with or without access to concept labels. Standard DGL datasets are not readily usable in CL, because of a lack of task labels. Standard CL datasets are not readily usable in DGL, as there is no guarantee that concepts are independent as assumed by data generation in DGL, where the data is generated based on an independent set of factors of variations. \n\n#### **Mean Concept AUC**  \nMean concept AUC is calculated by averaging over all the AUCs of predicting individual concepts (which are binary in our tasks) using only the overall set of learnt concepts. \n\n#### **Non-oracle impurity  baseline**  \nAUC of 1/2 is representative of random guessing. In off-diagonals, where a concept representation is predicting a concept which they are not aligned with, random guessing shows that the representation can predict the ground truth concepts no better than random, and therefore, there is no leakage. \n\n#### **The last sentence of the \"oracle impurity demonstrates**  \nWe agree that some rephrasing and clarification is required here - thanks for pointing out. Explicit supervision does encode purer concepts, with the exception of CW feature-map, as reported by others cited. We will clarify this exception in the revised version of the manuscript. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "txtXwLK1mem",
                "writer": "author",
                "reply_to": "JB6NoZjps1g",
                "title": "Reply to Reviewer 7LHy - Part 2/2",
                "comment": " ### Combining CL and DGL for better concept learning\n\nIn situations where concepts are known to be fully disentangled, using DGL with some type of weak supervision related to concepts or task labels can indeed prove fruitful as a direction for future work. As mentioned in the paper though, it is hard to realistically assume real-world concepts are disentangled.\n\n\n### Capacity of encoder  vs. decoder\n\nRunning experiments on increasing the capacity of the encoder and decoder separately confirms that the encoder's capacity has a much more important role in the quality of the representations than that of the decoder as pointed out by the reviewer. This can be seen in the following figure https://i.imgur.com/cRcc4bG.png where the capacity of the encoder/decoder is varied while its counterpart's capacity is left fixed. We will highlight this in the revised version of the manuscript. \n\n\n### References\n[1] Diederik P. Kingma and Max Welling.   Auto-encoding variational Bayes.   In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations (ICLR), 2014.\n\n\n[2] Irina Higgins,  Lo\u0131c Matthey,  Arka Pal,  Christopher Burgess,  Xavier Glorot,  Matthew Botvinick, Shakir  Mohamed,  and  Alexander  Lerchner.   beta-vae:  Learning  basic  visual  concepts  with  a constrained  variational  framework. In International  Conference  on  Learning  Representations (ICLR). OpenReview.net, 2017\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JB6NoZjps1g",
                "writer": "author",
                "reply_to": "qWBUkTMlW4m",
                "title": "Reply to Reviewer 7LHy - Part 1/2",
                "comment": " We thank the reviewer for their constructive comments and feedback.\n\n\n### Concept property (ii)\nProperty (ii) states that concepts should preserve the amount of mutual information observed in ground truth concepts or factors of variation. The Oracle impurity score is precisely doing that by measuring the divergence of the learnt concepts from the ground truth ones. We agree that we need to clarify that whilst in absence of access to ground truth concepts, the niching-based metrics reveal information about the task separability and minimality of concepts, they do not fully realise property (ii). This will be reflected in the revised version of the paper.\nIn the meantime, to motivate why task separability and minimality are important for evaluating concept quality, below we show that these properties are often empirically observed in real-world data. The figure in https://i.imgur.com/GmJhj61.png, which we will include in our paper, shows the absolute values of concepts-to-tasks linear correlation coefficients in CUB, as a representative of real-world datasets. As evident, tasks indeed rely on an often non-overlapping set of concepts. Thus, the concept niches for each task do not tend to intersect. If this is preserved by the learnt concept representation then the Niche Impurity Score (NIS), capturing the amount of undesirable mutual information amongst concepts, should be low, while Niche Purity Score (NPS), capturing the amount of inevitable mutual information amongst concepts, should be high. On the other hand, simultaneous high NIS and NPS suggest that there may be unnecessary mutual information between concepts that counts as leakage. \n\n\n\n### Purity matrix entries\nWe compute the (i, j)-th entry of the purity matrix as follows: we split the original testing data $(X_\\text{test}, Y_\\text{test}, C_\\text{test})$ into two disjoint sets, a new training set $(X_\\text{train}^\\prime, Y_\\text{train}^\\prime, C_\\text{train}^\\prime)$ and a new testing set $(X_\\text{test}^\\prime, Y_\\text{test}^\\prime, C_\\text{test}^\\prime)$, using a traditional 80%-20% split. We then use the concept representations learnt for the i-th concept for samples in $X_\\text{test}^\\prime$ to train a ReLU MLP $\\psi(\\cdot)$ with a single hidden layer with 32 activations to predict to truth value of the j-th ground-truth concept. In other words, we train  $\\psi(\\cdot)$ using labelled samples {$\\Big( g \\big( \\phi ( \\mathbf{x}^{(l)} ) \\big)_i, \\mathbf{c}_j^{(l)} \\Big)   |   \\mathbf{x}^{(l)} \\in X_\\text{train}^\\prime \\wedge \\mathbf{c}^{(l)} \\in C_\\text{train}^\\prime $}. Finally, we set the (i, j)-th entry of the purity matrix as the AUC obtained when evaluating $\\psi(\\cdot)$ on the new testing set $\\big( g \\big(\\phi(X_\\text{test}^\\prime) \\big), C_\\text{test}^\\prime)$. Note that we therefore do not require all of the data to compute the entries of this matrix but we rather depend on using the original testing split exclusively. We will address this lack of clarity in the revised version of the manuscript. \n\n\n\n### Missing related work\n\nWe acknowledge the relevance of the missing work (SENN in short) mentioned. Thanks for pointing this out. We have now added SENN as a benchmark and will update the manuscript with this accordingly. See figures (Oracle Impurity Score: https://i.imgur.com/BHbLULq.png; Niche Impurity Score: https://i.imgur.com/6AkkX1a.png, Niche Purity Score:  https://i.imgur.com/tFC8URI.png) for updated plots for both of our metrics which now include SENN results as well as two unsupervised DGLs (both vanilla VAE [1] and $\\beta$-VAE ($\\beta = 10$) [2]). The Oracle Impurity Score reveals that representations learnt by SENN are considerably more prone to unnecessary leakage (i.e., impurity) than those learnt by CCD, which is the closest approach to it. We observe that this is because SENN\u2019s concept representations are able to predict non-aligned concepts better than those learnt by CCD. We believe that the decrease in leakage in CCD is a consequence of CCD\u2019s training process including a regularization term that encourages coherence between concept representations in similar samples and misalignment between concept representations in dissimilar samples. Niching scores corroborate this hypothesis by showing a similar trend to that observed in oracle impurity.\n\n### Proposed metrics in various learning regimes\n\nThe applicability of metrics depends on access to ground truth concepts in the case of oracle impurity metric and access to task labels in the case of niching-based ones. Thus any learning regime that gives access to either requirement can be assessed based on the proposed metrics, including those that may merge CL and DGL in various ways.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cv7YDc7qFiZ",
                "writer": "author",
                "reply_to": "yvROdG4Zo8O",
                "title": "Reply to Reviewer iQK1 - Part 3/3",
                "comment": " **weak supervision provided by DGL methods**  \nWe are not aware of any supervised DGL. It would be great if some references are provided. Unsupervised DGL methods were avoided on the basis of the theoretical impossibility of learning disentangled representations in an unsupervised manner [1], as pointed out in the last paragraph of Section 2, hence using weakly supervised DGLs only. For completeness sake, we have now added benchmarks on unsupervised DGLs too (both vanilla VAE [2] and $\\beta$-VAE [3]), as well as for another unsupervised CL method (SENN [5]). We will update our manuscript accordingly with these new benchmarks. See figures (Oracle Impurity Score: https://i.imgur.com/BHbLULq.png; Niche Impurity Score: https://i.imgur.com/6AkkX1a.png, Niche Purity Score:  https://i.imgur.com/tFC8URI.png) for updated plots for both of our metrics. Interestingly enough, in absence of dependencies, unsupervised DGL approaches encode less impurity than the weakly supervised ones. However, when the dependency is high $\\beta$-VAE, in particular, tends to struggle more than the semi-supervised DGLs, which is not surprising as the approach is evolved around disentangling factors of variations that indeed have a lot of entanglement. \n\n[1] Francesco  Locatello,   Stefan  Bauer,   Mario  Lucic,   Gunnar  Ratsch,   Sylvain  Gelly,   Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations.  In International Conference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research, pp. 4114\u20134124. PMLR, 2019.\n\n[2] Diederik P. Kingma and Max Welling.   Auto-encoding variational bayes.   In Yoshua Bengio and Yann LeCun (eds.), International Conference on Learning Representations (ICLR), 2014.\n\n[3] Irina Higgins,  Lo\u0131c Matthey,  Arka Pal,  Christopher Burgess,  Xavier Glorot,  Matthew Botvinick, Shakir  Mohamed,  and  Alexander  Lerchner.   beta-vae:  Learning  basic  visual  concepts  with  a constrained  variational  framework. In International  Conference  on  Learning  Representations (ICLR). OpenReview.net, 2017\n\n[4] Chih-Kuan Yeh, Been Kim, Sercan Omer Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Raviku-mar. On completeness-aware concept-based explanations in deep neural networks. InNeuralInformation Processing Systems (NeurIPS), 2020.\n\n[5] David Alvarez-Melis and Tommi S. Jaakkola.  Towards robust interpretability with self-explaining neural networks.  In Advances in Neural Information Processing Systems (NeurIPS), pp. 7786\u20137795, 2018.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yvROdG4Zo8O",
                "writer": "author",
                "reply_to": "SpYrYWWtAQ",
                "title": "Reply to Reviewer iQK1 - Part 2/3",
                "comment": " **Points related to Definition 1**  \n1. Aligned component-wise:\nIntuitively, saying that two concept representations, say $\\hat{c}_1$ and $\\hat{c}_2$, are aligned component-wise is just saying that the i-th entry in $\\hat{c}_1$ encodes for the same concept as the i-th entry of $\\hat{c}_2$. The alignment between learnt concepts and ground truth ones is explained in the following extract from the paper. Once such alignment is made, the order of learnt concepts is adjusted to match that of the ground truth ones they are aligned with. Thus there will be a component-wise alignment between the learnt and ground truth concepts. \n\u201cThe alignment between the learnt and ground truth concepts is clear in CBM and CW. In CCD and DGL, where there is no alignment, we compute a greedy alignment between the learnt and ground truth concepts based on the predictive AUC of using a learnt concept to predict each ground truth concept (i.e., a ground truth concept is assumed to be represented by the learnt one that can predict it best). \u201d\n\n2. argmax notation:\nThis is correct and we appreciate that you pointed this out to us. We have updated our manuscript to make it clear that our metrics, as currently formulated, are applicable only when the concept-to-label model outputs a categorical distribution.\n\n3. The usage of AUC: \nWe use AUC to measure the probability that one is able to correctly predict the j-th ground truth concept using the i-th learnt concept representation when we train a model for this particular task. Although this is technically speaking defined only for binary concepts (and so is most of the concept-explainability literature), note that it can easily be generalised to categorical concepts by either (1) computing the mean one-vs-all AUC across all possible labels a concept may take or, if one has a choice in the training and design of the model which will be evaluated, (2) binarising all concepts so that each label a concept may take is assigned its own binary concept representation.\n\n4. Implementation concerns:\nIt is true that our estimation of $\\pi$, which limits the function class of \\psi to a one-layered-MLP, can be biased due to our choice of class functions. Nevertheless, we opted to proceed this way for two main reasons: (1) efficiency, as it is computationally intractable to explore the set of all functions $\\psi$ with that domain and codomain, and (2)  we are only interested in measuring the divergence between the $\\pi$ matrix and the oracle impurity matrix. Because in practice both $\\pi$ and the oracle impurity matrix are computed using a search over the same constrained set of functions $\\psi$, we expect that any inductive bias will appear in both computations. This means that when measuring the divergence between the two of them, which is what we are interested in measuring for the OIS, this bias should not have a significant effect on the score itself. Finally, note that there is precedence for such a constraint in the family of functions used for similar metrics as seen in [4].\n\n**Non-oracle impurity \"misleadingly\" shows more impurity**  \nFor each method in Figure 3, the red colour shows the divergence of learnt concepts from ground truth ones, as the dependency between concepts increases. The blue colour shows the divergence of learnt concepts from ground truth ones, assuming that the ground truth ones were independent, despite the increasing independence between concepts. While comparing red/blue across models allows comparing how well models do comparatively, the point here is to compare the red and blue within each model, where the gap shows that, if the non-oracle impurity is used to evaluate the quality of a concept representation, then all models misleadingly show much less leakage than encoded and recorded by the oracle-impurity. This extra leakage is sourced from the fact that the assumption that all concepts are independent is broken for higher values of $\\lambda$ rather than from changes in the representations themselves.\n\n**Why should we prefer niching-based metrics**  \nIf this is intended to be read as why should we prefer niching-based metrics to oracle impurity, we outline in the second paragraph of Section 5 that when ensuring the overall quality of concept representation is sufficient, niching-based metrics should be favoured to oracle-impurity, due to computational efficiency when one has access to both concept and label annotations. It can be computationally expensive to compute the impurity matrix $\\pi$ (and its corresponding oracle matrix) whereas one can quickly compute the linear correlation coefficients needed to evaluate our instantiation of the correlation-nicher. We will update our manuscript to clearly highlight where the efficiency of using our niching metrics comes from.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DHMbCif1GMv",
                "writer": "author",
                "reply_to": "qF1QJn4ulE",
                "title": "Reply to Reviewer yfkD - Part 2/3",
                "comment": " ## Replies to Niching Scores Concerns\n### Assuming access to ground truth concepts\nAssuming access to ground truth concepts may not be a weakness but it is definitely a limitation when learning concepts in a not fully supervised manner. Metrics that make such assumptions are fully compatible with approaches such as CBM and CW that benefit from concept supervision, however, to be used in other paradigms where concept annotations are not available or are available partially, one still needs to assume access to full concept annotation. It is hard to imagine a real-world case where concept labels are available and used for verification, but not in the learning. Consequently, previous work has attempted to introduce \"metrics/quality control mechanisms\" that do not rely on ground truth concepts. These include importance score in [3] and completeness score in [4].\n\n### High NPS and low NIS\nHigh NPS is not necessarily accompanied by low NIS. In cases where every learnt concept representation encodes information about nearly all ground truth ones, both NPS and NIS tend to be high (i.e., when a concept representation is able to predict with high probability both aligned and misaligned concepts). We observe this in CW, especially when using the entire concept maps as concept representations, as well as in Self-Explaining Neural Networks (SENN) [5], an unsupervised concept learning method we will include as part of our updated manuscript. You can see both SENN\u2019s and CW\u2019s NIS in https://i.imgur.com/6AkkX1a.png and their NPS in  https://i.imgur.com/tFC8URI.png. Note that the results we obtain for SENN differ from those obtained for CCD, our other unsupervised CL method. We believe this to be a consequence of CCD\u2019s training process including a regularization term that encourages coherence between concept representations in similar samples and misalignment between concept representations in dissimilar samples.\n\n### Selecting $\\beta$\nOne way to select beta, and also to measure the quality of the concept representations, is to compute niching scores for different values of $\\beta$ and generate a plot as we have done in Fig. 7. Such a plot highlights how robust your concept representation is and allows you to see whether or not reducing the niche sizes by increasing $\\beta$ quickly results in worse niching scores. One can then select $\\beta$ by picking a value for which the niche size becomes relatively stable, or at least linear as a function of $\\beta$. For example, we selected $\\beta = 0.2$ in our evaluation by noticing from our plots in Figure 7 that niche sizes, and niche scores, remain relatively linear and stable around that value. We will update section 6.3 accordingly to include a suggestive strategy for how to select $\\beta$.\n\n### Defining Concept Nicher\nThe attempt to give a general definition of a nicher is intentional to leave room for various instantiations. We chose to use a specific instantiation of the nicher using linear correlation coefficients for the sake of simplicity and efficiency. Nevertheless, one can opt for a non-linear model in a different instantiation and is something we would like to explore as part of future research.\n\n## Replies to Other Concerns\n### Unsupervised DGL\nWe have now added two unsupervised DGL methods to our experiments, vanilla VAE [3] and $\\beta$-VAE [4] (with $\\beta = 10$). You can see our updated plot for OIS in https://i.imgur.com/BHbLULq.png while our plots for NIS and NPS can be found at https://i.imgur.com/6AkkX1a.png and https://i.imgur.com/tFC8URI.png, respectively. Interestingly enough, we observe that, in absence of dependencies, unsupervised DGL approaches encode less impurity than the weakly supervised ones. However, when the dependency is high, $\\beta$-VAE, in particular, tends to struggle more than the semi-supervised DGLs, which is not surprising as the approach has the underlying assumption that the different factors of variation are independent and incentivizes this separation by the multiplier $\\beta$ in its loss function. \n\n### OIS and alignment between learned and ground truth concepts\n\nYou are correct in your understanding that an alignment is required to compute the OIS. Furthermore, this alignment is not known for the DGL methods we evaluate within our work (both semi-supervised and unsupervised). To address this, as pointed out under the heading \u201cMetrics that assume concept independence may be misleading\u201d, in CCD and DGL (weakly or unsupervised), where there is no alignment, we compute a greedy alignment between the learnt and ground truth concepts based on the predictive AUC of using a learnt concept to predict each ground truth concept (i.e., a ground truth concept is assumed to be represented by the learnt one that can predict it best). We opted for this greedy strategy for efficiency purposes mainly. Nevertheless, future work may explore better alignment mechanisms, as well as fully exploit the whole search space when the number of concepts is small. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qWBUkTMlW4m",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ehhk6jyas6v",
                "title": "",
                "comment": "The authors have put decent effort to bring concept-based representation learning and disentanglement learning together under one umbrella in terms of the quality of generated concepts in presence as well as absence of ground truth concept labels. Some related metrics were proposed for evaluation of the quality of concepts for both these methods. Based on these studies, presented in the paper, some important conclusions were made based on requirements of concept supervision and their effects on final concept quality as well as the predictive performance of the model. This paper introduced metrics to evaluate the concept quality generated by both methods i.e. concept-based representation learning and disentanglement learning in the scinerio of concept supervision and correlation of concepts. For this purpose, mainly three family of methods were considered i.e. concept-based representation learning both with and without concept supervision and semi-supervised disentanglement learning method. Based on this metrics, some recommendations were made regarding requirements of explicit concept supervision for concept-based representation learning methods and weak supervision for disentanglement learning methods.\n\nThis paper has below weaknesses/clarifications/suggestions:\n1)It's not very clear how the concept property (ii) is related to the contributions made based on concept correlations and availability of ground truth cocnepts. The property is definitely a desirable property for the concepts, but how the proposed metrics capture whether a set of generated concepts follow this property, specially when the ground truth concepts are not available.\n\n2)How each entry of the purity matrix is calculated? Do all the data points are required to calculate each matrix entry? Please elaborate the line \"In practice, we parameterise the family of functions \u03c8 j by training, via gradient descent, a ReLU MLP with 32 hidden activations\" in this context.\n\n3)A very important work (https://arxiv.org/abs/1806.07538) is missing from the paper, which can be a very good representative of unsupervided concept learning methods. This work has used both label supervision and decoder based network for better concept learning. Extending in the same line, What is your opinion on (possibility of) merging these two types of methods (CL and DGL) for better concept learning? If you think it's possible, then how would the proposed metrics would be helpful here?\n\n4)Figure 8 shows the effect of network capacity. On top of that, it would be interesting to see the individual effects of capacities of encoder and the decoder. To be precise, improving the capacities of both the encoder and the decoder would result low impurity and higher cocnept accuracy, but it's important to check if capacity of the encoder has more effect on concept quality than capacity of the decoder. This is an interesting work that tried to unify concept-based representation learning and disentanglement learning and proposed corresponding metrics that helped to find some important conclusions. Answering/commenting on the points, that I posted under the main review, can significantly improve the quality of this work.",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "8_NK8OUhp21",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ehhk6jyas6v",
                "title": "",
                "comment": "The authors consider the question of whether recent concept-based learning algorithms, as well disentangled representation learning algorithms, result in high-quality representations. In particular, they consider what high-quality should mean in terms of the relationship with ground truth concepts and the ability to make accurate predictions for a downstream task. To this end, they propose two main metrics for representations that are explicitly or implicitly encouraged to encode concepts: 1) a score that captures how well the learned representation preserves the relationships between concepts (which may be correlated), and 2) a score that captures how well concepts can be split into groups that are useful/useless for predicting particular label dimensions. Concept-based explanations are a relatively new idea; the first papers on this topic, including TCAV and its extensions, were post-hoc methods that did not require specialized model architectures. More recently, the concept bottleneck idea suggested training a model that explicitly encodes concepts at an intermediate layer (enabling interventions, etc). In parallel, as the authors say, there has been a rich literature on unsupervised/semi-supervised disentangled representation learning, where models are trained that may or may not capture semantically meaningful, decorrelated concepts in their latent spaces. It is natural to wonder to what extent these various methods can result in representations that are useful from a concept-based model explanation perspective. \n\nBefore getting into the authors' contributions, it's probably worth saying that the intuitive answer here (for me, at least) is that concept supervision is key, and without it we can't expect a model to learn a latent space that is well-aligned with concepts that we don't tell it about. Ultimately, this appears to be the takeaway from the experiments (see figures 2, 4, 5), but it's not a particularly exciting or surprising result; it's showing what people would expect to begin with, but with a little bit of rigor.\n\nBelow, I'll summarize and comment on the authors' two proposed metrics.\n\n### Oracle impurity score\n\nThe proposed OIS score basically assesses to what extent the learned concepts predict the ground truth concepts equivalently to how the ground truth concepts would. Some previous work suggests that concepts should not be predictive of one another because this would constitute \"leakage,\" which is problematic for experiments with concept interventions. To remedy that, because concepts in fact are related (this can likely be seen in the ground truth annotations for most datasets), the authors consider how well each ground truth concept can predict another (using AUC, for example) and then ask whether the learned concept achieves the same predictive performance.\n\nMore specifically, the authors build a matrix of predictive accuracies for all pairs of concepts, using both the ground truth and learned concepts as predictors. They then take the Frobenius norm of the difference between these two matrices and apply a couple constant factors.\n\nThis is fine, it seems like an improvement on previous metrics for measuring leakage. It also seems related to the idea of measuring how well the learned concepts can predict the ground truth concepts in a 1-1 manner, which is one of the main metrics in the CBM paper. While that metric effectively assesses the mutual information between matched pairs of learned and ground truth concepts while ignoring inter-concept relationships, this one measures (roughly) the delta in mutual information across all pairs.\n\nA couple questions and concerns about this:\n- The bounds used to guarantee that the score is in [0, 1] seem to require the use of AUC as a performance metric. What if we want to use something else, e.g. because the concept is multivariate or real-valued?\n- If you want to quantify the mutual information for a discrete concept, why not use cross entropy loss? This would approximate $H(c_i | c_j)$ and $H(c_i | \\hat c_j)$, whose difference is equal to $I(c_i | c_j) - I(c_i | \\hat c_j)$. This seems more aligned with what you're talking about than AUC.\n- The use of a Frobenius norm on the difference in matrices of prediction accuracies seems a bit heuristic. It's fine, it just seems a bit arbitrary and I wonder if there's a better way to do this depending on what the particular accuracy metric is (e.g., if we had exact mutual information values).\n- The notation in definition 1 is a pretty odd way of saying you train a model $\\psi_j$ to predict one concept from another. I would not have been able to understand this without the explanatory text below. Please consider rewriting this a different way.\n\n### Niching scores\n\nThe next metric tries to provide some measure of concept-based representation quality without requiring access to ground truth concepts. This is quite a leap, because it requires resorting to some notion of quality that departs from how previous work has assessed concept-based representations. \n\nThe authors suggest that for a good concept-based representation, we should be able to identify a small number of concepts that are predictive of each output dimension, and that the remaining concepts should not be predictive (because this would constitute leakage). I read this part of the paper a couple times and I just don't buy this argument, I don't see how the ability to \"niche\" concepts into groups that are useful/useless for each output dimension is a measure of the representation's quality. This is not what I would think of as a measure of whether my learned concepts capture what they're supposed to capture. And the ability to do well on these scores seems dataset-dependent: what if there are output dimensions where every concept is useful, or where no concept is useful? It's a nice idea to try to make a metric that doesn't require access to ground truth concepts, but I don't quite agree with the premise of this particular approach. (Though I'm curious to hear if other reviewers disagree.)\n\nSome other questions and concerns:\n- I'm not so sure about framing the fact that previous metrics require access to ground truth concepts as a weakness. First off, if you're using some of these methods (e.g., CBM) we know you have access to ground truth concepts, so it's not an unrealistic assumption. Second, existing notions of high-quality concept-based representations are defined specifically based on learned representations capturing known concepts, so access to those concepts is clearly necessary; you haven't gotten around that need, you've just opted for an orthogonal notion of representation quality.\n- Isn't a high NPS and low NIS virtually guaranteed by the construction of your concept niche? You're effectively doing feature selection to ensure that the predictive concepts are in one group and the useless ones are in another (for a given output dimension). For any representation at all, it should not be surprising if NPS is high and NIS is low. Or am I missing something here?\n\nAs explained above, I have some qualms about this metric, but given its current state here are a couple other comments:\n- When defining your \"concept nicher\" (i.e., a way to decide which concepts are relevant to a given output dimension), there are a couple problems with your use of correlation. First off, it's worth noticing that correlation is essentially a measure of the MSE from a fitted linear regression model (check the math on this for the specific relationship). With that in mind, there's a better option than taking the max score when you have multivariate learned concepts: derive the MSE from a fitted multivariate linear model that takes in all the concept dimensions. But why not allow a nonlinear model? Also, what if the label is not real-valued but a classification label? Rather than using correlation, wouldn't it be better to fit a classifier and check the cross entropy loss? Yes, this requires fitting a model, but that step is only required once per method evaluation.\n- How are we supposed to choose beta? Different beta values will result in very different outcomes. For example, if I set beta close to 0, I expect we'll see great NPS/NIS scores (because everything will be deemed relevant to every output dim). \n- Overall, there's a lot of wiggle room in your definition of \"concept niching\" and it would be nice if there were a better justified default option. On the other hand, this aspect of your paper is essentially feature selection, and that's a hard problem with approximations of varying quality and computational cost.\n\n### Experiments\n\nThe experiments section uses the proposed metrics to compare some SOTA concept-based and DGL algorithms. As mentioned at the beginning of my review, these experiments overall felt a bit too much like sanity checks (particularly figure 3), because the outcome was mostly what I would have expected to begin with. Also, it could have been nice to use some non-toy datasets.\n\n### Other comments\n\n- While you're at it evaluating various methods, why not include any unsupervised (rather than semi-supervised) DGL algorithms? It would be interesting to see how much worse these do than the other approaches (which is perhaps to be expected because of the works cited).\n- The OIS score appears to require a known alignment between learned and ground truth concepts, is that right? Am I correct in understanding that this is somehow known for semi-supervised DGL approaches but not unsupervised ones? \n- The definitions in this paper are not easy to read. They're too verbose and too long, please try to make them more concise.\n It's a good idea to assess whether unsupervised and semi-supervised DGL algorithms can learn high-quality concept-based representations, and in doing so the authors developed two metrics that they suggest can be used to compare concept-based representations both with and without access to ground truth concepts. However, the results from their study do not seem very surprising or impactful, and I was not convinced of the rationale behind these metrics (for the reasons described above). For that reason, my rating is that this work is marginally below the acceptance threshold.",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "fPJ2mn9yJh",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_Ehhk6jyas6v",
                "title": "",
                "comment": "The authors propose new metrics for methods in disentanglement learning and concept learning,  which have some nice properties, including robustness to correlations in the underlying factors, and experimentally probe the properties of these metrics on a number of different proposed methods in the disentanglemnt/concept learning literatures. \nStrengths\n\n- metrics which work for correlated underlying factors are really valuable and I think a big blind spot in this space, glad to see this is a focus of this paper\n- proposed metrics have some nice underlying intuitions and seem sensible to me\n- experiments seem quite extensive, comparing many prior methods from the literature in both CL and DGL\n\nWeaknesses\n\n- paper could greatly benefit from improved clarity throughout: there are a number of points where I struggle to understand exactly what is being communicated\n- In particular, clearer descriptions of methods and how those differences manifest in the results section would be helpful - I don't have a deep understanding of all the related work discussed and I have a lot of trouble knowing what the takeaways from the results section should be. Some figures which accent the particular points in question in each section could help, or more analysis of the overall concepts in question for each experiment\n- it's not entirely clear conceptually how we're supposed to think about CL and DGL in this paper. Is the paper intended to unify the methods? Contrast them? I don't fully understand what the authors see as the main differences or similarities, or what they want to communicate to the reader on this front. It would be good to spend more time developing a framework and some common language to describe the two approaches\n- I give many more specific notes below\n\nOther\n\nSec 1:\n- the acronym DGL for disentanglement learning is strange - I'd personally prefer something like DEL but up to you\nSec 2:\n- a working example at the top of Section 2 explaining what each concept might represent would be very helpful, especially to someone like me who is familiar with the overall literature but not on the bleeding-edge methods\n-  not sure why we need both g and \\phi in this framework?\n- can you clarify the role of the downstream task here? it's never really made clear - are there many tasks? just one? do we expect to know it at training time?\n- would be useful to define TCAV\n- you state that if a function g contains sufficient statistics to perform well on a downstream task f, then g must be invertible; I don't see why this is true, since g may reduce the input \\phi to just the sufficient statistics, and much of \\phi may not be recoverable\n- you list the operating assumption of DGL, what is the analogous assumption for CL?\n- you focus on weakly supervised disentanglement, why not weakly supervised CL as well?\nSec 3:\n- some sloppiness in language at the top - I'm not sure what the \"quality of their learnt concepts\" is meant to refer to: only CL (since you used the word \"concept\")? or both, since you refer to all the \"approaches above\". This is where I think it would be useful to clarify a particular framework for how you relate these two literatures\n- may be beyond the scope of the work, but I agree that the no-leakage assumption is unrealistic - however there are some cases where we want it (the two concepts are truly related) and some cases where we don't (if it's a relic of sampling bias). Just an interesting thought, not sure if there are ramifications in your paper\n- it would be good to discuss the CUB example in the main body since it is an important point for the rest of the paper\nSec 3.1\n- Def. 1: what does it mean for two representations to be \"aligned element-wise\"?\n- Def. 1: the argmax notation loses me - is the idea that the concept should be categorical? I don't think this was specified earlier\n- I don't understand the usage of AUC here - are you assuming binary concepts?\n- Implementation concern which should probably be discussed: your estimate of \\pi depends on calculating a \\sup over functions \\psi - this will probably be only approximate, and if this is a related function class (or less powerful) than the encoder in question g, then you might systematically bias your estimate of \\pi and OIS\nSec 3.2\n- I'm not totally sure the analogy with evolutionary biology is necessary here\n- there's an assumption implicit in niching which is that all tasks you care about will necessarily rely on a specific subset of individually predictive dimensions - worth motivating this assumption, it's a little bit different from the standard DGL assumptions\n- Def. 6: technically N_j is not an input to NPS, just clean up formalization here to clarify what you mean by \"the NPS of \\not N_j (v_f)\"\nSec 4.2\n- \"in order to have datasets compatible with both CL + DGL ... construct datasets full described by ground truth gen. factors\" - I'm not sure what this means, can't you use any dataset with either of these methods?\nSec 4.3\n- clarify exactly what the metric \"Mean Concept AUC\" refers to - I'm not quite sure how it's calculated\n- why is the non-oracle impurity baseline matrix using 1/2 on its off-diagonals - wouldn't the baseline be 0 on the off-diagonals to mimic the situation where we assume no leakage?\n- you say that the non-oracle impurity \"misleadingly\" shows more impurity as dependency increases. However, it's not clear this is misleading, it's possible that the methods are truly doing worse on that data (even relative to oracle). It would be nice to have something external to tell us which is the case\n- suggestion: indicate in figures which methods are supervised, this will help interpreting them a lot\n- the last sentence of the \"oracle impurity demonstrates\" paragraph seems to contradict the first: the first says that explicit supervision \"encodes purer individual concepts\", the last says explicit supervision \"does not translate to purer concepts\"  - I'm confused which should be true\n- the reference to \"concept loudness\" should be explained to those not familiar with the literature\n- the point at the bottom about robustness to concept inter-dependencies seems really important - I would love to see this fleshed out more\nSec 5.\n- (iii) you discuss the \"weak supervision provided by DGL methods\" but aren't there supervised, weakly, and un-supervised DGL methods?\n- why should we prefer niching-based metrics?\n- this point about efficiency goes over my head, would be good to have a quick discussion in the paper\n\n The authors propose metrics which address a very important problem in the relevant literatures (in particular, underlying factors which may be correlated), but the communication of the ideas and experimental results are not currently clear enough for me to accept the paper as is.",
                "rating": 5,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the acceptance of the paper",
                "Sentiment Expression": "unable to recommend",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "ZDaSIkWT-AP": {
        "paper_id": "iclr_2022_ZDaSIkWT-AP",
        "paper_title": "Case-based reasoning for better generalization in textual reinforcement learning",
        "paper_abstract": "Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world and later reuses the collected experiences to act efficiently. The method can be used in conjunction with any existing on-policy neural agent introduced in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization and achieves new state-of-the-art results on widely used environments.",
        "paper_acceptance": "Accept (Poster)",
        "meta_review": "This paper describes how to apply a combination of case-based reasoning and RL methods to improve the performance of agents in text-adventure games.  The reviewers unanimously recommend acceptance.  This work is both insightful and practical.  This is a valuable contribution.  Well done!",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "saAdd8c9e4_",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_ZDaSIkWT-AP",
                "title": "",
                "comment": "This paper describes how to apply  a combination of case-based reasoning and RL methods to improve performance of agents on text-adventure game type tasks. It introduces a GNN representation of state and a vector-quantized encoding scheme so that contextual information about successful actions from the past can be retrieved and re-used. Experiments show a significant increase in performance, along with ablations showing value of the CBR  especially on OOD environments.Finally there is qualitative insights  of the memory representation showing how the CBR helps. Using the Neurips rubric:\n\nOriginality: As the related work shows,  CBR to speed up RL has been tried before, however those methods are very different and not meant for the modern scale of deep RL and contextualized representations+retrieval methods. The actual details of the CBR model used in this paper, seeded graph attention and vector quantization to aid retrieval seem relevant at least in their application to doing CBR for RL. \nThe basic experiments are straightforward, applying combinations of CBR and sota RL methods on the 2 TAG domains. However, there are a couple of novelties in the ablations and a very good qualitative analysis to show where the gains from CBR are coming from (fig 3).\n\nQuality: I thought the model described in sec 4 was interesting and mostly well-motivated. Some minor questions:\n  a. comparing the 2 equations in seeded graph attention, it looks like the h's dependency on $\\alpha$ is quadratic, which seems unusual for attention?\nb. why sum for the final representation and not average? it could lead to wierd biases when the number of entities are different between states.\nc. The VQ approach is interesting, but I am a bit skeptical that it is necesary. Very efficient methods exist to do retrieval in large dim spaces, with some effort. So for practical applications, it might just be easier to do that. Can you share some quantitative measurements to show that this code splitting approach is necessary?\nd.  The claim that CBR lets you generalize OOD seems not supported and my intuition is almost the opposite. Can you elaborate?\ne. The CBR and neueral agent seem to be separately trained. Can they be jointly optimized or at least can the neural agent be trained with  some knowledge of when the CBR will over-ride it so that it's policy can adapt to that?\nf. Fig 4: it would be more interesting if this could show the \"counterfactual\" probabilties. since the neural agent always loses to the CBR in action selection, what if the neural agent had been allowed to execute, would it still have been successful?\ng. Is locality sensitive hashing another way to do efficient retrieval that can generalize?\n\nClarity: the paper is well written and all major parts are well-explained. A few details could use some clarity:\na.  when you say \"the final set of all retrieved actions and the corresponding relevance\", can you be more precise and define the tuple or whatever exactly the data structure is.\nb. sec 4.3: \"applied to the entities\": how does it choose which entities to use?\nc. Please elaborate on the Adolphs and Hofmann method, even though it's not an original contribution it is a core part of your system.\nd. The jump from 24% to 73% in win rate on Jericho seems really big ! You should highlight it more. Fairly well-thought out scalable approach to adding CBR to a tough RL setting which has got some attention recently. Strong results and good empirical analysis. It would be beneficial to get RL practitioners to seriously consider adding CBR type approaches as a new type of strategy for boosting Deep RL.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "lLt2FVJvJlr",
                "writer": "official_reviewer",
                "reply_to": "Bo9uyH9nbRR",
                "title": "Response",
                "comment": " \nI think HscQ's questions were good ones, and the authors seem to have addressed them by comparison with other transformer-based methods. It seems that their particular architecture in fig 1 (KG, seeded attention etc) is required to get the results they did.\n\nOne remaining confusion I have is philosophical rather than technical. Revisiting the idea of CBR again as described in sec 2, how is that different from the generic notion of a policy? Is the author's method better seen as simply a new approach to representing policies in structured enviroments that take max advantage of textual representations in order to do better IND and OOD generalization?\n\nThe authors have done a pretty exhaustive job addressing other concerns, I am raising my score by 1.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_b104DD7URz",
                "writer": "author",
                "reply_to": "saAdd8c9e4_",
                "title": "Kind reminder",
                "comment": " Thanks again for your valuable feedback and your positive review. As today is the end of the final stage of the discussion, we would be glad if you could acknowledge that you have read our rebuttal. We believe that we have comprehensively addressed all your comments, but we remain available in case you have any further inquiry. Thanks!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nsGrYH1ia33",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_ZDaSIkWT-AP",
                "title": "",
                "comment": "The paper presents, at its core, a case-based reasoning (CBR) centered approach to solving text-based games. The CBR process shown is a 4 step process consisting of: retrieve (useful past experiences), reuse (the past experiences in a meaningful manner), revise (modify the current policy to account for prior experiences), and retain (decide which experiences to keep). This method is applied to an A2C agent that uses a knowledge graph based state representation, though it can be used along with other types of text game agents too.\n Strengths:\n\n- The core idea is well defined and motivated. \"Lets get agents to use previous useful experiences to better improve the current policy\"\n\n- The paper is well written overall I was mostly able to follow along, model design choices are mostly explained.\n\n- The paper compares to multiple existing methods and the line up of related work baselines strengthens the work.\n\nClarifications/Concerns/Weaknesses:\n\n- Section 3&4 in particular are a bit unclear and can be consolidated more. I think providing details of each portion under the relevant subsection of the 4 step CBR algorithm would make it a lot more clear.\n\n- The retrieve portion in particular seems very similar to many other retrieval methods recently seen in text based game literature and the wider NLP community. It would be nice to see a comparison/ablation in *just the retrieve portion* to not use the knowledge graph and only text on *Jericho* (one obvious way would be to frame it as a Machine Reading Comprehension task to retrieve passages as in Guo et al. 2020 https://arxiv.org/abs/2010.02386 [which is cited and compared to for TWC]). Without such experiments it is relatively unclear whether the portions shown in Sec 4.1 are aiding, and if so how.\n\n- The \"Baseline agent + CBR\" comparisons are good to have but would probably be more useful to have in Jericho than TWC. The main reason being that TWC is a singular \"home\" domain in which the commonsense knowledge (the text descriptions + genre knowledge) are more likely to be uniform throughout than with the varying genres like Jericho. This tests the limits of the CBR process and gives you the ability to analyze exactly in what portion of cases in certain types of knowledge transferable. In my opinion, such an analysis would likely prove to be the most valuable contribution of a work like this. The paper is well written and presents a useful paradigm for thinking about text game agents and the results are interesting. The main thing holding the paper back is the lack of analysis/ablations on certain portions of the proposed algorithm that make it unclear where the gain are coming from.\n\n\n====Rebuttal Update====\nI've read the rebuttal across the reviewers and am satisfied enough to increase my score.",
                "rating": 8,
                "confidence": 5
            },
            {
                "review_id": "w_sFO4JOHdq",
                "writer": "author",
                "reply_to": "hp7_fVzi0hA",
                "title": "Thanks for your suggestions and review",
                "comment": " Thanks for appreciating our work. We will make the changes you suggested in the next version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "w2Gt5vAaQ6p",
                "writer": "author",
                "reply_to": "nsGrYH1ia33",
                "title": "Thanks again",
                "comment": " Dear Reviewer,\n\nThanks again for your time and for appreciating our paper. We hope you had a chance to look at our rebuttal. We would like to know whether our response and the new experimental results are sufficient to address your concerns. Are there any remaining points that you would like to discuss?\n\nThanks again for your consideration,\n\nAuthors of the submission",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EAbZuBY5Tsg",
                "writer": "author",
                "reply_to": "saAdd8c9e4_",
                "title": "Thanks again",
                "comment": " Dear Reviewer,\n\nThanks again for your time and for appreciating our paper. We hope you had a chance to look at our rebuttal. We would like to know whether our response and the new experimental results are sufficient to address your concerns. Are there any remaining points that you would like to discuss?\n\nThanks again for your consideration,\n\nAuthors of the submission\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "vqcohPwDVfl",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_ZDaSIkWT-AP",
                "title": "",
                "comment": "In this paper, the authors propose to improve the existing policy-based RL algorithms for the text-based world environments by incorporating knowledge via a case-based reasoning module. This improves out of distribution performance. The key idea in Case-Based Reasoning is to collect interactions that led to positive rewards in the past and try to map a novel situation to one of these past interactions to decide on the action in the current situation. In particular, authors represent the state of a text-based game by a knowledge graph and use message propagation (focussed on the sub-set of the nodes) to get the most similar representation of a new context.  Strengths:\n1. Authors show that combining CBR with existing RL methods improves agent performance significantly. \n2. Authors conduct detailed experiments (existing RL models + CBR) which show the effectiveness of the proposed approach. The proposed model is more sample efficient and generalizes better.    \n\nWeaknesses:\n1. The different components proposed in the paper, (e.g., CBR, KG) are not novel per se for the text-based RL agents. The paper has limited novelty. \n2. There has been recent work that has used transformer-based LMs for text-based RL agents [e.g., 1,2,3]. Since Transformers are trained on very large corpora, they have been shown to encode world knowledge and hence work well in text-based world settings when combined with an RL agent. The authors do not compare to these approaches. It would be nice to have a comparison with these knowledge-based approaches as well.   \n\n[1] Deep reinforcement learning with transformers for text adventure games, Y. Xu, et al., 2020: https://ieeexplore.ieee.org/document/9231622  \n[2] Pre-trained Language Models as Prior Knowledge for\nPlaying Text-based Games, Singh, et al., 2020: https://arxiv.org/abs/2107.08408\n[3] Stabilizing transformers for reinforcement learning. Parisotto, et al., 2020: https://arxiv.org/abs/1910.06764 \n\nSuggestion:\n\nAuthors say that they use A2C as the RL agent, however, they cite the paper that describes A3C, so it is not exactly clear if they are using a synchronous or asynchronous version of the Advantage Actor-Critic agent. More details on this would help.  The paper proposes the use of Case-Based Reasoning combined with Knowledge Graphs for improving RL agents for text-based environments. However, the approach has limited novelty as similar approaches have been proposed in the past. On the evaluation side, authors should also compare with other approaches (e.g., based on Transformers) that make use of external knowledge to generalize better. ",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "hp7_fVzi0hA",
                "writer": "official_reviewer",
                "reply_to": "yP5b-CKqNqp",
                "title": "Thanks for the clarifications",
                "comment": " Thank you for your responses, which help answer my questions, and I encourage you to add in these additional clarifications to the paper. I also appreciate the additional experiments in the Appendix, which further strengthens the paper's findings and provides better perspective to evaluate the design choices made in the paper. Overall, I'm happy to raise my score!\nOne last suggestion I have with respect to the 'Best Agent' row in Table 3 is to discount games where none of the agents get off the ground and get 0 scores or all of them basically get the same score (e.g. 1). This might be a more accurate way to convey the performance comparison between the different agents.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "euOAlQ66Iv5",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_ZDaSIkWT-AP",
                "title": "",
                "comment": "This paper proposes an approach to combine case-based reasoning with reinforcement learning for text-based games. The method works by keeping track of states that received positive rewards in the game, and then having a retrieval mechanism to retrieve similar contexts at a new state in the game. The authors use a quantization technique for efficient storage and retrieval, and a fallback neural policy (trained with RL) in cases where none of the retrieved action templates result in an admissible action in the current state. The writing is quite clear and the experiments are mostly convincing, barring a few questions I have. Strengths:\n- Solid approach combining CBR and RL\n- Experiments are thorough and convincing.\n- Writing is quite clear\n\nWeaknesses:\n- Some design choices may need more justification/analysis (see below, mainly points 3 and 4)\n\nComments:\n1. In the retriever module, how is the threshold $\\tau$ chosen?\n2. Algorithm 1 describes performing retrieval using multiple context selectors $\\mathcal{C}_t$. How are these specified and how are they different from each other? Do they store different types of contexts?\n3. The \u2018Retain\u2019 module seemed a bit ad-hoc to me since the module stores actions with positive rewards only. This goes against the key premise of performing good credit assignments in RL since the crucial action that led to this positive reward may occur 5 or 10 or even 20 steps before the transition with the positive reward. I suspect this is why storing previous actions helps empirically. So, to me, this design choice seems arbitrary and not well motivated. In fact, why not store negative actions as well or even use a prioritization scheme based on TD error similar to that in https://arxiv.org/abs/1511.05952 ? An ablation on this (even on a small subset of the games) may be helpful.\n4. The context discretization scheme in 4.2 reminds me of this paper (https://arxiv.org/abs/2103.13552) that used hash functions to represent the state. Would just hashing the state (e.g. with locality sensitive hashing (LSH) on fixed pre-trained BERT representations) with a random function work as well as the learned discretization scheme? Again, an ablation on just a subset of the games might be useful here. \n5. On the same point as above, the paper mentions that the discretization scheme helps tackle the issue of changing representations over the course of training. However, since the discretization is also a learned function, wouldn\u2019t that also potentially change over time? It wasn\u2019t clear how this solves the issue of the same context mapping to two different entries in the memory.\n6. In table 3, the term \u201cwin rate/win count\u201d for the last row was confusing. I took it to be the win rate on the games themselves, but it seems to be a comparison across the different models. I suggest changing it to something clearer.\n7. It is also unclear how exactly the model chooses between two actions which have the same template, since they have the same relevance $\\delta$ value (which depends only on the template?). For example, in figure 3, I\u2019m not sure how the agent picks `Put emerald in case` over `Put jewels in case`. I think this paper is exploring a novel direction, has good empirical results and would be a valuable addition to the literature. Addressing the points mentioned above would further strengthen the paper and I\u2019m happy to raise my score if the authors can provide convincing responses.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "7elrjbem22b",
                "writer": "author",
                "reply_to": "iclr_2022_ZDaSIkWT-AP",
                "title": "General comments",
                "comment": " We thank all the reviewers for the overall positive feedback, the valuable suggestions, and the time spent on our manuscript. Following your comments, we believe we have significantly improved the paper with additional ablation studies and experiments. The main improvements we made to the paper are the following.\n\n* We investigated the performance gains obtained by different baseline agents when enhanced with case-based reasoning on the Jericho environment (**Reviewer eX6x**). The results (**Appendix B**, **Table 5**) show that CBR consistently boosts the performance of the neural agents, similarly to what we observed in TWC.\n* We evaluated different techniques to efficiently retrieve previous experiences from the case memory, including locality sensitive hashing, random projection, and sign random projection (**Reviewers Da5u and aw9P**). We found that the original VQ-based approach performs best and is more efficient (see **Appendix C**, **Table 6**).\n* We assessed alternative options for selecting which actions should be retained in the memory, such as sampling actions based on the TD error (**Reviewer Da5u**). We observed that the method based on the TD error could be a viable alternative, but our original implementation achieves the best results (see **Appendix D**, **Table 7**). \n* We reported the results obtained by a version of the CBR method that trains the agent and the retriever jointly (**Reviewer aw9P**). This requires changing the architecture of the neural agent and does not yield any noticeable improvement (see **Appendix E**, **Tables 8 and 9**)\n* We replaced our graph-based retriever with a multi-paragraph text-based implementation (as in Guo et al. 2020) to assess the effectiveness of the knowledge graph and the seeded graph attention (**Reviewer eX6x**). The results (**Appendix F**, **Table 10**) show that modeling the state as a knowledge graph allows better capturing contextual information compared to the text-based retriever.\n* We compared to additional baselines for Jericho (**Reviewer HscQ**), confirming that our approach outperforms the references given by the reviewer (see **Appendix I**, **Table 13**).\n* We clarified some points, added a discussion on out-of-distribution generalization (**Appendix H**, **Reviewer aw9P**), and made additional minor improvements.\n\nOverall, **all the additional experiments and analyses we included in the paper confirmed the effectiveness of our implementation**. We highly appreciated the opportunity to improve our work and we would like to request the reviewers to have a look at the updated version of our paper. We are looking forward to your feedback on our additional experiments and we hope that our new analyses will further convince you about our work.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yP5b-CKqNqp",
                "writer": "author",
                "reply_to": "euOAlQ66Iv5",
                "title": "Reply to Reviewer Da5u ",
                "comment": " We thank the reviewer for the thorough review and the valuable suggestions. We reply to each of your questions below. Also, please note that, besides the ablation studies you suggested, we performed several additional experiments that may still be of interest to you. Please see the updated version of the paper for more details.\n\n> **1.** In the retriever module, how is the threshold $\\tau$ chosen?\n\nThe main purpose of the threshold $\\tau$ is to avoid retrieving irrelevant context-action pairs from the memory (e.g., in the early training stages). In order to avoid this issue, we set $\\tau$ to a value that implies a large similarity between the query context and the retrieved ones. We opted to set $\\tau = 0.7$ based on preliminary experiments on a subset of the TWC games. \n\n> **2.** Algorithm 1 describes performing retrieval using multiple context selectors. How are these specified and how are they different from each other? [...]\n\nContext selectors need to be action-specific, as they are meant to represent the portion of the state that is relevant to the execution of a given action. Hence, we compute a different context representation for each valid action (we assume that the set of valid actions is given at each time step). In our implementation, this is achieved using a graph attention seeded at the entities that the action is applied to.\n\n> **3.** The \u2018Retain\u2019 module seemed a bit ad-hoc to me since the module stores actions with positive rewards only. [...] Why not [...] use a prioritization scheme based on TD error [...]? \n\nThanks for this very valuable suggestion. Indeed, our implementation is carefully designed for TBGs and works well for our use case, showing that in this context the main challenge lies in grounded language understanding and in the accurate modeling of the state of the game. However, we agree with you that other implementations of the retain module are possible. While the architecture described in Section 4 is specific to our use case, the algorithm described in Section 3 aims to be more general. Therefore, following your suggestion, we slightly changed the retain step to use a $\\textit{retain}$ function that selects which of the previous actions should be retained. Both our original implementation (storing the last $k$ actions) and your suggestion fall into this general formulation. Then, we performed an ablation study, where we compared different versions of the retain module. One of these variants samples the context-action pairs to be retained according to the TD error as you suggested. Overall, we found that our original implementation works best in practice, but the other variants also obtain competitive results. The results of the ablation study are reported in **Appendix D** (**Table 7**).\n\n> **4.** [...] Would just hashing the state (e.g. with LSH [...] work as well as the learned discretization scheme? [...]\n\nThanks again for another valuable comment! We performed an ablation study where we replaced the vector quantization with other techniques, like LSH, random projection and sign random projection. The results are reported in **Appendix C** (**Table 6**). Overall, we found that the vector quantization achieves better performance, but LSH also obtains good results. However, note that LSH requires storing the full continuous context representations as well, in order to detect false positives falling in the same bucket as the query context, but having small cosine similarity with it. This makes vector quantization a more efficient alternative.\n\n> **5.** [...] The paper mentions that the discretization scheme helps tackle the issue of changing representations over the course of training. However, [...] wouldn\u2019t that also potentially change over time? [...]\n\nThe discretization scheme helps to tackle the issue of representations changing over time, but the other reason why this works in practice is that the retriever is pretrained, as mentioned in Section 5. This minimizes large fluctuations in the continuous context representations and the discretization function further cancels small variations.\n\n> **6.** In table 3, the term \u201cwin rate/win count\u201d for the last row was confusing [...]\n\nThe terminology is taken from Guo et al. (2020). We agree that it can be confusing and we changed it in the updated version of the paper.\n\n> **7.** It is also unclear how exactly the model chooses between two actions which have the same template, since they have the same relevance [...]\n\nThe relevance $\\delta$ does not depend on the template. It depends on the similarity between the context representation stored in the memory and the current context. In the example in Figure 3, the agent picks the action \u201cPut emerald in case\u201d because the context in which that action was executed is more similar to the current context, where the action that will give positive reward is \u201cPut diamond in case\u201d.\n\nThanks for appreciating our paper. Hope our response further convinces you about our work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6FH7diWZ6vd",
                "writer": "author",
                "reply_to": "vqcohPwDVfl",
                "title": "Reply to Reviewer HscQ ",
                "comment": " We thank the reviewer for the feedback and for the time spent on our manuscript. We reply to the two main concerns in your review below.\n\n**Novelty.** To the best of our knowledge, we believe that case-based reasoning has not been explored to train RL agents in the way proposed in our  paper. This has also been mentioned by the other reviewers. _\u201cI think this paper is exploring a novel direction, has good empirical results and would be a valuable addition to the literature\u201d_ (**Reviewer Da5u**); _\u201cAs the related work shows, CBR to speed up RL has been tried before, however those methods are very different and not meant for the modern scale of deep RL [...] It would be beneficial to get RL practitioners to seriously consider adding CBR type approaches as a new type of strategy for boosting Deep RL\u201d_ (**Reviewer aw9P**); _\u201cThe core idea is well defined and motivated: let\u2019s get agents to use previous useful experiences to better improve the current policy. [...] The paper [...] presents a useful paradigm for thinking about text game agents and the results are interesting\u201d_ (**Reviewer eX6x**). Therefore, we politely disagree on the lack of novelty of our approach.\n\n\n**Comparison to additional baselines.** Thanks for the references to additional baselines relying on transformer-based LMs for Jericho. We have included a comparison with the models that you suggested in **Appendix I** (**Table 13**). Note  that our approach outperforms Trans-v-DRRN and DBERT-DRRN in the vast majority of the games. Moreover, Trans-v-DRRN fails to perform better than the baselines in Table 3 on 12 out of  the 15 games it was evaluated on. Similarly, DBERT-DRRN only performs better than the baselines on 4 games out of the 14 games for which the paper reports results.\n\nAnyway, we agree that more analyses are beneficial to the reader to have a sense of how the model works and where the performance gains come from. Hence, we performed several additional experiments and ablation studies. As an example, **Appendix B** shows the results obtained by different baselines when enhanced with CBR on a subset of the Jericho games. **Appendix C, D, E, and F** further discuss several additional experiments. \n\nWe hope that these additional analyses and our response will clarify some concerns and that you will reconsider your evaluation of our work.\n\nMinor comment: thanks for spotting the wrong reference to the A3C method. We fixed it!\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1hHcD6oRjpz",
                "writer": "author",
                "reply_to": "nsGrYH1ia33",
                "title": "Reply to Reviewer eX6x",
                "comment": " Thanks for the positive comments and constructive suggestions. We really appreciate reviews that can result in an opportunity to improve the paper with additional experiments. We reply to your main points below.\n\n> The \"Baseline agent + CBR\" comparisons are good to have but would probably be more useful to have in Jericho than TWC. [...]\n\nThanks for this comment. Following your suggestion, we included an experiment where we compare several baseline agents to their counterparts enhanced with CBR on a subset of the Jericho games. The results show that CBR consistently boosts the performance of the baseline agents on Jericho as well. More details are provided in **Appendix B** (**Table 5**)\n\n>  [...] It would be nice to see a comparison/ablation in just the retrieve portion to not use the knowledge graph and only text on Jericho [...]\n\nWe liked the suggestion to include a study where we replace the graph-based retriever with a text-based approach similar to Guo et al. 2020 (note that we are already comparing to this approach for Jericho, not for TWC). The ablation study (described in **Appendix F**) shows that the retriever that does not use the knowledge graph (but only text) achieves slightly worse performance than our original implementation. The results of the experiment are reported in **Table 10**. Please see the updated version of the paper for more details.\n\nFinally, we would like to point out that, following the suggestion of the other reviewers, we performed several additional analyses and ablations. For instance, in **Appendix C** we investigate alternative techniques for accessing the memory and **Appendix D** discusses alternative methods to select which context-action pairs should be retained. We believe these analyses could be of interest to you as well and should make it clear where the performance gains in our approach are coming from.\n\nWe appreciated the overall positive feedback on our work. Since you thought the main thing holding the paper back was the lack of analysis/ablations on portions of the proposed algorithm, we hope that our new analysis allays your concerns and you would reconsider your evaluation of our work.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bo9uyH9nbRR",
                "writer": "author",
                "reply_to": "saAdd8c9e4_",
                "title": "Reply to Reviewer aw9P",
                "comment": " Thanks for the overall positive feedback and the valuable comments. We appreciate your suggestions and the opportunity to further improve the paper.  We reply to your main questions below.\n\n> **a.** Comparing the 2 equations in seeded graph attention, it looks like the h's dependency on $\\alpha$ is quadratic [...]\n\nThe $\\alpha$ in the first equation is only meant to do a weighted average over the $\\beta$ coefficients of the neighbors. This is inspired by the similar architecture of Sun et al. [1], who used a directed message propagation for open-domain question answering and employed the attention coefficients in the same way.\n\n> **b.** Why sum for the final representation and not average? it could lead to wierd biases when the number of entities are different between states.\n\nWe only sum on the representations of the seed entities. These entities are taken from each admissible action separately, to produce different action-specific representations. Hence, in practice, the number of entities is only 2 or 1, and we pass the result to a FFN.\n\n> **c.** The VQ approach is interesting, but I am a bit skeptical that it is necessary. [...] Can you share some quantitative measurements to show that this code splitting approach is necessary?\n\nThanks for the valuable suggestion! We performed an ablation study where we compare the vector quantization with other techniques, including random projection, sign random projection and locality sensitive hashing. The results of the experiment are reported in **Appendix C** (**Table 6**) and show that our VQ-based implementation achieves better results compared to the other approaches.\n\n> **d.** The claim that CBR lets you generalize OOD seems not supported and my intuition is almost the opposite. Can you elaborate?\n\nThanks for this comment. We have made this point more clear by adding an Appendix that explains why case-based reasoning obtains good OOD generalization. In short, the reason is that it forces the agent to map contexts from the OOD test set to the most similar contexts in the training set and effectively reuses the retrieved experiences to detect the most viable valid action in the OOD setting. More details are provided in **Appendix H**.\n\n> **e.** The CBR and neueral agent seem to be separately trained. Can they be jointly optimized [...]?\n\nYes, we had already tried training the CBR retriever and the neural agent jointly. However, this does not yield any performance gain and requires changing the architecture of the agent to make it aware of the action candidates produced by the CBR, so that this information can be included in the action scoring mechanism. For these reasons, we switched to the implementation described in the main paper. We have included in **Appendix E** the results obtained by optimizing the retriever and the neural agent jointly, both on TWC (**Table 8**) and Jericho (**Table 9**)\n\n> **f.** Fig 4: it would be more interesting if this could show the \"counterfactual\" probabilties. [...] what if the neural agent had been allowed to execute, would it still have been successful?\n\nThanks for this suggestion. We have incorporated the analysis you suggested in Figure 4.\n\n> **g.** Is locality sensitive hashing another way to do efficient retrieval that can generalize?\n\nYes, see the reply to point c and Appendix C for more details.\n\n**Clarity.** Thanks for the comments!\n\n> **a.** when you say \"the final set of all retrieved actions and the corresponding relevance\", can you be more precise [...]?\n\nThanks for the suggestion, we described more formally the set of retrieved actions in Section 3 (although the precise data structure is already shown in Alg. 1)\n\n> **b.** sec 4.3: \"applied to the entities\": how does it choose which entities to use? \n\nIn Sec 4.3, the reused action is applied to the same entities mentioned in the action used to build the context selector ($\\mathcal{V}_{a_t}$).\n\n> **c.** Please elaborate on the Adolphs and Hofmann method, even though it's not an original contribution it is a core part of your system. \n\nThe method used to train our model (from Adolphs & Hofmann) is explained in **Appendix A**.  We included a reference to Appendix A in Section 5.\n\n> **d.** The jump from 24% to 73% in win rate on Jericho seems really big ! You should highlight it more\n\nThanks, we have highlighted this in Section 6.3!\n\nThanks for appreciating our paper and our empirical results. Hope our response further convinces you about our work.\n\n\n\n[1] Haitian  Sun,  Bhuwan  Dhingra,  Manzil  Zaheer,  Kathryn  Mazaitis,  Ruslan  Salakhutdinov,  and William Cohen. Open domain question answering using early fusion of knowledge bases and text. EMNLP 2018.\n",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This work",
                "Sentiment Expression": "insightful and practical",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "GjWDguPZRmr": {
        "paper_id": "nips_2022_GjWDguPZRmr",
        "paper_title": "Improving Variational Autoencoders with Density Gap-based Regularization",
        "paper_abstract": "Variational autoencoders (VAEs) are one of the most powerful unsupervised learning frameworks in NLP for latent representation learning and latent-directed generation. The classic optimization goal of VAEs is to maximize the Evidence Lower Bound (ELBo), which consists of a conditional likelihood for generation and a negative Kullback-Leibler (KL) divergence for regularization. In practice, optimizing ELBo often leads the posterior distribution of all samples converging to the same degenerated local optimum, namely posterior collapse or KL vanishing. There are effective ways proposed to prevent posterior collapse in VAEs, but we observe that they in essence make trade-offs between posterior collapse and the hole problem, i.e., the mismatch between the aggregated posterior distribution and the prior distribution. To this end, we introduce new training objectives to tackle both problems through a novel regularization based on the probabilistic density gap between the aggregated posterior distribution and the prior distribution. Through experiments on language modeling, latent space visualization, and interpolation, we show that our proposed method can solve both problems effectively and thus outperforms the existing methods in latent-directed generation. To the best of our knowledge, we are the first to jointly solve the hole problem and posterior collapse.",
        "paper_acceptance": "Accept",
        "meta_review": "The paper addresses the KL collapse of VAE models by proposing a new regularization. Reviewers generally acknowledge the novelty of the work and have the tendency of recommending acceptance.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "jJ_2Sx3V7uj",
                "writer": "author",
                "reply_to": "szl37BZVkRl",
                "title": "Response to Reviewer FTv2",
                "comment": " Thank you very much for the encouraging comment! We are glad that you found our reply helpful.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "szl37BZVkRl",
                "writer": "official_reviewer",
                "reply_to": "3So4PCaGdfU",
                "title": "Thanks for your response",
                "comment": " Thanks to the authors for their response to my review (and apologies for my late response to the authors). I found the response helpful -- specifically, the authors' comments on runtime have given me a better idea about the practicality of the proposed approach.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "44mMCswbSm4",
                "writer": "author",
                "reply_to": "nOYQlXDKHP",
                "title": "Response to Reviewer xN7d",
                "comment": " Thank you very much for increasing the score and for the insightful feedback! We have updated the paper in the revised version to clarify all the points raised, including using the standard VAE notation and adding the citations from your review.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nOYQlXDKHP",
                "writer": "official_reviewer",
                "reply_to": "Tx1jdhD5yUH",
                "title": "Answer",
                "comment": " I upgrade my grade from 7 to 8. It would be nice to update the paper to clarify these points + add the citations from my review.\n\nAlso, I think the paper would be more clear without these two X and Y RVs, and move to the \"standard\" notation for VAE, as this is sufficient for the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jbsntyREtGe",
                "writer": "author",
                "reply_to": "GVwwHw6OLs",
                "title": "Response to Reviewer rX6Q",
                "comment": " Thanks for your comment! We approximated the test set log likelihoods with $16$ importance weighted samples of $\\mathbf{z}$ from both the variational distribution and prior distribution for each datapoint $\\mathbf{x}$ (i.e., $8$ samples from each distribution), which yields robust approximations empirically. Please see also the newly provided Appendix C in the revised version for the detailed sampling process. To give more details, we conducted evaluations for all models across $10$ different random seeds under this setting and reported the mean values of log likelihoods at the precision of $0.1$, where the variances are all less than $0.01$.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "GVwwHw6OLs",
                "writer": "official_reviewer",
                "reply_to": "WFhw_INFmyx",
                "title": "Increased score",
                "comment": " Thanks to the authors for addressing the points in the review; I am happy to increase my score.\n\nRegarding the number of samples in the evaluations, I did not mean the number of data points but rather how many samples of $\\mathbf{z}$ from the variational distribution are used in order to approximate the test set log likelihoods?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WFhw_INFmyx",
                "writer": "author",
                "reply_to": "e6gW1OGY6-",
                "title": "Response to Reviewer rX6Q",
                "comment": " Thank you very much for your positive feedback and constructive criticism! In the following we address it point by point.\n\n> **Q:**\n\t\"Throughout, the use of $\\mathbf{x}$ and $\\mathbf{y}$ to refer to instances of the same variable is confusing. Why not just use x throughout as is commonly done in the literature, avoiding the introduction of unnecessary notation?\"\n\n**A:**\n\tWe apologize for the confusion on our notations. We actually followed the notations used in [1], which used two variables $\\mathbf{x}$ and $\\mathbf{y}$ to represent the input and output of VAEs. Indeed, we agree that it will be a much better choice to use $\\mathbf{x}$ only, which follows the standard formulation of VAEs. We have corrected the notations in the revised version of our paper.\n\n> **Q:**\n\t\"Although it is clear why we would expect the proposed method to solve the hole problem, it is not explained why we would expect it to solve posterior collapse? Surely, as with the vanilla VAE, there is a local optimum where $q_{\\phi}(\\mathbf{z}|\\mathbf{x})=p(\\mathbf{z})\\forall\\mathbf{x}$?\"\n\n**A:**\n\tThanks for your question. The proposed objective seeks to optimize both the log-likelihood of data and the sum of marginal mutual information between the latent variable and the data. With such an objective design, the posterior collapse (or KL vanishing) can be solved effectively as the mutual information sub-objective is a lower bound of the KL divergence term in ELBo according to Hoffman et al.'s formulation.\n\n> **Q:**\n\t\"When reading the paper from beginning to end in order, it is unclear what the point is of Equation (3) and the paragraph just preceding it. I believe the clarity would be improved were this part moved to the part before Equation (8).\"\n\n**A:**\n\tThanks for your valuable suggestion. We have made changes according to your suggestion in the revised version.\n\n> **Q:**\n\t\"In Figure 1, should the $p_{\\phi}$ terms not be $q_{\\phi}$?\"\n\n**A:**\n\tThanks for pointing out the typo, which has been fixed in our revised version.\n\n> **Q:**\n\t\"Are the results in Table 2 computed on the test set?\"\n\n**A:**\n\tYes, we computed the results in Table 2 on the test set following prior work [2].\n\n\n> **Q:**\n\t\"How many samples are used for the evaluation?\"\n\n**A:**\n\tWe used the full test set for evaluation. The numbers of samples of different datasets are given in Table 1.\n\n\n> **Q:**\n\t\"Why not report the (importance weighted) ELBO taken with a large number of samples, as is commonly done in the literature?\"\n\n**A:**\n\tWe computed the prior Log-Likelihood $priorLL(\\theta)$, which shares the same information with the Negative Log-Likelihood (NLL) estimated by (importance weighted) ELBO.\n\n\n> **Q:**\n\t\"The examples shown in Figures 6 and 7 are extremely short sentences, and at least qualitatively it is not clear that the DG-VAE is better than the $\\beta$-VAE\"\n\n**A:**\n\tThanks for your valuable suggestion. We have added cases of long sentences on each dataset in our revised version, and highlighted tokens of the longest common subsequences for clear comparisons. \n\n\n> **Q:**\n\t\"Why only show examples of the $\\beta$-VAE and not the other baselines?\"\n\n**A:**\n\tWe only compared our method with $\\beta$-VAE(0.1) in the case study as it is the best overall performing baseline model according to the automatic metrics. We will add additional results from other models in the revised version to address your comment.\n\n\n> **Q:**\n\t\"Is your method the first which intends to jointly solve the hole problem and posterior collapse?\" & \"If so, this would be a valuable statement to add.\"\n\n**A:**\n\tThanks for your valuable suggestion. To the best of our knowledge, we are indeed the first to jointly solve the hole problem and posterior collapse. We have added this statement in our revised version.\n\n\n> **Q:**\n\t\"Arguably, the most important limitation of the proposed method is that the training objective is no longer a lower bound on the true log-likelihood of the data. Does this mean that the model is no longer suitable for tasks such as density estimation, out-of-distribution detection, etc. which the vanilla VAE is otherwise useful for?\"\n\n**A:**\n\tThanks for your suggestions, we don't have a definite answer to the question but we do agree that these are very intersting directions for exploration.\n\n\n[1] Yu W, Wu L, Zeng Q, et al. Crossing Variational Autoencoders for Answer Retrieval[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 5635-5641.\n\n[2] Zhu Q, Bi W, Liu X, et al. A Batch Normalized Inference Network Keeps the KL Vanishing Away[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 2636-2649.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3So4PCaGdfU",
                "writer": "author",
                "reply_to": "Pe6Ro9En2Co",
                "title": "Response to Reviewer FTv2",
                "comment": " Thank you for the detailed and insightful review. Below, we address your points individually.\n\n> **Q:**\n\t\"The interpolation study shows is interesting and the proposed approach seems to be better than the baselines on the chosen metric for interpolation. However, this metric is not directly related to the mutual information between latent variables and text or the diversity one can expect from the samples of the generative model.\"\n\n**A:**\n\tThanks for your comment. Empirically, we do observe that the metric for interpolation (i.e., Rouge-L F1-score) has a strong correlation with the mutual information between latent variables and text. This is intuitive as the generated sentences tend to be irrelevant to the ground truth when the mutual information is poor, hence leading to a low Rouge-L F1-score (and vice versa).\n\n> **Q:**\n\t\"The only metric BN-VAE does slightly worse is CU, and I am unsure how to interpret this unconventional metric.\"\n\n**A:**\n\tWe propose CU to quantify the severity of the hole problem, by measuring the degree of matching between the aggregated posterior distribution and the prior distribution in a dimension-wise perspective. A lower value of CU indicates a larger mismatch between these two distributions, and hence a severer hole issue.\n\n> **Q:**\n\t\"The proposed approach also is more expensive due to the quantities involved with the aggregated posterior. A comparison of runtime with respect to other approaches would be helpful.\" & \"Please address the runtime of the proposed approach.\"\n\n**A:**\n\tThanks for your insightful comment! Indeed, our approach is a bit more expensive than the baseline models due to the density gap-based regularization. However, this additional computational cost is very affordable. For instance, we compared the training time of our model and the vanilla VAE based on the default setting of batch size $|B|=32$, latent dimension $Dim=32$, and the number of samplings $M=32$ for Monte Carlo approximation in Eq. 9 and Eq. 11. The averaged training time of our model (over all experimental datasets) is only $11\\\\%$ higher than that of the vanilla VAE. We will include run time anlaysis results of our model and the baselines in the revised version of our paper.\n\n> **Q:**\n\t\"some notes on the presentation: Line 53: q(z|x) instead of y, rather x and y are confusing, its the same thing Indicator operator used for MI Line 82 Text in math is poorly formatted\"\n\n**A:**\n\tWe apologize for the confusion on our notations. We actually followed the notations used in [1], which used two variables $\\mathbf{x}$ and $\\mathbf{y}$ to represent the input and output of VAEs. Indeed, we agree that it will be a much better choice to use $\\mathbf{x}$ only, which follows the standard formulation of VAEs. We have corrected the notations in the revised version of our paper.\n\n> **Q:**\n\t\"Some related work: VAE with a VampPrior, Tomczak et al. Learning to Explain: An Information-Theoretic Perspective on Model Interpretation, Chen et al.\"\n\n**A:**\n\tThanks for your valuable sharing. We have added these related work to appropriate places in our revised version.\n\n\n[1] Yu W, Wu L, Zeng Q, et al. Crossing Variational Autoencoders for Answer Retrieval[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 5635-5641.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Tx1jdhD5yUH",
                "writer": "author",
                "reply_to": "FBUX9rf-Ta",
                "title": "Response to Reviewer xN7d",
                "comment": " Thanks a lot for your positive review and valuable feedback. Here are our answers for your questions:\n\n> **Q:**\n\t\"not sure to understand whether the proposed objective is still a lower bound to the log-likelihood\" & \"is the proposed objective still a lower bound to the log-likelihood?\"\n\n**A:**\n\tThanks for your question. The proposed objective is no longer a lower bound as it seeks to optimize both the log-likelihood of data and the sum of marginal mutual information between the latent variable and the data. With such an objective design, the posterior collapse (or KL vanishing) can be solved effectively as the mutual information sub-objective is a lower bound of the KL divergence term in ELBo according to Hoffman et al.'s formulation.\n\n\n> **Q:**\n\t\"weird notation, e.g. the use of X and Y variables, it should be X only, I don't understand why to introduce a second RV for the same observation.\"\n\n**A:**\n\tWe apologize for the confusion on our notations. We actually followed the notations used in [1], which used two variables $\\mathbf{x}$ and $\\mathbf{y}$ to represent the input and output of VAEs. Indeed, we agree that it will be a much better choice to use $\\mathbf{x}$ only, which follows the standard formulation of VAEs. We have corrected the notations in the revised version of our paper.\n\n\n> **Q:**\n\t\"Also, q(n) is the same as p(n), no? Why a proposal distribution here? It is fixed(?)\"\n\n**A:**\n\tThanks for pointing out the typo. We have fixed this typo by changing $p(\\mathbf{x}=x_n)$ to $q_{\\phi}(n)$ to in the revised version of our paper. As stated in line 64, $n$ is the index (or identity) of datapoints whose posterior distributions compose the aggregated posterior distribution, so it is fixed to the discrete uniform distribution $q_{\\phi}(n) \\equiv \\frac{1}{N}$.\n\n\n> **Q:**\n\t\"l. 123: I don't understand what is the point of stating that \"we only consider z \\in ...\" => q(z) is Gaussian, so we can't have q(z) = 0 anyway, no?\"\n\n**A:**\n\tBesides the commonly used Gaussian distribution, we also consider von Mises-Fisher (vMF) distribution and propose corresponding variants, e.g., DG-vMF-VAEs. In theory, we can have $q_{\\phi}(z)=0$ in DG-vMF-VAEs, but the domain of $DG(\\theta,\\phi;z)$ is $\\\\{z|q_{\\phi}(z)>0\\\\}$, as we only need to compute this term for samples from $q_{\\phi}(\\mathbf{z})$, as stated in Equation 7. That is why we stated that we only consider $z \\in \\\\{z|q_{\\phi}(z)>0\\\\}$.\n\t\n\n> **Q:**\n\t\"in the end, the objective is only a combination of ELBo + mutual information? Could you explain a little bit why is this novel? Especially, what is the difference with [1]?\"\n\n**A:**\n\tOur proposed model seeks to optimize both the log-likelihood of data and the sum of marginal mutual information between the latent variable and the data. The key novelties of our models are twofold: (1) in contrast to existing models such as Adversarial Autoencoder [2] whose regularizer is merely based on sampling sets, and thus is sub-optimal, our model innovatively takes the perspective of PDFs, as we discuss in line 111, which is proved to form a continuous latent space that matches the prior much better, as illustrated in Appendix A; (2) for Gaussian settings, we present a regularizer for modelling more aggressive mutual information between the latent variable and data by imposing regularisation over marginal distributions over each dimension of the latent variable, as we introduce in line 159. This intends to make full use of the latent dimensions instead of only activating part of them.\n\nAccording to [3], our training objective differs from that of Asymmetric MIM mainly on the following two points: (1) MIM introduces parameterized approximate priors on data and latent variables to avoid the need for unstable adversarial training and the estimation of mutual information, while our method only adopts the anchor distributions (following vanilla VAE) and replaces adversarial training (as Adversarial Autoencoder [2] does) with our Density-Gap based regularizer; (2) Asymmetric MIM maximizes the mutual information between the data distribution and the latent variable distribution, which is similar to our Equation 10, but we further extend this to the sum of mutual information between the data distribution and the marginal distributions over each dimension of the latent variable (as illustrated in our Equation 13), so as to capture richer mutual information.\n\n\n> **Q:**\n\t\"The following citations are missing and they provide a broader view on how researchers tackle the problem in the NLP community:\"\n\n**A:**\n\tThanks for your valuable sharing. We have added these related work to appropriate places in our revised version.\n\n\n[1] Yu W, Wu L, Zeng Q, et al. Crossing Variational Autoencoders for Answer Retrieval[C]//ACL. 2020: 5635-5641.\n\n[2] Makhzani A, Shlens J, Jaitly N, et al. Adversarial autoencoders[J], 2015.\n\n[3] Livne M, Swersky K, Fleet D J. MIM: Mutual Information Machine[J], 2019.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Pe6Ro9En2Co",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_GjWDguPZRmr",
                "title": "",
                "comment": " This paper addresses posterior collapse in VAE and also tries to mitigate the issue with many existing solutions for this which is the trade off with poor fit to the prior.\n\nThe authors propose a novel regularization to substitute the KL regularization in ELBo for VAEs, which is based on the density gap between the aggregated posterior and the prior. Since quantities related to aggregated posterior need for the regularizer depend on the whole dataset are expensive to compute, this paper further changes the objective to consider aggregation over minibatches only. \n\nThis regularizer maximizes the ELBO as well as the mutual information between the input and the latent variable.\n\nFor Gaussian settings, the authors present a regularizer for more aggressive mutual information between the latent variable and data by imposing a regularizer over marginal distributions over each dimension of the latent variable.\n\nThe empirical comparison is done with relevant baselines on text datasets.\n The paper is sound and the baselines are well chosen covering a range of related work.\n\nThe technical contribution hinges very heavily on the paper by Hoffman et al.'s formulation, but I believe the proposed regularizer is novel.\n\nThe interpolation study shows is interesting and the proposed approach seems to be better than the baselines on the chosen metric for interpolation. However, this metric is not directly related to mutual information between latent variables and text or the diversity one can expect from the samples of the generative model.\n\nMy main concern is that the main empirical results show that baseline/competing models are as good/better than the proposed method -- especially BN-VAE. The only metric BN-VAE does slightly worse is CU, and I am unsure how to interpret this unconventional metric.\n\nThe proposed approach also is more expensive due to the quantities involved with the aggregated posterior. A comparison of runtime with respect to other approaches would be helpful.\n\nSome related work:\nVAE with a VampPrior, Tomczak et al.\nLearning to Explain: An Information-Theoretic Perspective on Model Interpretation, Chen et al. some notes on the presentation:\nLine 53: q(z|x) instead of y, rather x and y are confusing, its the same thing\nIndicator operator used for MI\nLine 82\nText in math is poorly formatted\n Please address the runtime of the proposed approach.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "FBUX9rf-Ta",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_GjWDguPZRmr",
                "title": "",
                "comment": " This paper focuses on a well-known problem in variational auto-encoders:\u00a0the learned latent representation often has \"gaps\" in its prior, i.e. there are latent samples with high prior PDF values that fail to generate coherent outputs.\n\nThe authors analyse the problem and propose a novel objective to fix the issue that combines the ELBO with a mutual information term. Importantly, the proposed surrogate objective is well-motivated. **Strengths**\n\n- important problem in deep generative modeling\n- contribution is interesting and well-motivated\n- good experimental section\n\n**Weaknesses**\n\n- not sure to understand whether the proposed objective is still a lower bound to the log-likelihood\n- weird notation, e.g. the use of X and Y variables, it should be X only, I\u00a0don't understand why to introduce a second RV for the same observation. Also, q(n) is the same as p(n), no? Why a proposal distribution here? It is fixed(?)\n\n**Missing citations**\n\nThe following citations are missing and they provide a broader view on how researchers tackle the problem in the NLP community:\n- SentenceMIM: A Latent Variable Language Model (Micha Livne, Kevin Swersky, David J. Fleet)\n- A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text (Bohan Li, Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick, Yiming Yang)\n- Preventing posterior collapse in variational autoencoders for text generation via decoder regularization (Alban Petit, Caio Corro)\n- Preventing Posterior Collapse with Levenshtein Variational Autoencoder (Serhii Havrylov, Ivan Titov) - is the proposed objective still a lower bound to the log-likelihood?\n- l. 123: I don't understand what is the point of stating that \"we only consider  z \\in ...\" => q(z) is Gaussian, so we can't have q(z) = 0 anyway, no?\n- in the end, the objective is only a combination of ELBo + mutual information? Could you explain a little bit why is this novel? Especially, what is the difference with [1]?\n\n[1] MIM: Mutual Information Machine (Micha Livne et al.) nothing to report\n",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "e6gW1OGY6-",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_GjWDguPZRmr",
                "title": "",
                "comment": " This paper proposes a unified solution to two problems which can occur when training VAEs - the hole problem (where the aggregated variational distribution fails to fit the prior) and posterior collapse (where the variational distribution becomes the same for every data point and is therefore uninformative).\n\nThe proposed solution is a modification to the VAE's training objective, where the per-data-point KL divergence term is replaced by the KL divergence from the aggregated posterior to the prior. This aggregate KL divergence term is expressed as the sum of KL divergences over the dimensions of the latent variable.\n\nThe authors compare their method to various baselines, all of which were designed to solve the posterior collapse issue. The proposed method appears to address the posterior collapse problem, while suffering the hole problem to a lesser extent than the baselines. Strengths:\n- This is a conceptually simple method which appears to be effective at solving both the hole problem and posterior collapse when training VAEs.\n- The authors do a good job at explaining the hole problem and posterior collapse, as well as the apparent trade-off between the two.\n    - Figure 1 is particularly clear for this.\n- The empirical results appear to outperform the baselines, in terms of having all of the latent dimensions being both 'active' and 'consistent' (as defined in Appendix C), as well as the latent representations and observations having a high amount of mutual information.\n    - The latent space visualisations appear to indicate that the proposed method clearly outperforms the baselines at solving both the hole problem and posterior collapse.\n    - In addition, the proposed method appears to consistently outperform baselines at generating interpolations between sentences.\n- Although this may not be the most significant advance in generative modelling by itself, it does seem to be a piece of work that the community could easily build on in order to make incremental advances in the field.\n\n\nWeaknesses:\n- Although the authors explain the hole problem and posterior collapse well, the explanation of their actual method difficult is to follow. Some examples include:\n    - Throughout, the use of $\\mathbf{x}$ and $\\mathbf{y}$ to refer to instances of the same variable is confusing. Why not just use $\\mathbf{x}$ throughout as is commonly done in the literature, avoiding the introduction of unnecessary notation?\n    - Although it is clear why we would expect the proposed method to solve the hole problem, it is not explained why we would expect it to solve posterior collapse? Surely, as with the vanilla VAE, there is a local optimum where $q_{\\phi}(\\mathbf{z}|\\mathbf{x}) = p(\\mathbf{z}) \\forall \\mathbf{x}$? \n    - When reading the paper from beginning to end in order, it is unclear what the point is of Equation (3) and the paragraph just preceding it. I believe the clarity would be improved were this part moved to the part before Equation (8).\n    - In Figure 1, should the $p_{\\phi}$ terms not be $q_{\\phi}$?\n- The evaluation in the experiments section is not entirely convincing.\n    - There are several missing details, e.g.\n        - Are the results in Table 2 computed on the test set?\n        - How many samples are used for the evaluation?\n        - Why not report the (importance weighted) ELBO taken with a large number of samples, as is commonly done in the literature?\n    - The interpolation task used to measure the ability of the model to do latent-guided generation isn't totally convincing.\n        - The examples shown in Figures 6 and 7 are extremely short sentences, and at least qualitatively it is not clear that the DG-VAE is better than the $\\beta$-VAE.\n        - Why only show examples of the $\\beta$-VAE and not the other baselines?\n\nUPDATE: Increased score post rebuttal. - Is your method the first which intends to jointly solve the hole problem and posterior collapse?\n    - If so, this would be a valuable statement to add.\n\nOther questions included in the strengths and weaknesses section. - The authors do not discuss the limitations of their method.\n    - Arguably, the most important limitation of the proposed method is that the training objective is no longer a lower bound on the true log likelihood of the data. Does this mean that the model is no longer suitable for tasks such as density estimation, out-of-distribution detection, etc. which the vanilla VAE is otherwise useful for?",
                "rating": 5,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "addresses the KL collapse of VAE models by proposing a new regularization",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the novelty of the work",
                "Sentiment Expression": "Reviewers generally acknowledge",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "_A4-JP8d_f": {
        "paper_id": "nips_2021__A4-JP8d_f",
        "paper_title": "Challenges and Opportunities in High Dimensional Variational Inference",
        "paper_abstract": "Current black-box variational inference (BBVI) methods require the user to make numerous design choices \u2013 such as the selection of variational objective and approximating family \u2013 yet there is little principled guidance on how to do so. We develop a conceptual framework and set of experimental tools to understand the effects of these choices, which we leverage to propose best practices for maximizing posterior approximation accuracy. Our approach is based on studying the pre-asymptotic tail behavior of the density ratios between the joint distribution and the variational approximation, then exploiting insights and tools from the importance sampling literature. Our framework and supporting experiments help to distinguish between the behavior of BBVI methods for approximating low-dimensional versus moderate-to-high-dimensional posteriors. In the latter case, we show that mass-covering variational objectives are difficult to optimize and do not improve accuracy, but flexible variational families can improve accuracy and the effectiveness of importance sampling \u2013 at the cost of additional optimization challenges. Therefore, for moderate-to-high-dimensional posteriors we recommend using the (mode-seeking) exclusive KL divergence since it is the easiest to optimize, and improving the variational family or using model parameter transformations to make the posterior and optimal variational approximation more similar. On the other hand, in low-dimensional settings, we show that heavy-tailed variational families and mass-covering divergences are effective and can increase the chances that the approximation can be improved by importance sampling. \n",
        "paper_acceptance": "accept",
        "meta_review": "This paper is concerned with some broad questions in variational inference, namely what variational families should be used (e.g. light-tailed, heavy-tailed, flows) what divergence should be optimized (e.g. inclusive KL, exclusive KL) and how performance could be diagnosed after inference is complete. On a strict technical level, this paper appears to contain few contributions. Nevertheless, reviewers were overall positive about the paper's attempt to establish and support some grand (albeit somewhat informal) themes. These are 1) that mode-seeking divergences are easier to optimize than mode-spanning divergences 2) that this difficulty can be understood by considering the polynomial dependence of the divergence on importance weights and 3) that one can fit a generalized Pareto distribution and use the k statistic to diagnose inference success.\n\nOne weakness identified by several reviewers agreed on was an inadequate discussion of prior work. The paper would be stronger with a better review of prior work related to all three of the themes, here, i.e. on the difficulty of optimizing different divergences, on using different variational families for VI and on inference diagnostics. There are some citations now, but with somewhat cursory discussions. In particular, the paper would be stronger if it was self-contained so that someone not familiar with the PSIS framework can follow it. The authors have agreed to expand their discussion of prior work.\n\nReviewers also had some specific comments about the experimental results (both what was done and the presentation). The authors have also been receptive to this feedback.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "LYDPySY16cM",
                "writer": "author",
                "reply_to": "zqw3284Ysb",
                "title": "related work",
                "comment": " Apologies for the oversight! We will be sure expand our discussion of prior work. While we borrow some ideas from [14,30], the goal of our paper is quite different: to understand the trade-offs and limitations of different choices for variational objective and approximating family. That is, we aim to provide guidance to end-users *before* they run VI, whereas [14,30] primarily* aim to provide accuracy diagnostics for use *after* running VI. In addition, [14,30] focus on low-dimensional examples while our focus is on approximation high-dimensional posterior distributions.\n\n*[14] does recommend using heavy-tailed approximations and [30] does discuss simulation-based calibration. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ReelISzRX_",
                "writer": "author",
                "reply_to": "L3uYsU1qmN",
                "title": "response",
                "comment": " Thank you very much for your reply, comments, updated score and review.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zqw3284Ysb",
                "writer": "official_reviewer",
                "reply_to": "QF8BBwJalXQ",
                "title": "Response",
                "comment": " Thank you for your response. After reading it along with the other reviews and responses I maintain that this is a good paper worthy of acceptance. However, I note that the authors did not reply to my point about the lack of an in-depth treatment of related work, which was really my main issue with the paper. I highly encourage the authors to consider adding this. Nevertheless, this is a good paper and I look forward to seeing how it turns out.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Ms_2rnwN8F",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__A4-JP8d_f",
                "title": "",
                "comment": "The authors propose a conceptual framework and design guidelines for variational inference based on the pre-asymptotic tail behaviour of the unnormalized importance weight. This framework is used to study the behaviour of commonly variational families and f-divergences in mid-to-high dimensions. The authors observe that mass-covering divergences, while often superior theoretically and for low dimensional problems, are hard to optimize in mid-to-high dimensions compared to the mode-seeking exclusive KL-divergence. These observations are consistent with the prediction made using their framework based on estimating the tail-index of the best-fit generalized pareto distribution.  **Originality and Significance:**\nAs far as I can tell, the proposed framework does not contain novel contributions on a technical level, i.e. the work builds heavily on previous work on pareto smoothed importance sampling (PSIS) [1] and PSIS based diagnostics [2] (as adequately cited by the authors), and the authors use standard estimators for the different f-divergences and their gradients.\nThat said, I believe there is value, especially for new partitioners, in presenting the above in a coherent and unified framework. The presented framework makes it easy to study the behaviour of commonly used variational families and divergences and helps making educated predictions based on the order of the (estimated) tail index and f-divergence. Hence, I do believe the setup and presentation are original and valuable for the community.\n\n**Clarity:**\nThe writing of the paper is clear and easy to follow.\nOne thing that I noticed is that the authors do not explain how to actually use the PSIS correction to improve the estimator. Adding a paragraph explaining this would make the paper more self-contained and enable readers not familiar with [1, 2] to better understand the experiments.\n\n**Quality:**\nOverall, the methodology is technically sound and the observations and conclusions drawn by the authors seems correct but sometimes slightly to general. \n\nSpecifically, the statements P1, P2 made in section 3.3 are too general to be supported by the actual experiments.\n- While *P1* mention 'mode-seeking'-divergences and 'mass-covering'-divergences the experiments only evaluate the exclusive KL-divergence as an example for 'mode-seeking' and the inclusive KL-divergence and $\\chi^2$-divergence as examples for 'mass-covering' divergences. The experiments are still insightful, but the claim should be less general to be supported by the actual experiments.\nThe main text mentions additional results for the other divergences in the appendix, however, the appendix does only include additional results for the evaluation presented in Figure 3c, but not for for the evaluations presented in Figure 3a and 3b.\n- The claim made in *P2* would be better supported if the plot in the main text would include the results for the $1/2$-divergence (mentioned on Line 194) and the importance weight $w$ itself. The corresponding plot for Figure 3c can be found in the appendix (Figure B.1.). \n   - Is there a reason why these results are not reported? \n   - For each of the line plots in Figure 3, what was the variational objective used for training? The figures in the appendix are explicit about what variational objective was used. It would be helpful to mention this in the figure caption in the main text as well. \n  - For clarity it would also be helpful to include the degree of the variational objective in the legend similar to the figure in the appendix.\n\nIt is also unclear to me if the results are averaged over multiple independently trained models. Given that this paper consideres non-convex stochastic optimization, I consider this necessary to reach any conclusions. Specifically,\n- Figure 4, Figure 6 and Table 1 do not report any metric of variability and I can not find any statement in the text mentioning if the presented values are averages over multiple runs.\n- How are the confidence intervals computed in Figure 2?\n- How is the reported variance computed in Figure 3?\n\nSome clarification would be helpful here.\n\n\n**Additional Questions and Comments:**\n- Regarding the intuition gained by observing the distance from the mode (Figure 1): The overlap between the distribution over distances from the mode is already less for the inclusive KL-divergence in 2D and this difference seems to carries over to the higher dimensional examples.  Even the 2D illustration does not give the impression that the approximation based on the inclusive KL would do better w.r.t. this metric (overlap between distributions over distances). For other metrics, i.e. the variance of the resulting importance weight, I'd agree that the approximation based on the inclusive KL-divergence seems preferable. Overall, I'm not sure how Figure 1 demonstrates that 'the benefits of heavy-tailed approximate families and divergences favoring mass-covering behaviour diminishes as the dimensionality of the target distribution increases'.\nAm I missing something here?\n- Figure 1; caption: The approximation is red for the exclusive KL and green for the inclusive KL but the caption only mentions that the approximation is shown in red.\n- Line 38, 29; some words seem to be missing in that sentence. 'the benefits of heavy-tailed approximate families and divergences favoring mass-covering *behavior* diminishes as *the* dimensionality of the target distribution increases.\n- Line 205: I think this should reference Figure 3a instead of Figure 2\n\n\n*[1] Aki Vehtari, Daniel Simpson, Andrew Gelman, Yao Yuling, and Jonah Gabry. Pareto smoothed importance sampling. arXiv preprint arXiv:1507.02646, 2019.*\n\n*[2] Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but did it work?: Evalu- ating variational inference. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5581\u20135590. PMLR, 2018.*\n adequate",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "L3uYsU1qmN",
                "writer": "official_reviewer",
                "reply_to": "joh-6lJZ5P",
                "title": "Response",
                "comment": " Thank you very much for your detailed answers.\n\nAfter reading the other reviews and responses, it seems to me that the rebuttal addresses most concerns. For my part, I'm happy with the rebuttal and will increase my score, recommending to 'accept' the paper, accordingly.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "joh-6lJZ5P",
                "writer": "author",
                "reply_to": "Ms_2rnwN8F",
                "title": "Reply to reviewer",
                "comment": " We thank the reviewer for the thorough review, feedback and useful comments. We will include a section on PSIS. Here, we reply to the comments:\n\nC1: We will include results for \u00bd divergence to further substantiate our predictions: P1 and P2. That said, however, we find that the growth rate of the f-function is a key defining property of the behavior of f-divergences, and we can generalise the results of our experiments based on this fact. As pointed out in the paper, the exclusive KL divergence depends on the logarithm of the importance weights, whereas mass-covering divergences depend on higher polynomial degrees, which ultimately determine the results.\n\nC2: We thank the reviewer for their suggestion. The main reason to not include 1/2 divergences is clarity and avoiding overloading information in the plots. See reply to C1.\nEach of the lines in figure 3 is the result of fitting the approximation using the variational objective corresponding to the divergence in their label. We will update the text to clarify this point. We will also update the caption to include the degree of the polynomial involved. Some results were left out due to space issues. We will move Fig B1 to the main text.\n\nC3: Results shown in figure 3 and 4 are the average of 50 independent simulations. Regarding Figure 6, we run the experiments with 3 different seeds. We will include standard deviation in table 1. We compute the quantiles in figure 2 as the empirical quantiles in the sample size of size S. In figure 3, the quantiles are empirically computed from 100,000 draws generated from the approximation. We will update the text to include all of this information. \n\nC4: We agree with the reviewer in their assessment. We want to point out that figure 1b is the result of a particular posterior, with fairly low correlation, which explains why exclusive KL behaves that well. In the low-dimensional setting, the inclusive KL does cover the target posterior with a wider and heavier tailed approximation, which is usually a desirable property. \n\nC5-C7:We will update the text and fix these inconsistencies. Thanks for pointing these out.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WghxXEImqFc",
                "writer": "author",
                "reply_to": "A3ld2tFGJo4",
                "title": "Reply to Reviewer",
                "comment": " We thank the reviewer for raising important questions and positive review.\n\nC1: This is a good question, we will add the discussion to the main text. $\\hat{k}$ is scale invariant, depending only on the distribution of important weights. While relative mean and covariance errors are good summary statistics for mean field gaussian posteriors, it does not tell the full story in more complex posteriors. In practice, this means that for posteriors other than mean field Gaussians, relative mean and covariance errors are looking at specific summaries of the distribution, which may not be the best indicator of overall accuracy, as indicated by $\\hat{k}$. An example of this can be seen in Fig. C.2(e)\n\nC2: The issue for inclusive KL and F-div is that some of these models did not even converge because of numerical instabilities(and resulting divergences), and therefore we could not compute summary errors for any of them. We will update the text and clarify whether a model is simply not converging or if the approximation is very bad, but it converged. As we point out in the paper, exclusive KL tends to be more stable, even though in some cases the approximation is not great. We also point out that we attribute all $\\hat{k}$ to infinite when they are over 2.1 with/without convergence issues(as recommended in [26] and [30]), which we will also clarify in the text. The computation budget was limited to 15000 iterations. We will distinguish the two pathologies by an asterisk(where convergence was not achieved) in the final draft. \n\nC3:Thank you for pointing out this omission. Table 1 refers only to mean field Gaussians approximations. Bold indicates the best overall method (including HMC), whereas underline shows the best variational result for each experiment. We will clarify both points in the caption. \n\nC4: The results we show in figure 4 are computed with a slightly different step size, which explain the differences with figure 5 results. We have updated all results to show in figure 4 results computed with a step size included in figure 5, so both figures are relatable. Note that even if $\\hat{k}$ is very large, we can still compute some error summaries for some methods.\n\nC5: Yes the SNIS is a biased estimator, we will clarify this in the text.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QF8BBwJalXQ",
                "writer": "author",
                "reply_to": "mVvHe6sFILu",
                "title": "Reply to reviewer",
                "comment": " We thank the reviewer for asking some important questions and their positive review. \n\nFollowing are the replies to the questions/comments.\nC1: We found that initialization played a more significant role in flow based approximations in contrast to the more restricted variational families, due to the presence of a large number of parameters. We found that initializing the weights such that each flow-transformation resembles the identity function  provided more stable results. However, we made a point of testing the methods in fairly general settings without carefully fine tuning the methods. Initialization is non-trivial for more complex models, and it is difficult to make it general enough to perform well in many scenarios. \nWe should also point out that we call this \u2018pre-asymptotics\u2019 because the number of samples drawn to estimate the objectives and gradients are not sufficient in high dimensions as reflected by $\\hat{k}$, (a safe estimate will need an infeasible amount of samples to be drawn.) This will hold true even after the optimisation has been run for sufficient iterations reducing the effect of initialisation.  \n\nC2: We thank the reviewer for raising this point. As we show in our experiments, it is definitely possible to have good performing methods with bad $\\hat{k}$ diagnostics. As the reviewer points out, both measures reflect different aspects of the approximating distributions, and it is often a matter of the specific application to trust one or another. We focus on posterior accuracy as it often translates into more general usefulness, and in these cases $\\hat{k}$ is a good indicator of how good an approximation is.\n\nC3-C5: We will fix these issues in the camera ready version, we thank the reviewer for pointing these out. Table 1 uses MF Gaussian as the approximating family.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xlvIB8xoNqq",
                "writer": "author",
                "reply_to": "VEJQlKNRbGD",
                "title": "Reply to Reviewer",
                "comment": " Thank you for your suggestions and feedback.\n\nC1. We agree with the reviewer that further work in this direction is definitely needed in order to establish more strict theoretical criteria. We have considered many diverse models, like auto regressive models (arK), hierarchical Gaussian (radon) and non-Gaussian(8-schools) models, logistic regressions(dogs) which should be sufficient for the scope of this work, a study for neural networks is probably better suited for the longer version of this work.\n\nC2. This is an excellent idea and we will include such a table in the camera ready version.\n\nC3 and C4: Thanks for pointing these out, we will fix these issues in the camera ready version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mVvHe6sFILu",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__A4-JP8d_f",
                "title": "",
                "comment": "The paper discusses the accuracy of variational inference and the choices that need to be me made with regard to divergence and approximating family, particularly as they relate to the dimensionality of the problem. Based on an analysis of the pre-asymptotic behavior of the density ratios involved, the authors propose a framework based on Pareto smoothed importance sampling (PSIS) to help practitioners analyze the success of their approximate inference procedure. The authors provide experiments demonstrating general findings for variational inference for a range of divergences and approximate posteriors.   Overall, I found the paper to be quite clear and well-written. I believe the paper to be quite significant as I am not aware of much work on trying to provide a framework for understanding variational inference and the choices that need to be made, particularly in a way that could be used by a practitioner with relative ease. I believe the insights on dimensionality, mode-seeking vs mode-covering, and the differences in the quality of approximation from different approximate posteriors (e.g. MF Gaussian, Student's t, NVP), are novel, valuable and provide good guidance for practitioners and those who regularly use VI in their research.\n\nThe main issue I had with the paper was the lack of a proper review of prior work. The paper does reference some prior work in passing, but it does not really discuss the prior work (for instance [14, 30]) in the detail that I think would be required. For instance, [30] quite similarly proposes using the $\\hat{k}$ obtained from PSIS to evaluate how successful VI is - although they did not analyze different divergences from the standard mode-seeking one nor the effect of dimensionality. I think the paper would really require a proper discussion of these papers, as well as other attempts to assess the quality of VI approximations.\n\nOverall, as someone who focuses on VI in their research, I think this is quite a good paper and represents a useful and needed direction for the field. Therefore I believe the paper deserves acceptance, but I would of course be curious to hear what the other reviewers say.\n\nBelow are some more minor/tangential comments/questions:\n- The authors frequently make reference to the fact that they are interested in the \"pre-asymptotic\" regime. To me it would seem intuitive that this would depend quite significantly on the initialization of the approximating distribution, particularly as the divergences and dimensionality are changed. Do the authors have any intuition as to what the best way of initializing the approximate posterior are for these different cases?\n- For more complex models, it is often the case that there are symmetries that result in many modes that are identical when it comes to predictions, for instance permuting neurons in the same layer of a neural network would result in such modes. In the case of a unimodal approximate posterior, it would in principle therefore be possible to model one of these modes while missing the others in a way that the predictions are actually reflective of the true posterior. It would seem to me that in such a case the $\\hat{k}$ would be quite bad even though the predictions are fine - do the authors have any ideas on how this could be addressed?\n- It seems as though the labels are incorrect for Figs. 2 and 3. For instance, in line 138 Fig. 3a is referred to when I believe Fig. 2 is the correct one. \n- In Table 1, which approximate posterior is being used?\n- l. 259 - \"but it less\" The authors addressed some limitations of their work in that the discussion is limited to a typical f-divergences used in the literature and does not discuss all types of posteriors seen in the literature, such as semi-implicit posteriors. While not an extensive discussion, I found this description of limitations to be adequate for the goals of the work. I am not aware of potential negative societal impacts for this work.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "A3ld2tFGJo4",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__A4-JP8d_f",
                "title": "",
                "comment": "In this paper, the authors study the pre-asymptotic behavior of the density ratios between the joint distribution and the variational approximation. The authors use generalized Pareto distribution to model this density ratio in the pre-asymptotic regime and make three major predictions for how BBVI methods should behave. They verify their predictions through extensive experimentation, and finally, provide actionable advice to BBVI practitioners (use flows with exclusive KL and PSIS as a good default.)   I appreciate the authors for taking up this important direction in BBVI research. In the recent years, the number of components in BBVI methods have increased dramatically and it is important that we, as a community, analyze the existing methods. This paper takes some very concrete steps towards it.\n\nOverall, I like the paper. It is clearly written for most part. However, I have a few concerns and seek explanations.\n\n- Figure 6. I am not sure how to make sense of $\\hat k$ when using flows. Even with exclusive KL, the $\\hat k$ value reaches $\\infty$ for moderate dimensions (less than 100). Still, the relative errors seem to suggest they work better than the other methods. How reliable is $\\hat k$ when comparing different methods while keeping the variational objective same? Since $\\hat k$ value for flows seems to be $\\infty$ for moderately large models, the use of $\\hat k$ as a diagnosis tool is rather limited?  \n - Figure 4b. I don't see the Planar flow and NVP flow lines (Green lines) for Fdiv and Inclusive KL? Were they not calculated because the $\\hat k$ is $\\infty$? If so, I believe something is not consistent. There is Relative error estimates for NVP Flow even though the $\\hat k$ estimate is $\\infty$ for exclusive KL and D = 50. Maybe I missed the explanation somewhere; it will be great if the authors can point it out or provide one. \n - Table 1. I am not entirely sure the bold and the underline are supposed to be. Also, is the entire table only for NVP based method? I could not find, in text or in the table caption, the family that was used for the reported numbers. Are there any error bars for these numbers? \n - Figure 5. Should not there be at least on entry corresponding to the planar and NVP flows for higher dimensions (>30)? Did none of these step sizes converged for flows? If so, how did we get the numbers in Figure 4? \n - At L69, what is $\\hat I$? an unbiased or biased estimator? This is more of a notational consistency issue.  Yes",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "VEJQlKNRbGD",
                "writer": "official_reviewer",
                "reply_to": "nips_2021__A4-JP8d_f",
                "title": "",
                "comment": "This paper provides a statistical tool to analyze different variational inference methods in the f-divergence family. It showed that for high dimensional and low dimensional problems, the behavior of the variational inference methods may differ, and the analysis based on the GPD statistics can suggestion different strategies for different situations. The predictions from this theory were validated by some commonly used models (e.g., linear regression). In summary, this paper can provide useful guidelines for variational inference practitioners.  The main novelty of this paper is to use GPD to model the density ratio, which reveals important properties of different variational inference methods in the f-divergence family, which includes the inclusive KL and exclusive KL approaches. This provides a practical tool for people to use for variational inference diagnosis. One particular conclusion from this analysis is that, for moderate-to-high-dimensional posteriors, exclusive KL divergence is a better choice than inclusive KL divergence. These conclusion can be very helpful for people to choose the right variational inference method in different scenarios. Here are some more detailed commons of the paper:\n\n1. The three predictions (P1, P2, P3) are reasonable and are verified by some experiments. However, more rigorous proofs or justifications are needed to generalize the conclusions to other applications scenarios. For example, the authors mentioned that application to neural networks is not covered theoretically or empirically. \n\n2. It would be better to provide a table to summarize the conclusions in the paper, for low dimensional and high dimensional problems. Properties and behaviors of different variational inference method could be included in the table, together with the recommended inference method in the f-divergence family.\n\n3. A minor technical issue, in line 103, for the normalized importance weight, there should be a 1/S term in the denominator.\n\n4. A problem in notation: D is used for divergence, but also used for dimension.",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "appears to contain few contributions",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the paper's attempt to establish and support some grand (albeit somewhat informal) themes",
                "Sentiment Expression": "reviewers were overall positive",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "discussion of prior work",
                "Sentiment Expression": "inadequate",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "a better review of prior work related to all three of the themes",
                "Sentiment Expression": "would be stronger",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "if it was self-contained so that someone not familiar with the PSIS framework can follow it",
                "Sentiment Expression": "would be stronger",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the experimental results (both what was done and the presentation)",
                "Sentiment Expression": "Reviewers also had some specific comments",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            }
        ]
    },
    "zgMPc_48Zb": {
        "paper_id": "iclr_2021_zgMPc_48Zb",
        "paper_title": "Differentially Private Generative Models Through Optimal Transport",
        "paper_abstract": "Although machine learning models trained on massive data have led to breakthroughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge and provide indirect access to the private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differential privacy. DP-Sinkhorn relies on minimizing the Sinkhorn divergence---a computationally efficient approximation to the exact optimal transport distance---between the model and the data in a differentially private manner and also uses a novel technique for conditional generation in the Sinkhorn framework. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difficult to optimize, especially in the presence of noise imposed by the privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Experimentally, despite our method's simplicity we improve upon the state-of-the-art on multiple image modeling benchmarks. We also show differentially private synthesis of informative RGB images, which has not been demonstrated before by differentially private generative models without the use of auxiliary public data.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The paper proposes a DP method for generative modelling based on optimal transport. The reviewers agree that the novelty is limited in relation to prior work, while the results are not especially compelling either. So, even though this is a valid approach, correctness is not sufficient for acceptance at ICLR. \n",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "mvH2Murct1m",
                "reply_to": "t0H-YwwPbCa",
                "title": "Response to reviewer 4 continued",
                "comment": "* Experimental analysis of robustness \n\nRegarding our claim about robustness, we have experimentally verified that our method converges stably on a wide range of learning rates, batch sizes and for three different optimizers (SGD, Rmsprop, Adam). We have added these results to the updated manuscript in Appendix E, Tables 4 and 5, and briefly discuss them at the end of Section 4.2. Our method also has fewer hyperparameters than GANs that involve separate discriminator learning rates, update schedules, and gradient penalty. Overall, DP-Sinkhorn can be trained with minimal hyperparameter optimization, aiding its prospect for adoptance by practitioners. Please also see section 2.2 of the updated manuscript for why learning with Sinkhorn divergence is naturally stabler than GANs.\n\n* Motivation for privately learning generative models\n\nRegarding why we choose to train generative models rather than discriminative models, we agree that if a discriminative model can be trained directly, then it will (likely always) perform better than first training a generator, and then training a discriminator on generated data, irrespective of whether privacy is required. However, there are practical considerations where this is not always feasible. Please see our general response that is addressed towards all reviewers regarding this point.\nS_hat is the empirical Sinkhorn divergence computed on a given batch, we have updated our manuscript to define this in the main text (Section 3).\n\nWe would like to thank the reviewer again for the helpful feedback. If there are any other remarks or questions, we would be happy to discuss them in this forum.\n\n[1] Yuqing Zhu, Yu-Xiang Wang. \u201cPoisson Subsampled Renyi Differential Privacy\u201d (2019)\n\n[2] Frederik Harder, Kamil Adamczewski, Mijung Park. \u201cDP-MERF: Differentially Private Mean Embeddings with Random Features for Practical Privacy-Preserving Data Generation\u201d (2020)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "t0H-YwwPbCa",
                "reply_to": "pf6MMEK8z2h",
                "title": "Thank you for the constructive feedback",
                "comment": "We thank the reviewer for the constructive feedback. Below is our response to some specific concerns raised in your review.\n* The guarantee of differential privacy\n\nRegarding the guarantee of privacy, we used poisson subsampling with moments accountant for composing Renyi Differential Privacy (RDP) over training iterations, as proposed in Zhu et al. 2019 [1]. We used the implementation of this mechanism in Tensorflow-DP, with further details provided in appendix D.4. As this moments accountant technique is well established and our implementation by working with the Tensorflow-DP library is fairly standard, we are certain that our method is differentially private in both theory and practice. We will release our source code once the paper is published. We added a proof of our method\u2019s privacy at the end of section 3. We chose to not include this in the original manuscript, since the technique is a standard application of privacy results for the Gaussian mechanism in RDP. The goal of our paper is indeed not to introduce a novel privacy mechanism, but rather to introduce optimal transport to the field of differential privacy as a promising method for learning differentially private generative models (see below for motivation for that). \n\n* Originality of the proposed method\n\nRegarding originality, our work highlights that we can learn private generative models that are more useful than recent state-of-art methods (GS-WGAN) using an easy-to-train, non-adversarial loss formulation (Sinkhorn Divergence) and standard differential privacy tools. Our motivation is that as a non-adversarial optimal transport loss, Sinkhorn Divergence converges more stably than GANs, which makes it naturally suited for adapting to differential privacy. We are particularly interested in the usefulness of the synthesized data for downstream tasks, for which we choose classification of MNIST and FashionMNIST. This has emerged as the standard benchmark for differentially private generative models. A strong classifier that generalizes to real test data can only be trained if the data used for training is diverse. GANs, which are used by most competitive works, tend to suffer from instabilities during training due to their adversarial training scheme. These instabilities can manifest as mode dropping and imbalanced covering of the data distribution, thereby synthesizing imbalanced data for training the classifier. This would result in a sub-optimal classifier. The advantage of our optimal transport-based method is precisely that it does not require any adversarial objectives in its standard form and does not suffer from any noticeable mode-dropping problems. We indeed found our images to be diverse, as seen by samples (see variance of shape and style of digits and grayscale color of outfit pieces in additional samples in Appendix E) and supported by the strong classification results. Our results are still worse in FID than the state-of-the-art due to slight blur. However, for downstream tasks diversity seems to trump sharpness. Generally, training instabilities and mode dropping problems in GANs may be exacerbated by the gradient noise applied during training for ensuring privacy. We think that it is unclear whether GANs will be as successful for training differentially private generative models as they are for non-private generative modeling. We believe that optimal transport provides a promising and very simple and robust framework for differentially private generative modeling, which we try to demonstrate with our results. We see our method\u2019s simplicity as its strength.\n\nMore generally, in non-private settings, a growing number of works have studied the application of optimal transport in learning deep generative models due to their desirable convergence properties, yet previous works on private generative learning have mostly focused on adversarial training methods. Our method fills the knowledge gap by bridging an important branch of generative models with the differentially private setting. Our work is conceptually most related to DP-MERF [2], which uses maximum mean discrepancy, another type of integral probability metric. Compared to DP-MERF, samples generated by our method are much more representative of the training data at a weaker privacy requirement of epsilon=10, whereas DP-MERF excels at the strong privacy regime of epsilon < 1, but fails to scale in utility with weaker privacy requirements. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aK0Lhn_nL-T",
                "reply_to": "t-W6KeDHHZ",
                "title": "Response to reviewer 3 continued",
                "comment": "* Is concatenation of data and labels in cost function a good strategy?\n\nWhile simple, our class-conditioning strategy by concatenation is theoretically grounded as it equates to performing optimal transport in the joint space of images and labels. We elaborate more on this point in section 3 of the revised manuscript. In more detail, though, the key modelling decision is in fact not whether the label is concatenated with the image, but which cost function to use when computing the Sinkhorn loss. In our case, by concatenating the label to the image, we are also assuming a squared L2 norm for measuring similarly between labels. One could argue that squared L2 norm is an awkward choice for measuring similarity between one-hot vectors. However, we found that squared L2 was indeed sufficient for our experiments, as we have not observed any instances of the class-conditioning failure. Hence, we chose to stick to the simplest choice, in particular because we are using the L2 norm already for the image cost itself. That being that, we completely agree that more tailored cost functions may be needed for more complex label spaces (one could imagine, for example, a hierarchical softmax for hierarchical labels). We leave these explorations for future work. \n\nIf there are any other remarks or questions, we would be happy to discuss them in this forum.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "t-W6KeDHHZ",
                "reply_to": "jXANwv7tWKN",
                "title": "Thank you for the constructive comments",
                "comment": "We thank the reviewer for the constructive feedback. Below is our response to some specific concerns raised in your review.\n\n* \u201cTwo datasets are considered\u2026 [DP-Sinkhorn] performs poorly compared to GS-WGAN both in inception score and FID\u201d\n\nWe would like to first highlight that we conducted experiments on three datasets: MNIST, Fashion-MNIST, and CelebA. MNIST and Fashion-MNIST are the de-facto benchmarks for differentially private generative learning in the image domain, as they have been used by many related works. The improvement in downstream classifier accuracy with DP-Sinkhorn is most pronounced on the Fashion-MNIST dataset, with an absolute improvement of 5.4% (8% relative) over GS-WGAN under respective best case scenarios. The metrics we evaluated on were classification accuracies and FID; we chose not to evaluate Inception score since we couldn\u2019t determine how inception score was calculated in previous works when evaluating on Fashion-MNIST.\n\n* Regarding FID and sample diversity.\n\nRegarding FID, DP-Sinkhorn with L2 loss indeed produced images with higher FID than GS-WGAN, while comparing favorably against all other methods. We found that if we suppress pixels with brightness less than 0.5 in images generated by DP-Sinkhorn, we obtain images with worse visual quality. Clearly, this modification could not improve sample diversity either. Yet, the modified images score better FID (54.16) than GS-WGAN (61.3). This indicates that our method indeed produces diverse samples, but these images are slightly blurry and hence score poorly on FID which emphasizes texture. For fair comparison, we didn\u2019t do any such tricks for the results presented in the paper, though. This also suggests that FID is not a reliable metric for this low resolution grayscale data and we deem the classification task more important. Importantly, we believe that DP-Sinkhorn performs better on classification despite the blur precisely because our images are more diverse and hence provide better generalization for downstream classifiers. The importance of diversity for downstream accuracy is even more pronounced in our newly added ablation experiments where we add noise on parameter gradients instead of image gradients. DP-Sinkhorn with parameter gradients produced even noisier images, yet the downstream classifier accuracy is still better than GS-WGAN. We hypothesize that this actually suggests that GS-WGAN and the other GAN-based approaches suffer from a certain amount of mode-dropping, while our approach does not. We attribute this to our robust optimal transport-based training approach that does avoid any potentially unstable adversarial objectives (in fact, we don\u2019t consider our method a type of GAN, as it doesn\u2019t train in an adversarial fashion. Rather, it directly optimizes the primal formulation of the optimal transport distance between the target and generated distribution via the Sinkhorn divergence - also see expanded explanation in section 2.2).\nWe also added additional figures with more samples drawn from our model in Appendix E to demonstrate that it indeed produces diverse samples (see, for example, variations in shape and orientation of digits and grayscale color in outfit pieces).\n\n* Regarding the CelebA experiments\n\nWe emphasize that our goal is to produce \u201cuseful\u201d synthetic data for downstream tasks. As Anonreviewer 2 has also pointed out, it is impossible to perfectly learn a synthetic distribution (in the language of statistics, match a target distribution up to arbitrary order statistics) while also enforcing differential privacy.  DP-Sinkhorn shows that it is indeed possible to synthesize useful data when training private generators on RGB images. To our best knowledge, no previous works have attempted training differentially private generative models on RGB images without additional public data. We want to explore this important yet missing application with our proposed method. Although we find the generated data useful for downstream tasks, there is clearly still room for improvement. Future works might explore metrics (replacing pixel space L2) that algorithmically encode our priors about natural images. We couldn\u2019t evaluate GS-WGAN on CelebA as code for GS-WGAN was not available at the time of submission. We are currently trying to train GS-WGAN on CelebA, but we expect experiments to run past the end of the discussion period. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i5XzTPP4b2",
                "reply_to": "iclr_2021_zgMPc_48Zb",
                "title": "Overall response and summary of manuscript update",
                "comment": "We thank all reviewers for the constructive feedback. We reply to some points that have been discussed by multiple reviewers here.\n\nWe agree that if we were only interested in training a differentially private classifier, then using a generative model as we do, may not be necessary. However, we believe that generative models have significant potential as a data sharing medium with differential privacy. We have expanded on our motivation for learning generative models in a differentially private manner in the introduction of the manuscript, and provide a more detailed explanation here. While gradient perturbation methods like DPSGD can be applied generally to training machine learning models, they have practical requirements that restrict their application. For instance, in a common setting where a data analyst (party wanting to use the data, such as a marketing company, medical ML start-up, etc.) wants to train a differentially private model on personal data held by a data curator (party in charge of safekeeping the database, such as government agencies, hospitals, mobile device operators, etc.), the data analyst would either: A. send their machine learning model and code to the curator and have the curator run DPSGD locally, or B. establish a high speed connection between the two parties, send model weights to the curator, and receive gradients back from the curator for updating the model. In either case, the analyst would have to disclose their model architecture to the curator, and the curator needs to perform computations locally that could become expansive if they are sharing data with multiple analysts. While the curator could theoretically use local differential privacy to treat the data before sending it to the analyst, the amount of noise required for adequate privacy protection will likely render the data useless. Our goal is to use privately learned generative models as a data sharing medium, where the curator can train a generator once, and share it with any number of analysts. The analysts could then use the generative model to produce synthetic data for training downstream tasks. This way, the computational burden on the curator is reduced in the long run, and the analyst would not need to transfer technology (that is likely vital for competitiveness) to the curator. Hence, sharing data by sharing a generative model can be considered a very general and versatile form of data sharing. The data analyst can use the data synthesized by the differentially private generative model in any way they like. Training a classifier, as we do in our paper, is merely one example. In principle, this data could be used to train other models, too. Hence, we argue that data sharing by generative modeling is a promising research direction in the area of differential privacy, which is why this task has been explored by several previous works before, for example [1-5]. Training a classifier with generated data under differential privacy for MNIST as well as FashionMNIST classification has emerged as a standard benchmark. Therefore, we choose this benchmark in our work as well.\n\nIncluding further points addressed in detail in the other replies, we have made the following modifications to our manuscript:\n* Expanded Introduction on motivating private generative models,\n* Expanded Section 2.2 on differences between Sinkhorn divergence and other methods,\n* Further explained in Section 3 why concatenating the label and images works, including two interpretations,\n* Added a privacy proof of our method following standard arguments in Section 3,\n* Additional qualitative results (synthetic images) in Appendix E, Figures 5 and 6.\n* Ablation results on perturbing image gradients versus perturbing parameter gradients (Section 4.2),\n* Hyperparameter sweep demonstrating training stability of DP-Sinkhorn under multiple learning rates and optimizer choices (Section 4.2 and Appendix E, Tables 4 and 5).\n\n\nWe hope that we have been able to address all reviewers\u2019 feedback and we welcome further discussion. Thank you!\n\n[1] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, Jiayu Zhou. \u201cDifferentially Private Generative Adversarial Network\u201d (2018)\n\n[2] Yunhui Long, Suxin Lin, Zhuolin Yang, Carl A. Gunter, Bo Li. \u201cScalable Differentially Private Generative Student Model via PATE\u201d (2019)\n\n[3] Reihaneh Torkzadehmahani, Peter Kairouz, Benedict Paten \u201cDP-CGAN: Differentially Private Synthetic Data and Label Generation\u201d (2020)\n\n[4] Frederik Harder, Kamil Adamczewski, Mijung Park \u201cDP-MERF: Differentially Private Mean Embeddings with Random Features for Practical Privacy-Preserving Data Generation\u201d (2020)\n\n[5] Dingfan Chen, Tribhuvanesh Orekondy, Mario Fritz \u201cGS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators\u201d (2020)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "baayuazo1yx",
                "reply_to": "PIPWiMXiFH",
                "title": "Response to reviewer 2 continued",
                "comment": "* Adapting to task specific measures\n\nWe agree with the observation that differentially private training of generators that resemble the real distribution by every measure is impossible. Thus, it would make sense to use task-specific measures when training, such that the generator resembles the real distribution in features that are important for the downstream task. In fact, optimal transport is a very flexible framework and what the generator learns depends entirely on what the cost function penalizes. Hence, in principle we could influence the type of features learned by the generator by modifying the cost function. While our class conditioning method is a first step in this direction, more sophisticated methods could certainly be deployed under the same framework. For example, if a feature extractor trained for the classification task was available a priori, then we could use this extractor in the cost function (replacing the adversarial feature extractor presented in 4.3). This would cause the generator to match the real distribution not in pixels, but in features that are useful for the classification task. This feature extractor would have to be either trained on public data, or with differential privacy. While this is an exciting direction, we think this is complementary to our main goal of promoting optimal transport-based generative learning (for its stability and ease of training) under differentially private settings. Future work could definitely improve upon our current framework in this direction. Please also see our overall response regarding our motivation for why we chose to work on generative learning rather than training a classifier directly.\n\n* Can DP Sinkhorn be applied to anything except images\n\nAbsolutely! Although the architecture of the generator network would need to be tweaked to fit the specific data type, private learning with Sinkhorn divergence is a general approach that can be applied to any data type for which a cost function can be defined. For example, cross entropy can be used for categorical random variables, and L1 distance can be used for real vectors that are sparse. Optimal transport has been used as a metric on other complex data types such as language and graphs [3-5].\nWe chose not to perform evaluations on other data types, since recent related works have been mostly focused on image generation and we wanted to benchmark our framework against these works. However, this is indeed a very interesting direction for future research and arguably another strength of our OT-based approach. As described, adapting our method to other data types just means replacing the cost function and is therefore easy, whereas training GANs (the baselines are mostly based on GANs) on other data types can be very challenging, due to the adversarial training, which we avoid.\n\nIf there are any other remarks or questions, we would be happy to discuss them in this forum.\n\n[1] Marco Cuturi. \u201cSinkhorn Distances: Lightspeed Computation of Optimal Transport\u201d (2013)\n\n[2] Lars Mescheder, Sebastian Nowozin, Andreas Geiger. \u201cThe Numerics of GANs\u201d (2017)\n\n[3] Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, Kilian Q. Weinberger. \u201cFrom Word Embeddings To Document Distances\u201d (2015)\n\n[4] David Alvarez-Melis, Tommi S. Jaakkola, Stefanie Jegelka. \u201cStructured Optimal Transport\u201c (2018\uff09\n\n[5] Titouan Vayer, Laetitia Chapel, Remi Flamary, Romain Tavenard, Nicolas Courty. \u201cOptimal Transport for structured data with application on graphs\u201d (2019)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PIPWiMXiFH",
                "reply_to": "BZ5B7hvYf2z",
                "title": "Thank you for the constructive comments",
                "comment": "We thank the reviewer for the constructive feedback. Below is our response to some specific points raised in your review.\n\n* Why Sinkhorn divergence is easier to optimize than GANs.\n\nWhen compared to WGAN, learning with the Sinkhorn divergence has distinct differences. First, the Sinkhorn divergence is computed under the primal formulation of OT, whereas WGAN's loss is computed under the dual formulation. While both are approximations to the exact Wasserstein distance, the source of the approximation error differs. The Sinkhorn divergence uses entropic regularization to ensure linear convergence when finding the optimal transport plan $\\pi$ [1]. Its two sources of error are the suboptimality of the transport plan and bias introduced by entropic regularization. With the guarantee of linear convergence, $\\pi$ can converge to optimality by using enough iterations, thereby allowing control of the first error. \nThe second source of error can be controlled by using small values of $\\epsilon$, which we found to work well in practice. In contrast, WGAN's source of error lies in the sub-optimality of the dual potential function. Since this potential function is parameterized by an adversarially trained deep neural network, it enjoys neither convergence guarantees nor feasibility guarantees.\nFurthermore, the adversarial training scheme can produce oscillatory behavior, where the discriminator and generator change abruptly every iteration to counter the strategy of the other player from the previous iteration [2]. These shortcomings contribute to WGAN's problems of non-convergence, which in turn can lead to mode dropping. In contrast, training with the Sinkhorn divergence does not involve any adversarial training at all, converges more stably, and reaps the benefits of OT metrics at covering modes. This stability is our key motivation for using the Sinkhorn approach for learning differentially private generative models, where stability in the adversarial GAN setting may be even harder to achieve due to the additional perturbations for guaranteeing differential privacy.\nWe\u2019ve expanded our discussion on differences between Sinkhorn divergence and GANs in section 2.2 of the manuscript.\n\n* Impact of gradient perturbation on generated images\n\nWe have added an ablation study to our MNIST and FashionMNIST experiments in section 4.2. Our results indicate that gradient perturbation on generated images is still superior to doing so on the parameters, but DP-Sinkhorn with parameter gradient perturbation still slightly outperforms all baselines on the classification task. Qualitatively, the images generated by parameter gradient perturbation appear worse (see Appendix E), yet the downstream classification accuracy is close to the one achieved by image gradient perturbation. We think this is due to the fact that downstream classifiers are dependent on sample diversity to achieve generalization, and images generated by DP-Sinkhorn with parameter gradients are diverse enough to train (relatively) good classifiers despite having a noisier appearance. We hypothesize that the baselines, mostly being advarially trained GAN-based methods, suffer from some amount of mode dropping compared to our method. We attribute this to our robust optimal transport-based training approach (see previous bullet point).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "pf6MMEK8z2h",
                "reply_to": "iclr_2021_zgMPc_48Zb",
                "title": "Proposed method is not well motivated and is not novel",
                "comment": "This paper presents a differentially private method for training a generative model. The proposed method takes advantage of the Sinkhorn divergence to achieve robustness against the hyperparameters' choices. The authors also introduce a cost function enabling the generative model to generate images associated with a specific class label. The experimental results show that the proposed method outperforms the existing methods for learning a generative model in a differentially private manner. Furthermore, such a high accuracy can be achieved without the use of publicity available data.\n\nThe strong points of this paper are as follows:\n-  The authors bring the Sinkhorn divergence-based learning of a generative model into the differentially private generative model learning problem.\n\nThe weak points of this paper are as follows:\n- The proposed method is not clearly explained and thus is suspicious in the guarantee of differential privacy.\n- The presented method is a direct application of Wang et al.'s moment account technique with Zhu et al.'s Poisson sampling. The originality is considerably low.\n- This paper has no theoretical and experimental analysis about robustness against a hyperparameter choice, while the authors claim it as a contribution.\n- The proposed approach is not well motivated. We can use some differentially private classification algorithm if the objective is high accuracy in downstream classification.\n\nI recommend rejection of this paper because the proposed algorithm has low originality and is not well motivated. Also, the unclarity of the privacy guarantee is problematic.\n\nThe authors do not give a clear explanation of the proposed method. In particular, it is unclear if the proposed algorithm guarantees differential privacy. I guess the authors employ either the composition theorem or moment account technique to prove the algorithm's differential privacy; however, there is no privacy proof of the proposed algorithm. I could not confirm that the proposed method ensures differential privacy.\n\nThe proposed method is a straightforward application of the techniques from Wang et al. and Zhu et al. Also, defining the cost function as in Eq. 4 is a straightforward way to combine the multidimensional real-valued data and discrete label. I cannot find any original idea, except introducing the Sinkhorn divergence, in the proposed method.\n\nThe authors claim that the proposed method is robust against the choice of its hyperparameters. However, there is no evidence to support the claim. A theoretical or experimental analysis of robustness is necessary to claim it.\n\nWhy don't we employ the differentially private classification algorithm, such as M. Adabi et al. Deep Learning with Differential Privacy. In CCS'16. When the objective is high accuracy in the downstream classification, we can utilize such an algorithm directly. What is the benefit of employing the generative model-based privacy preservation? The differentially private classification algorithm can achieve high classification accuracy; for example, in Adabi et al.'s paper, the classification accuracy for MNIST with eps=10 is 97%; this value is significantly higher than that of the proposed method.\n\n\n### Minor comments\n\n- What is the definition of $\\hat{S}$? ",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "jXANwv7tWKN",
                "reply_to": "iclr_2021_zgMPc_48Zb",
                "title": "Method not so novel, experiments not so convincing",
                "comment": "The paper proposes a method for training OT GANs using differentially private sinkhorn algorithm. The idea is very simple - train GANs with sinkhorn divergences and add Gaussian noise to the gradient of output wrt generated samples. So, the novelty by itself is minimal as sinkhorn GANs have previously been proposed. The method merely adds Gaussian noise to gradients. The DP analysis is also not very different.\n\nI am generally ok with incremental improvements in method if experimental results are strong. This doesn't seem to be the case in this paper. Two datasets are considered - MNIST and CelebA. In MNIST, we observe that the algorithm performs poorly compared to GS-WGAN both in inception score and FID. The method gets better accuracy though. This suggests that the method does not produce diverse samples. For example, consider a case of a mode collapse where a GAN generates only one sample per class. In this case, FID scores will be poor. However, if the generated sample correcly corresponds to the class, classification score will be high. My guess is similar thing is happening here. This is evident even in Fig 2 where the method has much poor sample diversity compared to GS-WGAN. Hence, the model itself is not that great.\n\nIn CelebA, despite using BigGAN style architecture, there is significant blur. The real challenge in differential private GANs is to generate samples with good quality while still satisfying privacy constraints.  This doesnt seem to be happening here. Also, comparison with GS-WGAN or other differentially private GANs are missing in CelebA experiment.\n\nFor conditional generation, is concatenation of data and labels in cost function a good strategy? It looks like both data and label space is very different, and I am wondering if such type of concatenation might be weak. Can you comment on this.\n\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BZ5B7hvYf2z",
                "reply_to": "iclr_2021_zgMPc_48Zb",
                "title": "new loss function for DP synethic image data",
                "comment": "This paper proposes a novel architecture and training process for learning differentially private synthetic data generators. \n\nRather than taking an explicit adversarial GAN approach, the paper looks at minimizing a regularized Wasserstein distance (called the Sinkhorn loss) between the empirical distribution of the data and the synthetic distribution. \n\nTheir approach brings two benefits: first, the Sinkhorn loss is more straightforward to optimize than traditional GANs. The authors don\u2019t really discuss why this is, but my understanding is that the \u201cadversary\u201d is contained in the dual formulation of Wasserstein distance (as the minimum over Lipschitz functions of the difference in expectation between two distributions). Considering such a Lipschitz adversary simplifies optimization. \n\nSecond, the paper uses an idea first published by Chen et al (NeurIPS 2020) but described as independent work here: Instead of measuring the gradient of the full parametric distribution of the generator, the authors measure the gradient \u201cat the generated image level\u201d. This gives a lower-dimensional gradient (requiring less noise). \n\nOverall, the paper achieves a moderate improvement over the work of Chen et al with respect to a few simple measures of accuracy (namely, how well two DNN models do at classifying real data when they are trained on synthetic data). It does considerably worse than Chen et al\u2019s method with respect to the FID score, a measure of visual similarity. \n\nI generally like the idea of exploring alternate training strategies. Nevertheless, I have several reservations about the paper: \n\n1. The relationship to Chen et al. It seems like much of the gain relative to previous work comes from the way gradients are compressed. But this idea appears already in the NeurIPS paper of Chen et al. It is unclear to me how to handle the relative priority of the papers. \n2. Overly simplistic accuracy measures: Wasserstein (or Sinkhorn) distance is a natural loss function, but it is only likely to be a really good measure of accuracy when it is very small. The DP literature, since the pioneering work of Blum, Ligett, and Roth, has focused on using as a loss function the minimum over a (potentially enormous but) task-specific set of queries as the ultimate loss. We understand as a field that general-purpose synthetic data isn\u2019t really possible (it preserves \u201ctoo many statistics, too accurately\u201d to avoid attacks). How can the framework here be adapted to such accuracy measures? How well does DP Sinkhorn do when measured against such task-specific accuracy? Can DP Sinkhorn be applied to anything except images (and if so, how well does it do)?\n\nOverall, I find the paper interesting but perhaps borderline. \n",
                "rating": 6,
                "confidence": 2,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "DP method for generative modelling based on optimal transport",
                "Sentiment Expression": "proposes",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "novelty",
                "Sentiment Expression": "limited",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "results",
                "Sentiment Expression": "not especially compelling",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "valid approach",
                "Sentiment Expression": "not sufficient for acceptance at ICLR",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "SkxxIs0qY7": {
        "paper_id": "iclr_2019_SkxxIs0qY7",
        "paper_title": "CoT: Cooperative Training for Generative Modeling of Discrete Data",
        "paper_abstract": "We propose Cooperative Training (CoT) for training generative models that measure a tractable density for discrete data. CoT coordinately trains a generator G and an auxiliary predictive mediator M. The training target of M is to estimate a mixture density of the learned distribution G and the target distribution P, and that of G is to minimize the Jensen-Shannon divergence estimated through M. CoT achieves independent success without the necessity of pre-training via Maximum Likelihood Estimation or involving high-variance algorithms like REINFORCE. This low-variance algorithm is theoretically proved to be superior for both sample generation and likelihood prediction. We also theoretically and empirically show the superiority of CoT over most previous algorithms in terms of generative quality and diversity, predictive generalization ability and computational cost.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "The paper proposes an original and interesting alternative to GANs for optimizing a (proxy to) Jensen-Shannon divergence for discrete sequence data. Experimental results seem promising. Official reviewers were largely positive based on originality and results. However, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a \"low-variance, bias-free algorithm\" to optimize JS.  Given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue (review reposted on the current OpenReview thread on Nov. 6), the AC feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. The current version of the paper does neither. The algorithm is not unbiased for at least two reasons pointed out in discussions: a) in practice a parameterized mediator will be unable to match the true P+G, at best yielding a useful biased estimate (not unlike how GAN's parameterized discriminator induces bias). b) One would need to use REINFORCE (or similar) to get an unbiased estimate of the gradient in Eq. 13, a key detail omitted from the paper. From the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with Eq. 13 (they commented \"most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this.\"). But rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the REINFORCE term thus effectively trading variance for bias. \nOn December 4th authors posted a justification confirming heuristically dropping the REINFORCE terms when taking the gradient of Eq. 13, and said they could attach detailed analysis and experiment results in the camera-ready version.  However if one of the \"most important idea\" of the paper is how to avoid REINFORCE (as still implied and highlighted in the abstract), the AC finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by (unreportedly) dropping the term. \n\nThe approach remains original, interesting, and potentially promising, but as it currently stands, AC and SAC agreed that inexact theoretical over-claiming and insufficient justification and in-depth analysis of key heuristic shortcuts/tradeoffs (however useful) are too important for their fixing to be entrusted to a final camera-ready revision step. A major revision that clearly adresses these issues in depth (both in how the approach is presented and in supporting experiments) will constitute a much more convincing, sound, and impactful research contribution.\n\n",
        "meta_review_title": "Novel approach with promising results for generative modeling, but with incorrect claims and insufficiently analysed heuristic shortcuts",
        "reviews": [
            {
                "review_id": "SJefME_ZeN",
                "reply_to": "rkgfHtIWx4",
                "title": "Response to response",
                "comment": "Thank you for your advice.\n\n1. The paper was revised in the revision period to address most of the comments on experiments.  We consider it now to be more solid against the previous comments and criticisms. You may want to re-check it.\n\n2. We agree, as we promise the corresponding discussion would be included in the final version of the paper.\n\n3. Actually, the experiment of robustness (see Sec 4.1.1) shows that in pracice, whether the mediator is trained to be optimal is not so important. See Fig2(b) and its legend. We state that the algorithm still works well under a variety of g-m balancing settings. In the extreme case, even if the mediator is remarkably less trained (g-steps=3, m-steps=1), the algorithm still works well and suceeds to converge.\n\nWe appreciate your comments, as we promise that these content would definitely be added to the final version of the paper.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bygvu1w2jm",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "nice idea",
                "comment": "Pros:\nThis paper is easy to follow. The idea is nice in three folds. \n1. By changing the auxiliary model's role from a discriminator to a mediator, it directly optimizes the JSD measure, which is a symmetrized and smoothed version of KL divergence.  \n2. Moreover, the mediator and the generator follow similar predictive goals, rather than the opposite  goals of G and D in GANs. \n3. For discrete sequential data, it avoids approximating expected rewards using Markov rollouts.  \n \nCons:\nSome details are missing in the experiments. \n1. In Table 2 of [A], LeakGAN, SeqGAN and RankGAN all show significantly better performances in terms of BLEU on EMNLP2017 WMT, compared to results reported in Table 3 of the submission. Any difference?\n2. The Word Mover Distance is computed by training a discriminator, which could be unstable. Could you provide other metrics to evaluate diveristy like self-bleu?\n\n[A] Guo, Jiaxian, et al. \"Long text generation via adversarial training with leaked information.\" arXiv preprint arXiv:1709.08624 (2017).\n\nMisc:\n1. How will the number of samples (i.e. batch size) affect CoT ?\n2. How is the applicability of CoT for continuous data? It seems to me there is no theoretical difficulties to apply CoT on continuous data.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rkgfHtIWx4",
                "reply_to": "BklsMW0kpX",
                "title": "Response to rebuttal",
                "comment": "Generally the reply responses my concerns. \nI found the authors' comments towards other non-official reviewers' questions are very useful and important, including points that I have not considered at the first place. Some questions are well handled.\n1. it seems most people have questions towards the setting of experiments, so maybe some clearance is needed.\n2. the question and response on \"why reinforce is not needed\" is important and should be included in the final version.\n3. also the question and response on \"what if the mediator fails to learn optimally\".\n\nOverall, I think the authors well respond to others' concerns. But I think some content needs to be included in the final version. To encourage the authors add these content, I may slightly lower my rating.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1eZN4i4JN",
                "reply_to": "Skx7dQKjC7",
                "title": "Detailed analysis of why REINFORCE is dropped",
                "comment": "To get better reading experience, you may want to use a LaTeX compiler.\n\nEq 13:\n\\begin{align*}\n&\\nabla_\\theta J_g(\\theta)\\\\\n=&\\nabla_\\theta(\\sum_{t=0}^{n-1}\\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\pi_g(s_t)^T (\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})\n\\end{align*}\n\nFor time step $t$, each term of Eq 13 equals to:\n\\begin{align*}\n&\\nabla_\\theta J_{g, t}(\\theta)\\\\\n=&\\nabla_\\theta \\left[ \\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})\\right]\\\\\n=&\\nabla_\\theta \\left[ \\sum_{s_t} G_\\theta(s_t) (\\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})) \\right]\\\\\n=&\\sum_{s_t} \\nabla_\\theta \\left[ G_\\theta(s_t) (\\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})) \\right]\n\\end{align*}\nLet $L(s_t) = \\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})$\n\n\\begin{align*}\n  &\\nabla_\\theta J_{g, t}(\\theta) \\\\\n  =&\\sum_{s_t}(\\frac{\\partial G_\\theta (s_t)}{\\partial \\theta} L(s_t) + G_\\theta(s_t) \\frac{\\partial L(s_t)}{\\partial \\theta}) \\\\\n  =&\\sum_{s_t}(G_\\theta(s_t)(\\frac{\\partial \\log G_\\theta(s_t)}{\\partial \\theta}L(s_t) + \\frac{\\partial L(s_t)}{\\partial \\theta})) \\\\\n  =&\\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\nabla_\\theta (\\text{stop\\_gradient}(L(s_t))\\log G_\\theta(s_t) + L(s_t))\n\\end{align*}\n\nAs you may notice, the total gradient in each step consists of two terms. The first term $(\\text{stop\\_gradient}(L(s_t))\\log G_\\theta(s_t)$ behaves like REINFORCE, which introduces variance to the optimization process. The second non-REINFORCE term is comparitively less noisy, though for the first sight it seems not to be working alone.\n\nIf we think about the effects of the two terms, we may notice that they have similar optimization directions (towards minimization of $KL(G_\\theta || M_\\phi)$ ). Thus, empirically, basic version of CoT introduces an extra hyperparameter $\\gamma \\in [0, 1]$, to control the balance of the high-variance first term and low-variance second term. The objective in each time step thus becomes:\n\\begin{align*}\n  &\\nabla_\\theta J^{\\gamma}_{g, t}(\\theta) \\\\\n  =&\\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\left[ \\nabla_\\theta \\gamma (\\text{stop\\_gradient}(L(s_t))\\log G_\\theta(s_t) + L(s_t)) \\right]\n\\end{align*}\n\nHowever, in practice we find that in all our attempts, the algorithm works best when $\\gamma = 0.0$. Thus, we directly drop the REINFORCE term. If you think it necessary, we would attach detailed analysis and experiment results in the camera-ready version.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Skx7dQKjC7",
                "reply_to": "S1xBB06y6Q",
                "title": "major concerns remain, REINFORCE still needed",
                "comment": "Thank you for beginning to address some of the concerns I raised.\n\nHowever, the major concern about avoiding REINFORCE remains. In particular, computing the gradient of Eq. 13 with respect to the parameters of the generator \\theta still requires REINFORCE. \n\nWhy? Because Eq. 13 involves an expectation over discrete samples s drawn from the generative model G_\\theta:\n\\nabla_\\theta E_{s ~ G_theta} [ f(s, theta, phi)]\n\nHow are you actually computing the gradients of Eq. 13 for CoT? My guess is that you are dropping the REINFORCE term in the gradient, and just computing:\nE_{s ~ G_theta}[\\nabla_theta f(s, theta, phi)]\nwhich is NOT the gradient of Eq 13.\n\nCould you please clarify how you compute this gradient and how you are avoiding REINFORCE?",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BklMQW7oRQ",
                "reply_to": "rygI5D8q0Q",
                "title": "Please refer to the submitted version of the paper, instead of the preprint version.",
                "comment": "We've had several updates to address the arguments. Please refer to the submitted version of the paper.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rygI5D8q0Q",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "The idea is doubtful, and the proof is erroneous.",
                "comment": "The section explaining the suporiority of training a mediator over training a model P^ to directly predict true distribution P is not convincing. The paper says that there is no guarantee that P^ can be trained to provide correct predictions, but the paper doesn't explain why a mediator doesn't have similar problems.\n\nIn the proof of theorem 3 and 4, the paper assumes the mediator M to be trained to be optimal. But it is unjustified. Even if we can train M to be optimal, 2*M-G is already the true distribution P. Why bother to use CoT\uff1f\n\nAt the same time, I have some suspicions of the experimental results. I think the performance gap is much larger than what CoT can possibly achieve.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJgeArCZ0m",
                "reply_to": "rJelZVAJpX",
                "title": "Thank you",
                "comment": "Thank you for the reply!",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BklAEn3K67",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "Paper Revised by the Authors",
                "comment": "The major updates include:\n1. Most of the mentioned typos are fixed.\n2. The color scheme is changed for better readability in gray scale printing.\n3. Standard deviation of the calculated eWMD is provided, in order to make the comparison statistically sound. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyxkN2DDTm",
                "reply_to": "HJl2Eka7pX",
                "title": "Response to Response",
                "comment": "We ran your code and created a validation set in order to \"early stop\" and reported NLL_test on a test set. Because early stopping is a regularization technique, we thus applied some regularization to CoT. We kept the hyperparameters constant thinking that they were the results of a Cross-Validation. If not, would it be possible for you to update the repo with the best performing hyperparameters and then we could rerun the experiment?  Also, does this means that all the MLE results you report are not cross-validated and have no regularization (e.g. dropout)? If so, i'm not sure what kind of conclusions one can come to when outperforming an un-regularized un-cross-validated MLE baseline. \n\nI'd be happy to continue the conversation via email :)\n\nFYI, we're coding CoT and adding it to our current repo. We will properly cross-validate CoT on the real dataset EMNLP News 2017 and add CoT in the real data experiment part of our paper.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1l29bRkpm",
                "reply_to": "HkgJzPJP2X",
                "title": "Response to AnonReviewer2",
                "comment": "Thanks for reviewing our paper!\n\nResponse to your concerns:\n\nWe will provide with a carefully-revised version of the paper according to your generous suggestions.\n\nAnswer to the questions:\n1. If there is no constraint on the entropy of the solution of the objective function, the objective would not be equivalent to minimization of JSD. Instead, it would simply be calculating the entropy of M, which is useless.\n\n2. Figure 2 (a)(b) shows CoT is more robust under its main evaluation to g-m balance compared to the g-d balance of SeqGAN. Ideally, Figure 2(a) should also be showing SeqGAN's performance under evaluation of JSD, however, in our attempts, SeqGAN always diverges under such evaluation. Figure 2 (c) show that the convergence of CoT is steady and quite fast under evaluation of NLL_{oracle}, which is biased on quality. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJl2Eka7pX",
                "reply_to": "r1eMNq-faQ",
                "title": "Response to ''Language GANs Falling Short''",
                "comment": "One good property of CoT is that its entropy estimation is more accurate than that of MLE.\u00a0\n\nAs is analyzed in some previous work (e.g. Ian's GAN Tutorial in NIPS 2016), models trained to optimize forward KL, which is equivalent to MLE's objective, tend to over-estimate the entropy of the data.\n\nIn our opinion, the ability of correctly estimating data entropy is important for discrete generative models, since in practice, the entropy of the data is not directly available. For real data, the entropy of the model is not a hyperparameter, but a trainable parameter learned via gradient descent. The manipulation of temperature may help in producing better samples at inference, but it is not a reason to get satisfied in merely doing so.\n\nBesides, your synthetic experiment results with MLE and CoT is a bit different from ours, where CoT performs worse and MLE performs remarkably better than that in ours. In our observed results, LMs trained via MLE almost always tends to overfit quickly after about 40 epochs. Without adopting the training techniques you've incorporated in your repo (i.e. Variational Dropout and Many-fold Cross-validation), it is difficult to reproduce your results about MLE, especially for a naive one. While all other models share the same training/testing framework in your code, CoT is absent and instead you used our unfinished repository. \n\nThus, the comparison seems a little bit unfair. While the training techniques for LMs with MLE are well-studied for years, there is much space for investigation of that for LMs trained via CoT (and, of course, discrete GANs). As a result, in our opinion, your experiment shows that CoT with current progress is capable of obtaining comparable results to a well regularized MLE-LM, and even better under a range of entropy settings, where NLL_{oracle} ranges from about 7.1 to 8.8. Please notice that the estimated entropy of the data by CoT given limited observation (10000 samples) lies in such a range. We admit that we are not sure about the reason why CoT cannot keep such advantages in marginal entropy settings (lower and/or higher), but as an educated guess, it may due to that it is more easy for MLE to memorize the training samples while CoT enforces the network to explore-and-improve. As a consequence, if the mediator is not strong enough (i.e. not perfectly matching the assumptions in our theory), the trained model may behave slightly worse in extreme cases.\n\nHowever, despite these minor arguments, in general, we agree with the opinions in your paper and that there needs to be a revolution in the field of language GAN researches before it actually becomes fruitful. Good job!",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1eMNq-faQ",
                "reply_to": "SJx1up316X",
                "title": "Addressing:  \"(...) and to achieve improved sample quality in terms of BLEU the authors adjust the temperature. Based on the experimental results, it\u2019s not obvious that this set of techniques improves performance over the MLE baseline. \"",
                "comment": "I think you are right saying that the evaluation protocol is concerning. Shameless plug: We just wrote a paper on this https://arxiv.org/abs/1811.02549 showing that Textual GANs have wrongly claimed that they can outperform MLE baseline. \ntl;dr : the most effective way to compare NLG models is in quality-diversity space with respect to multiple temperatures. What we find is that MLE outperforms all textual GANs everywhere in the quality-diversity spectrum. MLE is however tied with the newly proposed CoT (see Figure 3 for a comparison of the models on the Oracle task). \n\nThat being said, having played with all the language GANs, I can definitely say that CoT works better than all the proposed GANs trained with REINFORCE. However, I am not convinced (yet!) that it outperforms MLE. \n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1xBB06y6Q",
                "reply_to": "SJx1up316X",
                "title": "Reply to \"major concerns with algorithm and evaluation\"",
                "comment": "Thanks for your attention!\n\nWe have seen your review before when submitted to a previous conference and here we would like to make an official response to it.\n\nIn case you haven't noticed, we want to emphasize that this version of the paper is quite different from the version you've already seen, especially in the parts you've mentioned.\n\nResponse to the first part of the review\n\nThe reviewer makes the claims that using the mediator is not necessary and the proposed algorithm still incorporates REINFORCE algorithm. According to our submitted version of the paper, however, our presented algorithm disagrees with either of the claims.\u00a0\n\nWe are confused why the reviewer claims so since one of the most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this. The paragraphs around Eq.13 describe our approach in details.\n\nFor the necessity of the mediator, please check Sec 3.4.2, which is quite different from the version you have read. Your suggested version of the model may not be practically implementable because it cannot provide a probability prediction IN EACH TIMESTEP, which is very important if this module is to be used by CoT. Notice that M(x|s_t) is not equal to (G(x|s_t) + P(x|s_t)) / 2, making the factorization actually non-trivial.\n\nResponse to the second part of the review\n\nBefore this version of the paper is submitted to ICLR, we have deleted Theorem 4 in an earlier version of the paper as we consider your concerns about it to be correct. (thanks!) In our current version, we treat Mediator as an IMPLICIT estimator of JSD. The reason why such estimation is implicit is that the calculation of entropy of the input data is non-trivial. However, if you pay attention to Figure 3 and the paragraphs around it, we've shown balanced NLL, which, in theory, is actually only different in a constant from the estimated JSD. In practice, as the model's estimation of data entropy also improves, such difference may also change steadily as the training proceeds.\n\nAbout the behavior of the model when the mediator is not trained to optimality, we have empirically shown that it is still stable. Note that GANs also do not have such a guarantee, and in practice it is much more unstable than our approach. Please refer to Figure 2(b).\n\n\nWe have collected samples from three typical models and shown them in the appendix. Please also check it.\n\nResponse to minor comments:\n\nWe appreciate your suggestions. However, as the paper has length limit, we are not able to cover all aspects. We will consider your suggestions seriously and incorporate them as much as possible.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BklsMW0kpX",
                "reply_to": "Bygvu1w2jm",
                "title": "Reply to ICLR AnonReviewer1",
                "comment": "Thanks for reviewing our paper!\n\nResponse to your concerns:\n\n1. We have contacted one of the authors of the LeakGAN, finding that the data pre-processing and post-processing of ours and theirs are different. This makes the results quite different.\n2. We will present with an error bar in the coming revised version.\n\nResponse to Misc:\n\n1. This is an interesting topic, we would have some discussion about it if we have found interesting conclusions.\n\n2. We've actually implemented a continous version of CoT, of which the prior distribution is replaced by Beta Distribution instead of Multinomial Distribution in the current discrete version of CoT. However such a model does not perform well. This is an interesting direction for further research and survey.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJelZVAJpX",
                "reply_to": "r1lfAuiyaQ",
                "title": "Response to AnonReviewer3",
                "comment": "Thanks for reviewing our paper.\n\n1. For reproducibility, we are preparing for a open-source code base. After the paper is de-anonymized, we will attach a link to it.\n\n2. Before the paper of CoT is completed, we have had attempts at several different divergences, including JSD(CoT), Reverse KL(as is described in the appendix), Wasserstein-1 distance, etc. However, only CoT and Reverse KL succeed in getting rid of pre-training via MLE. Reverse KL appears to have mode collapsing problem, therefore CoT is finally the chosen model. However, this is a good direction for further research. We are also interested.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hkghpphk6X",
                "reply_to": "SJx1up316X",
                "title": "major concerns with algorithm and evaluation (2/2) ",
                "comment": "Minor comments:\n- The introduction does not sufficiently motivate the problem. What\u2019s broken about MLE and why do we need to target different divergences or alter the training procedure?\n- non-standard notation for the x_t/s_t, maybe use s_{:t}\n- why is finite observations problematic for MLE? It is the only divergence that depends only on samples from the data distribution and not evaluating the data density.\n- Why does an ideal solution have to be symmetric or smooth? You should discuss the issues with KL and JSD when the distributions do not have overlapping support which is core to the Arjovsky and Bouttou paper. Also if the generative model G(\\theta) is sufficiently powerful, then you can get good quality and diversity from any divergence.\n- The presentation of the generator loss in eqn 5 is confusing. By analogy to the original GAN, I expected the generator loss to be -E_{s ~ G}[log(1-D(s))], but you\u2019re using the raw D(s) in this expression. This loss looks somewhat like REINFORCE with D(s) as a reward. Are you also doing a new rollout starting from each timestep t? I guess this is taken from SeqGAN but the objective is confusing to me and not presented clearly.\n- Again, I don\u2019t see why a symmetric divergence are needed to give you quality and diversity. \n- when the mediator is not optimal (which it never will be in practice), CoT does not provide an unbiased mechanism for training with the JSD.\n- if the mediator M does not equal 0.5 * (P + G), then the estimate of JS will be biased, as will the gradients. Furthermore, you are ignoring one of the gradient terms when you are plugging into estimate JS: dL/dM dM / dG * dG/ d\\theta. Yes, the quantities are equal, but the gradients are not necessarily equal (eq 18)\n- what do you mean by \"generatively and predictively\u201d? In practice, CoT is not consistent unless the mediator is exact, so I don\u2019t think it\u2019s fair to highlight consistency vs. scheduled sampling.\n- \u201csame order of computational complexity as MLE\u201d: isn\u2019t it twice as expensive? And critically, you have to sample from the autoregressive generative model which will make it even more expensive.\n- this is a good point that you don\u2019t require MLE pretraining, but a subroutine is training a model with MLE on data samples (mediator)\n-  This is a critical section highlighting why the mediator is necessary, but I have no idea what this paragraph is saying. \n- you should describe the synthetic data experiment so the paper is self-contained.\nTable 1: Please add error bars. Did you try early stopping with MLE?\nFigure 2: Would be interesting to plot these learning curves for MLE as well.\nFigure 3: Why not plot estimate of JSD using the current mediator? Does the estimate of JSD track the true JSD?\nTable 2: MLE baseline and CoT-basic performance look about the same. w/o error bars unclear if these are significant differences. To get good samples in terms of BLEU you adjust temperature. What happens if you do that with an MLE model?",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJx1up316X",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "major concerns with algorithm and evaluation (1/2)",
                "comment": "Disclaimer: I reviewed this paper for a previous conference. The authors have not updated the paper to address any of my concerns, so I have copied my review below.\n\nThis paper presents a new technique (CoT) for training gen. models with tractable densities on discrete data. Instead of minimizing the KL divergence as in MLE, CoT minimizes the JS divergence. Unlike GANs that learn a critic to approximate the density ratio between the generated and true samples, CoT learns a \u201cmediator\u201d that approximates the mixture of the two densities, and uses that to construct the density ratio. They evaluate the technique on a synthetic sequence dataset from SeqGAN, and the EMNLP 2017 WMT News Section, and show improved performance in terms of sample quality (oracle NLL, BLEU) and sample diversity (test NLL, word mover distance). \n\nOverall, I found the idea of using a mediator to construct the density ratio interesting, but the use of an extra mediator is not necessary (see below).  The core claim that the technique is \u201cunbiased, low-variance, and computationally efficient\u201d is not sufficiently demonstrated. In particular, computing gradients of the objective function still require REINFORCE to differentiate through the sampling process, and the technique requires training an additional tractable density model on the mixture. Furthermore, there are no samples in the paper, and to achieve improved sample quality in terms of BLEU the authors adjust the temperature. Based on the experimental results, it\u2019s not obvious that this set of techniques improves performance over the MLE baseline. The presented technique is also substantially more expensive as you have to sample from an autoregressive model at training time, and evaluate two densities instead of one (mediator and generative model). An improved version of the paper should clarify how gradients are computed efficiently, clean up the language and presentation throughout,  and present a more thorough evaluation of the technique versus MLE.\n\nMajor comments:\nThere is no reason to continually train and update an extra network to be the mediator. If you first train a network M with MLE, then that represents the best approximation in the model class to the data density, P. You can then create a new network G, and use G / ((P + G)/2.0) to construct the desired density ratio. This approach would require training M first, but removes the need for an explicit mediator network while training.\n\nPaper is missing references to related work that leverages a tractable density as noise to estimate another density, e.g. noise contrastive estimation Gutmann & Hyvarinen 2010, b-GAN Uehara et al., 2017, and especially \u201cOn the distinguishability criteria for estimating generative models\u201d Goodfellow 2015. The Goodfellow paper presents a technique they coin self-contrastive estimation which seems related to CoT.\n\nWhile Theorem 3 makes sense to me, Theorem 4 is not obvious. The proof (for both theorems?) shows that the *value* of the CoT loss equals the JSD, but it does not prove that the *gradients* of the CoT loss equals the gradients of the JSD. To prove this, you need to invoke Danskin\u2019s theorem. A simple example for why objective being equal at a point does not imply gradients are equal:\nLet f(x) = x**2\nLet m = x, and define g(x) = x * m\nThen f(x)= g(x) at x=m, but the gradient of f(x) at m is 2*m, while the gradient of g(x) at m is just m.\n\nThe objective and gradient presented in eqs 13 and 14 have a gradient w.r.t. the parameters of the generative model \\theta outside of an expectation over discrete samples s ~ G_\\theta. The paper does not explain how you compute this gradient! To form an unbiased approximation of this expectation, you would have to use REINFORCE. How did you actually train these models? Do you use REINFORCE? Do you ignore this term in the gradient? I can\u2019t see how to compute this without getting high bias or high variance gradients.\n\nAll the theory assumes the discriminator is trained to optimality. If is not, how does the technique fail? If the mediator can\u2019t model (P+G), then your estimate of the density ratio will be wrong, and all the guarantees go out the window. If the mediator can model (P+G), then it can likely also model P, and then you can just build a perfect model with MLE.\n\nThere are no samples presented for the News Section (or synthetic) experiments! If the argument is that sample quality is improved, it would be great to have examples. The CoT-basic model performs almost the same as MLE! The CoT-strong model doesn\u2019t present much of an improvement except when you use a different temperature for sampling. It looks like just using the strong model trained with MLE and tuning the temperature could achieve similar results.\n\nThere are a number of grammatical errors throughout, and the text is often confusing and unclear. ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1lfAuiyaQ",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "Original idea, clear presentation",
                "comment": "*Summary*\nA clear an interresting presentation on learning sequences distributions. It achieve this objective by replacing the discriminator with a \"mediator\", a mixture between the training distribution and the target distribution which is estimated via maximum likelihood.\n\n*Pros*\n- Original idea for modelling distribution of sequence data\n- Theoretical convergence in the Jensen Shanon divergence sense\n- Promising experiments\n\n*Cons*\n- No major cons to the best of my knowledge\n\n*Typos*\n- It would be very nice to have black and white / color blind friendly graphs\n- Eq 10 too long\n- Introduce J_m & J_g in  sentence\n- Coma at the end of Eq 5, and maybe align Generator and Discriminator in some position (e.g. at the semi colon).\n- missing dot at Eq 8.\n\n*Question*\n- How would you ensure reproducibility (e.g. link to some code?)\n- Is there any hope to obtain consistency (convergence) wrt other metrics?",
                "rating": 7,
                "confidence": 2,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HkgJzPJP2X",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "Interesting and promising method for generative modelling of sequence data without policy gradient",
                "comment": "The paper proposes an interesting method, where the discriminator is replaced by a component that estimates the density that is the mixture of the data and the generator's distributions. In a sense, that component is only a device that allows estimating a Jensen-Shannon divergence for the generator to then be optimized against. Other GAN papers have replaced their discriminator by a similar device (e.g., WGANs, ..), but the present formulation seems novel. The numerical experiments presented on a synthetic Turing test and text generation from EMNLP's 2017 news dataset appear promising. \n\nOverall, the mediator seems to allow to achieve lower Jensen-Shannon (JS) divergence values in the experiments (and is kind of designed for that). Although this may be an improvement with respect to existing methods for discrete sequential data, it may also be limited in that it may not easily extend to other types of divergences that have proved superior to JS in some continuous settings.\n\nThe paper is rather clear, although there are lots of small grammatical errors as well as odd formulations which end up being distracting or confusing. The language should be proof-read carefully. \n\nPros:\n- Generative modeling of sequence data still in its infancy\n- Potentially lower variance than policy gradient approaches\n- Experiments are promising\n\nCons:\n- Lots of grammatical errors and odd formulations\n\nQuestions:\n- Equation 14: what does it mean to find the \"maximum entropy solution\" for the given optimization problem?\n- Figure 2: how do (b) and (c) relate to each other?\n\nRemarks, small typos and odd formulations:\n- \"for measuring M_\\/phi\": what does measuring mean in this context?\n- What does small m refer to? Algorithm 1 says the total number of steps  but it is also used in the main text as an index for J and \\pi (for mediator?)\n- Equation block 8: J_m has not been defined yet\n- \"the supports of distributions G and P\"... -> G without subscript has now been defined in this context\n- \"if the training being perfect\"\n- \"tend to get stuck in some sub-optimals\"\n- the learned distribution \"collapseS\"\n- \"since  the data distribution is, thus ...\"\n- \"that measures a\" -> \"that estimates a ...\"?\n- \"a predictive module\": a bit unclear - generative v. discriminative is more usual terminology\n- \"is well ensured\"\n- \"with the cost of diversity\" -> \"at the cost of diversity\"?\n- \"has theoretical guarantee\"\n- in the references: \"ALIAS PARTH GOYAL\" (all caps)\n- \"let p denote the intermediate states\": I don't understand what this is. Where is \"p\" used? (proof of Theorem 3)\n- \"CoT theoretically guarantees the training effectiveness\": what does that mean?\n- Figure 3: \"epochs\" -> \"Epochs\"\n- Algorithm 1: what does \"mixed balanced samples\" mean? Make this more precise\n- \"wide-ranged\"\n- Equation 10 is too long and equation number is not properly formatted\n- Figures hard to read in black & white\n- Figure 2 doesn't use the same limits for the Y axis of the two NLL plots, making comparisons difficult. The two NLL plots are also not side-by-side",
                "rating": 7,
                "confidence": 2,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SJlvFB_anQ",
                "reply_to": "BJerU0WThX",
                "title": "Please refer to LeakGAN's original paper",
                "comment": "Thanks for your attention. We would like to explain our considerations when we were writting this part. In the paper of LeakGAN, as is proposed by the authors, a typical setting of LeakGAN shall be:\n\n1. When used for generative purposes, alpha is always set to be 1.5 (as they said ``conservative strategy'').\n2. When used for evaluating NLL or sampling trajectories for reinforcement learning, alpha is always set to be 1.0.\n\nIn other words, in LeakGAN's original paper, they do not guarantee the model would also perform well under the settings as you've described. \n\nHowever, the authors of LeakGAN did update a new version in their official github code base, where the temperature trick is completely removed. We would update a new version with related results when revision is enabled. Thank you.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJerU0WThX",
                "reply_to": "Bygfh6S2t7",
                "title": "Still Missing Critical Component for Table 2",
                "comment": "Hi,\n\nTo make a more fair comparison for Table 2, it's better to first set \\alpha = 1.0 for all models (including Leakgan) and compare them with CoT. Then set \\alpha = 1.5 for all models and compare the results again. \n\nBasically you need to show that your model outperforms Leakgan under both settings (\\alpha = 1.0 and \\alpha = 1.5) to claim that the new model is state-of-art. Since you have compared Leakgan's NLL under \\alpha = 1.0, why don't you also record the BLEU score and compare it with CoT under \\alpha = 1.0? ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyxDOevL2Q",
                "reply_to": "r1eBMvIInm",
                "title": "Please refer to our notations",
                "comment": "Please refer to our notations at Sec 2.\nIn this case, for any given sequence S_t, its t-1-length prefix S_{t-1} is unique.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1eBMvIInm",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "question relates to \"Detailed Derivation of The Algorithm\"",
                "comment": "for line 2 within the proof,  G[S_(t-1)]G[S_(t)|S_{t-1}] = G[S_{t}] seems to be wrong. It should be SUM(i) G[S_(t-1)^{i}]G[S_(t)|S_{t-1}^{i}] = G[S_{t}] ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1evoVp4sQ",
                "reply_to": "S1gFX_4NjQ",
                "title": "Awesome",
                "comment": "Great! Thanks you kindly",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1gFX_4NjQ",
                "reply_to": "Hyl6DtGNjm",
                "title": "Sequence length information has been already provided, with which reproducibility is actually possible",
                "comment": "If you pay attention to the paper, we've actually provided with data pre-processing details (see page 8 Sec 4.2). The sequence length limit is set to be 51.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hyl6DtGNjm",
                "reply_to": "HJx0MGM4sX",
                "title": "Reproducibility is impossible with such an approach",
                "comment": "Hi, \n\nthanks again for your fast response. While I agree that the comparison is fair across all models using the same - fixed - sequence length, my issue is the following: \n\nIn the Texygen repo you mentioned, the NLL values are calculated as : \n\n        NLL = -tf.reduce_sum(\n            tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_vocabulary, 1.0, 0.0) * tf.log(\n                tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_vocabulary]), 1e-20, 1.0)\n            )\n        ) / (self.sequence_length * self.batch_size)\n\nOn the last line, we see that we are dividing by the sequence length. Therefore, by making the sentences arbitrarily large with <PAD> tokens, NLL_{test} becomes increasingly better, since predicting <PAD> tokens is trivial. Therefore, one can see that as self.sequence_length --> infinity , NLL --> 0.\n\nThe issue is that it is hard, if not impossible to replicate the results obtained in your paper without knowing the value of self.sequence_length. Therefore, benchmarking against your algorithm on NLL_{test} is unnecessarily hard. Results presented should be agnostic of such preprocessing details. Moreover, on a more personal note, Texygen has some major design flaws, such as constantly dumping words to text files instead of simply keeping then on GPU (or RAM for that matter), making it somewhat painful tool for research. Avoiding having to using Texygen to reproduce results would be great.\n\nThank you\n\n\n\n",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJx0MGM4sX",
                "reply_to": "BJxIadb4iQ",
                "title": "PAD tokens should be included in this case even in testing the NLL",
                "comment": "Hi,\n\nWhat we are trying to say is that even if when evaluting the test NLL, pad tokens should not be eliminated so that any unstability of keeping the padding segment consistent (i.e. the predicted likelihood of pad token should be almost always 1.0 since the first pad token is generated) would be detected and penalized by doing so. Such setting does not introduce any unfairness of the comparison, since for all evaluated models, the padded sequence length (max sequence length) is the same. One could not have arbitrarily good NLL performance by doing what you've described, since in our setting it is not allowed to place additional <PAD> after the sequence (otherwise it makes the padded sequence longer than the maximum sequence length limit).\n\nHere is an example of our consideration:\n\nSuppose there are two evaluated models, namely A and B.\n\nEvaluated Sequence: I have a pen . <PAD> <PAD> <PAD>\n\nProbability prediction of each step:\n     I      have     a     pen    .       <PAD>   <PAD>  <PAD>  <PAD>\nA: 0.3    0.5     0.1   0.5    0.7       0.99      0.8         0.999   0.9999\nB: 0.3    0.5     0.1   0.5    0.7       0.99      0.999     0.999   0.9999\n\nWe prefer model B, since if the prefix \"I have a pen . <PAD>\" is given, model B would have almost 1.0 probability to generate a correctly padded sequence, while for model A it would have only 0.8 probability to do so, even if for non-padding part the two models are actually the same.\n\nAs for making it easier for other researchers to make correct comparison, we recommend implementing and evaluating with Texygen, which can automatically deal with these data preprocessing issues.\n\nBest",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJxIadb4iQ",
                "reply_to": "rJxfLb-4j7",
                "title": "Why on the test set?",
                "comment": "Hi, \n\nthanks for the answer. While I understand why you could need this for training, why not mask the tokens when calculating the likelihood on the test set ?  One could have arbitrarily good NLL performance by simply increasing the sentence length with PAD tokens, since the distribution becomes fully deterministic after the first PAD token. \n\nIt would be greatly appreciated if, for reproducibility purposes, you could update the paper with the correct likelihood. This will allow other researchers to correctly compare their model with yours. \n\nThank you",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJxfLb-4j7",
                "reply_to": "S1xrZlyNjQ",
                "title": "PAD tokens are included.",
                "comment": "Hi,\n\nFor all evalutated models, the padding tokens are included in the calculation of NLL. The reason of doing so is that we observed some model incorrectly generate non-padding tokens even if the model has generated a few padding tokens. The reason may be that in text generation tasks, generated tokens are SAMPLED from each time's probability prediction over the vocabulary instead of directly MAGINALIZED to be the argmax token. This, however, could be considered as an important feature when evaluating different training algorithms.\n\nExample:\nOne typical failure:\n\nA cat is sleeping on the table . <PAD> <PAD> with <PAD> <PAD> <PAD> . <PAD> ......\n\nTo penalize such failures, when calculating the NLL, all paddings are not eliminated. We hope our such consideration makes sense to you.\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1xrZlyNjQ",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "Are pad tokens included in the calculation of NLL_{test} (Table 3) ?",
                "comment": "Hi, \n\nI am somewhat familiar with the EMNLP2017 WMT News dataset, and the NLL values reported are quite lower than what I'm used to seeing. It is possible that the PAD tokens are included in the calculation of the likelihood ?\n\nThank you",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bygfh6S2t7",
                "reply_to": "H1lnfB-ntX",
                "title": "Table 2 and 3 are designed as such to make fair comparison with LeakGAN",
                "comment": "Thank you for your attention! As we would like to point out, there is one thing special about LeakGAN when compared to other baseline models.  In LeakGAN's default settings,when it is used to generate samples, the temperature of the generator is adjusted to be a little bit lower (i.e. \\alpha = 1.5). However, when LeakGAN is used to compute the predicted NLL, \\alpha will be set to 1.0. To keep the comparison fair, we show results of CoT-strong under different settings (\\alpha = 1.0 and \\alpha = 1.5) to support our claim that:\n\n1. When we set \\alpha = 1.0 i.e. keep the temperature parameters as they originally are, CoT-basic and CoT-strong outperforms baseline models with the same settings, including MLE, SeqGAN, RankGAN and MaliGAN. \n\n2. When \\alpha is set to 1.5, CoT-strong outperforms LeakGAN.\n\nThe proposed CoT does reach the state-of-the-art, since in both cases CoT reaches the state-of-the-art. As for diversity benchmarks, since every evaluated model sets \\alpha to 1.0 in this case, such classified discussion is not necessary.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1lnfB-ntX",
                "reply_to": "iclr_2019_SkxxIs0qY7",
                "title": "Table 2 and 3 are misleading!",
                "comment": "Table 2 shows 'quality' performance. By decreasing the temperature (decreased entropy) of your model you can outperform your baselines. However, in Table 3 you present 'diversity' performance but you don't report your model at the lower temperature (\\alpha=1.5) and this model has to do worse because of the quality/diversity trade-off.\n\nThus, there is no way to know if CoT is state-of-the-art or for that manner actually outperforms any other model.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "an original and interesting alternative to GANs for optimizing a (proxy to) Jensen-Shannon divergence for discrete sequence data",
                "Sentiment Expression": "proposes",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Experimental results",
                "Sentiment Expression": "seem promising",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "false claims that are not well explained or supported",
                "Sentiment Expression": "still makes",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The current version of the paper",
                "Sentiment Expression": "does neither",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The approach",
                "Sentiment Expression": "remains original, interesting, and potentially promising",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "5dHQyEcYDgA": {
        "paper_id": "nips_2022_5dHQyEcYDgA",
        "paper_title": "Additive MIL: Intrinsically Interpretable Multiple Instance Learning for Pathology",
        "paper_abstract": "Multiple Instance Learning (MIL) has been widely applied in pathology towards solving critical problems such as automating cancer diagnosis and grading, predicting patient prognosis, and therapy response. Deploying these models in a clinical setting requires careful inspection of these black boxes during development and deployment to identify failures and maintain physician trust. In this work, we propose a simple formulation of MIL models, which enables interpretability while maintaining similar predictive performance. Our Additive MIL models enable spatial credit assignment such that the contribution of each region in the image can be exactly computed and visualized. We show that our spatial credit assignment coincides with regions used by pathologists during diagnosis and improves upon classical attention heatmaps from attention MIL models. We show that any existing MIL model can be made additive with a simple change in function composition. We also show how these models can debug model failures, identify spurious features, and highlight class-wise regions of interest, enabling their use in high-stakes environments such as clinical decision-making.",
        "paper_acceptance": "Accept",
        "meta_review": "generalized additive models, for performing MIL on digital pathology datasets in order to improve model interpretability.\n\nThe reviewers find that the paper is well written and describes the problem setting as well as their solution well.\nThe method is considered a valuable way to provide interpretability, is general and novel.\nThe empirical evaluation shows the benefit on three different data sets.\nThe reviewers agree on acceptance of the paper.\n\n",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "MOzLUtiaGnS",
                "writer": "official_reviewer",
                "reply_to": "Xhwbl6PyYoV",
                "title": "Post Rebuttal",
                "comment": " Thanks for addressing my comments. The additional experiment in the supplement is helpful, and could be worth highlighting in the main text. All of my other concerns are address, and I am keeping my original rating.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gVUfduIKUrd",
                "writer": "official_reviewer",
                "reply_to": "S5UGc66H3vp",
                "title": "acknowledgement of author's response",
                "comment": " I appreciate the author's response. In particular the clarification of the theoretical analysis with Shapley values. I also think that moving the proof to the appendix is good as the focus of the paper is not on this aspect of the work. Overall, I keep my good rating.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Xhwbl6PyYoV",
                "writer": "author",
                "reply_to": "EWSJ0NkG9Vq",
                "title": "Author Response",
                "comment": " Thanks for your extensive comments and thoughtful suggestions! We wanted to take this opportunity to respond to some of your questions:\n\n1. We agree that the value of the heatmaps and additional interpretability is best evaluated by an expert pathologist. To show that the regions highlighted by a pathologist coincide better with our additive MIL heatmaps, we densely annotated the Camelyon16 slides to conduct the study mentioned in Section 3.3.2. However due to time constraints, we could not get exhaustive annotations for tumor subtype. A significant complication of tumor subtyping is inter-pathologist variability in assigning tissue regions to a subtype, indeed some cases clinically require a specialized immunohistochemistry stain to make this evaluation. Nevertheless, we did attempt to address this in our supplementary submission where in Section 4, Table 1, we conduct a qualitative assessment of the attention and the additive MIL heatmaps with a board-certified expert pathologist. The expert reviewed 50 slides from Camelyon16 (micrometastasis detection) and 39 slides from TCGA RCC (kidney cancer subtyping) and answered which heatmap of the two would be most useful. The pathologist preferred the additive heatmap in 33/39 slides for subtyping and 49/50 for micro metastasis detection. While qualitative we believe this data strongly suggests that the additive heatmaps better align with human expectation and will be more useful in a clinical context.\n2. We have updated our figures by adding colorbar legends to them. Thanks for this suggestion.\n3. We\u2019ve also added the details around the number of patches, bags and slides in each dataset in Section 3.1.\n4. You rightly noted that even though the number of parameters in both the Additive and non-Additive models stay the same, the computation needed for an Additive model is slightly more due to the function being applied to every patch and being summed later. However, this is all still a vectorized matrix multiplication operation on the GPU and does not add a lot of additional computation. Specifically, the additive model had 30.04 GFLOPS as compared to the 29.98 GFLOPS of the non-additive model which resulted in an increase of the forward pass time by 0.08% using a Quadro RTX 8000 (averaged over 100 iterations with a bag size of 100).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Rn6_cLpqmii",
                "writer": "author",
                "reply_to": "S5UGc66H3vp",
                "title": "Author Response",
                "comment": " Thanks for your detailed comments! We appreciate that you liked reading the paper and noted the fact that it could be retrofitted to any previous MIL technique. We agree with your assessment that the equivalence of Additive MIL predictions with Shapley values could\u2019ve been better motivated. Shapley values represent the exact marginal contribution of patch towards a prediction, making them ideal for interpretation. However, computing Shapley values requires computing over combinatorial sets which, in practice, is computationally intractable. Therefore, previous methods in interpretability literature use approximations. In our proof we show that our additive MIL formulation is equivalent to Shapley values without approximation, we believe that this represents a significant technical contribution and provides theoretical support for our approach and results. We have re-written Section 2.3 to better highlight why this equivalence is an important property of Additive MIL models in the context of patch attribution. We have also moved the proof to the appendix so it\u2019s not distracting to the reader.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Qe-UemsD3Te",
                "writer": "author",
                "reply_to": "04GSqryF0gg",
                "title": "Author Response",
                "comment": " Thanks for your comments and thoughtful feedback! It\u2019s great that you share our view on how attention coefficients in attention MIL are not straightforward to interpret. To answer the two questions you mentioned in your comment:\n\n1. We agree that the equivalence of Additive MIL predictions with Shapley values could\u2019ve been better motivated. Shapley values represent the exact marginal contribution of patch towards a prediction, making them ideal for interpretation. However, computing Shapley values requires computing over combinatorial sets which, in practice, is computationally intractable. Therefore, previous methods in interpretability literature use approximations. In our proof we show that our additive MIL formulation is equivalent to Shapley values without approximation, we believe that this represents a significant technical contribution and provides theoretical support for our approach and results. We have re-written Section 2.3 to better highlight why this equivalence is an important property of Additive MIL models in the context of patch attribution. We have also moved the proof to the appendix so it\u2019s not distracting to the reader.\n2. We implemented the MIL model without attention using a fixed mean pooling function as you suggested. We agree that showing this disentangles the use of attention and the additive formulation. We have updated the paper with these results.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "04GSqryF0gg",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_5dHQyEcYDgA",
                "title": "",
                "comment": " The authors present a method, inspired by generalized additive models, for performing MIL on digital pathology datasets in order to improve model interpretability. Additive MIL models allow for straightforward patch credit assignment where spatial interpretability is improved over attention MIL methods alone. Additive MIL can be applied to any MIL method that includes a pooling operation.  Strengths: \n1) The authors provide an excellent explanation for the reasons why attention coefficients in attention-based MIL cannot be interpreted simply as the most positive representatives of given class. I have had a similar thought and the authors articulate this very well. \n2) The proposed methods does provide an additional and more informative level of interpretability that is demonstrated well in the figures. The authors are commended for emphasizing this work is about interpretability, not about improved predictive performance. \n3) The method is simple and general, which is the mark of a novel and important contribution. \n\nWeaknesses: \n1) I did not find the derivation of Theorem 1 (Shapley values) helpful for understanding the importance or major contributions of the paper. This derivation seemed out of place or added to increase the content/technical contribution of paper. I leave this to the other reviewers if they believe it is helpful. \n2) I would recommend the authors include results on the use of Additive MIL withOUT an attention module (w/o AB or transformers). I understand that this is not the emphasis of the paper, but it will help disentangle the importance of attention and additive MIL for predictive performance (i.e. is additive MIL sufficient?). These results can be included in the appendix or supplemental data. Decreased predictive performance with Additive MIL only does not weaken the paper.  Please see weakness above.  Yes. ",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "S5UGc66H3vp",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_5dHQyEcYDgA",
                "title": "",
                "comment": " This paper presents a method to improve interpretability of attention-based MIL models by using an additive predictor function (last layer head of the model). Although general, the method is applied to histological images where MIL is popular due to the weak nature of slide-level labels. The experimental section shows that making the predictor additive does not reduce the performance of MIL models on 3 histology datasets. The interpretability improvements are shown both quantitatively on a dataset of annotated cancer regions and qualitatively on several examples where class-level and excitatory/inhibitory saliency heatmaps are made possible.\n strengths:\n- clarity/quality: the paper is clear and a joy to read.\n- significance: the method is simple and can be retrofitted to existing attention-based MIL models such as ABMIL or TransMIL. It is shown to have little impact on accuracy but does provide significantly improved feedback to pathologists, which is key to the acceptance of AI models.\n- empirical study: the authors focus on digital pathology and make a compelling case of the benefit of the method using 3 different public datasets spanning different organs and diseases. First the method is shown to have little impact on the accuracy of two different SOT attention-based MIL methods, even showing improved accuracy in the case of ABMIL.\n- The authors went through the effort of annotating cancer regions on one dataset in order to quantify the improvement in explainability of additive MIL compared to attention MIL.\n- A theoretical effort is made to equate the additive MIL with Shapley sampling values, a game-theoretical framework for interpreting model prediction by decomposing by their feature contributions.\n\n\nweaknesses:\n- originality: the true novelty is somewhat limited. The paper combines two methods: attention-based MIL and additive attribution models. Nevertheless the combination of the two is original.\n- The added benefit over attention-MIL is relatively small, as measured quantitatively against pathologist's annotations: from 0.36 to 0.42 AUPRC. Nevertheless, it shows a significant improvement in false positive reduction.\n\n - I am not sure what the point is of the equivalence with Shapley Values. This is not well explained and the theorem proof comes a bit unmotivated.\n The authors do address some of the limitations of their approach, mostly the fact that the guarantee of not degrading performance through \"additivisation\" of a MIL method  was evaluated only on 2 methods and 3 datasets and thus may not hold true for any dataset or MIL method. \n\n",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "EWSJ0NkG9Vq",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_5dHQyEcYDgA",
                "title": "",
                "comment": " This paper propose an additive MIL technique to improve interpretability of MIL techniques. The key idea is to make prediction per instance, and summing (\"additive\") these predictions to get the final probability. This allows the author to get 1) excitatory/inhibitive signal for each patch 2) per-class contribution for each patch 3) a more linear instance contribution and 4) more straightforward way to understand interactive between patches (since their signal are getting added together). The author perform experiment on two TCGA cancer types and Camelyon 16, and demonstrate no loss of performance with respect to the state of the art, while displaying many desirable quality for interpreting the model's prediction. - The method is very simple and appear to have worked well. It consists only change in order of operation, where the prediction of each patch is summed after the final logit, rather than the input.\n- There is a theoretical guarantee that the per-instance prediction does represent the marginal prediction. \n- Because the prediction are being added together, the comparison/interpretation of the each per-instance prediction is direct, because each prediction contribute linearly to the final prediction.\n- The evaluation task is realistic, using sizeable datasets that are publicly available. The problem are varied enough, from micromet detection (which is more aligned with classical MIL problem) to cancer subtype classification (where the judgement may require taking average rather than the max).\n- My main concern is whether or not the additive MIL heatmap was really helpful for interpretation. It is true that we can see per-class excitation/inhibition from the heatmap, but it is unclear if these actually are meaningful. The authors claim (such as in Fig 4), that the heatmaps are aligned well with the tissue, but without extensive knowledge of tumor pathology, it is impossible for the reader to judge if the claim is correct or not. I recommend the author obtain pathologist segmentation of each tumor subtypes, so that we can evaluate better if the model heatmap actually align with human expectation.\n\nTo this point, the author did look at the problem on Camelyon, but only one small patch is shown on this binary classfication problem.\n- Figures can be labeled more clearly. It took me sometime to understand what each color is in Fig 3-6.  One idea is to add colorbar to clearly label every color used. In fig 3, there is only one legen in the top middle figure, when this hsould been to the side/top so it's more clear that this apply to every panel. - Because of the change in operation order, there seems to be much more computation that needs to happen (prediction for every patch). How does this affect run time of the method compared to SoA?\n- Why is the squares in Fig 3 appear to have different sizes in each column?\n- It could be helpful to give the reader the sense of scale for each dataset (how many instance per bag, how many bag total, how much of the instance is needed for the bag to be considered positive, etc). Some of these are already covered in text, but I think it could be helpful to see it side-by-side in a table. A problem with 10-instance per bag is going to be different from a problem with 10k-instance per bag (which I think is the case here and making the technique even more significant). Yes",
                "rating": 8,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The method",
                "Sentiment Expression": "is considered a valuable way to provide interpretability, is general and novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The empirical evaluation",
                "Sentiment Expression": "shows the benefit on three different data sets",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "is well written and describes the problem setting as well as their solution well",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "acceptance",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "0RDcd5Axok": {
        "paper_id": "iclr_2022_0RDcd5Axok",
        "paper_title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
        "paper_abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.",
        "paper_acceptance": "Accept (Spotlight)",
        "meta_review": "The paper reviews and draws connections between several parameter-efficient fine-tuning methods.\n\nAll reviewers found the paper addresses an important research problem, and the theoretical justification and empirical analyses are convincing.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "-ZIWcdyn-tP",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_0RDcd5Axok",
                "title": "",
                "comment": "This paper formulates different approaches for parameter-efficient transfer learning (such as adapters, prefix-tuning, LoRA) under a common framework. Approaches vary alongside different design dimensions: functional form, insertion form (sequential or parallel), which representation is directly modified by the approach and the composition function of the main representation with the adaptation one. \n\nThe authors describe how existing approaches fit this framework. Then, they show how changing these design choices leads to new approaches, including parallel adapters (PA), multi-head PA and scaled PA. \nThe authors then compare their newly introduced methods and existing ones empirically. They find that:\n1. Parallel insertion is generally better\n2. Under a low parameter budget (~0.1% of original) it is preferable to modify the attention of the transformer. Under a higher budget, changing the feed-forward network is better.\n3. Their methods generally perform on par or better than existing approaches.\n\nFinally, the authors combine their insights to develop \u201cMix-and-match\u201d adapters and show that it does well overall.\n Pros:\n\n- The common framework for different approaches is useful overall. In particular, I liked the section clarifying the connection between Prefix Tuning and Adapters. Although there is nothing groundbreaking here, I believe these frameworks can be useful and help researchers.\n- The authors show how the framework can be used to derive new approaches. They also show that these approaches can be more effective than existing ones.\n- The experimental setup is well-described overall, with hyperparameters being provided\n- This is a hard analysis to perform, with many possible things to change. I feel like the experiment set chosen is convincing overall, with the caveat mentioned below.\n\nCons:\n\n- Significance of results:\n\nThere are no standard deviations for any results in the paper, which makes it hard to assess significance of many results, esp. since some of the performance gaps are small.\nFor instance, in table 6, authors have a 0.4 BLEU discrepancy in their replication of full fine-tuning performance but draw conclusions on MAM being best based on a 0.2 BLEU gap. The gap is bigger when comparing MAM to methods not introduced by the authors, but still.\nThe story makes sense but I am not 100% confident in the robustness of the results. Using two decimal numbers for tasks (e.g: XSum) also gives a false impression of precision. \nI understand fine-tuning on some of these tasks (MT / XSUM) can be resource-intensive but having standard deviations in even a subset of the experiments would be useful.\n\n- Interpretation of Figure 4 in Section 5.2 and effectiveness of MAM Adapters for encoder models\n\nThe authors highlight that while existing methods perform well on MNLI/SST, they are underwhelming on en-ro / XSum. They conclude that this means existing parameter-efficient transfer learning (PETL) approaches are not great for higher-resource / more challenging tasks.\n\tHowever, I would point out that this data can also support a different conclusion: existing PETL approaches work well for encoder-only models but are not great for encoder-decoder models. Including the T5 datapoint is also not very relevant since in that case Superglue is treated as a single task (w/ one adapter). Answering \u201cFor which tasks / architectures do PETL methods perform well\u201d is an interesting question in itself and I feel like this section expedites this. It is likely that both the architecture and #datapoints in the task matter. \n\tThe author\u2019s choice at the end of section 4.2 is to focus on XSum / en-ro MT. It would be great to highlight that not only are those higher-resource tasks, but they are also generative ones, and thus all the conclusions of this paper might not apply widely to encoder models. Indeed, the results of MAM adapters in table 2 are quite mixed. Right now the paper is claiming more generality than deserved.\n\tTwo possible fixes here: (1) Make claims less general and specific to enc-dec models, (2) Conduct more experiments on encoder-only models \n\n- Writing:\n  - The writing is subpar right now. Example: section 4.4: \u201cits counterpart at attention\u201d -> \u201cits attention counterpart\u201d,  \u201cFFN can better utilize modification at larger capacities\u201d -> modifications or \u201cmodification of the FFN is better at larger capacities\u201d, etc. \n  - Minor: The citation style is often wrong for the sentence. Use \\citet more often.\n  - Minor: Replace \u201cFor classifications\u201d with \u201cfor classification tasks\u201d in the Appendix (twice)\n  - Typos: Section 5 name Discussions -> Discussion ;  Appendix A3 Learning -> Learning. \n\n- Minor:\n  - For Figure 2, I feel like the full fine-tuning number should come from the original baseline, not the replication.\n  - The protocol in 4.5 for choosing the scaling parameter for Scaled PA is a bit surprising. I would just suggest that it should be an hyperparameter, the current description makes it seem like it is based on hyperparam tuning for LoRA. \n  - It seems to me that 4.5 is about composition function *and* \\delta h functional form since LoRA changes both. \n\n\n~~~~~~~~\n\nRevised score upwards after response, see below\n Compelling framework to unify parameter-efficient transfer learning approaches, with interesting new methods emerging from it. The set of experiments is well-chosen overall, but the lack of standard deviations and the focus on generative tasks / enc-decoder models makes the results less robust and general. The writing would benefit from more work as well. ",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "9syxfYwIBa",
                "writer": "author",
                "reply_to": "XbEUCnnma0e",
                "title": "We have added standard deviation numbers in the revision",
                "comment": " Thank you for your time and encouraging review! We address your questions below:\n\n**-Q1: Variance of parameter-efficient tuning methods**\n\nThank you for the suggestion! We have added standard deviation numbers on MNLI and SST2 to Table 2 in the revision. MNLI and SST2 are relatively high-resource compared to other GLUE datasets, and the standard deviation on them is not large as shown in Table 2. [1] has reported similar standard deviation numbers on MNLI and SST2.     \n\nFor high-resource machine translation and summarization tasks such as en-ro and XSum, we have rerun the most performant methods in Table 6 with 3 different random seeds and reported the mean/standard deviation in Table 6, please check the revision for the update \u2013 the variance is quite small in XSum/en-ro. \n\n**-Q2: Typos**  \n\nThanks for catching those typos! We have fixed them in the revision.\n\n\n[1] Hu et al. LoRA: Low-rank adaptation of large language models. Preprint 2021\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qge8pmaXH54",
                "writer": "author",
                "reply_to": "oH79aBFHsUW",
                "title": "Cont'd ",
                "comment": " **-Q4: Experiments focus on generation tasks, and conclusions of this paper might not apply widely to encoder models**\n\nThis is a good point. Several previous works with different methods have shown comparable results to full fine-tuning on GLUE tasks when tuning only <1% of full parameters, thus we chose not to focus on GLUE since (1) a very small parameter budget (<1%) seems sufficient for different methods to match results of full fine-tuning on GLUE, thus it is difficult to obtain strong empirical differences; and (2) it may not be practically meaningful to further constrain the parameter budget (e.g. <0.01%) to distinguish approaches given that 1% parameters only take ~10MB on disk for RoBERTa-large. \n\nWe agree with the reviewer\u2019s point on the applicability of some of the results, and below we would like to categorize the main contributions of this paper, and highlight which ones of them are general, which ones are not, and how we revised the paper to reflect the reviewer\u2019s comments:\n\n1. The main technical contributions of this paper are the connections between existing methods and the proposed unified framework (Sections before experiments). This part is agnostic to architecture or tasks.\n\n2. The experimental examination of design dimensions (Section 4.3 - 4.5) is on summarization and MT tasks. We gave valid reasons why we do not perform them on GLUE, but still, the conclusions from this part could be potentially constrained to encoder-decoder models, as mentioned by the reviewer. We agree on this point, thus in the revision we have explicitly highlighted this limitation at the end of Section 4.2. Please check it out and let us know if you have any further concerns.\n\n3. The proposed MAM Adapter performs well, achieving better or comparable results to previous methods. This conclusion is general as reflected by both Table 2 on MNLI/SST2 and Table 6 on XSum/en-ro. The reviewer mentioned that \u201cthe results of MAM adapter in table 2 are quite mixed\u201d which we kindly disagree with. In Table 2 the only baseline number that outperformed MAM adapter was the SST2 number from Adapter (0.9%), but we note that it is tuning more parameters than MAM adapter thus the comparison was not fair (we set its bottleneck dimension to be the same as prefix tuning and thus Houlsby adapter [6] used 2x #params compared to others). In the revision, we have replaced it with a fair adapter baseline with 0.5% parameters in Table 2, which shows similar results to the MAM adapter. Therefore, the MAM adapter indeed works well generally, achieving better or comparable results to previous methods across MNLI, SST2, XSum, and en-ro benchmarks.\n\n**-Q5: Writing**  \n\nWe have revised the places you mentioned as well as some citation styles. Thanks for the suggestion!\n\n**-Q6: protocol in Section 4.5 for choosing scaling parameter** \n\nThe scaling factor is indeed a hyperparameter for both LoRA and Scaled PA. We select it within {1, 2, 4} depending on the validation performance. The choice of scaling factors in LoRA and Scaled PA are independent, we have revised the description in Section 4.5 to make it more clear.\n\n**-Q7: Section 4.5 is about composition function and \\delta h functional form**\n\nGood point. We can draw conclusions with respect to composition function by comparing LoRA (s=4) to LoRA (s=1), or comparing PA to Scaled PA; we can also argue about \\delta h functional form by comparing LoRA (s=1) to PA, or comparing LoRA (s=4) to Scaled PA. In this paper, we intend to focus on the former in Section 4.5, composition function, to simplify the analysis.\n\n[1] Rush et al. A Neural Attention Model for Abstractive Sentence Summarization. EMNLP 2015  \n[2] See et al. Get To The Point: Summarization with Pointer-Generator Networks. ACL 2017.  \n[3] Li et al. Prefix-tuning: Optimizing continuous prompts for generation. ACL 2021.  \n[4] Lewis et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. ACL 2020.  \n[5] Zhang et al. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. ICML 2020.  \n[6] Houlsby et al. Parameter-Efficient Transfer Learning for NLP. ICML 2019\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oH79aBFHsUW",
                "writer": "author",
                "reply_to": "-ZIWcdyn-tP",
                "title": "We have added standard deviation numbers and clarified some statements in the revision",
                "comment": " Thank you for your time and helpful comments! We address your concerns below:\n\n**-Q1: Standard deviation of results**\n\nWe have added standard deviation numbers of XSum and en-ro to Table 6 as well as those of MNLI and SST2 to Table 2 in the revision. For XSum and en-ro we have rerun the most performant methods in Table 6 with 3 different random seeds and updated mean/standard deviation numbers in Table 6 \u2013 as shown in Table 6, the standard deviation is mostly small for both XSum and en-ro.  For the analysis experiments (Section 4.3-4.5) the performance difference between methods is typically large, thus we do not rerun all of them with different random seeds due to the expensive computation. We hope the added standard deviation numbers would clear the concerns on the significance of the results.     \n\nRegarding the \u201csmall gap\u201d between some methods in Table 6, we would like to emphasize that, as the reviewer acknowledged, the relatively small gap is present only between the new variants presented in this paper which are part of our contributions. Compared to the best existing method in Table 6 (Pfeiffer adapter), MAM adapter outperforms it by 1 ROUGE-2 point on XSum and 0.6 BLEU point on en-ro on average across 3 random runs, which we believe is a significant improvement.\n\n**-Q2: Using two decimal numbers for tasks (e.g: XSum) also gives a false impression of precision**\n \nMost summarization work report ROUGE scores with two decimal numbers [1,2,3,4,5], thus we followed them to report two decimal numbers as well. We agree that using two decimal numbers may not be well-warranted, and we will consider changing it later while keeping it as it is in the rebuttal period.\n\n**-Q3: Interpretation of Figure 4 in Section 4.2** \n\nYou are right. The phenomenon present in Figure 4 and Table 2 may be not only due to #datapoints or task difficulties, but architectures (encoder-only v.s. encoder-decoder) may matter as well. Indeed, it is likely that the reason lies in the combination of these factors and there is not a single factor that could entirely explain the difference. As the reviewer noted, however, \u201cFor which tasks/architectures do PETL methods perform well\u201d is an interesting and non-trivial question itself, and answering this is beyond the scope of this paper. We emphasize that our goal of Section 4.2 is not to answer this question, but to point out that existing PETL methods still lag behind full-finetuning on some standard benchmark tasks, even though 10% of the parameters are tuned. This is because we feel that most previous work \u2013 by only reporting numbers on the GLUE benchmark \u2013 may unintentionally leave an impression that PETL methods can match full-finetuning easily with <1% of full parameters on any task, which is clearly not the case. We have revised the statement regarding this interpretation in Section 4.2 to make it more accurate by explicitly stating that the reasons could be related to architectures as well. Please check out our updates, and thank you for the advice, we feel the claims are more precise now.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oxVY7ETXmbJ",
                "writer": "author",
                "reply_to": "PBPjBdrsEve",
                "title": "Thank you!",
                "comment": " Thank you for your time and encouraging comments! \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PBPjBdrsEve",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_0RDcd5Axok",
                "title": "",
                "comment": "This paper breaks down the design of state-of-the-art parameter ef\ufb01cient transfer learning methods and presents a uni\ufb01ed framework that establishes connections between them. The paper re-frames them as modifications to specific hidden states in pre-trained models and defines a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Experiments on machine translation, text summarization, language understanding, and text classification have been conducted, which indicates that the unified framework enables can instantiate new parameter-efficient fine-tuning methods that tune fewer parameters. Strengths:\n\n1. The analysis and conclusion on Adapters, Prefix Tuning, and LoRA are very penetrating. Specifically, the paper derives an equivalent form of pre\ufb01x tuning to establish its connection with adapters in a unified view.\n\n\n2. The paper proposes a uni\ufb01ed framework for parameter-ef\ufb01cent tuning that includes several state-of-the-art methods as instantiations.\n\n3. The paper is well organized and well-motivated with theoretical analysis and proof. \n\n4. Experiment setting and analysis are comprehensive; the proposed method Scaled PA consistently shows its advantages against the other baselines.\n Overall, this paper is well-written and well-motivated. I think this paper attracts lots of researchers and will inspire future works in parameter efficient training. ",
                "rating": 10,
                "confidence": 5
            },
            {
                "review_id": "XbEUCnnma0e",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_0RDcd5Axok",
                "title": "",
                "comment": "This paper investigates recent parameter-efficient methods that have been shown to achieve good performance in a variety of NLP tasks. The authors analyse their connections and propose a unified framework which subsumes a number of existing approaches. The authors then compare their performance across four NLP tasks, with varying level of difficulty and amount of available resources. Here, compared to full fine-tuning, they find that parameter-efficient approaches perform well on simpler tasks (MNLI and SST2), while showing larger gaps on more challenging tasks (XSum and MT). A series of controlled experiments centred around the proposed framework shows that parallel adapters applied to the FFN module of a Transformer generally perform better. Applying them to the attention module leads, however, to better gains when the number of additional parameters is small. Finally, the authors combine these findings into a novel MAM adapter module that leads to further gains in every task. Strengths:\n- A new perspective of parameter-efficient methods within a unified framework.\n- A number of controlled studies which give insights into which components are meaningful and in which cases.\n- The released code base can help boost future research in this area.\n- Clear and well-written.\n\nWeaknesses:\n- No major weaknesses identified.\n\nQuestions:\n- It would be interesting to see the variance observed when applying parameter-efficient methods. Previous work found large variance in downstream performance across different seeds when performing full fine-tuning [1,2,3]. This is something you could add, for example, to Table 2.\n\nTypos:\n- In Figure 2, the performance of Adapter is 21.00, not 20.46\n- The \"v\" subscript is missing in Eq. 5\n\n[1] Dodge et al. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. 2021\n[2] Mosbach et al. On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. ICLR 2021\n[3] Bugliarello et al. Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs. TACL 2021 This paper provides a unified framework for parameter-efficient NLP, an important task when serving models at scale. The experiments provide new insights into the performance of existing methods, that, when combined, result in further performance improvements. I think this is a solid paper and recommend its acceptance.",
                "rating": 8,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper, the theoretical justification and empirical analyses",
                "Sentiment Expression": "addresses an important research problem, and are convincing",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "4azYdmhHCG": {
        "paper_id": "nips_2021_4azYdmhHCG",
        "paper_title": "Uncertainty Calibration for Ensemble-Based Debiasing Methods",
        "paper_abstract": "Ensemble-based debiasing methods have been shown effective in mitigating the reliance of classifiers on specific dataset bias, by exploiting the output of a bias-only model to adjust the learning target. In this paper, we focus on the bias-only model in these ensemble-based methods, which plays an important role but has not gained much attention in the existing literature. Theoretically, we prove that the debiasing performance can be damaged by inaccurate uncertainty estimations of the bias-only model. Empirically, we show that existing bias-only models fall short in producing accurate uncertainty estimations. Motivated by these findings, we propose to conduct calibration on the bias-only model, thus achieving a three-stage ensemble-based debiasing framework, including bias modeling, model calibrating, and debiasing. Experimental results on NLI and fact verification tasks show that our proposed three-stage debiasing framework consistently outperforms the traditional two-stage one in out-of-distribution accuracy.\n",
        "paper_acceptance": "accept",
        "meta_review": "This paper studies the uncertainty calibration properties of the bias-only model in ensemble-based de-biasing methods. The authors argue that the accurate estimation of uncertainty in the bias-only model has been overlooked in prior research, but can have an important influence on the overall de-biasing process. The method proposed in the paper applies calibration methods to the output of the bias-only model. Thus, from a novelty perspective, the method itself is a novel combination of existing approaches, while at a conceptual level the paper is addressing a previously overlooked issue and thus has stronger novelty at this level. The reviewers judged the approach and the theoretical development to be technically correct. The experiments were judged to provide adequate support for claims. The writing was judged to be clear. The reviewers indicated that all of their questions had been adequately addressed following the author response. As a result, the paper is recommended for acceptance. The authors should be sure to incorporate the discussed clarifications and updates in the final manuscript.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "u4_5cW1q_Rt",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_4azYdmhHCG",
                "title": "",
                "comment": "This paper shows an increasing connection between the uncertainty estimation of the biased only models and proposed MoCaD, a three-stage EBD approach considering model calibrating.  Originality: High. The combination of uncertainty calibration and EBD is a very novel idea and the impressive results shown in this paper validates the effectiveness of such combination.\n\nQuality: The submission is technically sound, with the major claims in Section 4 being backed up by both theoretical analysis and experimental results in Section 5. however, I do feel that the experiments section can be improved to reveal more insight into why MoCaD works well. \n\nClarity: This paper is well organized, although I struggled to remember the complicated notation system used in the paper. The authors should spend some time on improving the nomenclature even at the cost of abusing some notations, consider that various kinds of probabilities are needed in both main body and appendix. The paper does provide enough information to reproduce its results.\n\nSignificance: High. Debiasing is an increasingly important area in ML, and this work has the potential to serve as a foundational piece for future development. \n\nSome areas where improvement can be done:\n\nExperiments. From Theorem 1-2, the authors nicely connected the uncertainty calibration of bias-only models to the final performance of the debiased models, and motivated the use of calibration methods. In particular, in Section 6.2.1, the Dirichlet scaling method seems to work much better than the temperature scaling method. This is understandable, as Dirichlet scaling is more expressive, but it can also introduce some accuracy change that might hurt the debiasing model. Therefore, I would like to see  if more expressive calibration methods are indeed helpful, for example, the more expressive temperature scaling (http://proceedings.mlr.press/v119/zhang20k.html) and the nonparametric calibration <http://proceedings.mlr.press/v108/wenger20a.html>. Besides the class-wise ECE, the authors might also want to look into the confidence-based ECE (http://proceedings.mlr.press/v70/guo17a.html), which is related with theorem 2, as well as the accuracy of the calibrated bias-only model (to check if the calibration method introduces non-negligible hurt on that). Section 6.2.2 is indeed very informative and completely backs up Theorem 2, good job!\n\nThe title might be confusing. It might be interpreted to how to calibrate the uncertainty of final debiased model, while the uncertainty is in fact only calibrated for the bias-only model.\n\nSome typos, e.g., Figure 3 'Syntatic Bias'.\n\n------------After Rebuttal------------\nI have read the authors rebuttal and appreciate the new experiments comparing accuracies. This moves the scores upward.  Yes.",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "ePiJtn6-Std",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_4azYdmhHCG",
                "title": "",
                "comment": "The paper reveals that uncertainty calibration of the biased model is important for debiasing performance in EBD methods. By calibrating the biased model, the proposed method improves the performance of existing methods.   Originality: to the best of the reviewer's knowledge, this paper is the first to discuss calibration in EBD methods even though the topic of calibration has been studied extensively.  To some extent, the paper lacks novelty because the paper uses existing techniques without modification to improve existing EBD models. \n\nClarity: the paper is well organized and easy to follow. The method starts with the formalization of the EBD (Sec.3) method and theoretical (Sec.4.1) and empirical analysis (Sec.4.2) of the bias-only model. Then it introduces calibration techniques and the complete pipeline (Sec.5). However, there are some technical details that require further clarification. Please see the questions. \n\nQuality: the paper conducts theoretical analysis and verifies assumptions empirically. \n\nSignificance: the paper points out an overlooked problem in EBD methods. The work can inspire further investigation of calibration problems in ensemble-based debiasing methods. \n\nQuestions:\n\n1, Is the predicted label $Y(s)$ from the intrinsic model (line 136) different from the label $\\tilde{Y}(x)$ predicted by the ideal predictor (line 166)? Should they be the same? \n\n2, The global certainty level $\\alpha:=min_{X^s} max_{i\\in\\{0,1\\}} P_D(Y=i|X^s)$ (line 149) is the least confidence prediction from the true principle.  Can the author justify its role as a \"global\" certainty level? It's not clear why this quantity conveys a \"global\" view of certainty. \n\n2, Regarding Figure 3: the plot shows monotonically increasing performance with temperature. As the temperature increases,  the biased models will become more and more uncertain. At some point, this must hurt the debiasing performance because when the distribution of the biased model is almost flat, it does not provide any information to the unbiased model.  Can the authors comment on this thought? \n\n**--------------------post-rebuttal review--------------------**\n\nI appreciate the authors' response and discussion on my questions. I think the paper uncovers an overlooked problem in EBD methods and can inspire further research.  Yes, the authors have addressed the limitations of the paper especially the inconsistent results on image data. ",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "Kukex9DH3u",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_4azYdmhHCG",
                "title": "",
                "comment": "This paper provides theoretical and empirical proof that the bias-only model used for two-step debiasing methods does not provide adequate and accurate uncertainty estimation. Via both mathematical proofs and empirical experimentation on two NLP tasks, the authors show that bias-only models have been over-estimated and overlooked, and that further work is needed to improve them.   This paper provides a clear and interesting critique of the bias-only model used in two-step debiasing methods. \n\nSome comments that I had while reading the paper: \n\nPlease note that\u00a0 some recent works [22; 9] propose to jointly learn the bias-only model and the debiased main model\u00a0 in an end-to-end manner. Since it is difficult to quantify the impact of the bias-only model in this scheme, we mainly focus on the two-stage methods\u00a0 == How can your methodology be applied to these kinds of methods?\n\nFor Figure 1, wouldn't it be worth discussing the difference between the two plots (and the two tasks?)\n\nLine 228-229: \"In other words, it only changes the uncertainty estimation229 and *maintains* the model\u2019s accuracy?\u00a0\n== not sure what 'remain' means\n\nWhy were fact verification and NLI the tasks chosen?\n\nIs using a hand-crafted classifier for the NLI task comparable to a data-driven one for the fact verification one? Are they equivalent and comparable?\n\nAny hypotheses why the method doesn't work as well\u00a0on\u00a0Learnd-Mixin on HANS? Is there a difference between that approach and others?\n\nLine 359: \"poorly calibrated\"\n\n\"Further experimental results on image classification show inconsistent improvements\" == maybe it depends on the image task? would it be worth trying tasks that are more complex and multi-modal, as opposed to simple classification? e.g. captioning or VQA?\n\nWhat are some ways in which the bias-only model can be improved? I feel like there should be a lot more discussion of this, as well as the advantage of the joint bias and debiased model approach that was not studied at all in this paper. The fact that the paper focusses purely on NLP tasks and shows inconsistent improvement on image tasks is worrisome for me. What if the phenomena that were observed during the emprical experimentation were intrinsic to the nature of the two NLP tasks chosen (which is actually very few)? The fact that this method doesn't apply in image domains is worth exploring and explaining.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "Spd0wS0jjxr",
                "writer": "author",
                "reply_to": "Fh-Nk03fTs",
                "title": "Response for Reviewer x4rW",
                "comment": " We appreciate the positive feedback and we are happy the reviewer has highlighted the clarity of the paper, the novelty of our findings and method, as well as the thoroughness of our experiments. Below, we respond to each comment that was raised.\n\n**- For Theorem 1, what kind of values should I expect for the threshold? I understand it depends on latent constants, but it\u2019d be nice to get a rough range.**\n\nWe agree with the reviewer that a rough range can benefit. However, as the latent constants are related to the\u00a0unknown\u00a0posterior characteristics of the bias-only model, we can only empirically show\u00a0some\u00a0statistics of the threshold.\u00a0Among the three latent constants $\\alpha, l_0,$ and $\\epsilon$, $\\alpha$ can not be observed even empirically since the intrinsic principle $P_D(Y|X^S)$ is unavailable. As a result, we calculate the\u00a0empirical values of $l_0, \\epsilon$ of\u00a0the syntactic bias-only model on MNLI, and report the value of $2\\epsilon$ as an approximation of the threshold. Specifically, we use the Mean-shift algorithm\u00a0to cluster the instances represented by hand-crafted features to\u00a0discretization $X^B$, and make use\u00a0of\u00a0the data binning algorithm in [1] to create $S_{f_B}(l)$. The result shows that the\u00a0weighted average value of $2\\epsilon$ is 0.064, the maximum is 0.51, the minimum is 0, and the weighted median is 0.012.\nThe following table shows the statistics of the 5 biggest sets.\n\n| Ratio   |   $l_0$    |     $2\\epsilon$\t|    >$2\\epsilon?$ |\n| -- | -- | -- | -- |\n| 0.092 | 0.186\t| 0.024 | true |\n| 0.092 | 0.256\t| 0.079 | true |\n| 0.090 | 0.267\t| 0.012 | true |\n| 0.085 | 0.377 | 0.126 | true |\n| 0.084 | 0.143\t| 0.005 | true |\n\nHere the first column shows the relative size of the set with respect to the whole dataset. The last column shows whether $|l - P_D(Y=0|S_{f_B}(l))| > 2\\epsilon$ holds on the set.  The result shows this equation holds on sets that contain 83.4% MNLI samples (327352) in total. In conclusion, these empirical statistics suggest that the threshold can be relatively small in most cases so that the calibration can improve the debiasing performance.\n\n**- It took me some time to parse some of the theoretical notation and statements. For example, I found the notation for debiasing performance in Theorem 1 unclear.**\n\nWe thank the reviewer for raising this point. In Theorem 1 we denote the debiasing performance on $S_{f_B}(l)$ as $P_D(\\{x \\in S_{f_B}(l)|\\tilde{Y}(x) = Y(x)\\})$, where {$x \\in S_{f_B}(l)|\\tilde{Y}(x) = Y(x)$} denotes the subset of $S_{f_B}(l)$ on which the main model gives the same prediction as the unbiased model. The debiasing performance on $S_{f_B}(l)$ is then defined as the probability of this set. We will add the explanation to the paper for better readability here.\n\n[1] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning (ICML'17). 2017.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rc9yI6QAEne",
                "writer": "author",
                "reply_to": "u4_5cW1q_Rt",
                "title": "Response for Reviewer CYB7",
                "comment": " We thank the reviewer for their time and positive comments. Please see below for responses to each point:\n\n**- I would like to see if more expressive calibration methods are indeed helpful, for example, the more expressive temperature scaling (http://proceedings.mlr.press/v119/zhang20k.html) and the nonparametric calibration http://proceedings.mlr.press/v108/wenger20a.html. Besides the class-wise ECE, the authors might also want to look into the confidence-based ECE (http://proceedings.mlr.press/v70/guo17a.html), which is related with theorem 2.**\n\nWe thank the reviewer for this constructive comment. We would like to highlight related content in 6.1.1 where we show the effect of different extents of calibration (measured by the class-wise ECE). As our work inspires the combination of calibration and EBD methods, experimenting with more expressive calibration methods and more accurate calibration measurements could be an important future direction on the basis of this work, as suggested by the reviewer.\n\n**- the accuracy of the calibrated bias-only model (to check if the calibration method introduces non-negligible hurt on that).**\n\nWe checked the accuracy of the calibrated bias-only model and report it in the following table. FEVER denotes the claim-only bias-only model on the FEVER training set. HANS, MNLI, and unknown denotes syntactic bias-only model, hypothesis-only model, and unknown bias-only model on MNLI training set correspondingly.\n\n|\u00a0\u00a0|\u00a0FEVER\t| HANS\u00a0|\u00a0MNLI\u00a0|\u00a0Unknown |\n| -- | -- | -- | -- | -- |\n| Un-Cal | 60.6\u00a0|\u00a054.8\u00a0|\u00a063.8 |\u00a063.2 |\n| Temps\u00a0| 60.6\u00a0| 54.8\u00a0|\u00a063.8\u00a0| 63.2 |\n| Dirichlet\u00a0| 62.7\u00a0|\u00a069.9\u00a0| 64.0\u00a0| 63.4 |\n\nThe results show that the temperature scaling does not change the predicted label because the maximum of the softmax function remains unchanged. It only changes the uncertainty estimation. On the contrary, the Dirichlet calibrator will change the prediction accuracy. More precisely, the Dirichlet calibrator improves the accuracy of all bias-only models in our experiment.\u00a0\n\n**- Some typos, e.g., Figure 3 'Syntatic Bias'.**\n\nWe thank the reviewer for pointing this out. We will thoroughly check the paper and correct the typos.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3oaFq7BGLN8",
                "writer": "author",
                "reply_to": "Kukex9DH3u",
                "title": "Response for Reviewer P1n2",
                "comment": " We thank the reviewer for their time and constructive comments. We respond below to each comment that was raised (reordered).\n\n**- Please note that\u00a0some recent works [22; 9] propose to jointly learn the bias-only model and the debiased main model\u00a0in an end-to-end manner. Since it is difficult to quantify the impact of the bias-only model in this scheme, we mainly focus on the two-stage methods ==\u00a0How can your methodology be applied to these kinds of methods?**\n\nPlease note that calibration techniques for training ab-initio well-calibrated models [1] could be applied to the bias-only model in the end-to-end framework, instead of the post-hoc approach (Temperature Scaling and Dirichlet Calibration) used in this paper. However, a new theory needs to be developed to analyze the behavior of bias-only models in this case, as it involves learning dynamics. It could be an interesting new research direction, which is clearly beyond the scope of this paper.\n\n**- For Figure 1, wouldn't it be worth discussing the difference between the two plots (and the two tasks?)**\n\nThe two plots here are just used to show the poor calibration phenomena, so we did not include a detailed discussion. Nevertheless, it is interesting to discuss the difference between the two plots. Remind that most of the blue bars below the diagonal indicate that the\u00a0model is over-confident, otherwise is under-confident.\u00a0Therefore, the left diagram shows the syntactic bias-only model is over-confident, while the other one is under-confident. The property of being over-confident or under-confident varies among different bias-only models on MNLI, as shown by Figure 1 in the Appendix. The reason why different models show different behavior remains a question, which is worth future investigation.\n\n**-Line 228-229: \"In other words, it only changes the uncertainty estimation and maintains the model\u2019s accuracy? == not sure what 'remain' means**\n\nWe agree with the reviewer that \u201cmaintain\u201d should be used here. Thanks for the correction.\n\n**- Why were fact verification and NLI the tasks chosen?**\n\nFact verification and NLI are two common tasks used in debiasing, [2-4]. We follow these previous papers to choose these tasks and datasets for our experiments. We will add the explanations to the paper.\n\n**- What if the phenomena that were observed during the empirical experimentation were intrinsic to the nature of the two NLP tasks chosen (which is actually very few)?**\n\nWe\u00a0would\u00a0like to\u00a0illustrate\u00a0the generality of our observed\u00a0phenomena\u00a0from the following two folds:\u00a01) The existence of poorly\u00a0calibrated machine learning models\u00a0has been observed by much previous literature on calibration, as we mentioned in lines 181-184, which is independent of the task or dataset.\u00a02)\u00a0Our\u00a0theoretical\u00a0results are not restricted to any specific tasks or datasets. Though the latent constant $\\alpha$ relates to the task, it only affects the\u00a0numerical value\u00a0of the threshold\u00a0other than the conclusion of the theorem. In other words, calibration will benefit the debiasing performance for any task and bias-only models though\u00a0the\u00a0degree may vary.\u00a0We would like to emphasize that the two tasks are chosen because they are representative tasks in debiasing. We\u00a0have reported the result of\u00a04 different bias-only models, 4 different EBD methods, and 2 different calibration methods, in total 32 different instances on\u00a0the two\u00a0representative\u00a0tasks. These empirical results have shown consistent behavior of MoCaD on different configurations.\n\n**- Is using a hand-crafted classifier for the NLI task comparable to a data-driven one for the fact verification one? Are they equivalent and comparable?**\n\nThe biases we considered on the two datasets are different. That is why different models are used for the bias-only model. In our opinion, it is not fair to compare the two bias-only models. Nevertheless, we would like to list their accuracies here. The hand-crafted classifier for the NLI task is designed to capture the syntactic bias, and the accuracy of it on the MNLI training set is 0.548. The claim-only classifier for the fact verification task is designed to capture the claim-only bias, and the accuracy of it on the FEVER training set is 0.606.\n\n**- Any hypotheses why the method doesn't work as well\u00a0on\u00a0Learned-Mixin on HANS? Is there a difference between that approach and others?**\n\nYes, there is a difference. As we mentioned in line 127, in Learned-Mixin a trainable gate function is added as the exponent of the bias-only output. As a result, its optimal bias-only model is different from others and does not fit our theoretical assumptions. For a thorough comparison, we experimented with Learned-Mixin and reported the results. We will add these explanations to the result analysis part.\n\n**- Line 359: \"poorly calibrated\"**\n\nSorry for the typos. Thanks for the correction.\n\n**- What are some ways in which the bias-only model can be improved? I feel like there should be a lot more discussion of this, as well as the advantage of the joint bias and debiased model approach that was not studied at all in this paper.**\n\nWe would like to highlight\u00a0that\u00a0existing\u00a0improvements (in a different aspect from this paper) of the bias-only model mainly focus on\u00a0exploiting different prior knowledge to obtain bias-only models in the unknown bias case, as we introduced in Section 2, lines 79-85.\u00a0We\u00a0have\u00a0experimented our method with the bias-only model proposed by Utama et al. [35] for the unknown bias case, and the result is shown in Table 1. It shows that our method consistently improves its out-of-distribution performance.\nFor\u00a0the joint bias and debiased model approach, we would like to highlight that it\u00a0is an end-to-end debiasing framework, other than a method to improve the bias-only model. Currently, there is no conclusion to show the advantages of this framework with respect to the two-stage one\u00a0or its\u00a0effect on the bias-only model. For known dataset bias, [2-3] adopt the two-stage framework, while [5] adopts the end-to-end framework. For unknown dataset bias, [4,6] adopt the two-stage framework, while [7] adopts the end-to-end one. It could be an interesting new research direction to\u00a0apply our methodology to the end-to-end methods, however as we illustrated in the answer to the first point, it\u00a0is clearly beyond the scope of this paper.\n\n**- \"Further experimental results on image classification show inconsistent improvements\" == maybe it depends on the image task? would it be worth trying tasks that are more complex and multi-modal, as opposed to simple classification? e.g. captioning or VQA?**\n\nWe agree with the reviewer, more complex and multi-modal tasks are worth trying in the future.\n\n**-The fact that this method doesn't apply in image domains is worth exploring and explaining.**\n\nWe respectfully disagree that it has been shown as a fact that this\u00a0method doesn't apply in image domains. The experimental result on the image classification task shows that our method can achieve the best performance among all EBD methods, though the improvement is not consistent across the calibrators. As we discussed in lines 367-371, a possible reason is that the invariant mechanism for image classification has a high certainty, reducing the impact of calibration error on debiasing according to our theoretical analysis.  As the reviewer suggested, more complex tasks are worth trying.\n\n[1] Kumar, A., Sarawagi, S., and Jain, U. Trainable calibration measures for neural networks from kernel mean embeddings. In International Conference on Machine Learning (ICML), pp. 2810\u20132819, 2018.\n\n[2] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4060\u20134073, 2019.\n\n[3] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. Mind the trade-off: Debiasing nlu models without degrading the in-distribution performance. arXiv preprint arXiv:2005.00315, 2020.\n\n[4] Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. Towards debiasing nlu models from unknown biases. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7597\u20137610, 2020.\n\n[5] Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. End-to-end bias mitigation by modelling biases in corpora. In Annual Meeting of the Association for Computational Linguistics, 2020.\n\n[6] Victor Sanh, Thomas Wolf, Yonatan Belinkov, and Alexander M Rush. Learning from others\u2019 mistakes: Avoiding dataset biases without modeling them. In International Conference on Learning Representations, 2021.\n\n[7] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Learning to model and ignore dataset bias with mixed capacity ensembles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 3031\u20133045, 2020.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "t5kuskL0WpV",
                "writer": "author",
                "reply_to": "ePiJtn6-Std",
                "title": "Response for Reviewer 7E1r",
                "comment": " We thank the reviewer for the insightful comments. We are pleased to note the positive remarks on the significance of our work. Please see below for a detailed response to each point that was raised:\n\n**- Is the predicted label from the intrinsic model (line 136) different from the label predicted by the ideal predictor (line 166)? Should they be the same?**\n\nThey are different. Intuitively, the intrinsic model is unbiased and with invariant out-of-distribution performance, while the ideal predictor is biased to some features. Their relationship is shown in line 100.\n\n**- The global certainty level (line 149) is the least confidence prediction from the true principle. Can the author justify its role as a \"global\" certainty level? It's not clear why this quantity conveys a \"global\" view of certainty.**\n\nHere $\\alpha$ is taken as the \u201cglobal\u201d minimum of $\\max_{i \\in \\{0,1\\}} P_D(Y = i|X^S)$ on the whole space of $x^s$, so the word \u201cglobal\u201d is used for underlining this property.\n\n**- Regarding Figure 3: the plot shows monotonically increasing performance with temperature. As the temperature increases, the biased models will become more and more uncertain. At some point, this must hurt the debiasing performance because when the distribution of the biased model is almost flat, it does not provide any information to the unbiased model. Can the authors comment on this thought?**\n\nWe thank the reviewer for this insightful thought.\u00a0It is true that when the temperature in the temperature scaling is extremely high, the distribution of the bias-only model\u00a0(biased model)\u00a0is almost flat, and the debiasing performance will be hurt.\u00a0We would like to highlight that the Temperature Scaling algorithm generally converges to a finite temperature that minimize the calibration error [1].\u00a0For example in our experiments, the optimal temperatures given by the Temperature Scaling algorithm are 0.915, 1.585, 1.646, and 0.792, for the hypothesis-only model, syntactic bias-only model, unknown biased model on MNLI, and the claim-only model on FEVER, respectively. \n\n[1] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning (ICML'17).\u00a02017.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Fh-Nk03fTs",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_4azYdmhHCG",
                "title": "",
                "comment": "- The paper analyzes ensemble-based debiasing (EBD) methods for training models that don\u2019t rely on dataset biases and thus has good out-of-distribution performance\n- The theoretical analysis shows that the debiased model performance decreases monotonically as calibration error increases, and properly calibrating the model can improve not just out-of-distribution performance, but also in-distribution performance under certain conditions\n- Based on the theoretical analysis, the authors propose a modification to EBD methods, where the bias-only model is calibrated. Empirically, this improves the performance of debiased models.\n  - The paper highlights the importance of calibrating bias-only models for EBD methods. Although this is not so surprising given that EBD methods rely on probabilistic estimates outputted by bias-only models, the findings (both theoretical and empirical) are novel to my knowledge.\n- The theoretical analysis clearly illustrates the benefits of calibrating the bias-only model both for out-of-distribution and in-distribution model, and this directly motivates the proposed modification to the EBD methods. The modification (adding a calibration step) is simple, but well-motivated, practical, and novel to my knowledge.\n- The paper includes thorough, systematic experiments showing that the proposed modification improves the debiased model performance, in line with the theoretical analysis. The empirical improvement in performance is small, especially relative to the empirical gains yielded by the standard EBD methods, and this could make the key takeaway of the paper limited in its impact. However, the empirical gains are consistent across multiple datasets, calibration methods, and debiasing methods. They also have additional empirical analysis, which are also consistent with the theoretical results.\n- The paper is clearly written, and I enjoyed reading it! \n\nMinor comments\n- For Theorem 1, what kind of values should I expect for the threshold? I understand it depends on latent constants, but it\u2019d be nice to get a rough range.\n- It took me some time to parse some of the theoretical notation and statements. For example, I found the notation for debiasing performance in Theorem 1 unclear.\n Yes",
                "rating": 6,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "estimation of uncertainty in the bias-only model",
                "Sentiment Expression": "has been overlooked in prior research",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The method proposed in the paper",
                "Sentiment Expression": "applies calibration methods to the output of the bias-only model",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "approach and the theoretical development",
                "Sentiment Expression": "technically correct",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The writing",
                "Sentiment Expression": "clear",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "questions",
                "Sentiment Expression": "had been adequately addressed",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "recommended for acceptance",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "2zCRcTafea": {
        "paper_id": "nips_2021_2zCRcTafea",
        "paper_title": "Focal Attention for Long-Range Interactions in Vision Transformers",
        "paper_abstract": "Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability to capture local and global visual dependencies through self-attention is the key to its success. But it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks(e.g., object detection). Many recent works have attempted to reduce the cost and improve model performance by applying either coarse-grained global attention or fine-grained local attention. However, both approaches cripple the modeling power of the original self-attention mechanism of multi-layer Transformers, leading to sub-optimal solutions.  In this paper, we present focal attention, a new attention mechanism that incorporates both fine-grained local and coarse-grained global interactions.  In this new mechanism, each token attends its closest surrounding tokens at the fine granularity and the tokens far away at a coarse granularity and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal attention, we propose a new variant of Vision Transformer models, called Focal Transformers, which achieve superior performance over the state-of-the-art (SoTA) Vision Transformers on a range of public image classification and object detection benchmarks.  In particular, our Focal Transformer models with a moderate size of 51.1M and a large size of 89.8M achieve 83.6% and 84.0%Top-1 accuracy, respectively, on ImageNet classification at 224\u00d7224.  When employed as the backbones, Focal Transformers achieve consistent and substantial improvements over the current SoTA Swin Transformers [44] across 6 different object detection methods.  Our largest Focal Transformer yields58.7/59.0boxmAPs and50.9/51.3mask mAPs on COCO mini-val/test-dev, and55.4mIoU onADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks. \n",
        "paper_acceptance": "accept",
        "meta_review": "There is a unanimous agreement on the novelty/solid results of the paper and the AC agree with the reviewers. The proposed idea in this paper is interesting, well-motivated, and could make good contributions to the general vision and representation learning communities. In this regard, the AC recommends acceptance as spotlight.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "ecpEKMw12k",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_2zCRcTafea",
                "title": "",
                "comment": "This paper proposes a focal version of self-attention module for vision transformers. The query is kept at high resolution, but the number of keys is reduced depending on the relative position difference. This proposed attention is implemented within multi-scale windows. It shows good performance on top of recent transformer advances, on ImageNet and COCO.\n  ## Strength\n1. The proposed focal attention mechanism is original and novel.\n2. The idea is clearly presented with equations and illustrations.\n3. Training details included for fair comparison and reproducing. Comparison is fair with other methods.\n4. Various settings and tasks are explored and compared with solid baselines.\n\n## Weakness\n1. Runtime / throughput is not clear. The proposed attention requires extra subsampling layers, key value computation, and more importantly, concatenating keys and values from different resolution windows (according to the code). These might introduce extra runtime cost with the same FLOPs. But this is not sufficiently discussed in the paper.\n2. Scaling to larger models. The ImageNet performance of Focal-Base is the same as Focal-Small, and is similar to Swin with the same size. In addition, the improvement over Swin in the Small regime is limited as well. In this case, error bars may be reported to address result variances. However, Focal-Base improves consistently on detection segmentation tasks, given a limited ImageNet Top-1. Does it suggest that more context (larger focal size) is necessary for COCO than ImageNet? (Relates to the discussion in Line 281).\n3. What is the intuition behind sharing the same set of query key parameter for both small and large windows? It does save parameters and FLOPs, but the model might be extracting local edges/corners with fine detail and global semantic concepts with coarse context windows. This seems to suggest that different (not shared) parameters might be good for fine and coarse levels. It might be worth ablating that three independent attention could be used for the three levels.\n4. The training recipe seems modified slightly from DeiT/Swin, e.g. the drop path rate. Does it affect comparison fairness?\n\n------------------------------------------------------------Post Rebuttal-------------------------------------------------------------------\n\nAfter reading the author rebuttal and other reviews, I think the weakness 2 and 4 are mostly addressed. Nevertheless, the runtime/throughput is a confirmed weakness according to the authors' results in the rebuttal and is also a shared concern among reviewers. But I think this is ok given other contribution of the paper. In this case, I keep my original rating of 6. Yes.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "XJ7w5SJ2_Mc",
                "writer": "author",
                "reply_to": "GskYaL9dv2F",
                "title": "One more experimental result on number of focal levels",
                "comment": " We thank the reviewer again for the great suggestion of comparing two focal levels and three focal levels.\n\nIn our previous response, we had reported the comparison between two and three focal levels with window size=7. We did not observe noticeable improvement when adding the second focal level into our current Focal-Tiny model. We suspect one of the two reasons is that the odd window size 7 makes it hard to align the pooled window tokens at the second level to a window. Hence, we change it to an even number 8. Note that changing window size to 8 can ensure the correct alignment but requires padding the feature map at each layer so that it can be divided by 8. Below we report the performance for our Focal-Tiny models using two and three focal levels with window size=8.\n\n| Model | window size | focal levels | Top-1 Acc. | #params. | GFLOPs | Throughput (imgs/s) |\n| --- | --- |  --- | --- |  --- | --- | --- |\n| Focal-Tiny | 8 | 2 | 82.16 | 29.20 | 4.99 | 282 |\n| Focal-Tiny | 8 | 3 | 82.26 | 29.22 | 5.03 | 226 |\n||\n\nAccording to the above table, adding the second focal level does bring minor improvement to the original Focal-Tiny model. But still, the improvement is minor with respect to the extra overhead in terms of throughput. Along with our previous results, we can conclude that adding a second focal level can only bring minor improvement because the region is already covered by focal level 1 and 3 regarding the receptive field. Hence, we believe the gain brought by our proposed focal self-attention is mainly attributed to its ability to model both short- and long-term interactions across different visual tokens, rather than the pyramid of the feature map. As a result, we recommend using two focal levels as in our current Focal Transformers. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mzzHtLNuiEC",
                "writer": "author",
                "reply_to": "1Q2FgfyjqUL",
                "title": "Quick update with separate qkv embedding",
                "comment": " We thank the reviewer again for the suggestion to use separate qkv embedding layers for different focal levels. Following this suggestion, we did a quick experiment to use two separate qkv embedding layers for the two focal levels. For Focal-Tiny model, this change brings extra ~7M parameters. In the meantime, it achieves 82.4% top-1 accuracy, which is 0.2% better than the original Focal-Tiny model. Based on this result, we agree with the reviewer that using different qkv embedding layers should be better to capture the different granularities of visual contents, but at the cost of extra model parameters. We will add this experiment to our revision.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0F5ZSya7wLT",
                "writer": "author",
                "reply_to": "nips_2021_2zCRcTafea",
                "title": "We thank all reviewers for their valuable comments",
                "comment": " First of all, we thank all reviewers for their valuable comments! We are pleased all reviewers think our paper is well-written with a clear story. We are also encouraged that Reviewer rkny and B1T8 think our method is original and novel, and Reviewer HEkP thinks our design is reasonable to address an interesting research problem as acknowledged by Reviewer NEKg. \n\nBased on all the constructive comments, we can summarize two common concerns about our work. Our separate responses to these two concerns raised by different reviewers may be slightly different based on the specific questions, but here we attempt to address the common pieces at the beginning to help align the discussion. \n\n### 1. Concern about model efficiency/throughput\n\nWe thank the reviewers for raising this concern and suggesting us to report the throughputs. Indeed we found FLOPs cannot directly reflect the running speed of different methods. Below we report the throughputs for different methods, including Vision Transformer (ViT), DeiT, PvT, CvT, Vision-Longformer (ViL), Swin Transformers, and our own Focal Transformers. For comprehensiveness, we not only compare the throughputs with the conventional input resolution 224x224 for image classification but also compare on higher input resolutions 448x448 and 896x896 as suggested by Reviewer HEkP. We report the numbers below. \n\n| Model | Top-1 Acc | GFLOPs | 224x224 | 448x448 | 896x896 |\n| --- | --- | --- | --- | --- | --- |\nDeiT-Small/16 | 79.8 | 4.6 | 939 | 101 | 20\nPVT-Small | 79.8 | 3.8 | 794 | 172 | 31\nCvT-13 | 81.6 | 4.5 | 746 | 125 | 14 \nViL-Small | 82.0 | 5.1 | 397 | 87 | 17\nSwin-Tiny | 81.2 | 4.5 | 760 | 189 | 48\nFocal-Tiny | 82.2 | 4.9 | 319 | 105 | 27\n||\nPVT-Medium | 81.2 | 6.7 | 517 | 111 | 20\nCvT-21 | 82.5 | 7.1 | 480 | 85 | 10\nViL-Medium | 83.3 | 9.1 | 251 | 53 | 8 \nSwin-Small | 83.1 | 8.7 | 435 | 111 | 28\nFocal-Small | 83.5 | 9.1 | 192 | 63 | 17\n| |\nViT-Base/16 | 77.9 | 17.6 | 291 | 57 | 8\nDeit-Base/16 | 81.8 | 17.6 | 291 | 57 | 8 \nPVT-Large | 81.7 | 9.8 | 352 | 77 | 14\nViL-Base | 83.2 | 13.4 | 145 | 35 | 5\nSwin-Base | 83.4 | 15.4 | 291 | 70 | 17\nFocal-Base | 83.8 | 16.0 | 138 | 44 | 11\n| |\n\nIn the above table, we notice that Focal Transformers are slower than most of their counterparts except for ViL on low-resolution inputs. When the resolution increases, our models become more comparable and even faster than several methods. Particularly, we notice that ViT/DeiT, CvT and ViL become less efficient for high-resolution inputs. We suspect it is because ViT/DeiT performs fined-grained global self-attention, CvT performs convolution, and then the global self-attention across the whole feature map. Though ViL performs local self-attention, it has several global tokens which attend to the whole feature map. In contrast, our Focal Transformer models are more flexible to different input resolutions, since it performs window-wise attention and has predetermined focal region size and focal window size that are independent of the input resolution. Likewise, Swin Transformers also have such property and thus can maintain very good efficiency for higher resolution input. Giving the above table, we refer the reviewers to our separate responses for a more detailed analysis. We will add all these comparisons and analyses into our revision. Above all, we would like to highlight here that the main motivation for us to propose the focal self-attention to model both short- and long-range interactions is that we would like to apply it to high-resolution inputs which are required for various dense prediction tasks, such as object detection, instance segmentation, and semantic segmentation. \n\n### 2. Concern about Focal-Base model\n\nAnother main concern about our method is the seemingly saturated performance of our Focal-Base model. We also observed this when submitting our paper. Based on our follow-up experiments, we found this is not because our Focal Transformers is prone to overfit when the model becomes larger. Instead, it is mainly because of the sub-optimal hyperparameter setting, i.e., the drop path rates. Following Swin Transformer, when we changed the drop path rate 0.3 to 0.5 for our Focal-Base model, we obtained maximally 83.8% (83.7% on average over multiple runs) top-1 accuracy, which has a substantial margin to Swin-Base model. When we further increase the focal region size from [7,5,3,1] to [9,7,5,1] to capture more long-range dependencies for our Focal-Base model, we observed further improvement to 83.9% top-1 accuracy on average. These experiments clearly address the concerns raised by the reviewers.\n\n### 3. Implementation details\n\nBesides the above two main common concerns, some reviewers are confused by the implementation details of our Focal Transformer model. To resolve these concerns, we had replied separately and explained the computation process we used to implement the focal self-attention. More importantly, we will release the full code so that others can reproduce the results reported in our submission and here.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2U0RbQScYdk",
                "writer": "author",
                "reply_to": "GskYaL9dv2F",
                "title": "Thanks for your valuable comments",
                "comment": " First of all, we would like to thank the reviewer for pointing out the strengths of our work. We also thank the reviewer for the valuable comments and suggestions! We address the concerns one by one below.\n\n**Q 1&2. Model efficiency and throughputs.**\n\nA 1&2: Thanks for pointing this out. Below we report the throughput comparisons for different models below. All throughputs are measured on 1 V100 GPU with batch size 128 and input image resolution 224x224. \n\n| Model       | Top-1 Acc | GFLOPs | Throughput (imgs/s)  |\n| --- | --- | --- | --- |\nDeiT-Small/16 |      79.8       |  4.6     | 939 \nPVT-Small        |      79.8       | 3.8      | 794 \nCvT-13              |      81.6       | 4.5      | 746 \nViL-Small          |      82.0       | 5.1      | 397 \nSwin-Tiny         |     81.2       | 4.5       | 760 \nFocal-Tiny        |     82.2       | 4.9       | 319 \n||\nPVT-Medium  | 81.2        | 6.7      | 517 \nCvT-21             | 82.5        | 7.1      | 480 \nViL-Medium    | 83.3        | 9.1      | 251  \nSwin-Small      | 83.1        | 8.7      | 435  \nFocal-Small     | 83.5        | 9.1      | 192 \n||\nViT-Base/16    | 77.9        | 17.6     | 291 \nDeit-Base/16  | 81.8        | 17.6     | 291 \nPVT-Large       | 81.7        | 9.8       | 352  \nViL-Base          | 83.2        | 13.4     | 145  \nSwin-Base       | 83.4        | 15.4     | 291  \nFocal-Base      | 83.8        | 16.0     | 138  \n\nIn the above table, we do notice that Focal Transformers are slower than their counterparts except for ViL. We refer the reviewer to our response to Reviewer HEkP above for more comparisons on higher resolution inputs. Based on our investigations on different ablated models, we found the throughput decrease is because of two reasons: 1) the overhead to extract the fine-grain and coarse-grain tokens at each transformer layer; 2) the extra computation after adding more keys and values. By measuring the speed for each of the ablated Focal-Tiny models in Figure 5 of our main submission, we obtain the numbers in the below table. We further factorize the time cost of key and value extraction and attention computation by implementing pseudo versions of Focal-Tiny-local and Focal-TIny-global which retain the same number of keys and values by appending pseudo tokens instead of extracting them from the (pooled) feature map. As we can see, local fine-grained local attention leads to more computational cost than coarse-grained global attention. By comparing them with the corresponding pseudo versions, we can see that extracting fine-grained local tokens and coarse-grained global tokens both introduce more overheads than the attention computation. Moreover, extracting fine-grained tokens is relatively heavier than extract coarse-grained tokens.\n\n| Model                                    | Top-1 Acc | GFLOPs   | Throughput (imgs/s) |\n| --- | --- | --- | --- |\nFocal-Tiny                             | 82.2           | 4.92         | 319 \nFocal-Tiny-local                    | 81.4           | 4.90         | 417 \nFocal-Tiny-local-pseudo     | n/a             | 4.90         | 636 \nFocal-Tiny-global                 | 81.6            | 4.59        | 516 \nFocal-Tiny-global-pseudo   | n/a             | 4.59         | 633 \nFocal-Tiny-window              | 80.1            | 4.49        | 731  \n\nBased on the above comparisons, we notice the fine-grained local attention brings most of the extra time cost, and it is mainly due to the sub-optimal implementation of extracting surrounding fine-grained key and value tokens for each window. Currently, we implement this using rolling operations, which is two times faster than the conventional unfolding operation. However, we find this customized implementation is still under-optimized compared with a cuda kernel as used in the convolution layer, especially for the earlier stage which has relatively higher feature resolution. We believe when an efficient cuda kernel implementation is available, this extra time cost can be reduced significantly to approach the gap of FLOPs between different methods. Before that, we empirically studied how our models perform when removing a few time-consuming fine-grained local attentions. Specifically, we tried two variants, one is removing the fine-grained local attention at the first stage, the other one is removing local attention at all odd layers.  As we can see from the table below, these two variants have higher throughputs and slightly lower Top-1 accuracy, but still higher than the Swin-Transformer counterparts. These results imply that we may not need to have fine-grain local attention at all layers and all stages. We will have more systematic studies on this aspect.  \n\n| Model                                                                      | Top-1 Acc | GFLOPs | Throughput (imgs/s)  |\n| --- | --- | --- | --- |\nFocal-Tiny                                                               |     82.2       | 4.92        |  319 \nFocal-Tiny (no local fine-grain at first stage)    |     82.1       | 4.77        | 388 \nFocal-Tiny (local fine-grain at even layers)       |     81.9      | 4.75         | 399 \n||\nFocal-Small                                                               |    83.5       |  9.12       | 192 \nFocal-Small (no local fine-grain at first stage)    |   83.5       |  8.98      | 217 \nFocal-Small (local fine-grain at even layers)       |   83.3       |  8.85      | 240 \n||\nFocal-Base                                                          |    83.8       | 16.04    | 138  \nFocal-Base (no local fine-grain at first stage)     |    83.7       | 15.80   | 154 \nFocal-Base (local fine-grain at even layers)        |    83.5       |  15.56  | 172 \n\n**Q3. Comparing three focal levels with two focal levels used in the paper.**\n\nA3: Thanks for raising this question! The reason why we chose to use two focal levels in all our experiments is that we observed introducing the extra focal level 2 can slightly improve the performance in some cases but brings extra computational overheads in the meantime. Below we assume the finest level as focal level 1 and the coarsest level as focal level 3 and study whether adding the middle focal level can help improve the performance.\n\n| Model |  | focal level 1 | focal level 2 | focal level 3 | Top-1 acc | \\#Params. | GFLOPs | Throughputs |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | \nFocal-Tiny | | - | - | yes      | 81.6   | 28.7 | 4.59  | 516 \nFocal-Tiny | | - | yes | yes | 81.8   | 28.7 | 4.61   | 388\nFocal-Tiny | | yes | - | yes |  82.2 | 29.1 | 4.91 |  319\nFocal-Tiny | | yes | yes | yes | 82.2 | 29.2 | 4.93 | 269\n\nAs we can see from the table, when focal level 1 is missed, adding focal level 2 can improve the performance by 0.2% with the Focal-Tiny model. However, when we use focal level 1, adding focal level 2 cannot further have sensible improvement. We suspect there may be two reasons. First, as we mentioned in lines 159-161, we did not exclude the overlapped tokens across different focal levels. This means that the pooled window tokens at focal level 2 actually have a lot of overlaps to focal levels 1 and 3. This redundancy may explain why adding focal level 2 can improve the performance when merely using focal 3 but not for the combination of focal levels 1 and 3. Second, as we mentioned in our submission, we used the same window size=7 for a fair comparison with Swin Transformers. However, this odd window size hinders us to get a good alignment between each 7x7 window and the sub-windows at focal level 2 (because we need to halve the window size for pooling). We need to perform either padding or trimming for the original feature map so that the number of pooled windows at focal level 2 exactly matches the required one for each window-wise focal attention. We think this misalignment may introduce some noises to the focal self-attention which dismisses the potential benefits. For comprehensiveness, we will add this ablation study in our revision. Moreover, to verify our assumption, we will study whether focal level 2 can bring us some improvements when the windows are well-aligned for focal self-attention, e.g. we change the window size from 7 to 8. We will post the results during the discussion when the experiments are completed.\n\n**Q4: Confusion about handling size mismatch in focal attention computation.**\n\nA4: Thanks for raising this confusion! Given the feature map at a certain stage, e.g., 28x28 at the second stage, we first perform window pooling with s^l_{w}=7 to get 4x4 feature map as described at lines 142-149, then we use unfold operation with (filter_size=5, stride=1, padding=2) to perform the unfolding to get 5x5=25 window tokens for each of these 4x4=16 windows. As such, each of 7x7=49 tokens in a window shares the surrounding 25 window tokens when performing self-attention. Note that we always mask out the padding tokens during unfolding for precise self-attention computations. This strategy is used across all layers and all stages in our Focal Transformers, and the padding size is set to s^l_{r} //2 to adapt to different focal region sizes at different stages.\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DkutifARsjP",
                "writer": "author",
                "reply_to": "OaEv1g7ri8",
                "title": "Thanks for your valuable comments",
                "comment": " First of all, we would like to thank the reviewer for pointing out the contributions of our work. We also thank the reviewer for the valuable comments! We address the concerns one by one below.\n\n**Q1. Model efficiency comparison.**\n\nA1: Thanks for raising this concern! Below we report the throughputs for our Focal Transformers with different model sizes. For comparison, we also report the throughputs for PVT, Swin Transformer, Vision Longformer, CvT, etc. To compare the image classification efficiency, all throughputs are measured on 1 V100 GPU with batch size 128 and input image resolution 224x224. We also add the Top-1 accuracy and corresponding GFLOPs. Moreover, we measure the throughputs for higher input image resolutions 448x448 and 896x896 to simulate the object detection scenario. Note that we intend to **not** measure the full object detection models because all models share the same FPN/RPN/ROI head, which will discount the differences among different models.  \n\n| Model               | Top-1 Acc | GFLOPs |    224x224  | 448x448 | 896x896  |\n|--------|--------|--------|--------| -------| -------|\nDeiT-Small/16 |      79.8       |  4.6     | 939            |  101          | 20\nPVT-Small        |      79.8       | 3.8      | 794            |  172          | 31 \nCvT-13              |      81.6       | 4.5      | 746            |  125         | 14 \nViL-Small          |      82.0       | 5.1      | 397            |  87           | 17 \nSwin-Tiny         |     81.2       | 4.5       | 760            |  189         | 48 \nFocal-Tiny        |     82.2       | 4.9       | 319            |   105        | 27 \n||\nPVT-Medium  | 81.2        | 6.7      | 517                | 111          | 20 \nCvT-21             | 82.5        | 7.1      | 480                | 85            | 10 \nViL-Medium    | 83.3        | 9.1      | 251               | 53             | 8 \nSwin-Small      | 83.1        | 8.7      | 435               | 111           | 28 \nFocal-Small     | 83.5        | 9.1      | 192                | 63             | 17 \n||\nViT-Base/16    | 77.9        | 17.6     | 291             |  57          | 8\nDeit-Base/16  | 81.8        | 17.6     | 291             |   57         | 8\nPVT-Large       | 81.7        | 9.8       | 352              | 77            |  14 \nViL-Base          | 83.2        | 13.4     | 145              |  35           |  5 \nSwin-Base       | 83.4        | 15.4     | 291              | 70            |  17 \nFocal-Base      | 83.8        | 16.0     | 138              | 44            |  11 \n\nIn the above table, we do notice that Focal Transformers are slower than most of their counterparts except for ViL on low-resolution inputs. However, when the resolution increases, our models become more comparable and even faster than several methods. Particularly,  we notice that CvT and ViL become less efficient for high-resolution inputs. We suspect it is because CvT performs convolution and then the self-attention across the whole feature map, and ViL has several global tokens which attend to the whole feature map. In contrast, our Focal Transformer models are more flexible to different input resolutions, since it performs window-wise attention and has predetermined focal region size and focal window size that are independent of the input resolution. Likewise, Swin Transformers also have such property and thus can maintain good efficiency for higher resolution input.\n\nWe further study which components in our Focal Transformers mainly cause the lower throughputs. Based on our model design, we suspect this is because of two reasons: 1) the overhead to extract the fine-grain and coarse-grain tokens at each transformer layer; 2) the extra computation after adding more keys and values. To get more understanding, we investigate the time cost for different ablated models. By measuring the speed for each of the ablated Focal-Tiny models in Figure 5 of our main submission, we obtain the numbers in the below table. To further factorize the time cost of key and value extraction and attention computation, we implement pseudo versions of Focal-Tiny-local and Focal-TIny-global which retain the same number of keys and values by appending pseudo tokens instead of extracting them from the (pooled) feature map. As we can see, local fine-grained local attention causes more computational cost than coarse-grained global attention. Moreover, extracting fine-grained local tokens introduces more overheads than extracting the coarse-grain global tokens. This inspires one of our future directions on how to extract the local tokens more efficiently.\n\n| Model                                    | Top-1 Acc | GFLOPs   | Throughput (imgs/s) |\n|---|---|---|---|\nFocal-Tiny                             | 82.2           | 4.92         | 319 \nFocal-Tiny-local                    | 81.4           | 4.90         | 417 \nFocal-Tiny-local-pseudo     | n/a             | 4.90         | 636 \nFocal-Tiny-global                 | 81.6            | 4.59        | 516 \nFocal-Tiny-global-pseudo   | n/a             | 4.59         | 633 \nFocal-Tiny-window              | 80.1            | 4.49        | 731  \n\nBased on the above comparisons, we notice the fine-grained local attention brings most of the extra time cost, and it is mainly due to the sub-optimal implementation of extracting surrounding fine-grained key and value tokens for each window. Currently, we implement this using rolling operations, which is two times faster than the conventional unfolding operation. However, we find this customized implementation is still under-optimized compared with an efficient cuda kernel as used in the convolution layer, especially for the earlier stage which has relatively higher feature resolution. We believe when an efficient cuda kernel implementation is available, the efficiency gap can be closed to the gap of FLOPs between different methods. Before that, we empirically studied how our models perform when removing a few time-consuming fine-grained local attentions. Specifically, we tried two variants, one is removing the fine-grained local attention at the first stage, the other one is removing local attention at all odd layers.  As we can see from the table below, these two variants have higher throughputs and slightly lower Top-1 accuracy, but still higher than the Swin-Transformer counterparts. These results imply that we may not need to have fine-grain local attention at all layers and all stages. We will have more systematic studies on this aspect.  \n\n| Model                                                                      | Top-1 Acc | GFLOPs | Throughput (imgs/s) |\n|---|---|---|---|\nFocal-Tiny                                                               |     82.2       | 4.92        |  319 \nFocal-Tiny (no local fine-grain at first stage)    |     82.1       | 4.77        | 388 \nFocal-Tiny (local fine-grain at even layers)       |     81.9      | 4.75         | 399 \n||\nFocal-Small                                                               |    83.5       |  9.12       | 192 \nFocal-Small (no local fine-grain at first stage)    |   83.5       |  8.98      | 217 \nFocal-Small (local fine-grain at even layers)       |   83.3       |  8.85      | 240 \n||\nFocal-Base                                                            |    83.8       | 16.04    | 138  \nFocal-Base (no local fine-grain at first stage)     |    83.7       | 15.80   | 154 \nFocal-Base (local fine-grain at even layers)        |    83.5       |  15.56  | 172 \n\n**Q2. Performance saturates for Focal-Base model.**\n\nA2: Thanks for pointing this out! During our submission, we also observed these close Top-1 accuracies for our Focal-Small and Focal-Base model. Based on our further experiments, we found this is due to the sub-optimal hyperparameter setting. More specifically, in our submission, we set the drop path rate 0.3 for our base model without heavy hyperparameter tuning. But when we increased it to 0.5 following Swin Transformer, we can get a maximal 83.8% (83.7% on average) Top-1 accuracy on ImageNet-1K. This suggests that when vision transformer models become larger and deeper, the drop path rate should also be increased accordingly for a good regularization.\n\nBased on the new drop path rate, we study whether we can further improve the performance of our Focal-Base model. One change we did is that we increase the focal region size from the original [7,5,3,1] to [9,7,3,1], which enables more global interactions at the first two stages. This change achieves a better average Top-1 accuracy 83.9%. When we further change the convolutional patch embedding layer from default non-overlapped (kernel size = stride size) to overlapped ones (kernel size > stride size), the performance can be further improved to 84.1% top-1 accuracy on average. \n\nAll these ablation studies demonstrate that our Focal-Base model is not saturated when we use reasonable hyperparameters and can be further improved from various aspects.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1Q2FgfyjqUL",
                "writer": "author",
                "reply_to": "ecpEKMw12k",
                "title": "Thanks for your valuable comments ",
                "comment": " We thank the reviewer for acknowledging the novelty and contributions of our work! We also thank the reviewer for all the valuable comments and answer the questions one by one below.\n\n**Q1: Runtime / throughput is not clear.**\n\nA1:  Thanks for pointing out this! Below we report the throughputs for our Focal Transformers with different model sizes. For comparison, we also report the throughputs for PVT, Swin Transformer, Vision Longformer, CvT, etc. All throughputs are measured on 1 V100 GPU with batch size 128 and input image resolution 224x224. \n\n| Model               | Top-1 Acc | GFLOPs | Throughput (imgs/s) |\n| -----------| ----------- | ----------- | ----------- |\nDeiT-Small/16    |      79.8       |  4.6     | 939 \nPVT-Small         |      79.8       | 3.8      | 794 \nCvT-13              |      81.6       | 4.5      | 746 \nViL-Small           |      82.0       | 5.1      | 397 \nSwin-Tiny          |     81.2       | 4.5       | 760 \nFocal-Tiny         |     82.2       | 4.9       | 319 \n|  |\nPVT-Medium     | 81.2        | 6.7      | 517 \nCvT-21              | 82.5        | 7.1      | 480 \nViL-Medium       | 83.3        | 9.1      | 251  \nSwin-Small        | 83.1        | 8.7      | 435  \nFocal-Small     | 83.5        | 9.1      | 192 \n|  |\nViT-Base/16    | 77.9        | 17.6     | 291 \nDeit-Base/16  | 81.8        | 17.6     | 291 \nPVT-Large       | 81.7        | 9.8       | 352  \nViL-Base          | 83.2        | 13.4     | 145  \nSwin-Base       | 83.4        | 15.4     | 291  \nFocal-Base      | 83.8        | 16.0     | 138  \n\nIn the above table, we do notice that Focal Transformers are slower than most of their counterparts except for ViL though they have similar FLOPs. We found this is because of 1) the overhead to extract the fine-grain and coarse-grain tokens at each transformer layer; 2) the extra computation after adding more keys and values. To get more understanding, we investigate the time cost for different ablated models. By measuring the speed for each of the ablated Focal-Tiny models in Figure 5 of our main submission, we obtain the numbers in the below table. To further factorize the time cost of key and value extraction and attention computation, we implement pseudo versions of Focal-Tiny-local and Focal-TIny-global which retain the same number of keys and values by appending pseudo tokens instead of extracting them from the (pooled) feature map. As we can see, local fine-grained local attention causes more computational cost than coarse-grained global attention. Moreover, extracting fine-grained local tokens introduces more overheads than extracting the coarse-grain global tokens. This inspires one of our future directions on how to extract the local tokens more efficiently. \n\n| Model     | Top-1 Acc | GFLOPs   | Throughput (imgs/s) |\n| -----------           | ----------- | ----------- | ----------- |\nFocal-Tiny                             | 82.2           | 4.92         | 319 \nFocal-Tiny-local                    | 81.4           | 4.90         | 417 \nFocal-Tiny-local-pseudo     | n/a             | 4.90         | 636 \nFocal-Tiny-global                 | 81.6            | 4.59        | 516 \nFocal-Tiny-global-pseudo   | n/a             | 4.59         | 633 \nFocal-Tiny-window              | 80.1            | 4.49        | 731\n\nBased on the above analysis, the fine-grained local attention brings most of the extra time cost, and it is mainly due to the sub-optimal implementation of extracting surrounding fine-grained key and value tokens for each window. Currently, we implement this using rolling operation, which is two times faster than the conventional unfolding. However, we find this customized implementation is still under-optimized compared with a cuda kernel, especially for the high feature resolution at the first stage. We believe when an efficient cuda kernel implementation is available, the gap of time cost can be reduced to approach the gap of FLOPs. Before that, we empirically studied how our models perform when removing a few time-consuming fine-grained local attentions. Specifically, we tried two variants, one is removing the fine-grained local attention at the first stage, the other one is removing local attention at all odd layers.  As we can see from the table below, these two variants have higher throughputs and slightly lower Top-1 accuracy, but still higher than the Swin-Transformer counterparts. These results imply that we may not need to have fine-grain local attention at all layers. We will have more systematic studies on this aspect.  \n\n| Model    | Top-1 Acc | GFLOPs | Throughput (imgs/s) |\n| -----------           | ----------- | ----------- | ----------- |\nFocal-Tiny      |     82.2       | 4.92        |  319 \nFocal-Tiny (no local fine-grain at first stage)    |     82.1       | 4.77        | 388 \nFocal-Tiny (local fine-grain at even layers)       |     81.9      | 4.75         | 399\n|  |\nFocal-Small                                                       |    83.5       |  9.12       | 192 \nFocal-Small (no local fine-grain at first stage)    |   83.5       |  8.98      | 217 \nFocal-Small (local fine-grain at even layers)       |   83.3       |  8.85      | 240 \n|  |\nFocal-Base                                                         |    83.8       | 16.04    | 138  \nFocal-Base (no local fine-grain at first stage)     |    83.7       | 15.80   | 154 \nFocal-Base (local fine-grain at even layers)        |    83.5       |  15.56  | 172 \n\n**Q2: Scaling to larger models.**\n\nA2: Thanks for pointing out this! During our submission, we also observed this. Based on our further experiments, we found this is due to the sub-optimal hyperparameter setting. More specifically, in our submission, we set the drop path rate 0.3 in one shot for our base model without any further hyperparameter tuning. But when we increased it to 0.5 as in Swin Transformer, we can get maximal 83.8% Top-1 accuracy on ImageNet-1K. This suggests that when vision transformer models become larger and deeper, the drop path rate should also be increased accordingly for a good regularization to prevent overfitting.  \n\nFor consistency to Swin models, we change the drop path rates for our Focal-Small and Focal-Base to 0.3 and 0.5, respectively, and conduct multiple rounds of training with all our three models as the reviewer suggested. We report the mean Top-1 acc and std below. \n\n\n| Model              |   Top-1 Mean  | Top-1 Std  |\n| -----------           | ----------- | ----------- | \nFocal-Tiny       |    82.15             |  0.105 \nFocal-Small     |    83.51             |  0.055 \nFocal-Base      |    83.69             | 0.1002 \n\nClearly, with the same hyperparameter settings, our Focal-Base has better performance than Focal-Small, and these three models consistently outperform the corresponding Swin Transformers.  \n\nAs the reviewer pointed out, our models indeed have more gains over Swin Transformers on COCO object detection than ImageNet classification when the model becomes larger and deeper. We suspect the reason why we observe this trend is because larger Swin Transformer models can already capture sufficient context through the window-shift mechanism for image classification. Consider the input resolution 224x224. From the first stage to the last stage, the feature map resolutions are 56x56, 28x28, 14x14, and 7x7, respectively. At the earlier stage, our Focal Transformers can capture more long-range dependencies than Swin Transformers. But when it comes to the later stages, the stacked window-shift operation in Swin Transformers can (almost) have a receptive field that covers the whole feature map, especially for larger models. However, for object detection, the input image resolution is usually much higher, e.g., 800x1333. In this case, the feature map at the last stage is still much larger than a window size 7x7. Even the base Swin Transformer is still difficult to capture the global context sufficiently. In contrast, our focal self-attention is intentionally designed to model such long-range dependencies for high-resolution feature maps. As such, we can still observe considerable improvements over Swin Transformers on COCO detection. \n\n**Q3: Intuition behind sharing qkv embedding layer for different focal levels.**\n\nA3: Thanks for this valuable suggestion! The reason for us to share the qkv embedding is two-fold: 1) though the tokens are from different granularities, we would like to project them into the same qkv space so that the attention between queries and keys from different granularities can be well-calibrated; 2) Indeed, using different qkv embedding layers will introduce more parameters. For example, using two qkv embedding layers for the local and global focal levels in our Focal-Tiny models will bring extra ~7M parameters on the original 29M model. We agree with the reviewer that different granularities may need different qkv embedding layers because they may capture different levels of structures. Due to the limited computational resource and time, we cannot report the performance for this new design right now, but we will post the results here once we get the experiments finished. \n\n**Q4: Slight difference on training recipe.**\n\nA4: Thanks for pointing this out! In our submission, we used drop path rates 0.2, 0.2, and 0.3 for our tiny, small, and base Focal Transformers, respectively. In contrast, Swin Transformers used 0.2, 0.3, and 0.5 drop path rates. We observed these differences indeed affect the performance to some extent. As shown in the above table, we reported the numbers with these new settings. Clearly, it helps us to achieve even better performance than our previous setting, especially for our Focal-Base model (83.5->83.8 maximally). Note that even with the sub-optimal hyperparameter setting, our Focal Transformers already outperform Swin Transformers on both image classification and object detection tasks.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "MVMaqLkPec",
                "writer": "author",
                "reply_to": "tdi1e2bnUm8",
                "title": "Thanks for your valuable comments",
                "comment": " First of all, we thank the reviewer for acknowledging the novelty, quality, clarity of our work! We also thank the reviewer for all the valuable comments! We answer the questions one by one below.\n\n**Q1: Dimension inconsistency for convolutional patch embedding layer.**\n\nA1: Sorry for the confusion! By filter size equal to 4, we meant the kernel size for our convolution is set to 4x4. That being said, the convolutional layer we used for the first patch embedding is implemented with pytorch nn.Conv(in_channel=3, out_channel=d, kernel_size=4, stride=4). As such, the input image of size 3x224x224 will be transformed to dx56x56. Note that this convolutional layer is exactly the same as in Swin Transformer for a fair comparison. \n\n**Q2: Big model does not achieve better performance.**\n\nA2: Thanks for pointing this out! We also noticed this when working on our submission. Based on further experiments, we found this is due to the limited hyperparameter tuning, instead of the strong inductive bias or poor generalization ability of big models. More specifically, in our submission, we used a small drop path rate of 0.3 for our Focal-Base model. When we follow Swin Transformer to increase it to 0.5, we can get 83.8% Top-1 accuracy on ImageNet-1K. This indicates that when vision transformer models become larger, the drop path rate should also be increased accordingly for a good regularization on the big models.\n\n**Q3: Criterion to put CvT-21 in Table 2.**\n\nA3: Thanks for the suggestion! Though CvT-21 just has a slightly larger model size than our Focal-Tiny (32.0M v.s. 29.1M), we compare it with our Focal-Small for two reasons: 1) it has much higher FLOPs than Focal-Tiny (7.1 GFLOPs v.s. 4.9 GFLOPs), which means more computations are used in the model; 2) it has 21 transformer layers, which is comparable to 24 layers in Focal-Small but much more than 12 layers in Focal-Tiny. Admittedly, the comparison between CvT-21 and Focal-small is still not fair enough. To have a relatively fair comparison, we increase the depth of our Focal-Tiny model from 2-2-6-2 to 1-2-21-1 but decrease the hidden dimension from 96 to 64. These changes lead to a new version of Focal-Tiny model with 26.5M parameters and 4.67 GFLOPs. Based on this new Focal-Tiny model, we achieved 82.9% Top-1 accuracy, which is higher than 82.5 for CvT-21 but uses much fewer parameters and FLOPs. In our revision, we will carefully make fair comparisons with previous works including CvT by adding more variants of our Focal Transformer models to match the number of parameters and GFLOPs as possible as we can. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tdi1e2bnUm8",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_2zCRcTafea",
                "title": "",
                "comment": "This paper presents an efficient self-attention mechanism for vision transformer, which simultaneously capture both short and long-range visual dependencies by making the query to attend patches at different granularity levels. The authors demonstrate its performance on image classification and object detection.  Originality:\n- The idea of extracting visual features using multiple granularity levels is well-established. Previous work has also tried to build a hierarchy of features within vision transformer (e.g., PVT). However, it is novel to build a single self-attention head which can attend multiple granularity levels.\n\nQuality:\n- The submission is technically sound.\n\nClarity:\n- The paper is well written.\n\nSignificance:\n- The empirical results are compelling. \n\nDetailed comment:\n\n1,  In section 3.1, a \u201cpatch embedding layer which consists of a convolutional layer with filter size and stride both equal to 4\u201d could not transform input $\\frac{H}{4} \\times \\frac{W}{4} \\times (4\\times4\\times3)$ to output $\\frac{H}{4} \\times \\frac{W}{4} \\times d$.\n\n2,  In Table 2, Focal-Base does not achieve better result than Focal-Small. Do you think it\u2019s due to the limited set of hyperparameters that has been tried, or it\u2019s due to the strong induction bias within focal attention may hurt the generalization ability of big model?\n\n3, What is the criterion to organize Table 2? CvT-21 should be placed along with Focal-Tiny. Yes.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "OaEv1g7ri8",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_2zCRcTafea",
                "title": "",
                "comment": "This paper proposes Focal Transformer, which captures both local and global attention based on their design. It also follows the pyramid architecture so their model can be easily extended to detection and segmentation tasks. The result on image classification detection and segmentation are very promising.   Overall, the main contribution of this paper is proposing a new attention module for multi-level trasnformer, which seems efficient and can capture both local and global information. Then the architecture heavily follows PVT and Swin Transformer.  \n\nAbout performance, it is slightly better than Swin Transformer, which is reasonable because swin do not have global attention.\n\nAdvantages:\n\n1. clear paper writing, which is easy to follow.\n\n2. strong performance on several tasks.\n\n3. reasonable story to design an efficient self-attention but consider both global and local region.\n\nWeakness:\n\n1. I have some concerns about the inference speed of this method. If I have a misunderstanding please correct me. Mainly because calculating focal attention seems not very straightforward. I understand the theoretical computation complexity is efficient but it is not clear of real running speed. **It is necessary to report the speed and compare with other methods** such as Swin and PVT, especially on object detection and segmentation which need a high-resolution image as input.\n\n2. In Table2 when scaling model size from small to base, the accuracy stops to increase. So I wonder does this model's performance upper bound is lower than other methods? Because ViT can consistently improve the performance when enlarging the model size even the model size is super large.\n Yes",
                "rating": 6,
                "confidence": 5
            },
            {
                "review_id": "GskYaL9dv2F",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_2zCRcTafea",
                "title": "",
                "comment": "This paper proposed a new attention mechanism called focal attention to enable efficient long-range interactions in vision transformer. By plugging the proposed module into a multi-scale vision transformer model,  a new focal transformer was designed and its superiority was demonstrated over the state-of-the-art approaches on both image classification and object detection tasks.  1. This paper is well written, and has no obvious mistakes.\n2. For a vision transformer model, how to reduce the computational overhead while keeping ability of capturing the long-range dependencies is still an important and interesting research topic. \n3. The experiments are conducted on various benchmarks on image classification and object detection, with extensive ablation studies.\n 1. There are limited discussions on model efficiency. I suggest the authors to add more experimental comparisons with Sparse Transformer or some other efficient transformer methods. \n2. The focal windows and region partition operations do not bring any additional flops but may be time-consuming.\u00a0 It should better add throughoutput or latency results in Table2.\n3. Figure 4 shows three fine to coarse attention levels with short and long range interaction.\u00a0 However, in the actual model configuration the attention only has two level and mismatch Figure4. Why dose this happen?\u00a0 The authors should do ablation study about the number of level including acc and latency. \n4. In table 4 stage1-2, s^1_{w} * s^1_{r} is larger than input resolution, e.g, 7x5>28. How do you handle this situation. ",
                "rating": 6,
                "confidence": 5
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the novelty/solid results of the paper",
                "Sentiment Expression": "a unanimous agreement",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The proposed idea in this paper",
                "Sentiment Expression": "is interesting, well-motivated, and could make good contributions",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "TlS3LBoDj3Z": {
        "paper_id": "iclr_2021_TlS3LBoDj3Z",
        "paper_title": "QTRAN++: Improved Value Transformation for Cooperative Multi-Agent Reinforcement Learning",
        "paper_abstract": "QTRAN is a multi-agent reinforcement learning (MARL) algorithm capable of learning the largest class of joint-action value functions up to date. However, despite its strong theoretical guarantee, it has shown poor empirical performance in complex environments, such as Starcraft Multi-Agent Challenge (SMAC). In this paper, we identify the performance bottleneck of QTRAN and propose a substantially improved version, coined QTRAN++. Our gains come from (i) stabilizing the training objective of QTRAN, (ii) removing the strict role separation between the action-value estimators of QTRAN, and (iii) introducing a multi-head mixing network for value transformation. Through extensive evaluation, we confirm that our diagnosis is correct, and QTRAN++ successfully bridges the gap between empirical performance and theoretical guarantee. In particular, QTRAN++ newly achieves state-of-the-art performance in the SMAC environment. The code will be released.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "This paper proposes practical improvements to theoretically well founded QTRAN, which is a state-of-the-art technique of cooperative multi-agent reinforcement learning.  The improvements include new designs of loss function and action-value estimator, which might be widely applicable beyond QTRAN.  However, it is not obvious if the proposed improvements actually improves the performance of QTRAN, and experimental evaluation is essential to this work.  After the discussion, there remain some major concerns about the experimental results.  In particular, the performance of baselines in the experiments is not consistent with those reported in the prior work.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "l1gcpZMfCwK",
                "reply_to": "iclr_2021_TlS3LBoDj3Z",
                "title": "Improvement over QTRAN is demonstrated. Approach is complex. Feels somewhat incremental.",
                "comment": "### Summary and claims\n\nThis work proposes a MARL (multi-agent reinforcement learning) algorithm.\nIn the MARL setting, multiple agents have to make choices based on independent information to maximize a common objective. An existing algorithm in this space is QTRAN.\nThe authors propose several modifications to QTRAN: changing the architecture, adding two additional constraints to the loss function and also allowing gradients to flow from the QTRAN objective into the \"true\" action-value estimator.\nThe claims of the paper are:\n1) QTRAN++ achieves better performance than QTRAN\n2) The modifications introduced stabilize training compared to QTRAN\n\nIt took me some time to fully understand all of the components that go into QTRAN++ and I find the complexity of the overall algorithm pretty surprising, especially considering that other algorithms in this space are as simple as \"add up all the Q values of the individual agents\".\nI think the proposed changes consist of:\n - Rather than directly training action-value networks, a QMIX-like hypernet approach is used\n - The network that estimates the \"true\" (combined) action-value is implemented through what the authors call a \"semi-monotonic mixing network\", which is the sum of a non-monotonic (regular) hypernet and a monotonic hypernet as used in QMIX. This seems pretty arbitrary. Isn't the original idea behind QTRAN that this would accurately track the true values?\n - In QTRAN the separate network that aggregates the Q values of the individual agents is trained to track the \"true\" action-value. In QTRAN++ this is done through multiple hypernetworks (the authors call these \"heads\").\n - The loss function is modified to impose two additional constraints on the transforming value function.\n - Gradients are now also backpropagated from the \"tracking loss\" into the \"true\" action-value estimator, which makes it somewhat unclear what it is actually representing.\n\n### Relation to prior work\n\nThe paper is positioned sufficiently with respect to prior work. I've noticed that there is a larger section on related work in the appendix. I'm not sure what the purpose of moving the related work into the appendix is, especially if some of the papers mentioned there are not actually related to the work presented in this paper. I think it would be good to try to move as much as possible of that section into the main text, leaving out prior work that is not sufficiently related.\nSome of the additions in QTRAN++ seem very similar to ideas proposed in QMIX, but this is not directly acknowledged as far as I can tell. It would be good to point out which parts of the architecture come from QMIX.\n\n### Are the claims supported?\n\nThe experiments presented in the paper are reasonably thorough and show that\nQTRAN++ consistently outperforms QTRAN on the tasks that were tested. SMAC (StarCraft Multiagent Challenge) is a nontrivial benchmark, so I would agree that claim 1) has been shown sufficiently. But I'm not sure whether the ablation studies are thorough enough to really demonstrate that all of the components of the (rather complex) proposed algorithm are really needed.\n\nThe authors often make claims about improved stability and other properties of the algorithm throughout the paper, but these are not supported by any empirical evidence. If the authors want to claim that QTRAN++ outperforms QTRAN because of a specific mechanism then it would be good to provide some sort of empirical evidence or proof (the proof in appendix A doesn't count since it doesn't make any statements about stability). Therefore I think that claim 2) is currently not well supported and it would be good to either support it better or soften the statements in the paper to make statements in the form of \"we believe that the algorithm has improved stability\".\n\n### Presentation and clarity\n\nThe paper is reasonably clear and understandable. There are some cases where incorrect grammar or word choice made a sentence difficult to understand. For example the choice of \"affluent\" to describe a class of estimators. It would be good to address cases like this to improve the clarity of the paper.\n\n### Conclusions\n\nThe main claim of the paper (that QTRAN++ is an improvement over QTRAN) has been demonstrated sufficiently. But the high complexity of the approach (with several additions to the algorithm feeling somewhat arbitrary) and the fact that the paper \"merely\" presents an upgrade to QTRAN could be potential arguments against accepting it.\n\n\n### *Edit after author comments:*\n\nI have read the author comments and the latest paper revision. The authors have noticeably improved the clarity of the paper in several places and adjusted their claims about the stability of the algorithm, and the improved ablation studies are appreciated. Unfortunately, after thinking it through very carefully and despite the author comments, I have not been able to understand some aspects of the model, for example why gradients from the tracking loss are backpropagated into the value function that is supposed to track the \"true\" action-values. Several parts of the architecture seem to have a complicated dual purpose, which makes it difficult to understand what is going on and why the model is performing better. I suspect that other readers might also encounter similar issues, which makes it difficult for me to raise my rating. I've decided to leave the rating at 6 (marginal accept).",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "N3N1ASRnPr6",
                "reply_to": "iclr_2021_TlS3LBoDj3Z",
                "title": "Interesting work; evaluation can be improved",
                "comment": "### Summary\nThis paper presents an improved version of QTRAN [1]. The design is based on new loss function design, as well as new action-value estimator designs. The paper claims superior perfromance gains compared to previous methods on the Starcraft Multi-Agent Challenge (SMAC) environment.\n\n### Strengths\n+ The ideas proposed to improve the previous QTRAN (or might be applied to other MARL algorithms as well) seems novel and general.\n+ The authors perform comprehensive ablation studies for different components they proposed.\n+ The empirical performance on the SMAC benchmark is better and more stable across different runs.\n+ The writing is clear and easy to follow.\n\n### Weaknesses\n- Only one environment is evaluated, which might not be that convincing. It would be good to see more results on different benchmarks.\n- In Figure 3, some results seem to be not converged yet. Since the metric is the win rate, which is bounded, it would be interesting to see given enough training steps, whether all methods can actually converge to similar winning rate, or inherently the proposed scheme can lead to better results.\n\n### Minor issues\nTypo: Page 6, \"... for being \u201cselfish.\u201d This ...\" -> \"... for being \u201cselfish\u201d. This ...\"\n\n\nConsidering all the aspects, I tend to accept the paper in the current stage.\n\n\n### Reference\n1. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. 2019.\n\n----------------------------------------------------------------------\n\n**Updates**: After reading the other reviews and the rebuttal, I still maintain my current score. The additional experiments on the converged results are good to me. As I'm not very familiar with the performance in MARL literatures, I have decreased my confidence from 3 to 2 to reflect some of the concerns of other reviewers involving the performance for the baselines.",
                "rating": 6,
                "confidence": 2,
                "writer": "official_reviewer"
            },
            {
                "review_id": "oJw_iowQub",
                "reply_to": "iclr_2021_TlS3LBoDj3Z",
                "title": "QTRAN++ is a good improvement for QTRAN, but the expectation of the advanced version of QTRAN is higher than in 2019.",
                "comment": "This paper provides good improvements that make QTRAN more practical and can be applied to problems other than matrix games. Since QTRAN is a significant improvement for value-based multi-agent reinforcement learning after QMIX, the practical implementation of QTRAN is expected for a long time in the community. However, after the publication of QTRAN, especially in 2020, some other works have explored the question of how to extend QMIX to the full IGM function class. Due to these works (QPLEX is the major concern, given that weighted-QMIX has been compared in the experiments), the expectation of the advanced version of QTRAN is higher than before. I have several concerns regarding several core contributions of QTRAN++.\n\nQTRAN++ relies heavily on the true joint action-value function. (1.1) However, learning joint action-value functions is not an adorable choice in multi-agent problems. (1.2) To ease the training and representation of joint action-value functions, the authors condition $Q_{jt}$ on individual q values and use a semi-monotonic structure. However, it is difficult to tell the contribution of the monotonic part. It has been shown that monotonic functions can not represent some Q-values. Why should this part be included? I expect that I can find the answer from ablation studies, but on two out of three scenarios, FC-QTRAN++ is very similar to QTRAN++. The authors can provide a more serious discussion of this part to make their paper stronger.\n\nAbout the training of $Q_{tran}^{i}$. I have two questions about the training of this value function. (2.1) When training $Q_{tran}^{i}$, whether local utility function of agent $j$ ($q_j$ using the notation from the paper) is updated? (2.2) The training scheme is a midpoint between VDN and QMIX, which is similar to an attention mechanism that has been explored in multi-agent value decomposition settings. The formulation is quite different from previous papers (DOP [Wang et al., 2020] and REFIL [Iqbal et al. 2020]), but based on the results from these previous work, I think the multi-head structure may not improve the performance. Although the authors use a matrix game to illustrate their idea, which I appreciate, I can hardly tell whether this example is specially designed. I was expecting a convincing ablation study on SMAC, but I do not find them sufficient: (1) The authors did not record how many random seeds did they test, and SMAC tasks are typically sensitive to random seeds. (2) The gap between QTRAN and QTRAN++ is not significant. If the authors can provide results with more random seeds on more maps, I will consider revising my rating.\n\nMy last concern is about QPLEX, as cited by the authors. Similar to QTRAN, QPLEX provides full expressivity for the IGM function class. Nevertheless, the implementation of QPLEX seems to be much more lightweight than QTRAN++. Since QPLEX has provided codes that can be freely tested on the SMAC benchmark, I was wondering why the authors cited this paper but did not compare to it. At least, a detailed discussion of the differences can make the contribution of QTRAN++ clearer.\n\n** A minor concern about experiments.\nIt seems that the authors are using an older version of QMIX. In the latest version, QMIX can achieve a win rate pf 80% on MMM2. This fact is unknown for many, because the journal version of QMIX reports the same win rate as in this paper.\n\n[Wang et al., 2020] Wang, Y., Han, B., Wang, T., Dong, H. and Zhang, C., 2020. Off-Policy Multi-Agent Decomposed Policy Gradients. arXiv preprint arXiv:2007.12322.\n\n[Iqbal et al. 2020] Iqbal, S., de Witt, C.A.S., Peng, B., B\u00f6hmer, W., Whiteson, S. and Sha, F., 2020. AI-QMIX: Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2006.04222.\n\n\n\n##### ======================== UPDATE =========================\n\nThanks for the authors' clarifications.\n\nAfter a careful re-evaluation of the paper, I have many concerns about the performance of baselines on the StarCraft II benchmark tasks. The reported performance is not consistent with those reported in the SMAC benchmark paper (see Figure 4,5,6 in [1]) and QPLEX paper (Figure 5,8,19 in [2]). Moreover, I also evaluate the available GitHub codes of baselines on my own, which is consistent with [1,2].\n\nUsing results in [1,2], QTRAN++ significantly underperforms the baselines on the StarCraft II benchmark tasks. Moreover, the paper claims that it uses the standard StarCraft II benchmark, the latest version of SC2, and the default baseline codes.\n\nDue to these concerns, I tend to lower my rating.\n\n[1] Samvelyan M, Rashid T, de Witt C S, et al. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\n\n[2] Jianhao Wang, Zhizhou Ren, Terry Liu, Yu Yang, and Chongjie Zhang. Qplex: Duplex dueling multi-agent Q-learning. ICLR submission. https://openreview.net/forum?id=Rcmk0xxIQV",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ouCmkmZw5oU",
                "reply_to": "iclr_2021_TlS3LBoDj3Z",
                "title": "Common response (updated): our work contributes to bridging the gap between theory and practice of MARL.",
                "comment": "Dear reviewers, \n\nWe express our deepest gratitude for your constructive feedback and valuable comments. \n\nAs the reviewers highlighted, we strongly believe that our work provides a significant empirical improvement (R1, R3, R4) to a theoretically important algorithm (R2, R4) with novel and general ideas (R1), which is presented with clear writing (R1). Overall, we believe that our work delivers a significant contribution by bridging the gap between theory and practice, for MARL.\n\nIn response to the questions and concerns you raised, we have carefully revised and enhanced our paper with the following additional experiments and discussions.\n\n- additional explanation on the contribution of our QTRAN++ for R4 (Section 1)\n- softened claims on improving the stability of QTRAN for R3 (Section 1)\n- improved description of the prior works for R3 (Section 2 and Appendix B)\n- clear explanations for the algorithmic components of QTRAN++ for R2 and R3 (Section 3)\n- experiments using QPLEX as an additional baseline for R2 (Figure 3)\n- ablation studies on seven more scenarios for R2 and R3 (Figure 4)\n- description of the versions of StarCraft and QMIX being used for R2 (Appendix D)\n- experiments using an increased number of training steps for R1 (Appendix E)\n- experiments on an additional environment for R1 and R2 (Appendix F)\n- additional ablation study for the semi-monotonic mixing network on an addition environment for R2 (Appendix G)\n\nThe revisions made are marked with \u201cred\u201d in the revised paper. \n\nThanks, Authors.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S_3bywsM8V",
                "reply_to": "CDdepvvA7H",
                "title": "Thanks for your response!",
                "comment": "Thank you very much for sharing the concern. \n\nWe first clarify your statement: our proposal to use a semi-monotonic structure is not to **learn** the action-value functions efficiently in the monotonic class (for the given environment). Instead, we designed our semi-monotonic network for being efficiently **learned by** the (transformed) action-value functions in the monotonic class. \n\nWe also verify that the semi-monotonic network does not over-exploit the fact that monotonic functions are sufficient for most maps in the SMAC benchmark. To this end, we refer to Figure 4 of our revised paper; here, our QTRAN++ outperforms its variant without the semi-monotonic network (FC-QTRAN++) in the maps where the monotonic functions are not sufficient, i.e., MMM2 (negative) and 10m_vs_11m. We consider the monotonic functions to be insufficient for such maps since the (monotonic) QMIX performs considerably worse than the (non-monotonic) QTRAN for solving them in Figure 3. \n\nTo further alleviate your concern on the semi-monotonic network, we considered an additional task, i.e., multi-domain Gaussian squeeze (MGS) benchmark [8], to compare QTRAN++ and its variant without the semi-monotonic network (FC-QTRAN++). The MGS benchmark is meaningful since it is specifically designed to show the case where the monotonicity functions are insufficient, e.g., QMIX performs worse than QTRAN. We report the corresponding result in Figure 8 of the revised paper. Here, one observes how QTRAN++ performs at least as well as FC-QTRAN++. This again verifies how our semi-monotonic network does not hurt the performance of QTRAN++ for general tasks.  \n\n**References**  \n[8] Son et al. QTRAN: Learning to factorize with transformation for cooperative multi-agent reinforcement learning, ICML 2019",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CDdepvvA7H",
                "reply_to": "V3XuOFKOV1E",
                "title": "Re: Response",
                "comment": "I want to thank the authors for their careful revision and considerate response. \n\nI discussed the paper with some other researchers in the MARL community, and I think a common concern is about the semi-monotonic joint Q network. I am still unclear about the monotonic part. The authors clarify this can \"bias the true action-value estimator towards being learned easily by a monotonic function\". I didn't quite get the point. Is this generally good for all tasks apart from SC2 (explained in detail in the next paragraph)?\n\nIn the revised paper, I think the authors mean that, for those action-value functions in the monotonic class, such a structure may learn efficiently. Authors show more experimental results to show the superiority of the semi-monotonic structure, which I appreciate. However, this raises a further concern: whether the proposed structure bypasses the difficulty of estimating joint Q values by exploiting the fact that monotonic functions are sufficient for most maps in the SMAC benchmark? (some recent papers support this fact) The monotonic part may hurt performance on other tasks like Predator and Prey. ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "V3XuOFKOV1E",
                "reply_to": "oJw_iowQub",
                "title": "Response to R2 (2/2)",
                "comment": "\n\n\n**6. The ablation studies are not sufficient (for validating the contribution of the multi-head structure). The authors did not record how many random seeds they test. Authors can provide results with more random seeds on more maps.**\n\nThank you for the suggestion. To address your concern, we extended our ablation studies from 3 maps to 10 maps. We report the experimental results in Figure 4 of the revised paper. In the figure, one can observe how the multi-head component provides a concrete improvement to QTRAN++. To be specific, QTRAN++ performs significantly better than Mix-QTRAN++ (QTRAN++ without the multi-head component) in the 5m_vs_6m and 5m_vs_6m (negative) scenarios and performs at least as good as Mix-QTRAN++ for the other eight scenarios.\n\nFurthermore, we recorded that we use five random seeds for all the experiments in Section 4.1 of our paper (before revision). If you find this number of random seeds insufficient, we will be happy to run more experiments and increase the number of random seeds.\n\n---\n\n**7. The gap between QTRAN and QTRAN++ is not significant.**\n\nWe do believe that the gap between QTRAN and QTRAN++ is significant. As one can see in Figure 3 of our (original or revised) paper, QTRAN++ outperforms QTRAN for all the scenarios. Especially, QTRAN performs the worst among the baselines in five out of ten scenarios, i.e., MMM2, 3s5z, 3s5z (negative), 3s_vs_5z, 3s_vs_5z (negative). In contrast, QTRAN++ achieves the best performance for all ten scenarios.\n\n---\n\n**8. I was wondering why the authors did not compare with QPLEX. A detailed discussion of the differences between QPLEX and QTRAN++ can make the contribution clearer.**\n\nThank you for the suggestion. To address your concern, we additionally considered QPLEX (a concurrent submission at ICLR 2021, https://openreview.net/forum?id=Rcmk0xxIQV) as a baseline in Figure 3. One can observe how our QTRAN++ outperforms QPLEX for four scenarios, i.e., MMM2, 10m_vs_11m (negative), 5m_vs_6m (negative), 3z_vs_5z, and performs at least as good as QPLEX for the other six scenarios.\n\nTo further incorporate your suggestion, we discuss the algorithmic difference between QPLEX and QTRAN++. As you mentioned, QPLEX is similar to QTRAN++ since it removes the restriction on the true action-value estimator. However, they use different regularization for achieving this goal. To be specific, QTRAN++ imposes an additional loss function between the true and the transformed action-value estimators. In contrast, QPLEX introduces a structural constraint: the true action-value estimator is expressed as a summation of utility functions and a non-positive advantage estimator. A more detailed discussion is incorporated in Appendix B of our revised paper.\n\n---\n\n**9. It seems that the authors are using an older version of QMIX. In the latest version, QMIX can achieve a win rate of 80% on MMM2.**\n\nAfter careful inspection, we checked that our version of QMIX is identical to your suggested version. We hypothesize that the performance gap comes from different exploration hyperparameters and version of Starcraft II (performance of the algorithms may be sensitive to the version of Starcraft II as stated in [7]). To be specific, we use the same experimental setting across all algorithms for a fair comparison. Especially, we use the setting employed by the previous state-of-the-art work [4] to make the comparison more competitive. To incorporate your comment, we revised Appendix D of our paper by adding this discussion accordingly.\n\n---\n\n**References**  \n[1] Sunehag et al. Value-Decomposition Networks For Cooperative Multi-Agent Learning, AAMAS 2018  \n[2] Rashid et al. Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning, JMLR 2020  \n[3] Son et al. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning, ICML 2019  \n[4] Rashid et al. Weighted QMIX: Expanding Monotonic Value Function Factorisation, NeurIPS 2020  \n[5] Wang et al. Off-Policy Multi-Agent Decomposed Policy Gradients, preprint 2020  \n[6] Iqbal et al. AI-QMIX: Attention and Imagination for Dynamic Multi-Agent Reinforcement Learning, preprint 2020  \n[7] Python MARL framework (https://github.com/oxwhirl/pymarl)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KqX63asf529",
                "reply_to": "oJw_iowQub",
                "title": "Response to R2 (1/2)",
                "comment": "We express our deep appreciation for your time and insightful comments. We are grateful for all the positive comments: providing a practical improvement for the theoretically important algorithm (by you and R4), strong empirical performance (by R1, R3, and R4), novel and general ideas (by R1), and clear writing (by R1 and R3). In the revised manuscript, we have substantially updated or newly added (Section 2, Section 3, Figure 3, Figure 4, Appendix B, D, E, F) according to the initial reviews and colored them red. In the following, we address your comments one by one.\n\n---\n\n**1. The true joint action-value estimator is not an adorable choice in multi-agent problems.**\n\nWe agree that using the true joint action-value estimator is not adorable for some multi-agent frameworks, e.g., VDN [1] or QMIX [2], since the agent-wise policies cannot be extracted from the estimator. However, as we explained in Section 3, our QTRAN++ resolves this issue by extracting the agent-wise policies from the transformed action-value estimator: a projection of the true joint action-value estimator into a space of decentralizable functions. Note that QTRAN [3] and Weighted QMIX [4] also used the same approach to use the true joint action-value estimator. In our revised paper, we further clarified this point at the beginning of Section 3.\n\n---\n\n**2. It is difficult to tell the contribution of the monotonic part of the true joint action-value estimator. It has been shown that monotonic functions cannot represent some Q-values.**\n\nAs we explained in Section 3, the performance of QTRAN++ depends on the quality of the transformed action-value estimator for approximating the true action-value estimator. The monotonic part helps improve such quality since it implicitly biases the true action-value estimator towards being learned easily by a monotonic function, i.e., the transformed action-value estimator. In our revised paper, we further clarified this point in Section 3.2.\n\nFurthermore, although we agree on the limited representative power of the monotonic part in our true action-value estimator, our true action-value estimator does not suffer from such a limitation since it additionally has a non-monotonic network as a component.\n\n---\n\n**3. In two out of three scenarios in the ablation studies, FC-QTRAN++ is very similar to QTRAN++.**\n\nWe do believe that QTRAN++ demonstrates a concrete improvement over FC-QTRAN++ in our ablation studies; it outperforms FC-QTRAN++ significantly for the last of three scenarios, i.e., 3s_vs_5z. To further alleviate your concern, we refer to our revised paper where we increased the number of scenarios from three to ten. In Figure 4, one can observe a more concrete improvement: QTRAN++ outperforms FC-QTRAN++ for three scenarios, i.e., 10m_vs_11m, MMM2 (negative), 3s_vs_5z, and performs at least as good as FC-QTRAN++ for other seven scenarios.\n\n---\n\n**4. When training $i$-th head of the transformed action-value estimator, is the utility function of $j$-th agent updated?**\n\nYes, we update the utility function of $j$-th agent when training $i$-th head of the transformed action-value estimator. In our revised paper, we further clarify this point in Appendix D.\n\n---\n\n**5. Based on the results from DOP [5] and REFIL [6], I think the multi-head structure may not improve the performance.**\n\nAs explained in Section 3.2, our multi-head structure improves the performance of our QTRAN++ by alleviating the issues arising from partial observability. To be specific, it regularizes the agents to rely less on the underlying state information that is not observable during the execution of QTRAN++. While prior works such as DOP and REFIL use multi-head structures similar to ours, their motivations are quite different. DOP aims to reduce the variance in training agent-wise actors using a multi-head critic. REFIL uses a multi-head structure for generalizing the current value factorization to a dynamic number of agents. Hence, their results are inconclusive for assessing the performance improvements from our multi-head structure.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JTlz-Dee61",
                "reply_to": "l1gcpZMfCwK",
                "title": "Response to R3 (2/2)",
                "comment": "**4. I'm not sure whether the ablation studies are thorough enough.**\n\nTo alleviate your concerns, we extended our ablation studies from 3 maps to 10 maps. We report the experimental results in Figure 4 of the revised paper. In the figure, one can observe how each component of QTRAN++ provides a solid improvement. Especially, removing any component of QTRAN++ leads to significantly worse performance for at least one of the scenarios. Especially, as mentioned in our previous response, Fix-QTRAN++ underperforms significantly compared to QTRAN++ for all of the considered scenarios.\n\n---\n\n**5. The authors often make claims about improved stability of the algorithm. It would be good to soften the statements in the paper to make statements in the form of \"we believe that the algorithm has improved stability\".**\n\nThank you for pointing this out. We originally described QTRAN++ to improve the \u201cstability\u201d of QTRAN to reflect how our objective provides a denser training signal compared to QTRAN. Nevertheless, we deeply resonate with your concern and revised our paper to soften our claims on improving the stability of the QTRAN++. \n\n---\n\n**6. There are some cases where incorrect grammar or word choice made a sentence difficult to understand. For example, the choice of \"affluent\" to describe a class of estimators.**\n\nThank you for the helpful suggestion. To alleviate your concern, we carefully revised the paper to remove any incorrect grammar or word choice that makes a sentence difficult to understand. For example, we replaced the phrase \u201cmore affluent class of estimators\u201d with \u201ca larger class of estimators\u201d in our revised paper.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4P-0CrzELbf",
                "reply_to": "l1gcpZMfCwK",
                "title": "Response to R3 (1/2)",
                "comment": "We express our deep appreciation for your time and insightful comments. We are grateful for all the positive comments: providing a practical improvement for the theoretically important algorithm (by R2 and R4), strong empirical performance (by you, R1 and R4), novel and general ideas (by R1), and clear writing (by you and R1). In the revised manuscript, we have substantially updated or newly added (Section 2, Section 3, Figure 3, Figure 4, Appendix B, D, E, F) according to the initial reviews and colored them red. In the following, we address your comments one by one.\n\n---\n\n**1. Using a semi-monotonic mixing network for the true action-value estimator is quite arbitrary. Isn't the original idea behind QTRAN that the true action-value estimator would accurately track the true action-values?**\n\nAs we explained in Section 3, the performance of QTRAN++ (and QTRAN) is governed by two factors: (a) quality of true action-value estimator tracking the true action-values and (b) quality of transformed action-value estimator tracking the true action-value estimator. While the original QTRAN focused on designing the true action-value estimator to improve (a), we additionally use the semi-monotonic network to improve (b). To be specific, the semi-monotonic networks improve (b) by implicitly biasing the true action-value estimator to learn a monotonic function that can be easily approximated by the monotonic transformed action-value estimator. In our revised paper, we have further clarified this point in Section 3.2.\n\nWe also point out how the semi-monotonic network is necessary for achieving the best performance in our experiments. Indeed, in Figure 4 of our revised paper, one can observe a solid gap between QTRAN++ and FC-QTRAN++ (QTRAN++ without the semi-monotonic network) for three scenarios, i.e., 10m_vs_11m, MMM2 (negative), 3s_vs_5z, and performs at least as good as FC-QTRAN++ for other seven scenarios.\n\n---\n\n**2. Gradients are now also backpropagated from the \"tracking loss\" into the \"true\" action-value estimator, which makes it somewhat unclear what it is actually representing.**\n\nAs explained in Section 3, the performance of QTRAN++ depends on the quality of the transformed action-value estimator for tracking the true action-value estimator, i.e., the \u201ctracking loss.\u201d Through the backpropagation, our true action-value estimator can help in minimizing the tracking loss in addition to its original role, i.e., estimating the true action-value.\n\nFurthermore, the effectiveness of the backpropagation is supported by our empirical observation. Indeed, in Figure 4 of our revised paper, one can observe a significant gap between QTRAN++ and QTRAN++ without the backpropagation, i.e., Fix-QTRAN++ for all of the ten scenarios.\n\n---\n\n**3. I think it would be good to try to move as much as possible of Section B into the main text, leaving out prior work that is not sufficiently related. It would be good to point out which parts of the architecture in QTRAN++ come from QMIX.**\n\nThank you for the suggestion. To address your concern, we revised the paper accordingly as follows:\n- We moved relevant parts of Section B to Section 2.\n- We left out prior works that are not sufficiently related in Section B.\n- We explicitly stated which parts of our architecture come from QMIX in Section 3.2. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "VI03vpa08L",
                "reply_to": "AI_A8kgknFy",
                "title": "Response to R4",
                "comment": "We express our deep appreciation for your time and insightful comments. We are grateful for all the positive comments: providing a practical improvement for the theoretically important algorithm (by you and R2), strong empirical performance (by you, R1 and R3), novel and general ideas (by R1), and clear writing (by R1 and R3). In the revised manuscript, we have substantially updated or newly added (Section 2, Section 3, Figure 3, Figure 4, Appendix B, D, E, F) according to the initial reviews and colored them red. In the following, we address your comments one by one.\n\n---\n\n**1. The algorithmic contribution of the paper is relatively minor since it provides fairly simple modifications to an existing algorithm.**\n\nDespite being simple, we believe our QTRAN++ to deliver a significant contribution by closing the gap between theory and practice, i.e., QTRAN++ improves the empirical performance of a theoretically interesting algorithm. Furthermore, the algorithmic contribution of QTRAN++ stems not only from proposing the simple modifications but also from effectively combining the modifications to yield a large overall gain in performance. Indeed, as shown in Figure 4, the modifications are complementary, and omitting just one of our modifications results in a degradation of performance for at least one of the considered scenarios. We added the respective discussion in Section 1 of our revised paper.\n\n---\n\n**2. It would be good to see experiments on similar domains to those addressed in the original QTRAN paper.**\n\nThank you for the suggestion. To incorporate your comments, we evaluated our QTRAN++ using the multi-domain Gaussian squeeze environment from the original QTRAN paper [1]. The corresponding results are reported in Appendix F of our revised paper. In the experiments, one can observe how QTRAN++ consistently achieves the best result compared to the baselines including QTRAN. Hence, one may conclude that QTRAN++ achieves state-of-the-art performance across different environments and still retains the main strengths of QTRAN.\n\n---\n\n**References**\n\n[1] Son et al. QTRAN: Learning to factorize with transformation for cooperative multi-agent reinforcement learning, ICML 2019  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rtOdWlSSmT",
                "reply_to": "N3N1ASRnPr6",
                "title": "Response to R1",
                "comment": "We express our deep appreciation for your time and insightful comments. We are grateful for all the positive comments: providing a practical improvement for the theoretically important algorithm (by R2 and R4), strong empirical performance (by you, R3 and R4), novel and general ideas (by you), and clear writing (by you and R3). In the revised manuscript, we have substantially updated or newly added (Section 2, Section 3, Figure 3, Figure 4, Appendix B, D, E, F) according to the initial reviews and colored them red. In the following, we address your comments one by one.\n\n---\n\n**1. It would be good to see more results on different benchmarks.** \n\nThank you for the suggestion. We consider the SMAC benchmark to be sufficient in our experiments since it allows evaluating agents under diverse scenarios. Indeed, the scenarios have varying numbers and types of agents, and require the agents to learn diverse skills such as \u201ckiting\u201d and \u201cfocus fire.\u201d For this reason, most prior works such as QMIX [1], and MAVEN [2] only considered the SMAC environment for evaluation.\n\nNevertheless, to incorporate your comments, we additionally evaluated our QTRAN++ using the multi-domain Gaussian squeeze benchmark [3]. The corresponding results are reported in Appendix F of our revised paper. In the experiments, one can observe how QTRAN++ consistently achieves the best result compared to the baselines. This result is especially significant since the benchmark was designed specifically for demonstrating the strength of the original QTRAN algorithm [3]. This demonstrates that QTRAN++ achieves state-of-the-art performance across different benchmarks and still retains the main strengths of QTRAN.\n\n---\n\n**2. In Figure 3, some results seem to be not converged yet. It would be interesting to see experiments using enough training steps.** \n\nThank you for the suggestion. We follow the same number of training steps for the SMAC environment as the prior works, e.g., QMIX [1] and Weighted QMIX [4]. This can avoid a potential reproducibility issue when comparing with the prior works.\n\nNevertheless, to incorporate your comments, we evaluated our QTRAN++ by increasing the number of training steps twice (two million to four million steps) for the scenarios where algorithms have not converged in Figure 3. The corresponding results are reported in Appendix E of our revised paper. In the experiments, one can observe how our QTRAN++ maintains state-of-the-art performance even after the considered algorithms converge. \n\n---\n\n**3. Typo: Page 6, \"... for being \u201cselfish.\u201d This ...\" -> \"... for being \u201cselfish\u201d. This ...\"**\n\nThank you for pointing this out. We rephrased the corresponding sentence to alleviate your concern.\n\n---\n\n**References**\n\n[1] Rashid et al. Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning, JMLR 2020  \n[2] Mahajan et al., MAVEN: Multi-Agent Variational Exploration, NeurIPS 2019  \n[3] Son et al. QTRAN: Learning to factorize with transformation for cooperative multi-agent reinforcement learning, ICML 2019  \n[4] Rashid et al. Weighted QMIX: Expanding Monotonic Value Function Factorisation, NeurIPS 2020",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "AI_A8kgknFy",
                "reply_to": "iclr_2021_TlS3LBoDj3Z",
                "title": "Good paper - simple fixes to an important algorithm, yielding SOTA performance.",
                "comment": "## Summary\n\nThis paper addresses the domain of cooperative multiagent learning with centralised learning and decentralised execution. Specifically, it improves on the QTRAN algorithm, a theoretically justified algorithm which previously had not produced strong learning performance. With these improvements, QTRAN++ outperforms baselines on the SMAC environments.\n\nI recommend accepting this paper. It delivers strong performance on a popular benchmark for complex, cooperative multiagent learning (SMAC). While the algorithmic contribution is incremental, it still delivers insight into how to improve the empirical performance of a theoretically interesting and well-justified algorithm.\n\n ## Positives\n\nThe problem addressed - cooperative multiagent environments with CTDE - is a widely studied and important one. It is well set up in the paper, including discussion of related algorithms.\n\nThe base algorithm - QTRAN - should theoretically perform well in a wider variety of environments than other algorithms for these problems, so improving its performance is particularly valuable. The improvements made to the algorithm are clear and well motivated; section 3.1 in particular explains clearly the difference the modified loss is intended to make.\n\nThe empirical studies in the paper are strong. They show that QTRAN++ outperforms several baselines in data efficiency and final performance across a variety of domains. Further, a comprehensive ablation study shows that each of the improvements made to QTRAN is independently important (in at least some domains).\n\n## Negatives\n\nThe algorithmic contribution of the paper is relatively minor, since it provides fairly simple modifications to an existing algorithm.\n\nExperimentally, it would be good to see experiments on similar domains to those addressed in the original QTRAN paper, which are designed to probe the advantages QTRAN has over related algorithms. This would demonstrate that QTRAN++ retains the benefits of QTRAN in non-monotonic factorisable environments.\n\n## Sources of reviewer uncertainty\n\nI am not knowledgeable enough in this domain to be certain of the coverage of the baselines and domains in the paper. Since the empirical performance of the algorithm is central to the paper, this is important.",
                "rating": 7,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the proposed improvements",
                "Sentiment Expression": "it is not obvious if ... actually improves",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the experimental results",
                "Sentiment Expression": "there remain some major concerns",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the performance of baselines",
                "Sentiment Expression": "is not consistent with those reported in the prior work",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "SJw03ceRW": {
        "paper_id": "iclr_2018_SJw03ceRW",
        "paper_title": "GENERATIVE LOW-SHOT NETWORK EXPANSION",
        "paper_abstract": "Conventional deep learning classifiers are static in the sense that they are trained on\n      a predefined set of classes and learning to classify a novel class typically requires\n      re-training. In this work, we address the problem of Low-shot network-expansion\n      learning. We introduce a learning framework which enables expanding a pre-trained\n      (base) deep network to classify novel classes when the number of examples for the\n      novel classes is particularly small. We present a simple yet powerful distillation\n      method where the base network is augmented with additional weights to classify\n      the novel classes, while keeping the weights of the base network unchanged. We\n      term this learning hard distillation, since we preserve the response of the network\n      on the old classes to be equal in both the base and the expanded network. We\n      show that since only a small number of weights needs to be trained, the hard\n      distillation excels for low-shot training scenarios. Furthermore, hard distillation\n      avoids detriment to classification performance on the base classes. Finally, we\n      show that low-shot network expansion can be done with a very small memory\n      footprint by using a compact generative model of the base classes training data\n      with only a negligible degradation relative to learning with the full training set.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "Two reviewers recommended rejection, and one is slightly more positive. The main concern is that the experiments are not convincing (ie, the number of base and added classes is very small). Furthermore, while the paper introduces several interesting ideas, the AC agrees with the second reviewer that each of these could be explored in more detail. This work seems preliminary. The authors are encouraged to resubmit to a future conference.",
        "meta_review_title": "ICLR 2018 Conference Acceptance Decision",
        "reviews": [
            {
                "review_id": "r1R0L9Def",
                "reply_to": "iclr_2018_SJw03ceRW",
                "title": "a paper proposing hard-distillation for few-shot learning ",
                "comment": "On few-shot learning problem, this paper presents a simple yet powerful distillation method where the base network is augmented with additional weights to classify the novel classes, while keeping the weights of the base network unchanged. Thus the so-called hard distillation is proposed. This paper is well-written and well organized. The good points are as follows,\n\n1. The paper proposes a well-performance method for the important low-shot learning problem based on the transform learning.\n2. The Gen-LSNE maintains a small memory footprint using a generative model for base examples and requires a few more parameters to avoid overfitting and take less time to train.\n3. This paper builds up a benchmark for low-shot network expansion.\n\nThere are some problems,\n1. There still is drop in accuracy on the base classes after adding new classes, and the accuracy may still drop as adding more classes due to the fixed parameters corresponding to the base classes. This is slightly undesired.\n2. Grammatical mistake: page 3, line 5(\u201ca additional layers\u201d)\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1W-h8Flz",
                "reply_to": "iclr_2018_SJw03ceRW",
                "title": "Interesting, yet (by far) not sufficiently explored",
                "comment": "The goal of this paper is to study generalisation to novel classes. This paper stipulates some interesting ideas, using an idea of expansion layers (using a form of hard distillation, where the weights of known classes are fixed), a GMM to model the already learned classes (to reduce storage), and a form of gradient dropout (updating just a subset of the weights using a dropout mask). All of these assume a fixed representation, trained on the base classifier, then only the final classification layer is adjusted for the novel examples. \n\nThe major drawback is that none of these ideas are fully explored. Given fixed representation, for example the influence of forgetting on base classes, the number of components used in the GMM, the influence of the low-shot, the dropout rate, etc etc.  The second major drawback is that the experimental setting seems very unrealistic: 5 base classes and 2 novel classes. \n\nTo conclude: the ideas in this paper are very interesting, but difficult to gather insights given the focus of the experiments.\n\nMinor remarks\n- Sect 4.1 \"The randomly ... 5 novel classes\" is not a correct sentence.\n- The extended version of NCM (4.2.1), here uses as prototype-kNN (Hastie 2001) has also been explored in the paper of NCM, using k-means per class to extract prototypes.\n- Given fixed representations, plenty of work has focused on few-shot (linear) learning, this work should be compared to these. ",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BJK7re9ez",
                "reply_to": "iclr_2018_SJw03ceRW",
                "title": "The paper proposes a network/classifier expansion method to learn to classify with additional novel  classes in the future, without re-training with all the original data. It fine tunes the new parameters added with the new data (from novel classes), and with sampled examples from  simple generative models of the old classes. Overall the paper has a simple idea which is validated on limited settings (~10 classes only). ",
                "comment": "The paper proposes a method for adapting a pre-trained network, trained on a fixed number of\nclasses, to incorporate novel classes for doing classification, especially when the novel classes\nonly have a few training examples available. They propose to do a `hard' distillation, i.e. they\nintroduce new nodes and parameters to the network to add the new classes, but only fine-tune the new\nnetworks without modifying the original parameters. This ensures that, in the new expanded and\nfine-tuned network, the class confusions will only be between the old and new classes and not\nbetween the old classes, thus avoiding catastrophic forgetting. In addition they use GMMs trained on\nthe old classes during the fine-tuning process, thus avoiding saving all the original training data.\nThey show experiments on public benchmarks with three different scenarios, i.e.  base and novel\nclasses from different domains, base and novel classes from the same domain and novel classes have\nsimilarities among themselves, and base and novel classes from the same domain and each novel class\nhas similarities with at least one of the base class.                        \n                                                                             \n- The paper is generally well written and it is clear what is being done     \n- The idea is simple and novel; to the best of my knowledge it has not been tested before\n- The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation\n  (iCARL; where all weights are fine-tuned). The proposed method performs better in low-shot\n  settings and comparably when large number of training examples of the novel classes are available\n- My main criticism will be the limited dataset size on which the method is validated. The ILSVRC12\n  subset contains 5 base and 5 novel classes and the UT-Zappos50K subset also has 10 classes. The\n  idea is simple and novel, which is good, but the validation is limited and far from any realistic\n  use. Having only O(10) classes is not convincing, especially when the datasets used do have large\n  number of classes. I agree that this will not allow or will takes some involved manual effort to\n  curate subsets for the settings proposed, but it is necessary for being convincing.",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "B1Fj2sjmf",
                "reply_to": "r1R0L9Def",
                "title": "Regarding the Drop in accuracy on base classes",
                "comment": "Thank you for emphasizing the topic of performance drop when adding new classes.\nThe drop in the base classes is common in all of the competing methods, e.g. the  Soft-Dis/ICarl Adaptation where the base classes weights are adapted.\nIt is expected that as the number of categories increase, and classes that are similar to one another are introduced, there will be a drop in classification accuracy, even for a fully trained network. Please notice that the proposed method outperforms the competing methods in most cases.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJWd2ismz",
                "reply_to": "r1W-h8Flz",
                "title": "Experiments focus",
                "comment": "We thank the reviewer. We would like to draw the reviewer\u2019s attention to the following points:\n\n\u201cInfluence of forgetting on base classes:\u201c\nThe average accuracy of the base classes is presented in APPENDIX B BASE & NOVEL CLASSES TEST ERROR and in Figure 6,7,8,  In which we demonstrate the accuracy of the base classes as a function of the number of novel samples. We present the base class and novel class accuracy in each of the designed scenarios: generic classes from imagenet, Domain specific with similar novel classes,  Domain specific with similar class in base.\n\n\u201cInfluence of the number of components used in the GMM:\u201d\nIn section 4.5 RESULTS: DEEP-FEATURES GMM EVALUATION \nwe\u2019ve explored the effect of different number of GMM components on the ability to restore and re-create the accuracy on the base classes. We\u2019ve explored the effect of GMM components on 5 imagenet based dataset and 3 based UT-Zappos50K dataset.\n\n\u201cThe influence of the low-shot:\u201d\nIn Sections 4.3 and 4.4 the effect of the number of novel samples on the performance is presented. We\u2019ve experimented with a range of number of novel samples.\n\n\u201cThe dropout rate:\u201d\nIn Section 4.2.3 GRADIENT DROPOUT, we describe the experiment to evaluate the effect of Gradient Dropout in Low-Shot Expansion. We compare the results obtained with and without the use of gradient dropout, those are presented in Figures 2,3,4. Due to the already dense experimental section we did not add experiments done on various dropout ratios, though we agree that this is an interesting question.\n \n\n\u201cThe second major drawback is that the experimental setting seems very unrealistic: 5 base classes and 2 novel classes:\u201d \nPlease see answer above (to first reviewer) regarding the design of benchmark.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyTJhosXM",
                "reply_to": "BJK7re9ez",
                "title": "Design considerations of the proposed benchmark",
                "comment": "We would like to thank the reviewers for their commentary. We ask to draw the reviewer's attention to the following:\n\nIn this work, we focus on a robotic unit in real-life scenarios. It is often desired to be able to adapt to a single or two new classes available with only very few samples. We choose to establish the proposed benchmark for the task of Low-Shot Network expansion in a manner that will reflect this task. That is, we wanted the number of novel classes to be relatively small so the effect of Network adaptation to small quantities of new data can be studied. \n\nWe defined the class (base + novel) average accuracy to be our performance metric.  \nWe did not want the base classes accuracy to have overwhelmly more weight than the novel classes in the average accuracy metric, hence we defined the number of base classes to be in a range similar to the number of novel classes. We\u2019ve composed a dataset with a common cardinality (similar to CIFAR10 is O(10), MNIST is O(10) and SVHN is O(10)). While O(10) is considerably smaller than imagenet 1000 classes classification task. It is still a common classification task, which is also common in robotic unit applications. \n\nIn order to avoid bias in the constructed dataset, we\u2019ve performed numerous random experiment, and reported their average result: \n \nThe test on imagenet partitions was done by randomly selecting 50 classes, and then randomly partitioning to 5 groups of 10 , which are then further randomly partitioned to 5 base and 5 novel classes. The result of every experiment done with a given number of novel samples is averaged on 25 = 5X5 trails. That is Figure 2 is the result of 125 tests = 25 averaging X 5 #novel samples.\n\nFigure 5a Is the result of 25 (base,novel group avg) X 8 #novel samples = 200 trails.\n\nIn this unbiased test case, we addressed 3 typical scenarios: generic classes from imagenet, Domain specific with similar novel classes,  Domain specific with similar class in base. We\u2019ve further explored the effect and gain of Gen-LSNE compared to other methods in each of these scenarios. We concentrated our effort to analyze and explore the qualities of the proposed method in scenarios that are common in robotic unit real-life scenarios, and to the best of our knowledge the designed benchmark realistically reflect those.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the experiments",
                "Sentiment Expression": "not convincing",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "paper introduces several interesting ideas",
                "Sentiment Expression": "could be explored in more detail",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "This work",
                "Sentiment Expression": "seems preliminary",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "r111KtCp-": {
        "paper_id": "iclr_2018_r111KtCp-",
        "paper_title": "Taking Apart Autoencoders: How do They Encode Geometric Shapes ?",
        "paper_abstract": "We study the precise mechanisms which allow autoencoders to encode and decode a simple geometric shape, the disk. In this carefully controlled setting, we are able to describe the specific form of the optimal solution to the minimisation problem of the training step. We show that the autoencoder indeed approximates this solution during training. Secondly, we identify a clear failure in the generalisation capacity of the autoencoder, namely its inability to interpolate data. Finally, we explore several regularisation schemes to resolve the generalisation problem. Given the great attention that has been recently given to the generative capacity of neural networks, we believe that studying in depth simple geometric cases sheds some light on the generation process and can provide a minimal requirement experimental setup for more complex architectures. \n      ",
        "paper_acceptance": "rejected-papers",
        "meta_review": " + interesting approach for a detailed analysis of the limitations of autoencoders in solving a simple toy problem\n - resulting insights somewhat trivial, not really novel, nor practically useful => lacks demonstration of a gain on non-toy task\n - regularization study too limited in scope: lacking theoretical grounding, and more exhaustive comparison of regularization schemes.",
        "meta_review_title": "ICLR 2018 Conference Acceptance Decision",
        "reviews": [
            {
                "review_id": "H1s3VoOlf",
                "reply_to": "iclr_2018_r111KtCp-",
                "title": "This is an interesting study to understand the innerworkings of an autoencoder, however, the study is not quite convncing, yet.",
                "comment": "1. The idea is interesting, but the study is not comprehensive yet\n2. need to visualize the input data space, with the training data, test data, the 'gaps' in training data [see a recent related paper - Stoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017).]. \n3. What's the effect of training data size? \n4. How do the intermediate feature maps look like? \n5. Is there an effect of number of layers? Maybe the network architecture is too deep for the simple data characteristics and size of training set. \n6. Other shapes are said to be part of future work, but I am not convinced that serious conclusions can be drawn from this study only? \n7. What about the possible effects of Batch normalization and dropout?  \n8. size of 'd' is critical for autoencoders, only one example in appendix does not do justice, also it seems other color channels show up in the results (fig 10), wasn't it binary input?",
                "rating": 4,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SJQ2Xg2eM",
                "reply_to": "iclr_2018_r111KtCp-",
                "title": "a bit trivial and lacking in justification/insight as to the regularisation method",
                "comment": "The paper considers a toy problem: the space of images of discs of variable radius - a one dimensional manifold.\n\nAn autoencoder based on convolutional layers with ReLU is experimented with, with a 1D embedding.\n\nIt is shown that \n1) if the bias is not included, the resulting function is homogeneous (meaning f(ax)=af(x)), and so it fails because the 1D representation should be the radius, and the relationship from radius to image is more complex than a homogeneous function.\n- if we include the bias and L2 regularise only the encoder weights, it works better in terms of interpolation for a limited data sample.\n\nThe thing is that 1) is trivial (the composition of homogeneous functions is homogeneous... so their proof is overly messy btw). Then, they continue by further analysing (see proposition 2) the solution for this case. Such analysis does not seem to shed much light on anything relevant, given that we know the autoencoder fails in this case due to the trivial proposition 1.\n\nAnother point: since the homogeneous function problem will not arise for other non-linearities (such as the sigmoid), the focus on the bias as the culprit seems arbitrary.\n\nThen, the story about interpolation and regularisation is kind of orthogonal, and then is solved by an arbitrary regularisation scheme. The lesson learned from this case is basically the second last paragraph of section 3.2. In other words, it just works.\n\nSince it's a toy problem anyway, the insights seem somewhat trivial.\n\nOn the plus side, such a toy problem seems like it might lead somewhere interesting. I'd like to see a similar setup but with a suite of toy problems. e.g. vary the aspect ratio of an oval (rather than a disc), vary the position, intensity, etc etc.",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SJOrnJf-M",
                "reply_to": "iclr_2018_r111KtCp-",
                "title": "Interesting toy task but without real new insights or contribution to Autoencoders",
                "comment": "This paper proposes a simple task (learning the manifold of all the images of disks) to study some properties of Autoencoders. They show that Autoencoders don't generalize to disks of radius not in the training set and propose several regularization to improve generalisation.\n\nThe task proposed in the paper is interesting but the study made is somewhat limited:\n\n- They only studied one choice of Autoencoder architecture, and the results shown depends heavily on the choice of the activation, in particular sigmoid should not suffer from the same problem. \n\n- It would be interesting to study the generalization in terms of the size of the gap.\n\n- The regularization proposed is quite simple and already known, and other regularization have been proposed (e.g. dropout, ...). A more detailed comparison with all previous regularization scheme would be much needed. \n\n- The choice of regularization at the end seems quite arbitrary, it works better on this example but it's not clear at all why, and if this choice would work for other tasks.\n\nAlso Denoising Autoencoders (Pascal et al.) should probably be mentioned in the previous work section, as they propose a solution to the regularization of Autoencoder.\n\nOverall nothing really new was discovered or proposed, the lack of generalization of those kind of architecture is a well known problem and the regularization proposed was already known. ",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Hk4SO_-zG",
                "reply_to": "H1s3VoOlf",
                "title": "Reply to reviewer 3",
                "comment": "3/ What's the effect of training data size? \n\nIn this situation, the training data size is imposed; it is the total possible number of centred disks in a 64 x 64 image.\n\n4/ How do the intermediate feature maps look like?\n\nIn our case, the feature maps resemble disks in the encoding and decoding, which is natural. However we were not able to interpret them in a meaningful way. Thus, we turned to a different approach, the ablation study.\n\n5/ Is there an effect of number of layers? Maybe the network architecture is too deep for the simple data characteristics and size of training set.\n\nIn our case, the number of layers is imposed by the problem and the subsampling coefficient (1/2). We experimented with other, more drastic subsampling, with less success; we chose the minimal architecure which worked correctly.\n\n6/ Other shapes are said to be part of future work, but I am not convinced that serious conclusions can be drawn from this study only?\n\nWe have added some further experiments on ellipses in the supplementary material \nhttps://www.dropbox.com/s/hn2akqgqh9m0qxg/autoencoders_sup_mat.pdf?dl=0\n\n7/ What about the possible effects of Batch normalization and dropout?\n\nBatch normalisation is not explored in our case, it is true . In the first part of the paper, we are concerned with an optimal theoretical decoding solution, and we show that the network without biases is able to find this solution, so batch normalisation would not help here. In the second part, we are concerned with improving robustness to missing data. This can only be remedied by regularisation; batch normalisation may speed up convergence, but if it is to an incorrect solution, it is not that useful. Dropout can improve the generalisation capacities of the network, this is true, but only when the network has an excessive amount of neurons. In this paper, we restricted the architecture to be as minimal as possible. As an illustration, if we happened to dropout at the latent layer, we may disconnect the network during the current gradient step ! Thus, dropout did not seem a good idea to us.\n\n8/ size of 'd' is critical for autoencoders, only one example in appendix does not do justice, also it seems other color channels show up in the results (fig 10), wasn't it binary input?\n\nOne of the main goals of this paper is to put ourselves in a situation where we know the optimal value of d (in this case d=1). The results referred to in the Appendix are from another work by Zhu et al. whose network attempts to learn the image manifold. The colour channels come from the fact that we used their code which is designed for colour images. We showed their results for d=1 and d=100 so that we could not be criticised for giving their algorithm too much or too little freedom in terms of dimensionality.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1edwu-zf",
                "reply_to": "SJOrnJf-M",
                "title": "Reply to reviewer 2",
                "comment": "1/ They only studied one choice of Autoencoder architecture, and the results shown depends heavily on the choice of the activation, in particular sigmoid should not suffer from the same problem.\n\nThe main idea of our paper is to study in detail the minimal generalisable architecture needed for encoding and decoding a disk, the rationale being that if the autoencoder does not work in that situation, then there is a serious problem. If it does work (which is indeed the case), then how does it work ? There is no reason that the problems discussed in our paper, such as generalisation, would be resolved using a sigmoid. We believe that this may be a misunderstanding due to the fact that we are learning with binary images. In such a case, it may indeed make sense to use a sigmoid, however this is not connected to the ablation study or the generalisation problem. We have carried out the same ablation study using sigmoids instead of leaky ReLUs and this did not resolve the observed behaviour.\n\n2/ It would be interesting to study the generalization in terms of the size of the gap.\n\nThis is indeed an interesting point, as clearly there should be some limit point at which the autonencoder cannot generalise. However, this would significantly increase the scope of the paper, making it too long for publication in the ICLR format.\n\n3/ - The regularization proposed is quite simple and already known, and other regularization have been proposed (e.g. dropout, ...). A more detailed comparison with all previous regularization scheme would be much needed.\n\nWe agree that regularising filter weights is widely known and used. However, we have not observed in the literature the asymmetric approach which consists in regularising only the encoder; if there are any such references, we would be happy if the reviewers could indicate them. In Section 3.2.4, we point out that regularising both the encoder and the decoder leads to a less stable autoencoder. We have carried out similar experiments on a 2D latent space (with ellipses), and found that this marked behaviour is observed again. We propose to add these experiments in our supplementary material, see\nhttps://www.dropbox.com/s/hn2akqgqh9m0qxg/autoencoders_sup_mat.pdf?dl=0\nConcerning comparison with other regularisation schemes, we cannot compare with sparse autoencoders, since these try to encourage sparsity in the latent space and in our case the latent space cannot be any more sparse.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryPkDObfM",
                "reply_to": "SJQ2Xg2eM",
                "title": "Reply to reviewer 1",
                "comment": "1/ It is shown that 1) if the bias is not included, the resulting function is homogeneous (meaning f(ax)=af(x)), and so it fails because the 1D representation should be the radius, and the relationship from radius to image is more complex than a homogeneous function.\n2/ The thing is that 1) is trivial (the composition of homogeneous functions is homogeneous... so their proof is overly messy btw). Then, they continue by further analysing (see proposition 2) the solution for this case.\n3/ Another point: since the homogeneous function problem will not arise for other non-linearities (such as the sigmoid), the focus on the bias as the culprit seems arbitrary.\n\nYes, the fact that without biases the output of the AE is of the form alpha(r)F (where r summarises the input disk and F is a fixed image) is trivial, however it was necessary to state it. But we showed that in this case the AE actually finds the optimal possible F and function alpha(r), which is proportional to the dot product between the input disk and the function F, and not simply the area of the disk (see figures 7 and 8). The important fact here is that the ablated AE succeeds perfectly, in that it finds the best provable solution inside the range of its capacity. In deep learning it is rare to be able to show that the network is correctly approximating the best possible solution, therefore we found this point to be noteworthy. We address the question of using sigmoids in the reply to question 1/ of ``AnonReviewer2''.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJcjHdWzG",
                "reply_to": "iclr_2018_r111KtCp-",
                "title": "General reply to reviewers' comments",
                "comment": "We wish to thank the reviewers for their comments and criticisms, which we found to be useful and constructive. Before replying to the specific comments of the reviewers, we would first like to make some general points concerning the goal and scope of our work.\n\nFirstly, we would like to clarify that in Section 3.2.2. we study a situation where we can describe the optimal solution of the training problem analytically. The ablation study was carried out and analysed to show that the autoencoder finds the optimal solution in this case. To the best of our knowledge, few such optimality results exist in the deep learning literature. Nevertheless, we do recognize that the case is simple.\n\nSecondly, in Section 3.2.3. we investigate difficulties of the network to generalise. This is a very well-known problem concerning autoencoders (and GANs), which is often briefly discussed in the literature, but is rarely analysed in detail. In the case of complex images it is very difficult to decide whether a network is producing new examples or just copying examples from a database. We confirm the ubiquity of this problem by showing that it happens even in the case of the state-of-the-art work of Zhu et al. applied to disks.\n\nFinally, in Section 3.2.4, we identify a solution to this problem in the form of an assymmetric weight regularisation, the regularisation of the encoder weights. This greatly improves the autoencoder's generalisation capacity. This asymmetric regularisation has not been proposed in the literature, to the best of our knowledge (please correct us if we are wrong in this respect). It is possible that in the submitted version of the paper we did not highlight this enough, but we believe it to be significant.\n\n- New experiment : we have tested the asymmetric version of the regularization in a more complex case, with ellipses, and the improvement is even clearer than in the case of disks. We show these results in the following document\nhttps://www.dropbox.com/s/hn2akqgqh9m0qxg/autoencoders_sup_mat.pdf?dl=0 \nNumerically, this leads to an order of magnitude improvement in the l2 loss of the network on unobserved examples.\n\nBelow are our replies to the specific comments of the reviewers.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "approach for a detailed analysis of the limitations of autoencoders in solving a simple toy problem",
                "Sentiment Expression": "interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "resulting insights",
                "Sentiment Expression": "somewhat trivial, not really novel, nor practically useful",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "regularization study",
                "Sentiment Expression": "too limited in scope: lacking theoretical grounding, and more exhaustive comparison of regularization schemes.",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "uFk038O5wZ": {
        "paper_id": "iclr_2021_uFk038O5wZ",
        "paper_title": "Improving Abstractive Dialogue Summarization with Conversational Structure and Factual Knowledge",
        "paper_abstract": "Recently, people have been paying more attention to the abstractive dialogue summarization task. Compared with news text, the information flows of the dialogue exchange between at least two interlocutors, which leads to the necessity of capturing long-distance cross-sentence relations. In addition, the generated summaries commonly suffer from fake facts because the key elements of dialogues often scatter in multiple utterances. However, the existing sequence-to-sequence models are difficult to address these issues. Therefore, it is necessary for researchers to explore the implicit conversational structure to ensure the richness and faithfulness of generated contents. In this paper, we present a Knowledge Graph Enhanced Dual-Copy network (KGEDC), a novel framework for abstractive dialogue summarization with conversational structure and factual knowledge. We use a sequence encoder to draw local features and a graph encoder to integrate global features via the sparse relational graph self-attention network, complementing each other. Besides, a dual-copy mechanism is also designed in decoding process to force the generation conditioned on both the source text and extracted factual knowledge. The experimental results show that our method produces significantly higher ROUGE scores than most of the baselines on both SAMSum corpus and Automobile Master corpus. Human judges further evaluate that outputs of our model contain more richer and faithful information.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The authors address the important task of improving dialogue summarization using conversation structure and factual knowledge.\n\nPros:\n1) Clearly written and well motivated (as acknowledge by all reviewers)\n2) Technically sound (the proposed architecture is clearly in line with the problem that the authors are trying to solve)\n3) Significant upgrades to the paper after the reviewer comments (in particular the authors have added detailed ablation studies and results on non-dialogue datasets)\n\nCons:\n1) There is a significant difference between the results in the ablation studies in the original version and in the new version. Originally, the differences between KGEDCg and KGEDCg-GE and KGEDCg-FKG were very minor, but now the margins are as large as 7+ pts. I would request the authors to explain this in the final version\n\nThe reviewing team felt that while many Qs were sufficiently addressed by the authors, the large difference in the numbers reported for the ablation study in the initial and final version of the paper raises some new Qs which need to be addressed before the paper can be accepted. ",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "AT7jJ1BlQzN",
                "reply_to": "iclr_2021_uFk038O5wZ",
                "title": "Official blind review",
                "comment": "Summary: This paper proposes a knowledge graph enhanced network to improve abstractive dialog summarization with graphs constructed from the dialog structure and factual knowledge. The dialog graph is composed of utterances as nodes and 3 heuristic types of edges (such as utterances of the same speaker, adjacent utterances). The factual graph is constructed via openIE and dependency parsing, which the authors claim are complementary as the triplets (results of openIE) are not always available. \n\n---\nPros:\n+ The proposed method outperforms all the compared baselines on two dialog summarization datasets.\n+ Human evaluation shows that the proposed method leads to increased relevance and readability.\n\n---\nCons:\n- In the ablation study (Table 3), the performance of each variant is close to the full model, and removing either module still outperforms the compared baselines. Given such performance, I wonder if the difference between the ablated variants and the full model is statistically significant. Also, what is the performance of the proposed method without graph information? Is it effectively the same as a Pointer-Generator?\n- Details of human evaluation are lacking. Who are the annotators and how many annotators are there? What is the inter-annotator agreement?\n- The paper is poorly represented with unclear descriptions (not only typos/grammar but also definitions of various concepts), which makes it hard to follow. To name a few, in 3.2, the definition of edges gives one the impression that it is a fully connected graph. In 3.2.1, \u201ckeyword\u201d is defined after the description of the use of the keyword, and I can\u2019t really tell what is the \u201ckeyword neighborhood\u201d.  A general suggestion is to define them beforehand (when they first appear) instead of describing them later or in the footnote.\n\n---\nComments for rebuttal and revised paper\n\nThanks for providing a detailed response and an improved version of the paper. One thing that I am still concerned with is how come the updated ablation study is so different from the initial results. Originally, the differences between KGEDCg and KGEDCg-GE and KGEDCg-FKG were very minor (one of my questions above), but now the margins are as large as 7+ pts. Given such discrepancies without explanation, I'd hold my original evaluation.\n",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "qnWue5IpXqr",
                "reply_to": "iclr_2021_uFk038O5wZ",
                "title": "Improving Abstractive Dialogue Summarization",
                "comment": "This paper proposes a new neural pipeline for dialogue summarization that jointly includes word-by-word decoding, an utterance graph, and a factual knowledge graph. The proposed methods is assessed on the SamSUN and  Automobile Master dataset.\nThe method improves the top reported baselines by 0.5 to 2 points (RED) in both cases.\n\nThe paper contains a well-motivated introduction and perform a sound related work section (as far as my judge). They correctly detail their methods in a step-by-step pedagogical procedure. Overall the paper is pretty well-written. \n\nHowever, I have a few concerns about the experimental section. While this paper's main contribution is a (sensible) mixture of neural blocks, a single page of experiments to evaluate the method is not enough from my perspective. Besides, a 0.5-2 pts increase compared to a Pointer network may be a bit limited in light of the model complexity.\nHowever, my main concern is mostly the lack of analysis of the method. As there are many design choices, this requires multiple ablation studies to validate all of them. Yet, the authors only ablate the factual knowledge graph and graph encoder. Furthermore, the score differences are small, and a few run + std would have been useful to determine whether one change is significant.\nIdeas for other ablation studies could be:\n - the choice of edge\n - the attention block\n - removing both graphs\n - fitler-out some specific POS-tagging to see whether some keywords are more important.\nIt is also sometimes interesting to point out what part of the network may require additional capacity: should we make the bi-LSTM bigger first? The decoder? \n\nAnother interesting experiment would be to run the same architecture on a non-dialogue dataset (by naively setting some of the edges). Therefore, it could show whether the architecture can also perform well on non-dialogue long dependency.\nFinally, the authors (rightly) complain that there is a lack of abstract summarisation. However, it is still possible to take the Ubuntu dataset, run a baseline + KGDEC, and perform a human evaluation. \nI here list some potential experiments, and I am aware that it is unreasonable to perform them all. I mostly want to point out that there is a large spectrum of things that can be done to further demonstrate the validity of their model. \n\nOn a second note, some of the neural hyperparameters are missing, such as the convolution kernel/hidden_size to process the vertices. \n\nOverall, the paper has some merits. Easy to read, plenty of implementation details (although some parameters are missing). However, i cannot recommend paper acceptance without at least two or three additional experiments and potentially the error bar.\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "sv4mnGZKmAk",
                "reply_to": "2ay9m7PByns",
                "title": "Responses to AnonReviewer1",
                "comment": "Dear reviewer, we thank you for providing very valuable suggestions. We will explain your concerns and answer your questions point by point.\n\nQ1: It is not clear how the biLSTM encoding and the GE encoding are combined? Is it the same way as done with $h^S$ and $h^G$?\n\nA1: Maybe I didn't make it clear in the previous manuscript. The $h_n^S$ is the BiLSTM encoding and the $h^G$ is the graph encoding. We concatenate them to get the initial state of decoder $s_0=[h_n^S;h^G]$. Such information is in Sec 3.2.3.\n\nQ2: I am not satisfied with the ablation study. One important ablation to run would be an experiment without both GE and FKG. Without those two, the model simply becomes (seq2seq+attention). And from Table 1, we see that the R-L for that simple model is 28.16 and 29.37. It is hard for me to convince myself that adding either GE or FKG increases R-L by more than 12 points.\n\nA2: We further enrich the ablation studies. We do an ablation experiment on the Sequence Encoder (SE), Graph Encoder (GE), and Factual Knowledge Graph (FKG).  The model without GE and FKG is more similar to the Pointer Generator model instead of a Seq2Seq+Attention model and their performances are similar. The R-1, R-2, and R-L scores of the model without GE and FKG are 38.93, 14.16, and 34.88. The R-1, R-2, and R-L scores of the Pointer Generator are 38.55, 14.14, and 34.85. Compared to $\\rm KGEDC_g$,  the R-1, R-2, and R-L scores of the model without GE and FKG decreased by 4.94, 5.50, and 6.14 points.\n\n|SE|GE|FKG|R-1 ($\\sigma$)|R-2 ($\\sigma$)|R-L ($\\sigma$)|\n|----|----|-----|------|------|------|\n|\u2714|\u2714|\u2714|43.87(0.4)|19.66(0.5)|41.02(0.7)|\n|\u2714|\u2718|\u2714|39.62(0.6)|15.05(0.6)|35.87(0.5)|\n|\u2718|\u2714|\u2714|36.15(0.5)|12.38(0.8)|33.79(0.6)|\n|\u2718|\u2718|\u2714|19.04(1.0)|8.57(0.9)|17.82(1.2)|\n|\u2714|\u2714|\u2718|42.71(0.5)|19.15(0.4)|40.32(0.5)|\n|\u2714|\u2718|\u2718|38.93(0.7)|14.16(0.6)|34.88(0.6)|\n|\u2718|\u2714|\u2718|35.65(0.8)|11.49(0.7)|32.57(0.4)|\n|\u2718|\u2718|\u2718|17.52(1.2)|7.90(1.5)|16.26(1.0)|\n\nIt is worth noting that, for Seq2Seq+Attention model, the next predicted word entirely depends on $P_{vocab}$ which is a probability distribution over all words in the vocabulary, and $P(y_t)=P_{vocab}$. However, our model both copies words via pointing and generates words from a fixed vocabulary. $p_{gen}$ is used as a soft switch to choose between generating a word from the vocabulary by sampling from $P_{vocab}$, or copying a word from the inputs by sampling from the attention distributions $\\sum_{i:y_i=y_t} a_{i,t}^s+\\sum_{i:y_i=y_t} a_{i,t}^g$.\n\nQ3: It would also be interesting to know in what ratio $h_G$ and $h_S$ are combined in the case of gating fusion.\n\nA3: Considering there is no $h_G$ and $h_S$ in the paper, only $h^S$ and $h^G$ exist. Besides, we do not use a gate network to combine the $h^S$ and $h^G$. Therefore, we are not sure if you want to know what the ratio $c^s$ and $c^g$ are combined in the case of gating fusion. If we understand it correctly, the following statement will solve your question. We do a gate fusion analysis in Sec 5.3. We record the changes of the gate values $g_t$ for the development set during training in Figure 9 in A.6. For SAMSum dataset, at the beginning, the average gate value exceeds 0.5, which suggests generated contents are biased to choose source dialogues. As the training goes on, our model gradually realizes that fact descriptions are more reliable, which leads to a consecutive drop of the gate value. Finally, the average gate value is gradually stabilized to 0.422. Notably, the ratio of gate values is $(1-0.422)/0.422\\approx1.37$ and it is extremely close to the ratio of copying proportion $0.44/0.32\\approx1.38$ shown in Table 2 (0.32 and 0.44 mean the proportions of tokens and facts found in the summary), which seems that our model predicts the copy proportion relative accurately and normalizes it as the gate value. For the Automobile Master corpus, the experimental results are consistent with the data distribution.\n\nQ4: Since there is already sequential context dependency in GE, what additional advantage does the sequential encoder provide? Why not train only with the graph encoders?\n\nA4: In Graph Encoder (GE), the sequential context dependency captures the utterance-level contextual information and focuses on changes of speakers\u2019 views due to the continuous utterances. However, the sequence encoder processes the entire dialogue as a sequence and emphasizes the representation of each token in the token-level. The token and utterance granularities are cooperated to express conversational contents. Besides, token representations of the sequence encoder are also involved in the decoding process and the $h_i^s$ is used in the attention mechanism for sequence context vector $c_t^s$, which generates summaries by copying from the source tokens. Therefore, the sequence and graph encoder are both indispensable modules.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7g6fCoMZqdC",
                "reply_to": "n6KdF4NgFR_",
                "title": "Responses to AnonReviewer3",
                "comment": "Dear reviewer, we thank you for providing very valuable suggestions. We will explain your concerns and answer your questions point by point.\n\nQ1: Dialogue graph may not necessarily be sparse. In section 3.2.3, \u201cSparse Relational Graph Self-Attention Layer\u201d paragraph, the author writes \u201cHowever, our sparse self-attention operation ..., which reduces the quadratic computation to linear\u201d, but no proof is given, actually this seems not to be right. \n\nA1: We make a wrong expression on \"which reduces the quadratic computation to linear\". In fact, what we want to express is that our sparse self-attention operation reduces the computation amount to a certain extent. In our paper, the constructed dialogue graph is not a complete graph ($|E_d|<{|V_d|^2}$). Therefore, we regard the constructed dialogue graph as a sparse graph.\n\nQ2: The constructed factual knowledge graph seems to be sparse since the topics various in different dialogues. As shown in figure 2 (b), there are 7 different edge types in one dialogue session. How does the model deal with this issue? \n\nA2: We use the sparse relational graph self-attention and mask operation to deal with the factual knowledge graph, where if there is an edge between two nodes, the element of the mask matrix is set to 1, and if there is no edge between two nodes, the element of the mask matrix is set to 0. The edge labels are also encoded into a relational matrix $R$. The attention score is calculated as: $Attention(Q,K,V,R)=softmax(\\frac{Q\\times K}{\\sqrt d}+R)(V+R)$.\n\nQ3: Maybe the paper should provide more content on ablation and case study. For example, in equation (3), edge type is considered in attention computation, while the impact for distinguishing 3 different edges is not studied. Moreover, cases, where the proposed method doesn\u2019t perform well, may also be interesting while not included.\n\nA3: We add an ablation study for three different types of edges. Removal of the Sequential Context Dependency (SCD) edges does not significantly affect the performance (R-`1: 0.82 ($\\downarrow$), R-2: 0.73 ($\\downarrow$), R-3: 0.69 ($\\downarrow$)) because the sequence encoder can replace their impact to some extent. Removing either of the Speaker Dependency (SD) edges and Co-occurring Keyword Dependency (CKD) edges results in poor performance. With the SD edge removed, the R-1, R-2, and R-L scores decrease by 2.44, 2.12, and 2.76 points. With the CKD edge removed, the R-1, R-2, and R-L scores decrease by 1.49, 1.50, and 1.98 points. Besides, when two of the three edge types are arbitrarily removed, the performance will be greatly reduced, which demonstrates that capturing the long-distance cross sentence dependencies is important for making information flows in dialogues clearer. As we can see, removing three types of edges completely is equivalent to deleting the graph encoder, which can not aggregate utterance-level features.\n\n|SD|SCD|CKD|R-1 ($\\sigma$)|R-2 ($\\sigma$)|R-L ($\\sigma$)|\n|----|-----|-----|------|------|------|\n|\u2714|\u2714|\u2714|43.87(0.4)|19.66(0.5)|41.02(0.7)|\n|\u2714|\u2718|\u2714|43.05(0.5)|18.93(0.5)|40.33(0.6)|\n|\u2714|\u2714|\u2718|42.38(0.7)|18.16(0.5)|39.04(0.4)|\n|\u2718|\u2714|\u2714|41.43(0.6)|17.54(0.3)|38.26(0.5)|\n|\u2714|\u2718|\u2718|41.05(0.6)|16.78(0.4)|37.23(0.4)|\n|\u2718|\u2718|\u2714|40.66(0.5)|16.13(0.7)|36.85(0.7)|\n|\u2718|\u2714|\u2718|40.28(0.8)|15.71(0.9)|36.49(0.6)|\n|\u2718|\u2718|\u2718|39.62(0.6)|15.05(0.6)|35.87(0.5)|\n\nIn the case study, we add some analysis on the problems of summaries generated by our model. In Table 8 in A.7, the summary generated by our models is \"Lilly gonna be late. Gabriel order pasta with salmon and basil\". We find that there are some deficiencies in logical reasoning for our models. For example, the future tense is not inferred and the logical relationship between \u201corder food\u201d and \u201cfor Lilly\u201d is not recognized.\n\nQ4: In figure1, both Factual Knowledge Graph and Sequence Encoder are used in predicting the next word at decoder stage, in an attention manner, but why graph encoder is excluded here? Now attending to utterance representations encoded in a graph encoder help in doing better decoding.\n\nA4: The representation of graph encoder is used to initialize the initial state of decoder $s_0$, which passes utterance-level representations of dialogues to the decoder. Therefore, representations of graph encoder are used to do better decoding, as shown in $P_{vocab}$. However, in attention manner, we copy source tokens from the sequence encoder and the factual knowledge from the factual knowledge graph as a pointing process, as shown in $\\sum_{i:y_i=y_t}a_{i,t}^s+\\sum_{i:y_i=y_t}a_{i,t}^g$. In a word, although all of the sequence encoder, graph encoder, and factual knowledge graph are used in predicting the next word at the decoder stage, they are from two aspects. The sequence and graph encoders are used to generate the next token from the vocabulary table, while the sequence encoder and factual knowledge graph are used to generate the next token by copying from the source input and factual knowledge.\n\nWe modify the W_i^R in Eq. 3.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Gy2i8qUDLEi",
                "reply_to": "AT7jJ1BlQzN",
                "title": "Responses to AnonReviewer4",
                "comment": "Dear reviewer, we thank you for providing very valuable suggestions. We will explain your concerns and answer your questions point by point.\n\nQ1: In the ablation study (Table 3), the performance of each variant is close to the full model, and removing either module still outperforms the compared baselines. Given such performance, I wonder if the difference between the ablated variants and the full model is statistically significant. Also, what is the performance of the proposed method without graph information? Is it effectively the same as a Pointer-Generator?\n\nA1: We further enrich the ablation studies. We do an ablation experiment on the Sequence Encoder (SE), Graph Encoder (GE), and Factual Knowledge Graph (FKG) using the best-performing $\\rm KGEDC_g$ model on SAMSum corpus. As we can see, with the GE removed, the R-1, R-2, and R-L scores decrease by 4.25, 4.61, and 5.15 points. With the sequence encoder removed, the R-1, R-2, and R-L scores decrease by 7.72, 7.28, and 7.23 points. With the factual knowledge graph removed, the R-1, R-2, and R-L scores decrease by 1.16, 0.51, and 0.80 points. For all experiments, we run our models five times and report the average scores with the standard deviation, which suggests that the difference between the ablated variants and the full model is statistically significant. Besides, after getting rid of the GE and FKG (i.e. graph information), the R-1, R-2, and R-L scores decrease by 4.94, 5.50, and 6.14 points and. The R-1, R-2, and R-L scores of this ablated variant are 38.93, 14.16, and 34.88, which are similar to the Pointer Generator with 38.55, 14.14, and 34.85 for R-1, R-2, and R-L.\n\n|SE|GE|FKG|R-1 ($\\sigma$)|R-2 ($\\sigma$)|R-L ($\\sigma$)|\n|----|----|-----|------|------|------|\n|\u2714|\u2714|\u2714|43.87(0.4)|19.66(0.5)|41.02(0.7)|\n|\u2714|\u2718|\u2714|39.62(0.6)|15.05(0.6)|35.87(0.5)|\n|\u2718|\u2714|\u2714|36.15(0.5)|12.38(0.8)|33.79(0.6)|\n|\u2718|\u2718|\u2714|19.04(1.0)|8.57(0.9)|17.82(1.2)|\n|\u2714|\u2714|\u2718|42.71(0.5)|19.15(0.4)|40.32(0.5)|\n|\u2714|\u2718|\u2718|38.93(0.7)|14.16(0.6)|34.88(0.6)|\n|\u2718|\u2714|\u2718|35.65(0.8)|11.49(0.7)|32.57(0.4)|\n|\u2718|\u2718|\u2718|17.52(1.2)|7.90(1.5)|16.26(1.0)|\n\nQ2: Details of human evaluation are lacking. Who are the annotators and how many annotators are there? What is the inter-annotator agreement?\n\nA2: (1) We hire five native or fluent speakers of English as human annotators to rate summaries generated by our $\\rm KGEDC_g$, along with the outputs by Pointer Generator+Separator (PGS) and Fast Abs RL Enhanced (FARE). \n\n(2) For readability, we make the annotators focus on how fluent and grammatical the summary is and we provide them the following guidelines:\n\n1. First, the annotators judge whether the given sentence is complete or not. If the sentence is incomplete, the annotators rate the score as 1 for it.\n\n2. The annotators can understand the meaning of a complete sentence through their analysis, but there are many grammatical problems in the sentence. The annotators rate the score as 2 or 3 for this sentence.\n\n3. The annotator can easily understand the meaning of the sentence, and there are only minor grammatical problems in it. The annotators rate the score as 4 or 5 for this sentence.\n\nFor relevance, we make the annotators focus on how much faithful information the summary contains and we consider two types of unfaithful errors: (a) deletion or substitution error-mistakenly deleting or substituting subjects, objects, or clauses, and (b) hallucination error-creating content not present in the input. The guidelines are as follows:\n\n1. Firstly, we ask the annotators to avoid using general knowledge and check whether the given sentence is consistent with the source texts. \n\n2. If the given sentence is faithful to the source texts, the annotators rate the score as 4 or 5.\n\n3. If the given sentence is not faithful to the source texts, the annotators will label each type of unfaithful errors as 1 for the existence of errors and 0 otherwise, and give the scores (1, 2, or 3) of relevance according to the percentage of errors.\n\nQ3: The paper is poorly represented with unclear descriptions (not only typos/grammar but also definitions of various concepts), which makes it hard to follow. To name a few, in 3.2, the definition of edges gives one the impression that it is a fully connected graph. In 3.2.1, \u201ckeyword\u201d is defined after the description of the use of the keyword, and I can\u2019t really tell what is the \u201ckeyword neighborhood\u201d. A general suggestion is to define them beforehand (when they first appear) instead of describing them later or in the footnote.\n\nA3: We improve some definitions in our paper, such as the definition of edges given in Sec 3.2 and the definition of keywords in Sec 3.2.1.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6xifSH-mkeB",
                "reply_to": "qnWue5IpXqr",
                "title": "Responses to AnonReviewer2",
                "comment": "Dear reviewer, thank you for providing very valuable suggestions. We will explain your concerns and answer the point of your questions by point.\n\nQ1: The concern of lacking ablation studies?\n\nA1: We add some ablation experiments to verify the different neural blocks of our model. \nIn the first ablation experiment, we explore the contributions of the Sequence Encoder (SE), Graph Encoder (GE), and Factual Knowledge Graph (FKG), using the best-performing $\\rm KGEDC_g$ model on SAMSum corpus. Firstly, we study the effects of two encoders while keeping the FKG module. We only remove one encoder at a time and find that removing either of them leads performance to drop greatly. With the graph encoder removed, the R-1, R-2, and R-L scores decrease by 4.25, 4.61, and 5.15 points. With the sequence encoder removed, the R-1, R-2, and R-L scores decrease by 7.72, 7.28, and 7.23 points. Especially, the sequence encoder is slightly more important in overall performance, which suggests that the token-level information is indispensable for generating summaries. Removing both of them results in a very poor R-1, R-2, and R-L scores of 19.04\\%, 8.57\\%, and 17.82\\%, which is because the conversational context is not well represented. Next, after getting rid of the FKG module, all models could not keep as competitive as models with the FKG, which suggests the importance of factual knowledge. In particular, after only  FKG was removed, the R-1, R-2, and R-L scores decrease by 1.16, 0.51, and 0.70 points. The framework of the model without GE and FKG resembles the Pointer Generator and their performances are also similar. \n\n|SE|GE|FKG|R-1 ($\\sigma$)|R-2 ($\\sigma$)|R-L ($\\sigma$)|\n|----|----|-----|------|------|------|\n|\u2714|\u2714|\u2714|43.87(0.4)|19.66(0.5)|41.02(0.7)|\n|\u2714|\u2718|\u2714|39.62(0.6)|15.05(0.6)|35.87(0.5)|\n|\u2718|\u2714|\u2714|36.15(0.5)|12.38(0.8)|33.79(0.6)|\n|\u2718|\u2718|\u2714|19.04(1.0)|8.57(0.9)|17.82(1.2)|\n|\u2714|\u2714|\u2718|42.71(0.5)|19.15(0.4)|40.32(0.5)|\n|\u2714|\u2718|\u2718|38.93(0.7)|14.16(0.6)|34.88(0.6)|\n|\u2718|\u2714|\u2718|35.65(0.8)|11.49(0.7)|32.57(0.4)|\n|\u2718|\u2718|\u2718|17.52(1.2)|7.90(1.5)|16.26(1.0)|\n\nIn the second ablation experiment, we explore the effect of the edge labels of the dialogue graph. Removal of the Sequential Context Dependency (SCD) edges does not significantly affect the performance (R-1`: 0.82$\\downarrow$, R-2: 0.73$\\downarrow$, R-3: 0.69$\\downarrow$) because the sequence encoder can replace their impact to some extent. Removing either of Speaker Dependency (SD) edges and Co-occurring Keyword Dependency (CKD) edges results in poor performance. With the SD edges removed, the R-1, R-2, and R-L scores decrease by 2.44, 2.12, and 2.76 points. With the CKD edges removed, the R-1, R-2, and R-L scores decrease by 1.49, 1.50, and 1.98 points. Besides, when two of the three edge types are arbitrarily removed, the performance will be greatly reduced, which demonstrates that capturing the long-distance cross sentence dependencies is important for making information flows in dialogues clearer. As we can see, removing three types of edges completely is equivalent to deleting the graph encoder, which can not aggregate utterance-level features.\n\n|SD|SCD|CKD|R-1 ($\\sigma$)|R-2 ($\\sigma$)|R-L ($\\sigma$)|\n|----|-----|-----|------|------|------|\n|\u2714|\u2714|\u2714|43.87(0.4)|19.66(0.5)|41.02(0.7)|\n|\u2714|\u2718 |\u2714|43.05(0.5)|18.93(0.5)|40.33(0.6)|\n|\u2714|\u2714|\u2718|42.38(0.7)|18.16(0.5)|39.04(0.4)|\n|\u2718|\u2714|\u2714|41.43(0.6)|17.54(0.3)|38.26(0.5)|\n|\u2714|\u2718|\u2718|41.05(0.6)|16.78(0.4)|37.23(0.4)|\n|\u2718|\u2718|\u2714|40.66(0.5)|16.13(0.7)|36.85(0.7)|\n|\u2718|\u2714|\u2718|40.28(0.8)|15.71(0.9)|36.49(0.6)|\n|\u2718|\u2718|\u2718|39.62(0.6)|15.05(0.6)|35.87(0.5)|\n\nWe run our models five times and report average scores with the standard deviation.\n\nQ2: Your suggestion for adding experiments on the non-dialogic dataset?\n\nA2: We evaluate our model on a single-document news summarization dataset XSum. In this case, considering that there are no speaker dependencies in news texts, we remove this edge type. Besides, because the XSum dataset is more abstractive and most of the summaries are generated by paraphrasing the input content rather than copying from a single source sentence, we choose the XSum instead of the CNN/Dailymail dataset. The experimental results show that our model can handle long-distance cross-sentence dependencies on various datasets. \n\n|Model|R-1|R-2|R-L|\n|------|-----|-----|------|\n|PGS| 30.27|9.85|23.63|\n|FARE|32.03|11.64|26.11 |\n|$\\rm KGEDC_g$|34.71|14.38|29.56|\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "n6KdF4NgFR_",
                "reply_to": "iclr_2021_uFk038O5wZ",
                "title": "Official Blind Review #3",
                "comment": "What is the paper about, what contributions does it make, what are the main strengths and weakness?\n\nThe paper proposes a novel framework, Knowledge Graph Enhanced Dual-Copy network (KGEDC) for abstractive dialogue summarization. Conversational structure and factual knowledge are incorporated in this framework based on graph network to deal with long-distance cross-sentence dependencies and faithfulness respectively. This framework can be decomposed of 1) a sequence encoder to capture contextual information of dialogues flowing along the sequence, 2) a graph encoder via sparse relation graph self-attention network for cross-sentence dependencies, 3) a factual knowledge graph for representing relational tuples extracted from dialogues, 4) a dual-copy decoder to focus on both the input tokens and the factual knowledge. Experimental results on two datasets show the performance gains of the proposed methods over several previous baselines.\n\nSTRENGTHS:\n\n1. The motivation is clear and is in line with the model architecture. Two main unresolved problems in abstractive dialogue summarization are consistently concerned in the paper.\n\n2. The experimental results is rather strong and solid. The paper includes the results of different baseline methods for comparison, showing considerable boosts in both automatic and human evaluation metrics.\n\n3. The paper organization and writing is coherent. Besides, the model description is also well detailed.\n\nWEAKNESS:\n\n1. Dialogue graph may not necessarily be sparse. In section 3.2.3, \u201cSparse Relational Graph Self-Attention Layer\u201d paragraph, the author writes \u201cHowever, our sparse self-attention operation \u2026, which reduces the quadratic computation to linear\u201d, but no proof is given, actually this seems not to be right. In a dialog where only two persons interchange words in turn, the number of edges for speaker dependency will still be quadratic to turn length.\n\n2. The constructed factual knowledge graph seems to be sparse since the topics various in different dialogues. As shown in figure 2 (b), there are 7 different edge types in one dialogue session. How does the model deal with this issue? \n\n3. Maybe the paper should provide more content on ablation and case study. For example, in equation (3), edge type is considered in attention computation, while the impact for distinguishing 3 different edges is not studied. Moreover, cases, where the proposed method doesn\u2019t perform well, may also be interesting while not included.\n\n4. In figure 1, both Factual Knowledge Graph and Sequence Encoder are used in predicting the next word at the decoder stage, in an attention manner, but why graph encoder is excluded here? Now that utterance representations have been encoded in a graph encoder, attending to these representations may help in doing better decoding.\n\nTypos. Grammar, and Style\n\n1. just beneath equation (3) in section 3.2.3:  w_i^r should be w_i^R. \n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "2ay9m7PByns",
                "reply_to": "iclr_2021_uFk038O5wZ",
                "title": "Incorporating factual knowledge into dialogue summarization",
                "comment": "This paper proposes to improve dialogue summarization by encoding the text with a sequential encoder (for token-level contextualization) and a graph encoder (for long-distance and semantic contextualization). A KG is built and considered to be a surrogate for \"factual knowledge\". A dual-copy mechanism is used while decoding in the hope that direct access to this factual knowledge will enhance the faithfulness of the generated summaries.\n\nThe authors use a biLSTM to encoder the utterances. They build a dialogue graph where each utterance is a node and 2 nodes are connected if they have the same speaker, are within a distance d sequentially, or if they have common keywords (nouns, numerals,\nadjectives or notional verbs). They also build the KG using OpenIE triples and tuples from the utterance dependency trees. They use a GNN to encode both the utterance graph and the KG and use either concatenation or gated fusion to get the context vector. They copy from both the text sequence and KG during decoding.\n\nThey evaluate the method on two datasets: SAMSum corpus, Automobile Master corpus. They get a Rouge-L improvement of ~1.8 and ~2.5 respectively over string baselines. \n\nPros:\n- The method is interesting. Incorporating facts into text generation is an important and interesting area.\n- The results look good. They also perform human evaluation which is appreciated.\n\nCons/Questions:\n- It is not clear how the biLSTM encoding and the GE encoding are combined? Is it the same way as done with h^S and h^G?\n- I am not satisfied with the ablation study. One important ablation to run would be an experiment without both GE and FKG. Without those two, the model simply becomes (seq2seq + attention). And from Table 1, we see that the R-L for that simple model is 28.16 and 29.37. It is hard for me to convince myself that adding either GE or FKG increases R-L by more than 12 points.\n- It would also be interesting to know in what ratio h_G and h_S are combined in the case of gating fusion.\n- Since there is already sequential context dependency in GE, what additional advantage does the sequential encoder provide? Why not train only with the graph encoders?\n\nI would consider updating the rating once the authors respond.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "Clearly written and well motivated",
                "Sentiment Expression": "acknowledge by all reviewers",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "Technically sound",
                "Sentiment Expression": "the proposed architecture is clearly in line with the problem that the authors are trying to solve",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Significant upgrades to the paper",
                "Sentiment Expression": "the authors have added detailed ablation studies and results on non-dialogue datasets",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "significant difference between the results in the ablation studies",
                "Sentiment Expression": "in the original version and in the new version",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "while many Qs were sufficiently addressed by the authors, the large difference in the numbers",
                "Sentiment Expression": "reported for the ablation study in the initial and final version of the paper raises some new Qs which need to be addressed before the paper can be accepted",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "24-DxeAe2af": {
        "paper_id": "iclr_2021_24-DxeAe2af",
        "paper_title": "Accurate and fast detection of copy number variations from short-read whole-genome sequencing with deep convolutional neural network",
        "paper_abstract": "A copy number variant (CNV) is a type of genetic mutation where a stretch of DNA is lost or duplicated once or multiple times. CNVs play important roles in the development of diseases and complex traits. CNV detection with short-read DNA sequencing technology is challenging because CNVs significantly vary in size and are similar to DNA sequencing artifacts. Many methods have been developed but still yield unsatisfactory results with high computational costs. Here, we propose CNV-Net, a novel approach for CNV detection using a six-layer convolutional neural network. We encode DNA sequencing information into RGB images and train the convolutional neural network with these images. The fitted convolutional neural network can then be used to predict CNVs from DNA sequencing data. We benchmark CNV-Net with two high-quality whole-genome sequencing datasets available from the Genome in a Bottle Consortium, considered as gold standard benchmarking datasets for CNV detection. We demonstrate that CNV-Net is more accurate and efficient in CNV detection than current tools.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "Four knowledgeable referees have indicated reject. I agree with the most critical reviewer R4 that the model design lacks a clear and transparent motivation and that the experimental setup is not convincing, and so must reject.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "lG8CleiuM2D",
                "reply_to": "iclr_2021_24-DxeAe2af",
                "title": "Official Blind Review #1 ",
                "comment": "##########################################################################\n\nSummary:\nThe authors proposed CNV-Net, a deep learning-based approach for copy number variation identification. They encoded mapped DNA sequences into a pileup image that captures reference sequence, sequencing coverage, and mapped reads. Then, they used CNNs to classify it into deletions, duplications, or non-breakpoints. They benchmarked CNV-Net with two whole-genome sequencing datasets and claimed to obtain more accurate and efficient results than current tools.\n\n\n##########################################################################\n\nMajor comments:\nWhile the paper has its own merits, unfortunately, it has several issues that need to be addressed.\n- Although the authors claimed that CNV-Net is the first tool to use a CNN to detect CNVs, this is not true. Several previous works used CNNs for CNV detection. Furthermore, I think other machine learning and deep learning-based works should also be acknowledged. I\u2019d recommend authors properly cite and compare previous works with the proposed method. Some of the previous works include the followings:\n(1) Zhang, Yun Xiang, et al. \"DL-CNV: A deep learning method for identifying copy number variations based on next generation target sequencing.\" Mathematical biosciences and engineering: MBE 17.1 (2019): 202-215.\n(2) Cai, Lei, Yufeng Wu, and Jingyang Gao. \"DeepSV: accurate calling of genomic deletions from high-throughput sequencing data using deep convolutional neural network.\" BMC bioinformatics 20.1 (2019): 665.\n(3) Hill, Tom, and Robert L. Unckless. \"A deep learning approach for detecting copy number variation in next-generation sequencing data.\" G3: Genes, Genomes, Genetics 9.11 (2019): 3575-3582.\n(4) Pounraja, Vijay Kumar, et al. \"A machine-learning approach for accurate detection of copy number variants from exome sequencing.\" Genome research 29.7 (2019): 1134-1143.\n- The main contribution of the paper would be using a pileup image of mapped reads and a CNN to detect CNVs. However, in my view, the novelty of the paper is quite limited. As stated in the introduction, the pileup image encoding and CNNs have already been used in a couple of previous works for SNV detection. I could find any significant methodological differences in CNV and SNV detections; it seems like rather a straightforward application of previous methods on another similar problem. Otherwise, please clarify the differences between the two problems and what authors have done to overcome the new obstacles.\n- The core issue I have with this paper is that I do not think the experiment settings are realistic. As stated by the authors, CNV-Net must know the candidate CNV positions. I think this is a serious issue that must be handled rather than leaving it as a limitation of the work. Currently, the CNV-Net is evaluated with mapped and pre-preprocessed reads with CNV centered breakpoints. Compared to the experiments conducted in the previous works, the experiments of the proposed work seem limited, unrealistic, and biased in favor of the proposed method. \n- In my view, the authors left out too much information. For examples, it is quite difficult to understand how they used other tools for the experiments; Do they only use the CNV breakpoints that passed the quality control filter as CNV-Net? Or do they use all the mapped reads? What tool-specific arguments did the authors use for each tool? Currently, it is extremely difficult to reproduce the results presented in the paper.\n\n##########################################################################\n\nMinor comments:\n- How did the authors choose the specific numbers to encode individual base into R channel (e.g. A with 250, G with 180)\n- In the results section, the authors stated that they only used CNVs passing the quality control filter. Please provide more details for the filter explaining the filtering criteria and how they chose them.\n- In Table 2, how did the authors obtain the metrics for the multi-class problem? Please state whether they are macro or micro averages of scores.\n\n##########################################################################\n",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "hYTP2pyQfUb",
                "reply_to": "iclr_2021_24-DxeAe2af",
                "title": "Odd strategy, insufficiently described benchmark",
                "comment": "__Summary__\nThey authors describe a method to detect structural variation from aligned sequencing reads in a genome browser view. Their model encodes this genome browser view into an RGB image and applies a deep convolutional neural network to classify variant type (or no variant). They make use of curated variant annotations to train and test their model.\n\n__Major comments__\n* The RGB encoding is entirely arbitrary, unnecessary, and confusing. The authors should consider the actual range of the various input data and encode with a simple and interpretable strategy. For example, nucleotides are typically one hot encoded.\n\n* The improved accuracy on this task is abstract when only summarized in tables. Depicting an example of a structural variant whose prediction is improved by the authors\u2019 method would be very valuable. Ideally, both a false positive turned true negative and a false negative turned true positive could be shown.\n\n* The requirement that this method be provided candidate structural variation start and end points means that it's actually a module that would need to be plugged into a pipeline that also specified how those candidates are obtained. I would encourage the authors to develop that strategy before publishing their method.\n\n* The authors have not clearly described how the predictions of the other methods were used to annotate these curated variants. Doing so involves critical parameters, such as the allowed distance between the method prediction and true specified variant break points. Setting these parameters to strict values would be very unfair given that the authors method is not required to produce such breakpoints de novo. The Zook et al. paper describes this process in detail and discusses several software packages to do it.\n\n__Minor comments__\n* Does the authors\u2019 method make use of paired end read information?\n* How does Table 2 combine the accuracies for duplications and indels?",
                "rating": 2,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "bV13DskJMfC",
                "reply_to": "iclr_2021_24-DxeAe2af",
                "title": "A very short, trival paper, recommend rejection",
                "comment": "The authors in the paper describe a deep learning approach to detect copy number variants (CNVs) from DNA sequencing data, CNV-Net.  It described the approach by transforming the pileups into images and pass them through a CNN. This strategy has been proposed four or five years back to do SNPs and indels calling (DeepVariant).  It is challenging for SVs and CNVs as they can be arbitrarily large. However, there are also several existing models for this task (e.g. DeepSV, RDBKE). In this work, the authors only consider \"candidate CNV regions\" which are 201-bp small genomic regions that centered at the breakpoints. \n\nThe paper is only 4.5 pages. First, the authors did not explain or investigate many decisions in their model design. Then the experiments are flawed. I have many questions and concerns. \n1) First of all, this should not be called a CNV detector because it is in fact a breakpoint detector\n2) How did the authors do negative sampling? Were the negative samples randomly drawn from the genome? This creates bias as the sequence features might shift dramatically. \n3) Why are the negative samples not balanced with the positive samples?\n4) Are there differences in performance between deletion and duplications breakpoints? Does the model distinguish these two types? \n5) When splitting the training/validation/testing dataset, there will be significant data leakage if this is done randomly. Many SV/CNV regions have characteristic repeats patterns.\n6) The comparison between CNV-Net and other methods is not fair as the other methods are finding CNVs in the whole genome while CNV-Net is given a pre-defined set of \"candidate breakpoint regions\" in which positive samples have breakpoints perfectly centered in the middle. Moreover, such candidate regions might suffer from data leakage. \n7) Does HG002 really only has 174 duplications while NA12878 has 22,936? \n8) GIAB has more than two genomes available. Can we test on more genomes? \n9) Minor: RELU -> ReLU \n",
                "rating": 2,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "3eVcbqOxWPe",
                "reply_to": "iclr_2021_24-DxeAe2af",
                "title": "Interesting approach, but more evaluation is needed",
                "comment": "The authors present an innovative approach to CNV detection using CNNs. However, I believe that additional experiments need to performed before this paper is ready for publication. Below is a breakdown of strengths and weaknesses of this submission. \n\nStrengths: \n1. The empirical results seem encouraging compared to baselines. \n2. The method seems easy to implement and runs very fast. \n\nCons: \n\n1. There is only one comparison performed. I was wondering what the results are like in the opposite direction (training in HG002 and testing on NA12878)? Furthermore, the authors should consider other datasets, in particular the TCGA data where bam files, and copy number variations are available. The authors can train the method on one patient and test on another. \n\n2. There is lack of analysis on whether the method can recover known canonical copy number variations. In the TCGA dataset, and for certain cancer types, there are small and large copy-number variations that are known and have been validated in the lab. The authors should ensure that their method can recover these events using their method. \n\n3. The authors have considered baselines that do not take as input already known break points, which makes the comparison somewhat unfair. I was wondering if there is a way in which the authors can use the discovered breakpoints for each respective baseline and then determine whether their algorithm can improve the predictions? ",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the model design lacks a clear and transparent motivation and that the experimental setup",
                "Sentiment Expression": "is not convincing, and so must reject.",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "HyezmlBKwr": {
        "paper_id": "iclr_2020_HyezmlBKwr",
        "paper_title": "Test-Time Training for Out-of-Distribution Generalization",
        "paper_abstract": "We introduce a general approach, called test-time training, for improving the performance of predictive models when test and training data come from different distributions. Test-time training turns a single unlabeled test instance into a self-supervised learning problem, on which we update the model parameters before making a prediction on the test sample. We show that this simple idea leads to surprising improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts. Theoretical investigations on a convex model reveal helpful intuitions for when we can expect our approach to help.",
        "paper_acceptance": "reject",
        "meta_review": "The paper is on a new approach approach to transductive learning. Reviewers were a bit on the fence. Their most important objection is that the performance improvements that the authors report almost entirely come from the \"online\" version, which basically gets to see the test distribution.  That contribution is nevertheless, in itself, potentially interesting, but I was surprised not to see comparison with simple transductive learning from semi-supervised learning, learning with cache, or domain adaptation, e.g., using knowledge of the target distribution to reweigh the training sample, or [0], on using an adversary to select a distribution consistent with sample statistics. I encourage the authors to add more baselines, analyze differences with existing approaches, and, if their approach is superior to existing approaches, resubmit elsewhere. \n\n[0] http://papers.nips.cc/paper/5458-robust-classification-under-sample-selection-bias.pdf",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "BklX60fKtH",
                "reply_to": "iclr_2020_HyezmlBKwr",
                "title": "Official Blind Review #1",
                "comment": "The authors propose a method for adapting model parameters by doing self-supervised training on each individual test example. They show striking improvements in out-of-domain performance across a variety of image classification tasks while preserving in-domain performance; the latter is a marked difference from other robustness procedures which tend to sacrifice in-domain performance for out-of-domain (or adversarial) performance. These results are exciting, and I believe that this proposed test-time training method will spur a significant amount of further research into similar approaches.\n\nThe paper is well-written and the experiments are thorough, so I have no major concerns. Some remaining questions about the proposed approach are:\n\n1) How sensitive is test-time training to hyperparameters like splitting the parameters at the right location (i.e., the particular partitioning of $\\theta$ into $\\theta_e$, $\\theta_s$, and $\\theta_m$), or to the learning rate? Is there a good way to pick these hyperparameters, given that evaluation is on an out-of-domain distribution that we assume we do not have access to at training time? The paper proposes a particular split of parameters and a particular learning rate and number of steps (which differs for standard vs. online training). How were those chosen?\n\n2) How does test-time training compare to methods that assume access to the test distribution? I understand that a big benefit is that test-time training does not need to see the entire distribution (unlike standard domain adaptation approaches). But in cases where we do get to see parts of the test distribution -- say some unlabeled examples from it, or even some labeled examples -- how does test-time training compare? For example, should we see test-time training as providing the benefits of domain adaptation even when we're unable to access the unlabeled test distribution; or should we see it as doing something beyond what standard domain adaptation methods do, even when we have access to the unlabeled test distribution?\n\nMinor comments, no need to respond:\na) There are several minor typos in the paper, e.g.: p1, \"prediciton\"; eqn 8, v; p8, \"address\"; p9, \"orcale\"; appendix A, missing ref.\nb) The discussion in Appendix A seems a bit speculative and opinionated. For example, it is not obvious that one has to fall back on the space of all possible models in order to represent test-time training with a single gradient step as a fixed model. The discussion is useful but in my subjective opinion could be toned down; the experiments and discussion in the main paper are strong and less speculative.\n\n===\n\nEdit: Thank you for the response. The discussion about hyper-parameters makes sense. My rating edit comes from the realization that the performance improvements obtained are almost entirely from the \"online\" version, which gets to see the test distribution. So I think the baselines are in lacking in that sense: as a straw man baseline for example, one could simply run the normal model on half of the test set, and then use those observed test examples to do some other sort of domain adaptation training.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "B1gWRWfMcH",
                "reply_to": "iclr_2020_HyezmlBKwr",
                "title": "Official Blind Review #3",
                "comment": "The motivation is to increase accuracy of CNNs with unseen (unknown) distribution shifts. To this end, instead of the recent approach of training-time self-supervision, they adopted it for test-time [limited novelty]. More precisely, for a classification task, they considered two headed neural network with one head for main classification task and another for an auxiliary task (e.g. predicting rotation degree of rotated images). The feature extractor (up to k layers) is shared between these tasks thus updated using the two tasks, while two heads are updated according to each task. In test-time, the samples (drawn from shifted data distributions) are being used for updating the shared feature extractor through the auxiliary task. They have investigated their test-time approach in a series of experiments in online and offline settings on synthetic shift in data distribution of image classification tasks. The distributional shifts are synthesized adding some non-adversarial perturbation to clean images. The authors proof that their test-time approach can lead to lower error rate on given test-time samples when the underlying learning model is a linear regression model. \nThe authors incorrectly interchange OOD with domain shifts in their manuscript. The common usage of OOD set is to capture novel samples that are not properly following the training set distribution, e.g. from different concepts than the set of classes given in the training set. With an OOD set, a robust model dealing with it should be able to detect whether an instance is in-distribution or OOD, making a special decision for the latter case (e.g., rejecting the instance).\nIn domain shift, we look at the scenario where test objects are the same as used for training the model (i.e., correspond to one of the classes the model is processing), but might be perturbed or coming having a different distribution (e.g. SVHN for MNIST or vice versa). The approaches for domain shift concern to improve robustness of CNNs to such shifts in data distribution. Accordingly, the title of the paper inaccurately reflects of the claim of the paper and is misleading, this paper is not on learning with out-of-distribution instances.\nThe other important point is about catastrophic forgetting phenomena in online setting of their approach, which was not addressed thoroughly in the paper. How not to forget what the model has previously learnt a test-time training? I see this somewhat has been empirically shown in Fig 2 with accuracy on the original data, but what is the mechanism not to forget what have being learned so far?\nBesides generalization enhancement, the advantage of test-time self-supervision over training-time joint self-supervised is not clear for the readers, particularly considering the problem of the pitfall of catastrophic forgetting phenomena in test-time training. This pitfall does not exist for training-time joint self-supervised approach. What are the advantages of this approach?\nThe claims (about synthetic shift in distribution shift) are well supported in a series of experiments, where the distribution shifts are synthesized using adding some non-adversarial perturbation to clean images. However, the other common experiments on real shifts in distribution (e.g. SVHN for MNIST and vice versa, and those performed in Volpi et al  2018) are missing, which can help the paper to being better supported and justified for its practicality.\n[Volpi et al  2018]: Generalizing to Unseen Domains via Adversarial Data Augmentation, NIPS 2018\nI found the paper rather difficult to follow and not very coherent in its organization. The main idea is fundamentally simple, but it is still difficult to get it from the text. It needed me 2-3 readings before really getting the point of the paper.\n** Update ** I read other reviews and comments. The answer of the authors to my comments are somehow satisfactory, especially the point of changing from \"out-of-distribution\" to \"domain shift\", which avoid some confusion. I upgraded my rating to a \"weak accept\".",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HkltDr4oiS",
                "reply_to": "B1gWRWfMcH",
                "title": "Thank you and answers to your questions ",
                "comment": "Thank you for your feedback. It appears our use of the term \u201cout-of-distribution\u201d caused some confusion. Our algorithm works on what you call \u201cdomain shifts\u201d and does not deal with out-of-distribution detection. This terminology difference is secondary to the main contribution of the paper. In the revision, we use the term distribution shifts instead of out-of-distribution. \n\nForgetting has minimal impact on the performance of our method. This has been shown empirically in the paper, as you recognize in your review. Our model is jointly trained on both tasks, so our corrections during test-time training are tiny. This is in contrast to continual learning, where forgetting arises because the tasks have never been jointly trained and are learned one-by-one from scratch.\n\nWe also conducted the following experiment. The widely adopted oracle in continual learning is to jointly train on all the tasks. For test-time training, we experiment with the analogous oracle by mixing in training of the main task (on the training set) with the self-supervised task (on the test instance). This modification of our method takes a very long time to run, but should exhibit as little forgetting as possible. On all the benchmarks in CIFAR-10-C level 5, the standard and online versions of our method have the same performance both with and without this modification (up to random fluctuation), demonstrating that the impact of forgetting is minimal.\n\nRegarding experiments following [Volpi et al 2018]: Digit datasets (e.g. MNIST), especially those of small image dimensions, are generally not good fits for self-supervised learning. Rotation in particular can be poorly defined for digits. We experiment with real distribution shifts of natural scenes in the paper.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1ldGH4ioS",
                "reply_to": "SkgD9svKFS",
                "title": "Thank you and answers to your questions ",
                "comment": "Thank you for your positive feedback. We agree our method \u201chelps adjust for corruptions and modest dataset shifts\u201d. We understand the term out-of-distribution to broadly include distribution shifts of all kinds, including the small and modest ones in our experiments. To avoid this confusion, however, we now use the term distribution shifts instead of out-of-distribution in the revision.\n\nYou are  worried that \u201cthe more fine-tuned labels get (like the density of a tumor), the harder it gets to create auxiliary tasks.\u201d Even if the main task is highly specialized (e.g. the tumor density example), the auxiliary task can be fairly general (e.g. rotation). The self-supervised task only needs to share *features* with the main task without actually solving it, and features in computer vision can be as general as edges and shades. In ImageNet for example, improvements are aggregated across very specific problems such as distinguishing between 280 kinds of birds and 62 kinds of lizards, but rotation suffices for self-supervision. \nIn addition, the theoretically sufficient condition of our method -- gradient correlation -- is agnostic to the size of the label space; even if the label space is large (ImageNet with 1000 classes) or infinite (regression), using rotation still performs well.\n\nYou also asked for \u201c...a reasonable categorization of tasks where this method is expected to be applicable.\u201d We can provide reasonable rules of thumb for both the standard and the online version of our method. Standard: The self-supervised task, e.g. rotation, is both well defined and non-trivial on the new domain (in the sense discussed in Section 3.2). Online: All the test samples in the sequence are from the same (new) test distribution. Both conditions are easy to check in practice. Empirically, our method was shown to be effective in all of the experiments where these rules are met.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJgihEVsjr",
                "reply_to": "BklX60fKtH",
                "title": "Thank you and answers to your questions",
                "comment": "Thank you for your thoughtful comments. Here we answer your questions:\n\n1. We do not need information about the new distribution to choose the hyper-parameters. There are three hyper-parameters for test-time training: the splitting point, the learning rate and the number of steps. We select our splitting point to maximize performance of the joint-training baseline on the original distribution. The learning rate for test-time training is the same as during the last epoch of regular training; intuitively, this lets the model keep learning at the rate it has been accustomed to. Both these hyper-parameters can be selected without any knowledge of the test distribution, as done in our experiments. Empirically, we observe that performance is rather insensitive to the number of test-time training iterations once the self-supervised loss (on the test instance) converges. This also makes intuitive sense because once convergence is reached, the gradient from the self-supervised loss is small anyways. For the standard version, 10 steps is more than enough to reach convergence; we have in fact experimented with taking more steps and observe no difference in performance beyond random variations. For the online version, 1 step is in fact enough to reach convergence because the algorithm has already seen many previous samples from the same distribution. Practitioners can easily observe convergence of the self-supervised loss with the information revealed during testing.\n\n2. Thank you for acknowledging that we do not need to see the new distribution. In the case where some unlabeled samples are indeed available (as in the setting of unsupervised domain adaptation), yes they should be taken advantage of through methods for that setting. Test-time training can be applied on top of the model that has used these samples.\n\nYou minor comments: we really appreciate your careful reading and have incorporated them in our revision.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkgD9svKFS",
                "reply_to": "iclr_2020_HyezmlBKwr",
                "title": "Official Blind Review #2",
                "comment": "The paper proposes test time training, a method that uses an auxiliary task to provide a kind of loose supervision during test time. I say loose supervision because the theory suggests this is only useful when the gradients of the main and auxiliary losses are positively correlated.\n\nI'm not sure I totally believe that this is a method of out-of-distribution generalization, but rather it helps adjust for corruptions and modest dataset shifts which is an important problem itself. I suspect test-time training is fundamentally better suited for the latter because in general the assumption of correlation between the loss gradients cannot hold in the test if we allow for large shifts (like the airplane class in the video experiment). I note that the authors are aware of this limitation (page 5, last paragraph).\n\nMy main problem with this paper is that the more fine-tuned labels get (like the density of a tumor), the harder it gets to create auxiliary tasks. This will be a significant problem when the samples at test-time only share the highest level common characteristics with the true dataset; (like rotations do not impact or density of tumor, like face detection and similar fine-feature-based tasks).\n\nThat said, I do appreciate the experimental results which show promise; especially the CIFAR-10.1 results. So I'm inclined toward accepting this paper. I would be more so inclined if the authors could provide a reasonable categorization of tasks where this method is expected to be applicable. I ask this because, for practitioners, it is near impossible to verify the positive correlation of gradients assumption. If there are high-level targets that the distribution and/or task must satisfy that can act as indicators for applying this method, I believe that would be a valuable addition to the paper.\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SkxK93PFtr",
                "reply_to": "rJxhI4ktFB",
                "title": "Some questions",
                "comment": "Thanks very much. I will study the paper again based on your explanation to me.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJxhI4ktFB",
                "reply_to": "Bkx_qyUOtr",
                "title": "Answers",
                "comment": "Thank you for your kind words and we are glad to be able to inspire your research. Here we answer your questions:\n1. We cite and discuss this CVPR'19 paper in the related work section. Their method roughly corresponds to our joint training baseline, which we always compare with empirically. At a high level, the difference is simply that they do not perform test-time training i.e. modify the model parameters at test-time, which is exactly what we claim to contribute.\n2. We assume you meant to say \"corruption types\" instead of \"augmentation types\", and \"testing labels\" instead of \"testing data\". Then the case you described, of corruption types unknown in advance, is exactly the case we are trying to solve.\n3. The motivation is to make the analysis tractable, since we currently do not have sufficient tools to theoretically reason about realistic networks.\n4. We are not sure we understand your question, which asks us to compare the \"generalization problem\" with MMD-based methods; the comparison between a problem and a class of methods seems undefined. If you are looking for a comparison between the \"generalization problem\" and the problem of unsupervised domain adaptation, which often uses MMD-based methods and knows the target distribution in advance in the form of many unlabeled samples, we discuss this in the related work section. If you are looking for a comparison between our method and MMD-based methods, the most immediate difference is that the mean discrepancy is degenerate for a single sample thus cannot be used for test-time training.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Bkx_qyUOtr",
                "reply_to": "iclr_2020_HyezmlBKwr",
                "title": "Some questions",
                "comment": "First of all, I'd like to thank the author for the work, which inspire me to the generalization capability of real-world perturbation. I have few questions regarding the work. \n\n1. Based on my understanding, the formulation is very similar to the jigsaw puzzle published in CVPR2019 which also tackled the out of distribution generalization task. My impression is that this work adopted a image rotation for auxiliary task, while jigsaw puzzle adopted image patch shuffling as auxiliary task.  May I know whether there are any other differences in high level? \n\n2. When talking about out of distribution generalization, may I know whether the proposed problem can tackle the case where the augmentation types are unknown in advance (especially for testing samples). I guess this might not be a problem of jigsaw puzzle as it did not require testing data for parameter updating. \n\n3. I have difficulty understanding the theory 1 part. Based on the coarse of deep learning, the network is usually not convex. May I know whether there are any motivation by assuming x, y, l are all convex in \\theta?\n\n4. My research focus currently is on distribution based for generalization problem. May I know how this problem differentiate with the MMD based domain generalization method? such as the paper \"Exploiting Low-rank Structure from Latent Domains for Domain Generalization\" published in ECCV14 and other related works. \n\nFinally, I'd like to thank the author again for this inspiring work. I am looking forward to the explanation which I think will definitely help me with my future research. ",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the performance improvements that the authors report",
                "Sentiment Expression": "almost entirely come from the 'online' version",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "comparison with simple transductive learning from semi-supervised learning, learning with cache, or domain adaptation",
                "Sentiment Expression": "surprised not to see",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the authors to add more baselines, analyze differences with existing approaches",
                "Sentiment Expression": "encourage",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "qwjrO7Rewqy": {
        "paper_id": "nips_2022_qwjrO7Rewqy",
        "paper_title": "Neural Approximation of Graph Topological Features",
        "paper_abstract": "Topological features based on persistent homology capture high-order structural information so as to augment graph neural network methods. However, computing extended persistent homology summaries remains slow for large and dense graphs and can be a serious bottleneck for the learning pipeline. Inspired by recent success in neural algorithmic reasoning, we propose a novel graph neural network to estimate extended persistence diagrams (EPDs) on graphs efficiently. Our model is built on algorithmic insights, and benefits from better supervision and closer alignment with the EPD computation algorithm. We validate our method with convincing empirical results on approximating EPDs and downstream graph representation learning tasks. Our method is also efficient; on large and dense graphs, we accelerate the computation by nearly 100 times. ",
        "paper_acceptance": "Accept",
        "meta_review": "A reasonably interesting paper on deep models to approximate a popular class of objects, extended persistence diagrams, in topological data analysis. This is both an important problem, since these can be used throughout various parts of graph machine learning, and is a challenging one that requires technical innovations. The authors deliver on this, introducing quite good results.\n\nThe reviewers (and I) are in agreement about the paper, and the additional results the authors provided in response to the reviews are convincing. For example, on large sparse graphs, the proposed algorithms maintain a big speedup advantage.\n\nPerhaps the only question that came up is whether this work is more of a TDA-specific topic, but in general the paper clearly fits well within the machine learning community.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "-GMBohn-377",
                "writer": "author",
                "reply_to": "EB8RsPOukq6",
                "title": "Thanks for your support",
                "comment": " Thanks very much for your support!\n\nWe use the degree centrality function implemented with the package \"networkx==2.5\". The algorithm is available in https://networkx.org/documentation/stable/_modules/networkx/algorithms/centrality/degree_alg.html#degree_centrality.  And we will clarify this in the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EB8RsPOukq6",
                "writer": "official_reviewer",
                "reply_to": "k3HlBEMbxMZ",
                "title": "Thanks for your response",
                "comment": " Thank you for the detailed response. I would like to revise my rating as most of my questions are answered. A quick follow-up question: which kind of centrality did you use?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "k3HlBEMbxMZ",
                "writer": "author",
                "reply_to": "xnH12FGqyn",
                "title": "Soliciting further questions ",
                "comment": " Dear Reviewer,\n\nWe have tried our best to provide additional empirical results to address your concerns. Please do not hesitate to let us know if you have any further questions. We will be very happy to answer them during the discussion period.\n\nSincerely,\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4kj3G964RPa",
                "writer": "author",
                "reply_to": "Dzw7zCoo9N-",
                "title": "Thanks for your support",
                "comment": " Thanks very much for your support!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Dzw7zCoo9N-",
                "writer": "official_reviewer",
                "reply_to": "mtdIiAgptzVI",
                "title": "Thanks for the clarification",
                "comment": " Thank you for the clarification. I found it a very useful tool. Although this issue seems to be in the field of TDA, it is also useful in the field of AI, so I think it is fully acceptable.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "x7b4qRPn-v",
                "writer": "author",
                "reply_to": "0KitxKb10ZG",
                "title": "Discussion on the Wasserstein distance ",
                "comment": " Thank you very much for the quick response. We would like to clarify the point in our experimental details.\n\n**(4) \u201cYou compute an optimal matching between the collected pairs (that correspond to two \"filtration values\") and the target EPD, and then derive a gradient from this\u201d**\n\nYes, you are correct. Through the matching, the gradient is passed to each predicted persistence pair. Since we have established the one-to-one correspondence between pairs and edges, the gradient is then passed to the corresponding edge, and contributes to the representation learning. \n\n\n**(5) \"I agree this is not a major issue (though it shouldn't be ignored either).\"**\n\nYes, we agree.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mtdIiAgptzVI",
                "writer": "author",
                "reply_to": "ig7wPaz2vaF",
                "title": "Response to Reviewer cEes",
                "comment": " **(5) It is difficult to understand why only the filtration shown in Fig.1 is targeted**\n\nThank you very much for the quick response. We would like to clarify that the height-function-based filtration in Fig.1 is only for illustration purposes. We will add clarification in the caption of Fig.1.\n\nOur method applies to any filter function that can be assigned to a graph. In the paper, we cover 3 types of filter functions from state-of-the-art works, that is Ollivier-Ricci curvature [57, 51], heat kernel signature [5], and the node degree [17]. Details of these filter functions can be found in lines 255-256. When reporting approximation errors (Table 1), we report the mean score over diagrams of all filter functions. In addition, in our post-review response to Reviewer 5254 Q4, we add evaluations on 2 more filter functions: the clustering coefficient and the centrality. The empirical evidence shows that our method works well on all 5 types of filter functions. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0KitxKb10ZG",
                "writer": "official_reviewer",
                "reply_to": "rGvko1sDc3E",
                "title": "Still about the Wasserstein distance",
                "comment": " Thank you for your answer. Here I some following comments (mainly about the training loss) : \n\n> During training time, our model predicts persistence pairs for all edges. These pairs are collected as the predicted EPD. The predicted EPD is then compared with ground truth EPD using Wasserstein distance; this is the training loss.\n\nJust to be sure my understanding is correct: it means that at training time, you compute an optimal matching between the collected pairs (that correspond to two \"filtration values\") and the target EPD, and then derive a gradient from this. \nIsn't it how differentiation wrt the Wasserstein distance is already done (for instance in the work of Carri\u00e8re et al., ICML 2021)? In this work, from my understanding, authors identify points in the PDs with couple (b,d) of birth-death times in the input filtration, and differentiate through it. \nAnyway, that's definitely not a major concern, I was just wondering if I was missing something that would make the differentiation particularly efficient in your setting. \n\n>It is true that the computation of the Wasserstein distance is nontrivial and is indeed a bottleneck. But please note that this only happens during training.\n\nSure, I agree this is not a major issue (though it shouldn't be ignored either). ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ig7wPaz2vaF",
                "writer": "official_reviewer",
                "reply_to": "2CY8u2vKqw",
                "title": "Thanks for your response",
                "comment": " First, I apologize for any misunderstanding I may have caused you. For formatting purposes, we listed our concerns in \"Strengths And Weaknesses\" and provided concrete suggestions for resolving those concerns in \"Questions. After \"overall\", I write my impression of the reason for rating.\nTherefore, it is sufficient to answer the questions in the \"Questions\" section. In other words, it is enough to rebute the contents of \"weakness\".\n\nWith regard to my second question, I think the responses (1) and (4), as well as the responses to the references pointed out by the other two reviewers (which I was not aware of), are sufficient.\nAs to my additional questions, I understand your views. Although future research may yield different findings, I recognized that this is a reasonable policy at this time.\n\nAs for the first question, I think I did not fully convey my intention. This method assumes that EPD using the filtration shown in Fig. 1 is excellent for graph analysis. However, it is difficult to understand why only the filtration shown in fig.1 is targeted. For example, PersLay [5] uses a heat kernel. If other filtration methods are better, the value of the proposed method will be smaller. So I requested evidence that the filtration in Fig. 1 is very effective. My description of this point was problematic, so if you are short on time, just speeding up the method using the proposed filtration is certainly worth enough, so I will evaluate it from that perspective. If possible, please provide evidence. I will re-evaluate after receiving the answer to this question.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rGvko1sDc3E",
                "writer": "author",
                "reply_to": "H6J1IL71S0A",
                "title": "Response to Reviewer 8ijg",
                "comment": " Thanks very much for the constructive feedback and for pointing out the main idea of the paper \u201clearning to produce topological descriptors by approximating the inner-algorithms in a relevant way\u201d. \n\n**(1) How is the training Wasserstein loss implemented and how does it benefit from the \"edge decomposition\" properties of your approach?**\n\nThanks for pointing out the confusion. We will clarify this in the paper. \n\nThe key is that we can establish a one-to-one correspondence between edges and persistence pairs in the EPD. During the ascending filtration, each edge is either the destroyer of a 0D persistent homology class (i.e., connecting two connected components), or the creator of a 1D persistent homology class (i.e., creating a loop that will be destroyed during the descending filtration). This way, we can assign to each edge of the graph a unique persistence pair as its \u201clabel\u201d. The computation of the EPD is then reduced into an edge label prediction problem \u2013 predicting the persistence pair associated with each edge. \n\nDuring training time, our model predicts persistence pairs for all edges. These pairs are collected as the predicted EPD. The predicted EPD is then compared with ground truth EPD using Wasserstein distance; this is the training loss. It is true that the computation of the Wasserstein distance is nontrivial and is indeed a bottleneck. But please note that this only happens during training. At the inference stage, predicting edge labels and then the entire EPD does not involve the expensive Wasserstein distance computation. Also note that we observe good transferability (Section 4.1 in the appendix) of our model, meaning we can apply a pre-trained model to a new graph without training.\n\n\n**(2) Isn't it somewhat \"redundant\" to use the PIE as a second evaluation metric?**\n\nWe have to use PIE to evaluate some baseline methods [30, 39]. These methods directly approximate PIs, and thus cannot be evaluated using W2 distance. \n\n**(3) Related works on estimating persistent homology of point clouds.** \n\nThank you for suggesting the references. We will cite and discuss them in the paper. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2EHKJCgv_SS",
                "writer": "author",
                "reply_to": "2CY8u2vKqw",
                "title": "Response to Reviewer cEes",
                "comment": " **(1) EPD compared with other graph-oriented TDA methods.**\n\nSorry we are not sure which other TDA methods you are referring to. The learning power of EPDs has been well established in recent years. Recent works [5, 51, 57] have shown that EPDs can achieve SOTA performance in node and link prediction tasks. The success motivates accelerating the computation of diagrams, either through approximation [30, 39] or through better algorithm design [51]. We address this fundamental task in this paper.\n\n**(2) The reason to approximate part of the EPD computation algorithm**\n\n\nEmpirically, it is evident that direct approximation of the entire algorithm is very difficult. Please refer to the poor performance of baselines in Table 1 (GIN_PI, GAT_PI, GAT, GAT+MIN). The original EPD computation algorithm [11] involves a sequence of modulo-2 additions of boundary matrix columns that may involve edges and nodes far apart. Both the untypical modulo-2 arithmetic operation and the highly global computation are extremely hard to learn even for a deep neural network. \nOur idea of algorithmic decomposition is not simply a trade-off between approximation performance and computation. We use our algorithmic insights to circumvent the aforementioned challenges. Specifically, we decompose the algorithm into a set of parallel Union-Finds sub-routines. The benefit is threefold: (1) each Union-Find is only related to the persistence pairs originating from one node, and thus is a relatively local computation; (2) the Union-Find algorithm does not involve untypical modulo-2 arithmetic. Instead, it follows a certain sequential algorithm pattern that is known to be well aligned with GNN [44,49]; (3) these Union-Finds share common algorithm steps and potentially share similar information flows. Thus they can be approximated altogether with one single run of GNN. This is extremely efficient in practice. These benefits guarantee the superior performance of our algorithm, in both approximation error and running time.\n\n**(3) The evaluation is based on a limited set of assumptions that favor the proposed method and can be viewed as unfair.**\n\nSorry this comment is too vague for us to respond to. The way we use persistence diagrams for graph learning tasks has been well established in recent years [5, 9, 17, 51, 56, 57]. Combining topological features with GNN clearly outperforms the traditional way of directly feeding topological features into a classifier [19, 26]. Under such a premise, we evaluate the approximation error, running time, and representation quality of our method. On a broad spectrum of popular graph benchmarks, our method outperforms SOTA methods. \n\n**(4) Other speed-up methods, e.g., [Cufar and Virk 2021].** \n\nThank you for the reference. We will cite and discuss it. In fact, [Cufar and Virk 2021] does not accelerate the computation of persistence diagrams. It speeds up the computation of explicit cycle representatives for each persistent homology class. In terms of computing the diagram alone, it is not faster than known sequential matrix reduction algorithms. It is true that cycle representatives carry additional information and can potentially enrich persistence diagrams. We will be happy to explore how this can be leveraged for graph learning tasks in the future. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aTTk3pptci9",
                "writer": "author",
                "reply_to": "UFsTj5TZg4Z",
                "title": "Additional Response to Reviewer 5254",
                "comment": " **(8) The influence of k (k-hop) in efficiency.**\n\nWe evaluate the efficiency on vicinity graphs with different k\u2019s. In Table 4, we already reported the results when k=2. Here we report the results when k=1 and k=3. For k=1, following the same settings as Table 4, we report the statistics of the vicinity graphs, and the time to generate/estimate EPDs below.\n\n|Dataset | Cora | Citeseer | PubMed | Photo | Computers | CS | Physics |\n| :----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |\n| Avg. N/E | 4.9/10.6 | 3.8/7.7 | 5.6/11.7 | 31.1/333.6 | 38.7/430.7 | 9.6/32.0 | 13.1/53.2 |\n| Fast [51] | 0.194 | 0.097 | 0.115 | 4.083 |5.827 | 0.313 |0.713|\n| Gudhi [41] | **0.088** | **0.074** | **0.082** | **2.387** | 5.249 | **0.179** | **0.279**|\n| Ours | 4.086 | 4.012 | 4.073 | 4.087 | **4.097** | 4.085 | 4.095 | \n\nFor k = 3, we set the number of vicinity graphs to 100 (originally 1000), and report the results as below. \n\n|Dataset | Cora | Citeseer | PubMed | Photo | Computers | CS | Physics |\n| :----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |\n| Avg. N/E | 131.1/371.7 | 53.4/163.8 | 363.2/1380.2 | 2465.0/45497.5 | 8549.3/191947.6 | 813.1/4378.8 | 2028.9/18638.2 |\n| Fast [51] | 0.38 | 0.14 | 2.13 | 102.27 | 473.71 | 8.36 | 45.77 |\n| Gudhi [41] | **0.14** | **0.07** | 1.26 | 219.19 | 2499.24 | 8.09 | 137.83 | \n | Ours | 0.43 | 0.42 | **0.44** |  **0.62** | **1.21** | **0.47**| **0.54**|\n\n**Observation.** we can observe the same tendency as in Table 4 in the main paper. The performance is generally correlated with the size/density of the vicinity graphs. For k=1, for all benchmarks, the vicinity graph is small. Thus sequential algorithms are generally faster. For k=3, the vicinity graph is very large. Our method is significantly faster, especially on Photo, Computers, CS, and Physics.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UFsTj5TZg4Z",
                "writer": "author",
                "reply_to": "TUTXbkn1AwA",
                "title": "Additional Response to Reviewer 5254 ",
                "comment": " **(4) Experiment on other choices of filter functions.**\n\nBelow we set centrality and clustering coefficient as the filter function, follow the settings in Table 1, and report the approximation error on Cora, Citeseer, and PubMed.\n\n| Data | | Cora |  | Citeseer | | PubMed|\n| :----: |:----: |:----: |:----: |:----: |:----: |:----: |\n| Eval | $W_2$  | PIE | $W_2$ | PIE | $W_2$ | PIE|\n|Clustering |0.392 |1.53e-3 |0.178 |7.17e-4 |0.267 | 2.5e-3|\n|Centrality | 0.332 | 4.14e-4 | 0.237 | 4.65e-4 | 0.322 | 3.75e-4|\n\nWe also report computation time following the settings in Table 4. The only difference is that below we report the time to generate all vicinity graphs (rather than 1000 graphs as in Table 4).\n\n| Data | | Cora |  | Citeseer | | PubMed|\n| :----: |:----: |:----: |:----: |:----: |:----: |:----: |\n|Filter | Clustering | Centrality | Clustering | Centrality | Clustering |Centrality|\n| Fast [51] | 2.10 | 2.21 | 1.16 | 1.31 | 38.07 | 39.05 |\n| Gudhi [41] | 0.98 | 1.00 | 0.59 | 0.63 | 16.79 |16.24 |\n| Ours | 11.25 | 11.30 | 13.61 | 13.62 | 66.19 | 67.29 |\n\n**Observations.** We observe (1) filter function only has minor influence on inference/computation speed, for both the computation algorithm and ours; (2) filter function does influence the approximation error. The reason is that different filter functions have different ranges; functions with larger ranges tend to have larger approximation error especially on PIE. This is another evidence that the distance function on PIs is not very robust for learning (regarding Q2 above). \n\n\n**(5) EPD decomposition extended to simplicial complexes.**\n\nThe proposed algorithm is restricted to 0D/1D EPD computation. It does not generalize to high-dimensional simplices.\n\n**(6) How is the value of k (k-hop) chosen?**\n\nThe choice of k follows the settings of existing state-of-the-art works [51, 57]. In [57], Zhao et al. have already evaluated the influence of k, and selected the best k based on the empirical performance of the learning task. \n\n**(7) Experiments on the threshold value of average N/E or degree to decide which method is the fastest to compute/estimate the EPD.**\n\nTo find the threshold, we use the well-known Stochastic Block Model (SBM) to generate synthetic graphs. We set the number of nodes in these synthetic graphs from 200 to 300, with 10 as the step. In these graphs, we randomly generate 5 different clusters, and set the probability of edges intra-cluster to 0.4, and the probability of edges inter-cluster to 0.1. In this way, we can obtain 11 graphs with different nodes and edges. We set node degree as the filter function, and add experiments on the largest connected components of these 11 graphs. The information of the selected connected graphs and the running time (second) are listed below. As shown in the Table, the threshold is around 100 nodes / 820 edges.\n | Node | 80 | 84 | 88 | 92 | 96 | 100 | 104 | 108 | 112 | 116 |120 |\n| :----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |\n | Edge | 515 | 585 | 660 | 713 | 759 | 820 | 943 | 1012 | 1060 | 1152 |1231 |\n | Fast [51] | 6.8e-3 | 8.0e-3 | 8.9e-3 | 9.6e-3 | 1.0e-2 | 1.1e-2 | 1.3e-2 | 1.4e-2 | 1.4e-2 | 1.6e-2 | 1.7e-2|\n| Gudhi [41] | **2.5e-3** | **3.2e-3** | **3.6e-3** | **3.9e-3** | **4.0e-3** | 4.8e-3 | 5.5e-3 | 6.0e-3 | 6.4e-3 | 6.6e-3 |6.8e-3|\n | Ours | 4.5e-3 | 4.5e-3 | 4.6e-3 | 4.6e-3 | 4.6e-3 | **4.6e-3** | **4.7e-3** | **4.7e-3** | **4.7e-3** | **4.7e-3** |**4.8e-3** |\n\nWe then add experiments to evaluate the influence of density. We fix the node number of the SBM model to 250, and set the probability of edges intra-cluster from 0.5 to 0.7, and the probability of edges inter-cluster from 0.05 to 0.15. The steps for intra-cluster and inter-cluster are 0.02 and 0.01, respectively. In this way, we can obtain 11 graphs with the same nodes and different edges. We set node degree as the filter function, and add experiments on the largest connected components of these 11 graphs. The information of the selected connected graphs and the running time (second) are listed below. As shown in the Table, the threshold is around 100 nodes / 766 edges.\n\n |Node | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 | 100 |\n| :----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |:----: |\n| Edge | 489 | 529 | 595 | 652 | 766 | 842 | 968 | 1011 | 1082 | 1231 |1307 |\n| Fast [51] | 7.0e-3 | 7.4e-3 | 8.3e-3 | 9.0e-3 | 1.1e-2 | 1.2e-2 | 1.3e-2 | 1.4e-2 | 1.4e-2 | 1.6e-2 |1.7e-2| \n| Gudhi [41] | **2.8e-3** | **2.9e-3** | **3.0e-3** | **4.1e-3** | **4.2e-3** | 5.1e-3 | 5.5e-3 | 5.9e-3 | 6.2e-3 | 6.3e-3 | 6.7e-3 | \n| Ours | 4.1e-3 | 4.1e-3 | 4.2e-3 | 4.2e-3 | 4.3e-3 | **4.4e-3** | **4.7e-3** | **4.7e-3** | **4.8e-3** | **4.8e-3** | **4.8e-3** |\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "TUTXbkn1AwA",
                "writer": "author",
                "reply_to": "xnH12FGqyn",
                "title": "Response to Reviewer 5254",
                "comment": " **(1) Additional Downstream Tasks Baselines (\u201cPermanifold\u201d at https://github.com/pkyriakis/permanifold).**\n\nThanks for your suggestion. This paper proposes a new feature representation of persistence diagrams for learning tasks. We are happy to discuss it and use it as a baseline downstream method to verify the learning power of our estimated diagrams. We use the original code (https://github.com/pkyriakis/permanifold) and try our best to reproduce the results (details are explained later). We compare the original method, Permanifold, with Permanifold-PDGNN, in which we replace the original diagrams with the estimation of our method PDGNN. The table below shows that the two methods achieve similar classification performance. This confirms that the representation power of our estimated diagrams is on par with the original diagrams.\n\n\n| |  IMDB-BINARY | IMDB-MULT |\n| :----:| :----: | :----: |\n|Permanifold | $74.20\\pm1.60$ | $52.59\\pm0.56$ |\n|Permanifold-PDGNN| $73.50\\pm0.45$ |$ 51.06\\pm0.74$|\n\n**Experimental details.** We use the original repository of Permanifold (https://github.com/pkyriakis/permanifold). Note that in the repository, diagrams are generated using the package cechmate (for filter functions such as the node degree). However, it requires a specific version of package phat=1.5.0a0, which is not available through pip or conda. Therefore, we change the PD generation package from cechmate to another well-known package gudhi [41]. \nWe follow the settings from the paper: to select the hyper-parameters from \u201cmanifold = [Euclidean, Poincare], manifold dimension = [3,6,9,10],  projection bases = [200, 350, 500], batch size = 64, epoch = 100\u201d (other settings are the default setting of the code). We run the experiment with each set of hyper-parameters once and report the best one as the result. For IMDB-BINARY, we adopt the manifold as Euclidean, the manifold dimension as 3, and the projection bases as 500.  For IMDB-MULTI, we select the manifold as Euclidean, the manifold dimension as 9, and the projection bases as 500. \n\n**(2) Discuss and analyze why indirectly estimating the persistence image (PI) by estimating the EPD is better than directly estimating the PI**\n\nWe believe directly estimated PIs will lose important structural information that can be crucial for downstream tasks. PI is only an approximation of the persistence diagram. The L2 distance between PIs does not accurately reflect the true Wasserstein distance between diagrams. Therefore, using an L2-distance-based loss to directly learn the PI may lead to the loss of important structural information carried by a diagram. An example is provided in Figure 5 in the revised supplementary material. For a sample vicinity graph from Cora, we compare the ground truth PI (computed from the ground truth diagram), the PI computed from the diagram estimated by our method PDGNN, and the PI directly estimated by GIN.  Both estimated PIs have similar L2 distance from the ground truth PI. But we observe that the PI estimated by PDGNN has a very similar spatial distribution to the ground truth PI. This structural property, however, is not preserved by the directly estimated PI. Such loss of structural information of directly estimated PIs, although not captured by the L2 error, partially explains their worse representation power. In Table 2, the directly estimated PIs (PEGN(GIN_PI) and TLC-GNN(GIN_PI)) perform worse in the downstream task. \nThis choice of estimating diagrams instead of PIs is a part of the overarching theme of our paper. As pointed out by Reviewer 8ijg, the main contribution of our paper is to transfer a complicated and uncontrollable learning process to a controllable process with algorithmic insight. This general principle also applies to our learning algorithm. We decompose the diagram computation algorithm into sub-algorithms, which can be approximated well by a GNN. This is the reason PDGNN approximates the diagrams much better than other baselines in Table 1.\n\n**(3) The performance of PDGNN on large sparse graphs.**\n\nIn the experiments in the paper, the input is the k-hop vicinity graphs. On citation graphs, the vicinity graph remains small. On these small graphs, the exact computation algorithm like Gudhi has less overhead, and thus is unsurprisingly faster. We will clarify this in the paper.\nIndeed, on large and sparse graphs, our method outperforms strong baselines like Gudhi significantly. In the table below, we compare the running time (in seconds) on popular citation networks including Cora, Citeceer and PubMed. For each graph, we run experiments on the largest connected subgraph. We also report the number of nodes/edges of the selected subgraph.\n\n\n| |  Cora | Citeseer| PubMed | \n| :----:| :----: | :----: | :----: |\n|Node/Edge| 2485/5069 | 2120/3679 | 19717/44324 |\n| Fast [51] | 0.184 | 0.068 | 1.816 |\n| Gudhi [41] | 0.045 | 0.023 | 1.696 |\n|Ours | **0.006** | **0.005** | **0.007** | \n\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xnH12FGqyn",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_qwjrO7Rewqy",
                "title": "",
                "comment": " In this paper, the authors propose an efficient learning framework to estimate extended persistence diagrams (EPD). It decomposes a reformulated version of EPD as an edge-wise prediction task into independent pairing problems. The authors use GNN architecture to learn the union-find algorithm which can solve the pairing problems. Pros:\n\n1. The EPD approximation method is examined on varying datasets and achieves the best performance.\n2. The proposed method is efficient on large and dense graphs.\n3. The paper is well written and easy to follow.\n\nCons:\n\n1. I suggest the authors check\u00a0this related paper on using PD to learn graph representation and consider it as a baseline: https://openreview.net/forum?id=yqPnIRhHtZv\u00a0\n2. Large scale real-world graphs are usually sparse such as the brain connectome etc. However, in this paper the large graphs are mostly denser than the smaller ones. The authors claim that their estimation approach of EPD is faster than other methods on large and dense graphs, it would be worthwhile to check the performance on large sparse graphs, which are more common in real-world complex systems. The proposed method is actually slower when the graph is sparse (citation networks).\u00a0\n3. The major contribution of this paper is the efficient estimation of EPD using decomposition and GNN architecture to learn the reformulated edge prediction task. Downstream tasks in section 5.2 are more like\u00a0an indirect way to examine the approximation accuracy. As long as the EPD approximation error is low, PEGN/TLC-GNN (True Diagram) and PEGN/TLC-GNN (PDGNN) will have similar performance. Therefore I recommend the authors to focus more on the EPD estimation experiments. For example, the author could further discuss and analyze why indirectly estimating the persistence image (PI) by estimating the EPD is better than directly estimating the PI to explain their experiment result presented in table 1. Another thing worthy to look at is the choice of filter functions. Ablation study or discovery of other graph metrics such as clustering coefficient, centrality can help to make the experiment complete. 1. Can such EPD decomposition be extended to simplicial complexes (with 2-simplices, 3-simplices and more) apart from graph representations as simplicial complexes? If so, it would be interesting to also apply the proposed framework on some real-world higher-order data.\n2. How is the value of k (k-hop) chosen when extracting the |V| vicinity graphs?\n3.  The authors claim that their estimation method is much faster on large and dense datasets. Is there a threshold value of average N/E or degree to decide which method is the fastest to compute/estimate the EPD? Some additional experiment on real-world or synthetic graphs and a plot of computation time vs graph density may give an answer to this. The efficiency experiment is based on 2-hop neighborhoods. Does the value choice of k-hop affect such results? See question 1.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "2CY8u2vKqw",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_qwjrO7Rewqy",
                "title": "",
                "comment": " In this paper, the authors propose a mechanism to reduce the computation time, which is a major bottleneck of the extended persistence diagram that has recently been proven effective for graph structure learning tasks. The authors analyze the calculation algorithm of EPD, describe the key Union-Find algorithm in seqential, and approximate it with GNN to simplify the calculation. In their experiments, the authors evaluated the effect of EPD on graph analysis, the impact of pride of place, and computation time, and showed that EPD can analyze graphs efficiently and accurately, especially for large graphs. Strength\n- Algorithms to improve the efficiency of PED calculations were precisely constructed by analyzing the structure of PED calculations.\n- The authors evaluate the effect of PED and the speed-up effect of this algorithm, as well as the accuracy of the speed-up.\n\nWeakness\n- Although the subject is to improve the efficiency of EPD calculations, there are insufficient references to other acceleration methods.\n- This method is an EPD-only speed-up method, assuming that EPD has high performance, but there is no mention of whether EPD is superior to other graph-oriented TDA methods. If EPD is inferior to other TDA methods, the method is less valuable from an AI perspective, i.e., for graph analysis.\n\nOverall, the effect of graph EPD on non-traditional TDA-based methods and the speedup of EPD are accurately described, and the contribution is significant. On the other hand, the evaluation is based on a limited set of assumptions that favor the proposed method and can be viewed as unfair. Can you clarify the following two points?\n- Provide evidence that EPD is more effective than other TDA methods.\n- Discussion on whether there is any other research on EPD acceleration. Also, there are other speed-up methods for TDAs that are not EPD, such as [Cufar], etc. Can you mention why these methods cannot be used for EPD?\n\n[Cufar] MATIJA \u010cUFAR AND \u017dIGA VIRK, FAST COMPUTATION OF PERSISTENT HOMOLOGY REPRESENTATIVES WITH INVOLUTED PERSISTENT HOMOLOGY, https://arxiv.org/abs/2105.03629\n\nAlso, instead of approximating part of the algorithm with a GNN as an approximate computation of the diagram or PI, one could directly approximate the entire flow with a GNN or NN, but what is the reason for approximating only part of it? I know there is a trade-off between approximate performance and computation time, as well as the amount of data required, but is there anything you can mention? Please mention any that you can, although they may be related to issues to be considered in the future. The authors have clearly defined the application, clearly described the performance, and adequately addressed the limitation of their work.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "H6J1IL71S0A",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_qwjrO7Rewqy",
                "title": "",
                "comment": " The extraction of topological features (called persistence diagrams - PDs) from structured data (such as graphs, point clouds, time series, etc.) is a promising approach to develop new machine learning methods but suffers from high computational complexity (among other things) to be faithfully used in learning pipelines. This happens mostly because the computation of PDs relies on combinatorial algorithms (roughly, sparse matrix reduction) that do not scale well with the size of the considered data. \n\nFollowing a recent line of development, this article proposes to bypass the exact computation of topological features via combinatorial algorithms and to instead approximate it using neural networks. The difference with previous approaches is that instead of brutaly computing PDs using an ad-hoc architecture, authors decompose the (extended) PD computation in different algorithmic steps, which roughly rely on running a Union-Find. It invite them to design a Graph Neural Network (GNN) capable of learning to reproduce this sub-routine that they call PDGNN, essentially based on an adapted aggregation function (mixing a minimum and a maximum). \n\nThey showcase the benefits of the proposed approach on different experimental tasks.  - [Originality] While not being entirely novel for either the TDA nor the ML communities, the approach proposed in this paper is quite fresh and promising. At least from the TDA perspective, I think that the idea of \"learning to produce topological descriptors **by approximating the inner-algorithms** in a relevant way\" was somewhat in the minds, but this paper is the first one to do so in a convincing manner to the best of my knowledge. \n\n- [Clarity] The paper is well written an pleasant to read.\n\n-  [Significance] The approach developed in this work seems quite promising and may be a source of inspiration for future works in TDA, in particular showcasing the potential of GNN when it comes to extract topological information from data. \n\n- [Quality] I think this paper is of great quality overall. Aside from the interesting approach, I also like the experimental evaluation methodology which covers the important questions (i) Is our model capable of producing outputs similar to ground truth PDs (Sec. 5.1), (ii) are our approximated PDs still relevant as topological features in downstream tasks, (iii) what are the computational benefits/drawbacks of using this approach in terms of running time.  # Questions \n\n1. In line 271, it is said that the training loss was the $W_2$ metric rather than the L2 norm between some vectorization such as persistence images. Given that the $W_2$ requires to compute a matching to retrieve a gradient, doesn't it involve some computational burden in practice?\n2. (related to 1.) In line 49, it is said that \"The Wasserstein distance can be naturally decomposed into supervision loss for each edge (...) improve learning efficiency\" ; while I guess that I understand the underlying idea in an informal way, could you elaborate on this? In concrete terms, how is the training loss implemented and how does it benefits from the \"edge decomposition\" properties of your approach?\n3. Given that Persistence Images should be stable wrt the $W_2$ metric, isn't it somewhat \"redundant\" to use the PIE as a second evaluation metric? \n\n# Suggestions\n\n- The works _Learning persistent homology of 3D point clouds_ and _RipsNet: a general architecture for fast and robust estimation of the persistent homology of point clouds_ are also learning to produce topological features (on top of point cloud this times) and may be referenced along PI-Net and _Can Neural Networks learn persistent homology features_ in the introduction (line 38). I think the limitations of the work have been properly addressed (for instance the authors aknowledge that their approach is only beneficial on large graphs --- exact computation may be more efficient otherwise). \n\nI do not identify potential negative societal impact _specific to this work_. ",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "A paper on deep models to approximate a popular class of objects, extended persistence diagrams, in topological data analysis",
                "Sentiment Expression": "reasonably interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "This",
                "Sentiment Expression": "an important problem",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "The authors deliver on this",
                "Sentiment Expression": "introducing quite good results",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the proposed algorithms",
                "Sentiment Expression": "maintain a big speedup advantage",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "clearly fits well within the machine learning community",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "cZAi1yWpiXQ": {
        "paper_id": "iclr_2022_cZAi1yWpiXQ",
        "paper_title": "Adversarial Robustness Through the Lens of Causality",
        "paper_abstract": "The adversarial vulnerability of deep neural networks has attracted signi\ufb01cant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial attacks and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and define the adversarial distribution to formalize the intuition of adversarial attacks. From the causal perspective, we study the distinction between the natural and adversarial distribution and conclude that the origin of adversarial vulnerability is the focus of models on spurious correlations. Inspired by the causal understanding, we propose the \\emph{Causal}-inspired \\emph{Adv}ersarial distribution alignment method, CausalAdv, to eliminate the difference between natural and adversarial distributions by considering spurious correlations. Extensive experiments demonstrate the efficacy of the proposed method. Our work is the first attempt towards using causality to understand and mitigate the adversarial vulnerability.",
        "paper_acceptance": "Accept (Poster)",
        "meta_review": "The paper shows a causal perspective to the adversarial robustness problem. Based on a causal graph of the adversarial data creation process, which describes the perceived data as a function of content and style variables, the authors identified that the spurious correlation between style and label is the main reason for adversarial examples. Based on this observation, they propose a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Experiments on MNIST, CIFAR-10, and CIFAR-100 datasets show that the proposed method is better than two baselines, Madry and TRADES.\n\nOverall, the paper contains interesting ideas and tackles an important problem. Due to some concerns regarding the clarity and motivation of the paper, we strongly recommend the authors take the reviewers' comments to heart and incorporate their thoughts in preparing the camera-ready version of their manuscript.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "o5OVzCLDr9i",
                "writer": "official_reviewer",
                "reply_to": "ZFlxdJ1CdAJ",
                "title": "comments on the response",
                "comment": " this is an interesting experiment. thanks for adding this result. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "N6vBAghSoF5",
                "writer": "official_reviewer",
                "reply_to": "QHeDVwZx0BK",
                "title": "Thank you for your response",
                "comment": " Thank the authors for the response. Most of my concerns have been addressed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eUycyggQ1YO",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_cZAi1yWpiXQ",
                "title": "",
                "comment": "The work presents a causal perspective of adversarial attacks on image-based machine learning models by studying a causal graph of the adversarial data creation process and highlighting how such a process makes the learned models vulnerable. It argues that the main reason for adversarial vulnerability is the reliance of models on spurious correlations between labels and style. Accordingly, it proposes a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Empirically, the method is shown to be more robust than two baselines on three datasets. The work provides a plausible causal representation of the adversarial attack process and derives a useful explanation for vulnerability. The robust approach derived from their explanation is quite reasonable and is novel to my knowledge. It shows improvement on MNIST, CIFAR-10, and CIFAR-100 datasets. However, the confidence intervals are omitted which makes the results less convincing. \nThe choice of comparing against only two baselines, Madry and TRADES, can be better motivated, especially since many alternative methods exists. Consider comparing against the top performing method from RobustBench (https://robustbench.github.io/).\nScope of the analysis is not well defined. The types of adversarial attacks, types of classifiers, and types of data are not specified, and implicitly assumed to be image-based neural networks. \nThe description of the causal graph makes it hard to understand that two processes are being represented - the natural data distribution and its relation to the distribution of images constructed from adversarial attack for a given classifier(s). The robust learning method based on adversarial alignment is not explained well and involves choices that are unjustified. Statements on relationship of the proposed method to Madry et al. 2017 and Zhang et al. 2019 are imprecise.\n \n \n# Related work\n\nAdmittedly the literature on this problem area is vast, however, the following two related works will be important to discuss.\nA causal graph motivated adversarial learning approach is proposed in Heinze-Deml and Meinshausen 2021 (Conditional variance penalties and domain shift robustness. Machine Learning https://link.springer.com/article/10.1007/s10994-020-05924-1). The graph has core (content) and style nodes although it is different from the one proposed in this work.\nThe relationship between causal and adversarial approaches to robust learning was discussed in B\u00fchlmann 2020 (Invariance, Causality and Robustness. Statistical Science https://arxiv.org/abs/1812.08233). The discussion is for a different causal graph where X causes Y and different type of adversarial perturbations that are unbounded, however, the work is related as it also gives a causal interpretation to robust learning.\nAlthough non-causal in motivation, other robust learning work has also identified spurious correlation between labels and style features as reason for lack of generalizability. For example, see Sagawa et al. 2020 (Distributionally Robust Neural Networks for Group Shifts. ICLR https://arxiv.org/abs/1911.08731). Consider discussing this line of work into out-of-distribution generalization.\n \n \n# Questions that I would like the authors to respond to:\n \nWhat is the scope of domains for the proposed causal graph in Figure 1? Is it applicable to image or any high-dimensional data problems? Please specify the problems for which this causal graph is suitable. The graph makes unstated assumptions like Y is an effect not a cause of C (i.e. anti-causal learning) which should be motivated in context of some problem domains.\n\nWhy should the adversarial distribution be a result of an intervention on some causal graph, where the graph is shared with the one generating the natural distribution? This assumption does not seem necessary as the defintion of adversarial perturbation E_adv in Eq (2) does not rely on the causal graph. Please clarify if this is not a limiting assumption.\n\nWhat are type of adversarial attacks are in scope? This should be specified beforehand. The argument about origin of adversarial vulnerability relies on the assumption that conditional distribution of style P(s | X) does not vary much with adversarial attacks. However, some attacks change the image drastically such as ones that add image patches. \n \n \n# Questions below are minor and do not seriously affect my review:\n\nCan you test your hypothesis that spurious correlation between labels and style features is the reason for adversarial vulnesrabitliy such as by studying known attacks? This will provide more evidence to the analysis of the assumed causal graph.\n\nThe relation between the two causal models \\mathcal{M} and \\mathcal{M}_adv is not clear. Mention the specific hard or soft intervention that results in the latter one like do(E\\sim E_adv).\n\nOn the specific way of constructing adversarial distribution in Eq (2), the X and E are added instead of any other general function. Please state if this is a necessary assumption.\n\nCan you comment on the order of the standard deviations for numbers in Table 1, 2, 3? Ideally, these should be included in Appendix even if small.\n \n \n# Suggestions for improving the writing:\n\nThe statement claiming that objective in Eq. (7) is same as in Madry et al. 2017 for \\lambda and \\gamma equal to 0 is incorrect. The objective in Madry et al. solves a min-max problem maximising loss over perturbations of natural data points. Similarly, TRADES Zhang et al. 2019 also has a term containing maximum loss over perturbations.\n\nThe term nuisance factors is used in Sections 1,2 without defining it until later using the style features. Consider briefly mentioning an example of nuisance factors early on.\n\nThe term integrated representation s(X) in Eq (6) is not clear. Please give an example of such a function.\n\nI did not understand how Footnote 3 is an explanation for ignoring dependence between C and S. Even if the relation is not causal, it can be modelled as correlation exists based on the causal graph.\n \nRelationship with the work of Ilyas et al. 2019 on the origin of adversarial vulnerability can be moved to the main text from Appendix F as it is important to highlight what additional insights does a causal perspective provides. \n\nConsider mentioning the omitted explanation for change in P(Y | \\tilde{X}, s) since the path from S to Y given \\tilde{X} is open due to conditioning on a child of the collider X.\n\nSum notation in Eq (3) should clarify that \\mathbb{S} is discrete.\n \nThe remark from Gopnik et al. 2004 in the Introduction can be made more precise to the specific experimental conditions of that work instead of a claim for all of human cognition. The paper undertakes an original approach to studying the important problem of adversarial vulnerability. However, the description of the method, its design choices, and evaluation requires improvement. That said, the results are quite encouraging and authors should provide more evidence for their hypothesis on reasons for adversarial vulnerability and test their method against stronger baselines.\n\n---\nAfter the response\n\nMost of my concerns have been adequately addressed. The description of the method needs further clarification. I would encourage authors to clarify the choices made in Appendix A. Given that the approach to adversarial robustness is novel, empirically useful, and potentially will inspire further work, I have substantially improved the score and leaning towards a recommendation to Accept.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "eVGYHvM1l4",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "[Last three days reminder] Would you mind confirming if you have further questions? Thanks!",
                "comment": " Dear Reviewer aWQa,\n\nWe appreciate your comments and time! We have revised the paper following your suggestions. Would you mind checking it and confirming if you have further questions?\n\nBest Regards,\n\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "npzUr-Rtjt",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "[Last four days reminder] Would you mind confirming if you have further questions? Thanks!",
                "comment": " Dear Reviewer aWQa,\n\nWe appreciate your comments and time! We have revised the paper following your suggestions. Would you mind checking it and confirming if you have further questions?\n\nBest Regards,\n\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "l5_8V0To5aO",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "[Last five days reminder] Would you mind confirming if you have further questions? Thanks!",
                "comment": " Dear Reviewer aWQa,\n\nWe appreciate your comments and time! We have revised the paper following your suggestions. Would you mind checking it and confirming if you have further questions?\n\nBest Regards,\n\nAuthors\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_hJdp0XVC32",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "[Last six days reminder] Would you mind confirming if you have further questions? Thanks!",
                "comment": " Dear Reviewer aWQa,\n\nWe appreciate your comments and time! We have revised the paper following your suggestions. Would you mind checking it and confirming if you have further questions?\n\nBest Regards,\n\nAuthors\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "g5XBfJM2Y0",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "Would you mind confirming if you have further concerns? Thanks!",
                "comment": " Dear Reviewer aWQa,\n\nWe appreciate your comments and time! We have addressed all your questions with a summary and a detailed response below and revised the paper following your suggestion. Would you mind checking it and confirming if you have further questions?\n\nBest Regards,\n\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tM8qdkeZGLK",
                "writer": "author",
                "reply_to": "-Vrj5rXxuGx",
                "title": "Thank you very much",
                "comment": " Dear Reviewer iRoe,\n\nThank you very much for increasing the score. We are delighted to see that our response has addressed your concerns. Thanks again for your acceptance recommendation.\n\nBest Regards,\n\nAuthors",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DkwGjuHBwrT",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "Would you mind checking our response? Thanks! ",
                "comment": " Dear Reviewer aWQa,\n\nThanks for the comments! We have tried our best to address all the concerns and provided explanations to all questions. Here is a summary of our detailed response below. We humbly expect you can check it and confirm whether our response has addressed your concerns.\n\n- We have highlighted the scope of the proposed causal graph in our revision, i.e., we construct the causal graph for modeling the image data generation process and analyzing adversarial vulnerability.\n\n- We have explained why we regard the adversarial distribution as an intervention distribution.\n\n- We have highlighted which type of adversarial attacks is considered in our work.\n\n- We have revised the paper following your suggestions in both \u2018Questions below are minor and do not seriously affect my review\u2019 and \u2018Suggestions for improving the writing.\u2019\n\nThanks a lot for your feedback! Any further discussion/question is welcomed! Your support for a novel, simple, and initial attempt to think of adversarial vulnerability from a causal perspective is very important and we sincerely appreciate it!\n\nBest Regards,\n\nAuthors\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-Vrj5rXxuGx",
                "writer": "official_reviewer",
                "reply_to": "hbOXy0Omr1m",
                "title": "My concerns have been addressed",
                "comment": " The authors' excellent rebuttal addresses all my concerns and I have improved my score to 8.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2Vj8Ze_iit",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_cZAi1yWpiXQ",
                "title": "",
                "comment": "This paper presents a causal perspective on addressing adversarially vulnerability. It first constructs a causal graph, which then inspires the design of the distribution alignment method for reducing the gap between adversarial and natural data. Extensive experiments on CIFAR10, CIFAR100, and MNIST demonstrate the robustness of the proposed method against various attack methods. Strengths:\n* This paper provides a new perspective on analyzing adversarial robustness and it is novel for me.\n* This paper is well organized and easy to follow.\n* The distribution alignment method inspired by theory shows good robustness\n\nWeakness:\n* I noticed the Natural accuracy on CIFAR-10 and CIFAR-100 can sometimes be worse than TRADES and Madry. Given the trade-off between natural and adversarial accuracy is adjustable, it would be good to adjust the trade-off to see if the proposed method can simultaneously surpass TRADES in terms of both natural accuracy and robustness.\n* How to select the hyper-parameters $\\lambda$, $\\beta$ and $\\gamma$. Are they sensitive? Are they consistent among different datasets?\n\nDetail comments:\n* First paragraph of Page 9, \"we report robust accuracy of WRN-34-10 trained with CIFAR10 dataset on Auto-Attack, Madry: 49.58%, ADA-M: 51.56%, TRATDES: 52.46%\" TRATDES --> TRADES\n* Some \"Mardry\" should be \"Madry\" I enjoy reading the analysis of the paper. This new perspective is novel to me and I would tend to accept this paper. If the author can address my concerns on the experiments. I would be more convinced.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "ZFlxdJ1CdAJ",
                "writer": "author",
                "reply_to": "Y9rIuJ35pke",
                "title": "Response to reviewer 65Tx [3/3] ",
                "comment": " Q3: \u201cCan you provide examples of the kind of adversarial examples that ADA method is robust to, which prior work is not? That will be provide more intuition on what exactly is ADA method capturing. Right now, we see that the accuracy increases but not sure why.\u201d\n\nA3:\nThanks for your constructive suggestion! Inspired by your suggestion, we designed the following experiment to closely look at which kind of adversarial examples our method is robust to.\nTo further analyze the superiority of our method, we compare ADA-M and Madry to explore which kind of adversarial examples that ADA-M is more robust to. These analyses are based on the spurious perspective, as one main conclusion of our paper is that the origin of adversarial vulnerability is the excessive focus of DNNs on spurious correlations between labels and style variables. Specifically, we calculate the KL-divergence between $KL(P(Y\u2502h(x,s))||P(Y|h(x_adv,s)))$ for each input $x$, and divide samples into several (10 in our experiment) bins according to the KL-divergence. Then, we evaluate the robust accuracy of different models trained with ADA-M and Madry in each bin sample. We can see that ADA-M is more robust to samples leading to large KL-divergence than Madry. According to these empirical results, we can conclude that the proposed method is more robust to samples causing a significant difference between natural and adversarial distributions. \n\n|     KL       |     0.16    |     1.70    |     2.89    |     4.07    |     5.20    |     6.35    |     7.48    |     8.63    |     9.82    |     10.99    |     12.09    |\n|--------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|--------------|--------------|\n|     Madry    |     75.9    |     24.2    |     14.0    |     3.0     |     1.1     |     0.20    |     0.0     |     0.0     |     0.0     |     0.0      |     0.0      |\n|     ADA-M    |     73.5    |     38.2    |     30.8    |     24.7    |     18.9    |     17.3    |     12.0    |     10.6    |     7.4     |     1.8      |     0.0      |\n\nAgain, thanks for your constructive suggestion. We have revised our paper following your suggestions. Some changes are:\n\nSec. 3.2, Page 5: \u2018To make sure that\u2019.\n\nAppendix A.\n\nAppendix B, Table 5 and the corresponding explanation.\n\nAppendix F, Figure 3 and the corresponding explanation.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CwHAHGLhIi",
                "writer": "author",
                "reply_to": "Y9rIuJ35pke",
                "title": "Response to reviewer 65Tx [2/3]",
                "comment": " Q2: \u201cI do not see how Eqn 5 is derived. Are you using something similar to the triangle inequality for the divergences? In that case, will it be an upper bound. Also I do not think KL divergence supports triangle inequality. So this approximation step seems arbitrary--can you justify it? --Does it mean that g and h are the same models? Is there any justification for this choice, besides convenience?\u201d\n\nA2:\nThanks for pointing out the potentially confusing derivation of Eq. (5). We have added the following explanation to our revision.\nWithout loss of generality, the metric d in Eq. (4) can be realized as total variation distance (TVD). Thus, according to the definition of TVD, we have\n$d\\left(P\\left(Y|X\\right), P_{\\theta}\\left(Y|\\tilde{X}\\right)\\right) \\leq d\\left(P_{\\theta}\\left(Y|\\tilde{X}\\right), Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right) + d\\left(P\\left(Y|X\\right), Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right)$\n\nwhere $Q_{\\theta}(Y|X)$ is the conditional distribution specified by the classifier $h\\left(X;\\theta\\right)$.\nAccording to the Pinsker inequality, i.e., $d\\left(P,Q\\right) \\leq \\sqrt{\\frac{KL\\left(P||Q\\right)}{2}}$, where $KL$ is the Kullback-Leibler divergence, we have the following upper bound:\n\n$d\\left(P\\left(Y|X\\right), P_{\\theta}\\left(Y|\\tilde{X}\\right)\\right) \\leq \n\t\\sqrt{\\frac{KL\\left(P_{\\theta}\\left(Y|\\tilde{X}\\right)|| Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right)}{2}} + \n\t\\sqrt{\\frac{KL\\left(P\\left(Y|X\\right)|| Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right)}{2}}$\n\nNow, we can employ the upper bound as a surrogate loss. In practice, we can replace $\\sqrt{\\frac{KL\\left(P||Q\\right)}{2}}$ with $KL\\left(P||Q\\right)$. The intuition is that the difference between $\\sqrt{\\frac{KL\\left(P||Q\\right)}{2}}$ and $KL\\left(P||Q\\right)$ is relatively small when $KL\\left(P||Q\\right)$ is not extremely large, so the replacement will not introduce much difference. Thus, we have:\n$\n\tKL\\left(P_{\\theta}\\left(Y|\\tilde{X}\\right)|| Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right) \\nonumber + \n\t\\gamma KL\\left(P\\left(Y|X\\right) || Q_{\\theta}\\left(Y|X\\right)\\right),\n$\n\nwhere $\\gamma$ is a tunable hyperparameter. To verify the replacement will not introduce much difference, we evaluate the robustness of ADA-M and ADA-T on the CIFAR-10 dataset with two losses, i.e., $KL\\left(P||Q\\right)$ and $\\sqrt{\\frac{KL\\left(P||Q\\right)}{2}}$. The results evaluated on best checkpoints are shown in the following table. Here, * means that $\\sqrt{\\frac{KL\\left(P||Q\\right)}{2}}$ rather than  $KL\\left(P||Q\\right)$ is used for optimization.\n\n|               |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|---------------|----------------|--------------|--------------|--------------|\n|     Madry     |     83.56      |     56.69    |     51.92    |     51.00    |\n|     ADA-M     |     80.42      |     57.98    |     54.44    |     52.51    |\n|     ADA-M*    |     80.03      |     57.47    |     52.98    |     52.72    |\n|     TRADES    |     81.39      |     57.25    |     53.64    |     51.39    |\n|     ADA-T     |     81.22      |     58.97    |     54.55    |     52.95    |\n|     ADA-T*    |     79.94      |     58.46    |     54.12    |     52.28    |\n\nModel g is a neural network using $s$ to predict labels, and model $h$ is a neural network used for extracting features. Thus, $g$ and $h$ are different models. To ensure that model $g$ can contribute to the representation learning of model $h$, we apply model $g$ to the representation of model $h$. Simply setting $g$ to a linear model works well in our experiments. Following your suggestion, we compare different realizations of mode $g$, i.e., linear mapping and non-linear neural networks. The following results suggest that our method is robust to the selection of model $g$. We have added these results to our revision. Thanks for your valuable suggestion!\n\n|                           |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|---------------------------|----------------|--------------|--------------|--------------|\n|     Madry                 |     83.56      |     56.69    |     51.92    |     51.00    |\n|     ADA-M + Linear        |     80.68      |     57.18    |     53.36    |     51.41    |\n|     ADA-M + Non-linear    |     80.42      |     57.98    |     54.44    |     52.51    |\n|     TRADES                |     81.39      |     57.25    |     53.64    |     51.39    |\n|     ADA-T + Linear        |     80.31      |     58.43    |     54.31    |     52.25    |\n|     ADA-T + Non-linear    |     81.22      |     58.97    |     54.55    |     52.95    |\n\nThese results are evaluated on the best checkpoint models, which are adversarially trained on the CIFAR10 dataset.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Fp1Xv5m8_-X",
                "writer": "author",
                "reply_to": "Y9rIuJ35pke",
                "title": "Response to reviewer 65Tx [1/3]",
                "comment": " All authors are grateful for your time devoted to reviewing this paper and your constructive suggestions. Here are our detailed replies to your questions.\n\nQ1: \u201cWhile the theoretical justification makes sense, can you provide a (toy) empirical example to show how the learnt spurious correlation leads to adversarial examples, and not P(x|s)? I get the intuition, but the claim is vague. If you can provide one toy example supporting the claim, it will be stronger.\u201d\n\nA1:\nThanks for your valuable suggestion. In the context of imperceptible adversarial examples considered in this paper, neither P(x|s) nor P(s|x) leads to adversarial examples. The reasons are as follows, which has been added to our revision.\nIn this work, we focus on imperceptible adversarial examples, where drastic style changes hardly appear. Consequently, adversaries exploit the conditional association between Y and S. Considering that changing the style of samples, i.e., (P(s|x)), can make adversarial perturbations perceptible. Hence, adversaries need to identify perturbations where the conditional association between labels and styles is different from that of natural distributions.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DnytJ0Z4YEa",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "Response to reviewer aWQa [3/3]",
                "comment": " 4. Response to \u201cSuggestions for improving the writing\u201d\n\nQ4.1: \u201cThe statement claiming that objective in Eq. (7) is same as in Madry et al. 2017 for \\lambda and \\gamma equal to 0 is incorrect. The objective in Madry et al. solves a min-max problem maximising loss over perturbations of natural data points. Similarly, TRADES Zhang et al. 2019 also has a term containing maximum loss over perturbations.\u201d\n\nA4.1:\n$X+E_{adv}$ stands for adversarial example, which is obtained by maximizing loss over perturbation of natural data point, see Eq (2) and Sec. 3.2. Thus, our statement is correct.\n\nQ4.2: \u201cThe term nuisance factors is used in Sections 1,2 without defining it until later using the style features. Consider briefly mentioning an example of nuisance factors early on.\u201d\n\nQ4.2: \nWe give the explanation of nuisance factors when we first used this concept, see Sec. 1.\n\nQ4.3: \u201cThe term integrated representation s(X) in Eq (6) is not clear. Please give an example of such a function.\u201d\n\nA4.3:\nWe use a neural network to instantiate it in this paper, see Sec. 3.3.\n\nQ4.4: \u201cI did not understand how Footnote 3 is an explanation for ignoring dependence between C and S. Even if the relation is not causal, it can be modelled as correlation exists based on the causal graph.\u201d\n\nA4.4:\nWe aim to model the causal relationship between the content variable and the style variable and omit the spurious correlation between $\\hat{c}\\left(X\\right)$ and $\\hat{s}\\left(X\\right)$, although $\\hat{c}\\left(X\\right)$ and $\\hat{s}\\left(X\\right)$ are not statistically independent.\n\nQ4.5: \u201cRelationship with the work of Ilyas et al. 2019 on the origin of adversarial vulnerability can be moved to the main text from Appendix F as it is important to highlight what additional insights does a causal perspective provides.\u201d\n\nA4.5: Thanks for your suggestion. We have revised the paper according to your suggestion.\n\nQ4.6: \u201cConsider mentioning the omitted explanation for change in P(Y | \\tilde{X}, s) since the path from S to Y given \\tilde{X} is open due to conditioning on a child of the collider X.\u201d\n\nA4.6: Thanks for your suggestion.\n\nQ4.7: \u201cSum notation in Eq (3) should clarify that \\mathbb{S} is discrete.\u201d\n\nA4.7: Thanks for your valuable suggestion. Following your valuable suggestion, we have added it into our revision.\n\nQ4.8: \u201cThe remark from Gopnik et al. 2004 in the Introduction can be made more precise to the specific experimental conditions of that work instead of a claim for all of human cognition.\u201d\n\nA4.8: Thanks for your suggestion.\n\n[1] Conditional variance penalties and domain shift robustness. Machine Learning, 2021.\n\n[2] Invariance, Causality and Robustness. Statistical Science, 2020.\n\n[3] Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. ICLR, 2020.\n\n[4] On causal and anticausal learning. ICML, 2012.\n\n[5] Inference of cause and effect with unsupervised inverse regression. AISTATS, 2015.\n\n[6] A Simple Framework for Contrastive Learning of Visual Representations. ICML, 2020.\n\n[7] Unlabeled data improves adversarial robustness. NeurIPS, 2019.\n\n[8] Adversarial patch. NIPS, 2017.\n\n[9] One Pixel Attack for Fooling Deep Neural Networks. IEEE Transactions on Evolutionary Computation.\n\nWe have revised our paper following your suggestions. Some changes are:\n\nAppendix H, the second paragraph: ``A recent work \u2026 \u2019\u2019.\n\nSec. 2, the third paragraph: `` Specifically, we construct a causal graph \u2026 \u2019\u2019.\n\nSec. 3.1, the second paragraph: `` where we assume \u2026 \u2019\u2019.\n\nSec. 3.1, the last paragraph: `` Hence, the origin of adversarial vulnerability \u2026 \u2019\u2019.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cvjUXImETT",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "Response to reviewer aWQa [2/3]",
                "comment": " 3. Response to \u201cQuestions below are minor and do not seriously affect my review\u201d\n\nQ3.1: \u201cCan you test your hypothesis that spurious correlation between labels and style features is the reason for adversarial vulnesrabitliy such as by studying known attacks? This will provide more evidence to the analysis of the assumed causal graph.\u201d\n\nA3.1:\nThe spurious correlation is the key to understanding and mitigating adversarial venerability, which is the conclusion drawn from the constructed causal graph, see Sec. 3.1. In addition, our experiments can verify the effectiveness of considering the spurious correlation when designing a robust model, see Sec 4. \n\nQ3.2: \u201cThe relation between the two causal models \\mathcal{M} and \\mathcal{M}_adv is not clear. Mention the specific hard or soft intervention that results in the latter one like do(E\\sim E_adv).\u201d\n\nA3.2:\n$\\mathcal{M}$ is the mechanism of generating perturbation $E$. $\\mathcal{M}_{adv}$ is used for modifying $\\mathcal{M}$ to make the generated perturbation be adversarial.\n\nQ3.3: \u201cOn the specific way of constructing adversarial distribution in Eq (2), the X and E are added instead of any other general function. Please state if this is a necessary assumption.\u201d\n\nA3.3:\nFollowing the previous suggestions of the reviewer, i.e., Q2.1, Q2.2, and Q2.3, we are prone to specify the function in our revision. That is, we employ a specific approach to construct adversarial distribution, but we also highlight that the mechanism $\\mathcal{M}_{adv}$ can be instantiated by any functions that can generate adversarial examples.\n\nQ3.4: \u201cCan you comment on the order of the standard deviations for numbers in Table 1, 2, 3? Ideally, these should be included in Appendix even if small.\u201d\n\nA3.4:\nThanks for your valuable suggestion. Following your suggestion, we have added these results to our revision.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BeZoxgIV17e",
                "writer": "author",
                "reply_to": "eUycyggQ1YO",
                "title": "Response to reviewer aWQa [1/3]",
                "comment": " We appreciate your time and suggestions! Here are our detailed replies to your questions.\n\n1. Response to \u201cRelated work\u201d\n\nQ1: \u201cAdmittedly the literature on this problem area is vast, however, the following two related works [1, 2] will be important to discuss.\u201d\n\nA1:\nWe thank the reviewer for pointing out these valuable works. As pointed by the reviewer that the literature on this problem area is vast, merely discussing the most related papers may inadvertently miss some valuable work. We have added the following discussion into our revision. \n\n\u201cOne related work is CORE [1], a regularizer inspired by a causal graph is proposed to make models robust against domain shift. Specifically, [1] proposes minimizing the variance of prediction and loss conditions on label and ID information. Different from [1], our method is designed to eliminate the difference between the natural and adversarial distributions. A recent work [2] aims to connect robust learning and causality, but the main focus of [2] is on out-of-distribution generalization, which is different from adversarial learning. Another related work is [3], where spurious correlations are considered. Specifically, [3] proposes using prior knowledge to group the training data to avoid the reliance on spurious correlations. Difference from [3], our method does not rely on prior knowledge.\u201d\n\nThe problem of out-of-distribution (OOD) generalization is different from the adversarial robustness considered in our paper. This is because the test phase environment cannot be observed under the OOD setting, but in the literature of adversarial robustness, adversarial examples can be observed in the training phase. Therefore, we omit the discussion of this line of work.\n\n2. Response to \u201cQuestions that I would like the authors to respond to\u201d\n\nQ2.1: \u201cWhat is the scope of domains for the proposed causal graph in Figure 1? Is it applicable to image or any high-dimensional data problems? Please specify the problems for which this causal graph is suitable. The graph makes unstated assumptions like Y is an effect not a cause of C (i.e. anti-causal learning) which should be motivated in context of some problem domains.\u201d\n\nA2.1:\n - The causal graph is constructed for analyzing adversarial vulnerability. \n - We assume that the perceived data is generated from content and style variables when constructing the causal graph. Although no further assumptions are considered, only image datasets are used for evaluating the proposed method, so the graph is applicable to image data.\n - Considering the widely observed phenomenon that unlabeled data is helpful for learning, we can assume that the relationship between images and labels is anti-causal. We refer the reviewer to [4,5,6,7] for detailed theoretical and empirical supports.\n\nWe have highlighted the scope of the graph in our revision: The causal graph is constructed for modeling the image data generation process and analyzing adversarial vulnerability.\n\n Q2.2: \u201cWhy should the adversarial distribution be a result of an intervention on some causal graph, where the graph is shared with the one generating the natural distribution? This assumption does not seem necessary as the definition of adversarial perturbation E_adv in Eq (2) does not rely on the causal graph. Please clarify if this is not a limiting assumption.\u201d\n\nA2.2:\n - Our paper aims to connect causality with adversarial vulnerability, and we give a perspective, i.e., considering the difference between adversarial and natural distributions. Thus, we stated in the paper that: \u201cIn the context of adversarial learning, we desire the causal graph by which both the natural and the adversarial distributions can be generated\u201d. Consequently, we construct a causal graph, which can generate both the adversarial and the natural distribution.\n - We model the generation process of $E_{adv}$ by writing $E_{adv}$ as a mechanism $\\mathcal{M}_{adv}$ of $X$, $Y$, and $\\theta$, i.e., Eq. (2), which is consistent with the constructed causal graph.\n\nQ2.3: \u201cWhat are type of adversarial attacks are in scope? This should be specified beforehand. The argument about origin of adversarial vulnerability relies on the assumption that conditional distribution of style P(s| X) does not vary much with adversarial attacks. However, some attacks change the image drastically such as ones that add image patches.\u201d\n\nA2.3:\n - Thanks for pointing out the potentially confusing unstated assumption. We have highlighted the considered adversarial attacks as follows.\n\u201cIn this paper, we merely consider the imperceptible adversarial attacks, i.e, the widely studied \\ell_{p} norm bounded adversarial examples.\u201d\n - As mentioned by the reviewer, it is interesting to explore the connection between causality and more general adversarial attacks, e.g., adversarial patch [8] and pixel attack [9]. We leave it as our future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sm1uU0tuWJ4",
                "writer": "author",
                "reply_to": "2Vj8Ze_iit",
                "title": "Response to reviewer iRoe [3/3]",
                "comment": " Q3: \u201cDetail comments: First paragraph of Page 9: TRATDES --> TRADES. Some \"Mardry\" should be \"Madry\"\u201d\n\nA3: Thank you for your patience and careful attention to pointing out these typos. We have fixed all of these typos in our revision.\n\nFollowing your constructive suggestion, we have revised our paper. Some changes are:\n\nAppendix D, Training details.\n\nAppendix F, Figure 8, and Table 9 and their corresponding explanation.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wUDkdZEzTv",
                "writer": "author",
                "reply_to": "2Vj8Ze_iit",
                "title": "Response to reviewer iRoe [2/3]",
                "comment": " Q2: \u201cHow to select the hyper-parameters \\lambda, \\beta and \\gamma. Are they sensitive? Are they consistent among different datasets?\u201d\n\nA2:\nThanks for your valuable questions. We have added the following settings into our revision.\n - Consideration of consistency:\nIn all of our experiments, $\\beta$ is set to 1.0. For ADA-M, we set $\\gamma=0$, and $\\lambda$ is selected as follows:\nIn our experiments for CIFAR-10, $\\lambda$ is set to 1.0, and $\\lambda$ is set to 0.5 for CIFAR-100. For ADA-T, similar to TRADES, we set $\\gamma=1/6$, and $\\lambda$ is selected as follows: In our experiments for CIFAR-10, $\\lambda$ is set to 0.5 and $\\lambda$ is set to 1.0 for CIFAR-100.\n\n - Consideration of sensitivity:\nIn addition, following your suggestion, we evaluate the sensitivity of our method on these parameters on CIFAR-10 and CIFAR-100 datasets. The following results have been added to our revision.\n\nEvaluating the sensitivity of ADA-M on the CIFAR-10 dataset.\n\n|     CIFAR10             |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|-------------------------|----------------|--------------|--------------|--------------|\n|     Madry               |     83.56      |     56.69    |     51.92    |     51.00    |\n|     ADA-M $\\lambda=0.2$    |     82.17      |     57.13    |     53.28    |     51.80    |\n|     ADA-M $\\lambda=0.5$    |     80.83      |     57.28    |     53.65    |     51.97    |\n|     ADA-M $\\lambda=1.0$    |     80.42      |     57.98    |     54.44    |     52.51    |\n|     ADA-M $\\lambda=1.5$    |     80.17      |     57.53    |     53.24    |     51.40    |\n|     ADA-M $\\lambda=2.0$    |     80.35      |     58.29    |     53.16    |     52.71    |\n\nEvaluating the sensitivity of ADA-T on the CIFAR-10 dataset.\n\n|     CIFAR10              |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|--------------------------|----------------|--------------|--------------|--------------|\n|     TRADES               |     81.39      |     57.25    |     53.64    |     51.39    |\n|     ADA-T $\\lambda=0.2$    |     81.72      |     58.26    |     54.06    |     51.90    |\n|     ADA-T $\\lambda=0.5$       |     81.22      |     58.97    |     54.55    |     52.95    |\n|     ADA-T $\\lambda=1.0 $      |     79.65      |     58.48    |     54.45    |     52.89    |\n|     ADA-T $\\lambda=1.5 $      |     78.09      |     57.42    |     53.66    |     51.38    |\n|     ADA-T $\\lambda=2.0 $     |     74.42      |     55.29    |     52.07    |     50.04    |\n\nEvaluating the sensitivity of ADA-M on the CIFAR-100 dataset.\n\n|     CIFAR100            |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|-------------------------|----------------|--------------|--------------|--------------|\n|     Madry               |     55.98      |     28.39    |     25.15    |     24.04    |\n|     ADA-M $\\lambda=0.2$    |     54.68      |     29.85    |     27.32    |     25.88    |\n|     ADA-M $\\lambda=0.5$    |     54.07      |     29.76    |     27.62    |     25.44    |\n|     ADA-M $\\lambda=1.0$    |     52.69      |     29.77    |     27.80    |     26.04    |\n|     ADA-M $\\lambda=1.5$    |     52.37      |     30.74    |     28.49    |     26.71    |\n|     ADA-M $\\lambda=2.0$    |     48.40      |     30.07    |     28.52    |     25.97    |\n\nEvaluating the sensitivity of ADA-T on the CIFAR-100 dataset.\n\n|     CIFAR100          |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|-----------------------|----------------|--------------|--------------|--------------|\n|     TRADES            |     53.85      |     29.04    |     27.91    |     24.09    |\n|     ADA-T $\\lambda=0.2$    |     53.83      |     29.76    |     28.05    |     24.49    |\n|     ADA-T $\\lambda=0.5 $   |     53.91      |     30.19    |     28.11    |     24.90    |\n|     ADA-T $\\lambda=1.0$    |     53.17      |     30.66    |     28.57    |     25.74    |\n|     ADA-T $\\lambda=1.5 $   |     52.22      |     30.49    |     28.17    |     25.48    |\n|     ADA-T $\\lambda=2.0 $   |     50.40      |     30.22    |     28.14    |     25.49    |\n\nThanks for your constructive suggestion, all authors believe that your valuable comments would further improve our paper.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hbOXy0Omr1m",
                "writer": "author",
                "reply_to": "2Vj8Ze_iit",
                "title": "Response to reviewer iRoe [1/3]",
                "comment": " All authors are grateful for your time devoted to reviewing this paper and your constructive suggestions. Here are our detailed replies to your questions.\n\nQ1: \u201cGiven the trade-off between natural and adversarial accuracy is adjustable, it would be good to adjust the trade-off to see if the proposed method can simultaneously surpass TRADES in terms of both natural accuracy and robustness.\u201d\n\nA1:\nThanks for your constructive suggestion! Following your suggestion, we compare ADAT with TRADES on CIAFR10 and CIFAR100, where the robustness is evaluated on the best checkpoint model. The results are as follows, and the corresponding discussion has been added to our revision.\n\nComparison between ADA-T and TRADES on the CIFAR-10 dataset.\n\n|     CIFAR10    |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|----------------|----------------|--------------|--------------|--------------|\n|     TRADES     |     81.39      |     57.25    |     53.64    |     51.39    |\n|     ADA-T      |     81.72      |     58.26    |     54.06    |     51.90    |\n\nComparison between ADA-T and TRADES on the CIFAR-100 dataset.\n\n|     CIFAR100    |     Natural    |     FGSM     |     PGD20    |     CW20     |\n|-----------------|----------------|--------------|--------------|--------------|\n|     TRADES      |     53.85      |     29.04    |     27.91    |     24.09    |\n|     ADA-T       |     53.91      |     30.19    |     28.11    |     24.90    |\n\nWe can see that, compare to TRADES, the proposed ADAT can improve both the natural and adversarial accuracy, which is consistent with your intuition. These results further demonstrate the necessity of considering the spurious correlation when developing robust models.\nAll the authors believe that the solid suggestion would further improve our paper.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QHeDVwZx0BK",
                "writer": "author",
                "reply_to": "e6LxAjFM69r",
                "title": "Response to reviewer Jqu4",
                "comment": " All authors are grateful for your time devoted to reviewing this paper and your constructive suggestions. Here are our detailed replies to your questions.\n\nQ1: \u201cIn experiments, the current version only compares with SOTAs using attacks with 20 iterations. It would be helpful if attacks with more iterations are employed as the performance of some defense techniques is shown to drop significantly when the number of attack iterations increases.\u201d\n\nA1:\nThanks for your valuable suggestion. We evaluate the robustness of the best checkpoint model trained with ADAM on the cifar10 dataset, using PGD and CW attacks with up to 1000 iterations. We can see that the robust accuracy decreases as increasing the iteration, but the robust accuracy of our method under PGD-1000 and CW-1000 attacks is still higher than Madry under the PGD-20 attack, as the robust accuracy of Madry under PGD-20 and CW-20 attacks are 51.92% and 51.00%.\n\n|              |     K=20     |     K=50     |     K=100    |     K=500    |     K=1000    |\n|--------------|--------------|--------------|--------------|--------------|---------------|\n|     PGD-K    |     54.44    |     53.12    |     53.07    |     52.98    |     52.98     |\n|     CW-K     |     52.97    |     51.63    |     51.50    |     51.48    |     51.48     |\n\nQ2: \"why optimizing the anchor can push adversarial distributions to natural distribution.\"\n \nA2:\nThanks for pointing out the potentially confusing details. We have highlighted the following explanation in our revision.\n\nHere, we give an intuitive explanation of why optimizing the anchor can make the adversarial distribution similar to the natural distribution. Considering that both the adversarial distribution $P_{\\theta}\\left(Y|\\tilde{X}\\right)$ and the anchor $Q_{\\theta}\\left(Y|\\tilde{X}\\right)$ are parameterized by $\\theta$, so optimizing $\\theta$ to minimize $KL\\left(P_{\\theta}\\left(Y|\\tilde{X}\\right)||Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right)$ can force the adversarial distribution be similar to the distribution specified by the classifier $h\\left(X;\\theta\\right)$, i.e., $Q_{\\theta}\\left(Y|\\tilde{X}\\right)$. That is, optimizing $Q_{\\theta}\\left(Y|\\tilde{X}\\right)$ will also change the adversarial distribution $P_{\\theta}\\left(Y|\\tilde{X}\\right)$. In addition, minimizing the divergence $KL\\left(P\\left(Y|X\\right)||Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right)$ can endow the classifier with the ability to provide a good prediction performance. Thus, minimizing $KL\\left(P_{\\theta}\\left(Y|\\tilde{X}\\right)||Q_{\\theta}\\left(Y|\\tilde{X}\\right)\\right)$ will make the adversarial distribution be similar with the natural distribution.\n\nQ3: \u201cHow to connect g and h by sharing the same representation? It seems that g is a linear function while h is supposed to be parameterized by a neural network.\u201d\n\nA3:\nThanks for your valuable question. Your explanation is completely correct, Specifically, $h$ is parameterized by a neural network, where the extracted representation is $r$. And, $g$ is a linear (or non-linear) classifier with $r$ as its input.\n\nQ4: \u201cIn Eq 8, why $\\hat{s}\\left(X\\right)$ can approximate $s\\left(X\\right)$? It seems that there is no supervision signal to ensure this point.\u201d\n\nA4:\nThanks for your valuable question. To approximate $s(X)$, we employ the conclusion drawn from our causal graph. Specifically, we use the independence between the content variable and the style variable as the supervision to approximate $s(X)$. This is because the content variable is independent of the style variable. Consequently, given the representation space of content variables, we can identify a corresponding sub-space to sample style variables, where all samples in such sup-space are independent of content variables.\n\nQ5: \u201cIf $\\hat{s}\\left(X\\right)$ is the integrated representation of both $s$ and $X$, as mentioned in the paper, are $\\hat{c}\\left(X\\right)$\n and $\\hat{s}\\left(X\\right)$ supposed to be independent when $X$ is given?\u201d\n\nA5:\nThanks for pointing out this potentially confusing point. We have added the following explanation to our revision.\n\nThe underlying intuition is that we aim to model the causal relation between the content variable and the style variable and omit the spurious correlation between $\\hat{c}\\left(X\\right)$ and $\\hat{s}\\left(X\\right)$, because the spurious correlation is not a causal relation, although $\\hat{c}\\left(X\\right)$ and $\\hat{s}\\left(X\\right)$ are not statistically independent.\n\nFollowing your constructive suggestion, we have revised our paper. Some changes are:\n\nAppendix A, the second paragraph: ``Here, we give an intuitive explanation of why \u2026 \u2018\u2019.\n\nSec. 3.3, the last paragraph: ``The underlying intuition is that \u2026 \u2019\u2019.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Y9rIuJ35pke",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_cZAi1yWpiXQ",
                "title": "",
                "comment": "The paper shows a causal perspective to the adversarial robustness problem. It creates a graph over content and style variable sets. It identifies the spurious correlation between style and label as the main reason for adversarial examples, and then proposes a method to remove it from the trained model. Experiments on three datasets show that the proposed method is better than two baselines.  The paper has a lot of strengths:\n* While the relationship between causality and adversarial robustness is often stated, this is the first paper I've seen that formally characterizes it. I like the simple separation of content and style variables that allows this characterization. \n* The paper derives a method based on the graph criterion. I am not sure about all the assumptions used here (e.g., independent gaussians), but at least on the benchmarks reported, the method performs well.\n* The authors provide an interpretation of past methods within their framework, which I thought was a nice unifying insight. \n\nOverall, the strength of this paper is in the formulation of adversarial examples as a causal problem. This provides a different view and motivates better methods for modeling the style-based spurious correlations. My feedback for the authors is:\n* Section 3.1: While the theoretical justification makes sense, can you provide a (toy) empirical example to show how the learnt spurious correlation leads to adversarial examples, and not P(x|s)? I get the intuition, but the claim is vague. If you can provide one toy example supporting the claim, it will be stronger.\n* Section 3.2: \n --I do not see how Eqn 5 is derived. Are you using something similar to the triangle inequality for the divergences? In that case, will it be an upper bound. Also I do not think KL divergence supports triangle inequality. So this approximation step seems arbitrary--can you justify it?\n --Does it mean that g and h are the same models? Is there any justification for this choice, besides convenience?\n* Section 4.2: Can you provide examples of the kind of adversarial examples that ADA method is robust to, which prior work is not? That will be provide more intuition on what exactly is ADA method capturing. Right now, we see that the accuracy increases but not sure why.\n\n Great paper connecting causality and adversarial robustness",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "e6LxAjFM69r",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_cZAi1yWpiXQ",
                "title": "",
                "comment": "This paper proposes a causal graph to model the generation process of adversarial attacks. Based on the proposed causal graph, the authors identify the origin of adversarial vulnerability as the spurious correlation between style variable and class label. Under the adversarial distribution, such spurious correlation can be maliciously used to mislead a victim model. In this light, the authors propose a method to align the adversarial distribution and the natural distribution to prevent a model from learning spurious correlation. The proposed method is empirically validated on prevailing datasets under several attacks. Strengths\n1.\tOverall the paper is well written and easy to follow.\n2.\tThe proposed causal graph well models the generation process of adversarial attacks and sheds new light on how to understand and defend against attacks from a causal perspective.\n3.\tThe proposed approach is novel and well-motivated. \n4. The authors empirically validate the proposed method using several attacks on prevailing datasets.\n\nWeaknesses\n1.\tIn experiments, the current version only compares with SOTAs using attacks with 20 iterations. It would be helpful if attacks with more iterations are employed as the performance of some defense techniques is shown to drop significantly when the number of attack iterations increases.\n2.\tSome technical details may need to be clarified. See my questions below.\n\nQuestions\n 1. In the first term of Eq 4, the authors aim to minimize the divergence between $P(Y|X)$ and $P_{\\theta}(Y|\\widetilde{X})$. As  $P_{\\theta}(Y|\\widetilde{X})$ cannot be formulated analytically, the authors introduce $Q_{\\theta}$ as an anchor and minimize $KL(P_{\\theta}(Y|\\widetilde{X})||Q_{\\theta}(Y|\\widetilde{X}))$ and $KL(P(Y|X)||Q_{\\theta}(Y|X))$ instead. However, from my perspective this objective only optimize the anchor instead of pushing $P_{\\theta}(Y|\\widetilde{X})$ towards $P(Y|X)$, if I am not mistaken.\n\n2. How to connect $g$ and $h$ by sharing the same representation?  It seems that $g$ is a linear function while $h$ is supposed to be parameterized by a neural network.\n\n3. In Eq 8, why $\\hat{s}(X)$ can approximate $s(X)$? It seems that there is no supervision signal to ensure this point.\n\n4. If $\\hat{s}(X)$ is the integrated representation of both $s$ and $x$, as mentioned in the paper, are $\\hat{c}(X)$ and $\\hat{s}(X)$ supposed to be independent when $x$ is given?\n\nIt would be helpful if these questions can be addressed.\n Given the strengths listed, I tend to accept this paper. I suggest the authors concern the questions listed to be addressed.",
                "rating": 8,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "contains interesting ideas and tackles an important problem",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the clarity and motivation of the paper",
                "Sentiment Expression": "some concerns",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "B1grSREtDH": {
        "paper_id": "iclr_2020_B1grSREtDH",
        "paper_title": "Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts",
        "paper_abstract": "Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as a Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. We propose a scalable solution that builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble's recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods.",
        "paper_acceptance": "reject",
        "meta_review": "This paper constitutes interesting progress on an important topic; the reviewers identify certain improvements and directions for future work (see in particular the updates from AnonReviewer1), and I urge the authors to continue to develop refinements and extensions.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "BylbDBJaYH",
                "reply_to": "iclr_2020_B1grSREtDH",
                "title": "Official Blind Review #1",
                "comment": "This paper considers Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). The authors consider making decisions with experts, where each expert performs well under some latent MDPs. An ensemble of experts is constructed, and then a Bayesian residual policy is learned to balance exploration-exploitation tradeoff. Experiments on Maze and Door show the advantages of residual policy learning over some baselines.\n\n1. The Bayesian Reinforcement Learning problem this work considered is important. However, using experts immediately make the problem much easier. The original Bayesian Reinforcement Learning problem is then reduced to making decision with experts. Under this setting, there are many existing work with respect to exploration-exploitation tradeoff (OFU, Thompson Sampling) with theoretical guarantees. I did not see why using this residual policy learning (although as mentioned residual/boosting is useful under other settings) is reasonable here. There is not theoretical support showing that residual learning enjoys guaranteed performance. The motivation of introducing this heuristic is not clear.\n\n2. The comparisons with UPMLE and BPO seems not convincing. Both BPO and UPMLE do not use experts, and ensemble of experts outperforms them as shown in the experiments. And the ensemble baseline here is kind of weak (why sensing with probability 0.5 at each timestep?) Always 0.5 does not make sense (exploration should decrease as uncertainty reduced). Other exploration methods should be compared, to empirically show the advantages/necessities of residual policy learning.\n\nOverall, I consider the proposed BRPO a simple extension of BPO, with a heuristic of learning ensemble policy to make decisions. BRPO is lack of theoretical support, and it is not clear why residual policy learning here is necessary and what exactly the advantage is over other exploration methods. Comparisons with simple baseline like exploration with constant probability is not enough to justify the proposed method.\n\n=====Update=====\nThanks for the rebuttal. The comparison with PSRL improves the paper. However, I still think this paper needs more improvement as follows.\nTheorem 1 looks hasty to me. Batch policy optimization Alg is going to solve n_{sample} MDPs, which are generated from P_0. But Eq. (6) or Theorem 1 does not contain information about P_0, implying that P_0 has no impact, which is questionable (an uniform P_0 that can generate different MDPs and a deterministic P_0 can only generate one MDP should be very different). I suggest the authors do more detailed analysis.\nOn the other hand, I expected whether this special \"residual action\" heuristic has any guarantees in RL? Can decomposing action into a_r + a_e provide us a better exploration method (than others like PSRL, OFU...)? Since this is the main idea of this paper as an extension of BPO, I think this point is important. The experiments shows that it can work in some cases, but I do not see an explanation (the \"residual learning\" paragraph is high level and I do not get an insight from that.).",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BJl0Yoo2jr",
                "reply_to": "iclr_2020_B1grSREtDH",
                "title": "Reponse",
                "comment": "We\u2019d like to thank all reviewers for their thoughtful comments and feedback. We have updated our paper to address the questions and comments. Specific comments have been added to respond directly to each reviewer.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1xDuso3or",
                "reply_to": "BygdF6XLqH",
                "title": "Response",
                "comment": "Thank you for your thoughtful comments and feedback. We have updated our paper to address your questions and comments. Here\u2019s our summary.\n\n1. No theoretical support\nWe have updated our paper to highlight the theoretical contribution and to compare with other approaches that yield different theoretical guarantees. Please see the comments (1) and (3) under Reviewer 1 and the corresponding updated sections (Section 4.2, the Appendix).\n\n2. More experiments\nWe have added new experiments to compare against PSRL, as well as a more effective baseline ensemble policy as per Reviewer 1\u2019s comment. We have run another experiment to handle continuous latent parameters, as pointed out by Reviewer 2. Please check the added Appendix, as well as our response (2) to Reviewer 1 and response (1) to Reviewer 2.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1l0Vos3iS",
                "reply_to": "HJxPLs_qcr",
                "title": "Reponse",
                "comment": "Thank you for your thoughtful comments and feedback. We have updated our paper to address your questions and comments. Here\u2019s our summary.\n\n1. [Handling continuous latent parameters] \nThe maze tasks in the submission consider only a finite number of latent goals. This is analogous to settings in which we have strong structural priors about where to search for the target, so the belief is represented as a categorical distribution over those candidates. However, our algorithm is not limited to such settings. We have run another experiment in which the goal is continuous and can be anywhere in the maze. The goal is tracked with an EKF, with mean and covariance. The expert recommendation is a motion planner path to the EKF\u2019s mean goal. The BRPO agent results in average performance of 467.2, significantly outperforming BPO (152.7) or UPMLE (124.3). However, in this case, the BRPO agent does not improve significantly above the ensemble policy, which we believe is because it tracks only a unimodal belief. We plan to include experiments with multimodal belief representations, such as Gaussian Mixture Models, in our final submission.\n\n2. The agent only receives noisy L2 distance to the goal. We intentionally constrained the observation because otherwise the goal becomes too obvious. This is a common setup in similar tasks (e.g. LightDark) explored in previous work [1].\n\n3. The problem setup for Maze tasks is motivated by classical POMDP problems in discrete state-action spaces, such as Tiger [2] and RockSample [3]. Much like Maze tasks, these tasks also have a high reward for correct goal and a high penalty for incorrect goals, and low sensing costs. RockSample requires long-horizon navigation to get to the goals, which we also adopted.\n\n\n4. \u03c6 is a latent parameter that drives the transition or reward functions. For a robot system, that could be uncertain parameters such as friction coefficients or joint damping parameters, or the mass of unknown objects being manipulated. For the Maze problems, it corresponds to the true location of the goal (cheese).\n\n[1] Robert Platt, Russ Tedrake, Leslie Pack Kaelbling, and Tomas Lozano-Perez. Belief space planning assuming maximum likelihood observations. In Robotics: Science and Systems, 2010.\n[2] Leslie Pack Kaelbling, Michael Littman, and Anthony Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99\u2013134, 1998.\n[3] Trey Smith and Reid Simmons. Heuristic search value iteration for pomdps. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 520\u2013527. AUAI Press, 2004.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1xvIqjnjH",
                "reply_to": "BylbDBJaYH",
                "title": "Response ",
                "comment": "Thank you for your thoughtful comments and feedback. We have updated our paper to address your questions and comments. Here\u2019s our summary.\n\n1. What is the advantage of BRPO over Thompson Sampling (PSRL)?\t\n\nWe have added Section 3.1 and Appendices 6.1 and 6.3 to discuss the distinction. Those sections are repeated below for convenience.\n\nPosterior Sampling Reinforcement Learning (PSRL) [1] is an online RL algorithm that maintains a posterior over latent MDP parameters \u03c6. However, the problem setting it considers and how it uses this posterior are quite different than what we consider in this paper.\t\n\nIn this work, we are focused on zero-shot scenarios where the agent can only interact with the test MDP for a single episode; latent parameters are resampled for each episode. The PSRL regret analysis assumes MDPs with finite horizons and repeated episodes with the same test MDP, i.e. the latent parameters are fixed for all episodes.\n\t\t\t\t\t\nBefore each episode, PSRL samples an MDP from its posterior over MDPs, computes the optimal policy for the sampled MDP, and executes it on the fixed test MDP. Its posterior is updated after each episode, concentrating the distribution around the true latent parameters. During this exploration period, it can perform arbitrarily poorly (see Section 6.1 of the appendix for a concrete example). Furthermore, sampling a latent MDP from the posterior determinizes the parameters; as a result, there is no uncertainty in the sampled MDP, and the resulting optimal policies that are executed will never take sensing actions. In this work, we have focused on Bayesian RL problems where sensing is critical to performance. BRPO, like other Bayesian RL algorithms, focuses on learning the Bayes-optimal policy during training, which can be used at test time to immediately explore and exploit in a new environment. \n\t\t\t\t\nWe have run additional experiments to compare with PSRL. To make it handle the zero-shot scenario, we made PRSL sample from the posterior at every timestep and execute the corresponding optimal expert (aware of the penalties) . Since PSRL does not induce sensing, vanilla PSRL agent results in -124.4 \u00b1 11.3, as it suffers from a large number of penalties for reaching incorrect goals. When we run PSRL with sensing probability 0.5, this results in 464.1 \u00b1 5.5, with task completion rate of 94%. The task incompletion comes from belief occasionally not collapsing to one target. In comparison, our agent achieves an average return of 465.65 \u00b1 4.35, completing 100% of the episodes, even with suboptimal experts unaware of the penalties.\n\n2. Why use baseline with sensing probability 0.5? Other exploration methods should be compared.\t\t\nWe have added Appendix 6.2 to include results with a different exploration method, and duplicated those results here for convenience.\n\nThe ensemble we considered in Section 5 randomly senses with probability 0.5. A more effective sensing ensemble baseline policy could be designed manually, and used as the initial policy for the BRPO agent to improve on. Designing such a policy can be challenging: it requires either task-specific knowledge, or solving an approximate Bayesian RL problem. We bypass these requirements by using BRPO.\n\t\t\t\t\t\nOn the Maze10 environment, we have found via offline tuning that a more effective ensemble baseline agent senses only for the first 150 of 750 timesteps. The average return is 416.3 \u00b1 9.4, which outperforms the original ensemble baseline average return of 409.5 \u00b1 10.8. However, this is still lower than the BRPO agent that starts with that original ensemble, which accumulated an average return of 465.7 \u00b1 4.7. This trained BRPO agent also achieves a task completion rate of 100%, which is better than the 96.3% completed by the improved ensemble baseline. The performance gap comes from the suboptimality of the ensemble recommendation, as experts are unaware of the penalty for reaching incorrect goals. \n\t\t\t\t\n3. No theoretical support. \nWe show that the BRPO agent operates on its own MDP, which we refer to as Residual-MDP. Since this is an MDP, BRPO enjoys the theoretical guarantees provided by its underlying batch policy optimization algorithm. For example, if it runs TRPO, it inherits the same monotonic improvement guarantee.\n\nWe have updated Section 4.2 to clarify this.\n\n[1] Osband, Ian, Daniel Russo, and Benjamin Van Roy. \"(More) efficient reinforcement learning via posterior sampling.\" Advances in Neural Information Processing Systems. 2013.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BygdF6XLqH",
                "reply_to": "iclr_2020_B1grSREtDH",
                "title": "Official Blind Review #3",
                "comment": "In this paper, the authors motivate and propose a learning algorithm, called Bayesian Residual Policy Optimization (BRPO), for Bayesian reinforcement learning problems. Experiment results are demonstrated in Section 5.\n\nThe paper is well written in general, and the proposed algorithm is also interesting. However, I think the paper suffers from the following limitations:\n\n1) This paper does not have any theoretical analysis or justification. It would be much better if the authors can rigorously prove the advantages of BRPO under some simplifying assumptions.\n\n2) It would be better if the authors can provide more experiment results, like experiment results in more games.",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HJxPLs_qcr",
                "reply_to": "iclr_2020_B1grSREtDH",
                "title": "Official Blind Review #2",
                "comment": "The paper presents a Bayesian residual policy which improves a ensemble of expert policies by learning to reduce uncertainty. The algorithm is designed for reducing uncertainty due to the occluded objects and uncertainty about tasks. It is verified on two problems, cheese finding and door findiing, and compared with several different baselines. \n\nThe idea of the paper is good and Algorithm 1 sets out to learn the exploration policy when the expert policies do not agree. The exposition and writing are clear. The experiments are details and convey that the proposed method outperforms the baselines.\n\nThat said, the formulation of the task is a bit unusual and too specific, making me wonder if the method works for other tasks. Some questions to clarify the task formulation:\n1. Do agent start locations and cheese locations change during the training and evaluation? The figures suggest they remain the same, in which case the generality is limited.\n\n2. When an agent senses for cheese, does it receive orientation or only the distance? If it receives the distances, will that not be a signal that matches the goals with some noise. In other words, why does the agent to sense several times at the beginning to associate which expert policy should be active, and then follow that policy.\n\n3. Why and how was the reward for the cheese finding task determined? It seems very specific.\n\n4. I would be helpful to provide some intuition about \\psi\n\nOverall an interesting paper, but not sure how well it would perform on a wider set of tasks.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This paper",
                "Sentiment Expression": "constitutes interesting progress",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "rkQuFUmUOg3": {
        "paper_id": "iclr_2021_rkQuFUmUOg3",
        "paper_title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets",
        "paper_abstract": "Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform human-designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A.",
        "paper_acceptance": "poster-presentations",
        "meta_review": "The authors proposed a meta learning framework for NAS, namely MetaD2A (Meta Dataset-to-Architecture), that can stochastically generate graphs (architectures) from a given set (dataset) via a dataset-architecture latent space learned with amortized meta-learning. Each dataset is encoded via a set encoder and the architecutres are obtained via a graph decoder. MetaD2A is trained once on a database consisting of datasets and pretrained networks and can rapidly search a neural architecture for a novel dataset. While the set encoder and graph decoder for NAS have been introduced by existing work, the main contribution of the paper is to show that the meta-learning of a \"dataset-conditioned architecture generation\" framework can enable fast generation of a good architecture without training on the target dataset. The proposed method is interesting and effective, however it requires an existing pool of good architectures for a given task, which may limit its applicability to a diverse set of real-world problems. I strongly encourage the authors to include experiments on a larger pool of architectures than the NAS-Bench-201 search space to show the strength of their proposed method in generating good architectures. While training MetaD2A with pairs of MetaImageNet and randomly sampled graph shows that the proposed framework can generate graphs with different types of edges, it doesn't show that it can successfully meta-learn to produce better architectures for a new task from an existing pool of good architectures. \n\nWe believe that many of the reviewers comments were addressed in the rebuttal, so while the scores are low, they do not reflect neither the contribution nor the reviewers opinion well (e.g., R3, in his last post, seems to suggest that his review should be updated but it has not happened).",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "I7Ts5LD_IO",
                "reply_to": "Xi63zgnTCkk",
                "title": "Response to the Comment 3 (Correctness and diversity of generated architectures)",
                "comment": "**(3-1) According to Figure 8 and Figure 9, it is likely that your graph decoder can only generate one type of edge connections. Your graph decoder may fail.**\n\n- This is a misunderstanding and our graph decoder did not fail. We use **nodes to represent the operation types**, and thus our graph decoder generates the graph with fixed edges with **variable node types** such as the graphs in Figure 8 and Figure 9. Note that this is different from [Liu et al. 19 and Dong & Yang 20] where the operations are represented with edges. Thus our model did generate diverse neural networks with different operations. \n\n\n**(3-2) Since your framework needs other methods (GDAS / NAS-Bench-201) to provide training material. These materials are all good architectures. It is possible that your framework just gives architectures the same as those good architectures rather than using meta features. Your experiment should prove that your model can generate variety of architectures.**\n\n- Note that our method is a probabilistic approach and can generate multiple architectures even for a **single** dataset, rather than simply retrieving a memorized architecture from the training set. We further performed an additional experiment to show that MetaD2A can generate diverse architectures, instead of retrieving a memorized architecture from the training set. To this end, we generated 10000 neural architecture samples with our MetaD2A on the meta-training set of the MetaImageNet dataset, and measured the quality of the generated architecture by the 1) Validity and 2) Novelty, following [Zhang et al. 19]. \n\n- **1) Validity**measures the proportion of the neural architectures that are valid (e.g. graphs with all nodes connected to some other nodes). \n- **2) Novelty**measures the proportion of the valid neural architectures that are never seen in the training set. \n\n- The result in the Table below shows that MetaD2A can generate **diverse and novel**architectures that **never appeared in the training set**.\n\n#### **The Proportion of Novelty Graphs (%)**\n| Samples \t| Validity \t| Novelty \t|\n|:-------:\t|:--------:\t|:-------:\t|\n|  10000  \t|   100%   \t|  67.31% \t|\n\n[Zhang et al. 19] D-VAE: A Variational autoencoder for directed acyclic graphs. NeurIPS 2019.\n\n---\nWe have included this result in the Experiments Section 4.3 of the revision. We appreciate your constructive comments.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "R9taRccyIW-",
                "reply_to": "GVuW-hYIztc",
                "title": "The interactive discussion phase ending in less than 7 hours. ",
                "comment": "Dear reviewer, \n\nThe interactive discussion phase will end in less than 7 hours, and we cannot have discussions with you anymore after the deadline. Could you please check if our responses to your comments and the new experimental results you have requested? Please let us know if there is any other things that we need to clarify or provide. We thank you so much for your helpful and insightful suggestion. \n\nThank you,\nAuthors.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Op6gs7NHfQD",
                "reply_to": "_4NUwLdildu",
                "title": "The interactive discussion phase ending in less than 9 hours.",
                "comment": "Dear reviewer,\n\nCould you check the results we provide in **Figure 10 of the Appendix D.5**? We have shown that our model can generate graphs with diverse edges from the **DVAE search space**, which shows that the reason we had fixed edges (with only the nodes changing) was because of the**NAS Bench-201 search space**, and is not because of the limitation with our model. \n\nCould you also please let us know any other things we should clarify or address, since we have only 9 hours before the end of the interactive discussion phase? We thank you for your helpful comments which we believe have largely improved our paper. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yUe9rja4KuQ",
                "reply_to": "_4NUwLdildu",
                "title": "Response to new comment 3 (Diverse edge connection generation)",
                "comment": "- We appreciate your constructive comments. First, we would like to clarify that the fixed edge is the **property of the NAS-Bench-201 search space** and not the limitation of our method. Since we performs NAS on the NAS-Bench-201 search space, which is the standard benchmark dataset, the generated networks will have fixed edges. We have clarified the description of NAS 201 search space in the **Section B of the Appendix, in the revision**.\n\n- However, our model **is able to generate graphs with diverse edges**. To address R3's concern, we performed an experiment to show that MetaD2A generates diverse edge connections, and included **the experimental results in D.5 of the Appendix**of the revision. We train MetaD2A in the search space of [Zhang et al. 19], where architectures consist of 6 layers,  which each node represents one of 6 operations (e.g. 3\u00d73 convolutions,  5\u00d75convolutions, 3\u00d73 and 5\u00d75 depthwise-separable convolutions, 3\u00d73 max pooling, and 3\u00d73 average pooling), with diverse edge connections between nodes. In **Figure 10 of Appendix D.5**, we observe that MetaD2A successfully generates architectures with various edge connections including different types of operations of nodes. We hope that this result addresses your concern. We believe that including the results in the paper has further reduced confusion regarding the diversity of the generated graphs.\n\n[Zhang et al. 19] D-VAE: A Variational autoencoder for directed acyclic graphs. NeurIPS 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kNyl3xQRsGn",
                "reply_to": "GVuW-hYIztc",
                "title": "Responses to the comment 2 on the comparison against existing works",
                "comment": "**(2-1) In Table 1, MetaD2A is pretrained on Meta-ImageNet while other baselines are trained from the scratch. I think this is unfair to compare them.**\n\n- Please note that MetaD2A **does not** train on the target datasets at all after it is trained on Meta-ImageNet, while all existing approaches require training on the target datasets. MetaD2A only uses 20 samples per class from each target dataset for inference, **without any training regardless of the number of datasets**we target. \n\n- Thus, when considering the total number of examples to train on, when generating architectures for multiple datasets, if they, in sum, have a larger number of samples than MetaImageNet, the number of samples used by MetaD2A will be less. Moreover, as the number of datasets we consider becomes much larger, our method becomes even more sample-efficient since it only has to be trained once on the source dataset.\n\n- Please note that such a strategy to \"train only once and adapt to new datasets at no training cost\", is the main reason why MetaD2A can rapidly generate adaptive neural architecture for a given dataset, which is made possible by our **amortized meta-learning framework**.\n\n**(2-2) It's better to compare MetaD2A with some MetaNAS methods listed in related work part such as [2]. In my view, I think MetaD2A pays more emphasis on meta-learning in NAS field.**\n\n- MetaNAS methods listed in the related work target for few-shot classification, and cannot scale to large-scale learning with a large number of dataset-architecture pairs (1,679 and 3,752 tasks for training the generator and the performance predictor), each of which contains $260,000$ examples). One of our main contributions is in the proposal of amortized meta-learning of the cross-modal space of dataset and neural architectures (Page 2), which enables large-scale meta-learning of a NAS framework. \n\n- We further compared our method against Meta-NAS methods [3,4,5] on few-shot classification tasks (5 way 5 shot and 5 way 1 shot classifications on mini-imagenet), which is the main setting existing Meta-NAS methods consider. Note that we can generate an architecture for each task almost instantly with MetaD2A for the few-shot task. Following [4, 5], we adopt bi-level optimization (e.g., MAML framework) to learn initial weight of the searched architectures on a meta-training set of mini-imagenet. The results are as follow:\n\n|       Method       \t| Params (K) \t| 5way 1shot \t| 5way 5shot \t|\n|:------------------:\t|:----------:\t|:----------:\t|:----------:\t|\n|        MAML [1]       \t|    32.9    \t|    48.70   \t|    63.11   \t|\n|       MAML++ [2]     \t|    32.9    \t|    52.15   \t|    68.32   \t|\n|        BASE [3]       \t|    1200    \t|      -     \t|    66.20   \t|\n|       T-NAS++ [4]     \t|    26.5    \t|    54.11   \t|    69.59   \t|\n|       MetaNAS [5]     \t|     30     \t|    49.7    \t|    62.1    \t|\n| **MetaD2A (ours)** \t|    28.9    \t|  **54.71** \t|  **70.59** \t|\n\n\nThe few-shot classification results on MiniImageNet further clearly show the MetaD2A's effectiveness over existing Meta-NAS methods, as well as conventional meta-learning without NAS. \n\n[1] Finn, C, et al. Model-agnostic meta-learning for fast adaptation of deep networks, ICML17\n\n[2] Antoniou, A et al., How to train your MAML, ICLR19\n\n[3] Shaw, A. et al, Meta Architecture Search, NeruIPS 19\n\n[4] Lian, D. et al., Towards Fast Adaptation of Neural Architectures with Meta Learning, ICLR19\n\n[5] Elsken, T et al., Meta-Learning of Neural Architectures for Few-Shot Learning, CVPR20\n\n---\n\nWe add the discussions for scalability of the proposed method compared with conventional Meta-NAS approaches in Method Section 3.3 and include the above experimental results to Section D.2 of Appendix in the revision. We appreciate R1's helpful comments.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2efssj06we9",
                "reply_to": "GVuW-hYIztc",
                "title": "Responses to the comment 3 (ablation study)",
                "comment": "**(3) In ablation study section, they only compare MetaD2A with random, which is a somewhat weak baseline. However, the improvement of their method is not significant. I think there are some other perspectives to perform ablation study. For instance, they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component.**\n\n- We **do provide the ablation study in Table 5** for a total of 4 modules including random (random, random with predictor, generator without predictor, generator with predictor). In addition, we perform the ablation study to analyze components of the input type, the encoder, and the decoder in Table 6.\n\n- Moreover, in ablation study of Table 5, our method achieves **22.16%** higher performance over random on aircraft, which we believe is a more than significant improvement.\n\n- We **do provide the ablation study on the flat set encoding in Table 6** (Please see the fourth and the fifth row of Table 6). We can see that the hierarchical set encoding and bi-directional graph encoding both improves the performance of the predictor.\n\n---\n\nWe hope that the above responses clear up your confusion. Please let us know if there is anything else we should provide.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "x2ms2AHECYb",
                "reply_to": "GVuW-hYIztc",
                "title": "Responses to the comments on the Novelty and the Performance Predictor.",
                "comment": "Thank you for your reviewing efforts and constructive comments. We respond to your comments below:\n\n**I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios.**\n\n- Our main contribution, which largely differentiates it from the existing works in NAS, is the proposal of an **amortized meta-learning framework for learning a cross-modal latent space**of the datasets and architectures that allows to generate an architecture for a given dataset **without**training on it. Such meta-learning of the cross-modal embedding space has not been tried by any of the existing NAS approaches, due to the limited scalability of conventional meta-learning frameworks (e.g. MAML), and is made possible only by our amortized meta-learning algorithm.  \n\n---\n\n**(1-1) Since the predictor consists of two linear layers, we cannot take architecture with different nodes as input. Thus, it will limit the generalization ability of the whole algorithm.**\n\n- This is a misunderstanding and the performance predictor **does not only consist of two linear layers**. The two linear layers are placed on top of a dataset encoder and a graph encoder, where the latter can encode neural networks with different architectures. Please see Section 3.2 and concept Figure 1 and 2.\n\n**(1-2) There are also different kinds of performance predictors in NAS field like LSTM and GCN predictor [1].** \n\n- We would like to point out our performance predictor is **meta-learned**on multiple tasks from a single dataset, such that it can **generalize**to the performance prediction of a **novel neural architecture on a novel dataset without training**, which is not possible with any of the existing performance predictors. \n\n- To our knowledge, **the performance predictors proposed in the existing NAS works need re-training for the new dataset** since they have **no module to handle multiple datasets** (They are set-specific predictors that have encoders only for graphs.). On the other hand, our proposed meta-performance predictor consists of dataset encoder and graph encoder to address such a problem with one training. Moreover, the proposed meta-performance predictor learns meta-knowledge accumulated from multiple tasks of MetaImageNet. Thus, the predictor can predict performance for the new unseen dataset. As the number of target datasets increases, **the time complexity of other predictors linearly increases while our predictor has a constant time complexity of O(1).** In addition, none of the existing works in both NAS or meta-learning can handle the scale of meta-learning (training over 1,679 datasets, each of which contains 2,000 samples) as our MetaD2A does.\n\n**(1-3) I prefer to see the effect of predictor part with different predictors.**\n\n- Yet, following R1\u2019s suggestion, we compare our meta-predictor against an LSTM predictor and a GCN predictor, as well as a two-layer linear predictor (to show that this is not our model). We train all of them on the multiple tasks from MetaImageNet, and use them to predict the accuracy on 400 unseen MetaImageNet tasks. We report the performance as the Pearson correlation coefficient between the predicted accuracy and the actuary accuracy (higher the better). The results are given in the table below:\n\n#### **Predictor Correlation**\n|                    | Linear Layer |  LSTM  |   GCN  | MetaD2A-G | MetaD2A-DG |\n|:-----------:   |:------------:       |:------:   |:------:   |:-----------:      |:----------:           |\n| Correlation |    0.0420      | 0.0064 | 0.6076 |   0.6426        | **0.7976** |\n\n- The linear or LSTM-based predictors completely fail to generalize to new tasks, which is expected since they are too simple and ignore the change of accuracy of the same architecture on different datasets. GCN-based predictor works reasonably well, obtaining the Pearson correlation score of 0.6076. Yet, it significantly underperforms MetaD2A-G, which predicts the performance only based on the graph of the neural architecture, without consideration of the dataset. Finally, our full model, MetaD2A-DG which predicts the performance based on both **the dataset and the architecture** achieves the highest Pearson correlation score, significantly outperforming all baselines.\n\n---\n\nWe clarify the contribution of our meta-performance predictor in Section 3.2 of Method and include the above experimental results in Section D.1. of Appendix. Thank you for the constructive suggestion. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3dePuB-Ad-c",
                "reply_to": "d9Bg6Qh9Qq",
                "title": "Response to new comment 2",
                "comment": "Thank you for your helpful comments. We believe that our paper became much stronger after including the additional experiments you have suggested.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "W1NzJFk5AmP",
                "reply_to": "6KEmCx_3v5h",
                "title": "Response to new comment 1",
                "comment": "Thank you for appreciating the novelty of our work. We believe that the proposed method makes important contributions in both NAS and meta-learning fields. \n\n- Our model is the first NAS method that can rapidly search for architectures for **novel (unseen) datasets without training on the target dataset** (only with **forward passes**) rapidly by transferring meta-knowledge learned over large number of tasks sampled from the source dataset.\n\n- Moreover, such meta-learning is possible with our **scalable amortized meta-learning framework** which can meta-learn over **larger number of datasets**with **large number of samples** without any gradient update steps. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_4NUwLdildu",
                "reply_to": "I7Ts5LD_IO",
                "title": "Response 3",
                "comment": "Concern 3: The authors give experiment about the novelty of the generated architectures. But my concern here is the fixed edges of the generated architectures. I think it means the edge decoding part of your model cannot generate different kinds of edge connections, can you show that your model is able to generate different types of edge connection, or you can prove that this type of edge connection is good whatever the node operation is and wherever the model adapts. ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "d9Bg6Qh9Qq",
                "reply_to": "K1q9LesMjLr",
                "title": "Response 2",
                "comment": "Concern 2-1: The authors give experiment results compared with [2], which shows the effectiveness and the efficiency of the proposed model.\n\nConcern 2-2: The authors give experiment results in few-shot learning setting and compared with meta learning methods. The results show the proposed model can also be applied in this setting and achieve SOTA performance.\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6KEmCx_3v5h",
                "reply_to": "Z7NLkvB5rsF",
                "title": "Response 1 ",
                "comment": "Concern 1: The authors address that although similar components have appeared in previous work, they do not contain meta mechanism can be applied on such a scene where new datasets are not trained. The contribution of this model is adding this mechanism. I think it is of novelty and OK.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eD-jnj3wvMH",
                "reply_to": "Xi63zgnTCkk",
                "title": "Responses and Revision uploaded ",
                "comment": "Dear reviewer,\n\nCould you check our responses to your comments as well as the revision that reflects them? We have clarified the novelty, clearly described the correctness of the generated architectures, provided the results on the diversity of the generated architectures you suggested, and included the experiments for comparison against Meta-NAS approaches and FNA. We would like your feedback since we cannot interact with you after this Tuesday, which is the end of the interactive discussion phase.\n\nThanks, Authors",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Ejsz3H0ZVC_",
                "reply_to": "GVuW-hYIztc",
                "title": "Responses and the revision uploaded",
                "comment": "Dear reviewer,\n\nCould you check our responses to your comments as well as the revision that reflects them? We have answered all your questions, provided the experimental results you have requested (comparison with existing predictor and comparison with conventional Meta-NAS approaches), and clarified your concern regarding the novelty over conventional NAS, and ablation study.\n\nThanks, Authors",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "K1q9LesMjLr",
                "reply_to": "Xi63zgnTCkk",
                "title": "Response to the Comment 2 (Missing comparison to [2] and Meta-NAS methods)",
                "comment": "**(2-1) No comparing to other methods on fast adaptation by NAS such as [2].** \n\n- As a conceptual difference, [2] starts from a pretrained seed network and adapts it to a target dataset using a differentiable NAS. Therefore, whenever a new dataset is given, **[2] needs to train on the target dataset** in order to adapt and finetune the architecture, which takes a large amount of training time  (Please see the search time of DARTS and PC-DARTS in Table 1 and 2), while ours can generate the architecture by performing multiple forward steps, that is extremely fast.\n\n- Moreover, as you suggested, we compared our model with [2] on multiple datasets. For FNA, we report the results of the models pre-trained on two datasets, CIFAR-100 (FNA-CIFAR100) and MetaImageNet (FNA-ImageNet) which is also used in MetaD2A. The results are given in the Table below, which shows that MetaD2A **significantly outperforms it** with **orders of magnitude**smaller search time:\n\n#### **Accuracy (%)**\n|    Methods   |   MNIST   |    SVHN   |  CIFAR10  |  CIFAR100 |  AIRCRAFT |    Pets   |\n|:------------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|\n|     DARTS    |   98.82   |   65.71   |   54.30   |   15.61   |   22.50   |   18.14   |\n| FNA-ImageNet |   93.11   |   94.75   |   39.64   |   39.19   |   22.46   |   11.95   |\n| FNA-CIFAR100 |   99.51   |   94.19   |   70.41   |     -     |   26.43   |   13.57   |\n| MetaD2A      | **99.70** | **96.56** | **94.37** | **73.51** | **60.31** | **38.74** |\n#### **Search Time (s)**\n|    Methods   \t| MNIST \t|  SVHN \t| CIFAR10 \t| CIFAR100 \t| AIRCRAFT \t| Pets \t|\n|:------------:\t|:-----:\t|:-----:\t|:-------:\t|:--------:\t|:--------:\t|:----:\t|\n|     DARTS    \t| 20606 \t| 25286 \t|   7046  \t|   16747  \t|   2899   \t| 2469 \t|\n| FNA-ImageNet \t| 19945 \t| 23248 \t|   9016  \t|   13732  \t|   2762   \t| 1808 \t|\n| FNA-CIFAR100 \t| 19945 \t| 15169 \t|   9462  \t|     -    \t|   3178   \t| 2357 \t|\n|    MetaD2A   \t|   **32**  \t|   **30**  \t|    **26**   \t|    **68**    \t|    **34**   \t|  **38**  \t|\n\n- While both FNA models achieves better performance over DARTS with reduced search time over it, since FNA **requires to train on each dataset**, it inevitably requires a large amount of computation during the adaptation process. Contrarily, MetaD2A generates a neural architecture for most of each given task with **less than a minute** as it requires only forward passes, **without any training**on the target dataset.\n\n- We include the discussion of [2] in the related work section, and add [2] as a baseline in the main Table 2 and Table 3 of the revision.\n\n---\n\n**(2-2) Besides, meta-learning methods may also be compared.**\n\n- Conventional Meta-NAS approaches [Lian et al. 19, Shaw et al. 19, Elsken et al. 20] mainly targets few-shot classification, and **cannot meta-learn over a large number of dataset-architecture pairs** since they resort to gradient-based meta-learning, which requires expensive lookahead steps (Please see Figure 1(b)). Overcoming such a limitation of conventional Meta-NAS methods, and meta-learning in general, is one of the main contributions of this work, which we believe is highly novel, in both the perspective of NAS and meta-learning. \n\n- We further compared our method against Meta-NAS methods [3,4,5] on few-shot classification tasks (5 way 5 shot and 5 way 1 shot classifications on mini-imagenet), which is the main setting existing Meta-NAS methods consider. Note that we can generate an architecture for each task almost instantly with MetaD2A for the few-shot task. Following [4, 5], we adopt bi-level optimization (e.g., MAML framework) to learn initial weight of the searched architectures on a meta-training set of mini-imagenet. The results are as follow:\n\n|       Method       \t| Params (K) \t| 5way 1shot \t| 5way 5shot \t|\n|:------------------:\t|:----------:\t|:----------:\t|:----------:\t|\n|        MAML [1]       \t|    32.9    \t|    48.70   \t|    63.11   \t|\n|       MAML++ [2]     \t|    32.9    \t|    52.15   \t|    68.32   \t|\n|        BASE [3]       \t|    1200    \t|      -     \t|    66.20   \t|\n|       T-NAS++ [4]     \t|    26.5    \t|    54.11   \t|    69.59   \t|\n|       MetaNAS [5]     \t|     30     \t|    49.7    \t|    62.1    \t|\n| **MetaD2A (ours)** \t|    28.9    \t|  **54.71** \t|  **70.59** \t|\n\nThe few-shot classification results on MiniImageNet further clearly show the MetaD2A's effectiveness over existing Meta-NAS methods, as well as conventional meta-learning without NAS. We add this result to the Appendix of the revision. \n\n[1] Finn, C, et al. Model-agnostic meta-learning for fast adaptation of deep networks, ICML17\n\n[2] Antoniou, A et al., How to train your MAML, ICLR19\n\n[3] Shaw, A. et al, Meta Architecture Search, NeruIPS 19\n\n[4] Lian, D. et al., Towards Fast Adaptation of Neural Architectures with Meta-Learning, ICLR19\n\n[5] Elsken, T et al., Meta-Learning of Neural Architectures for Few-Shot Learning, CVPR20",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Z7NLkvB5rsF",
                "reply_to": "Xi63zgnTCkk",
                "title": "Response to the Comment 1 (Novelty)",
                "comment": "We thank you for your constructive comments. We provide responses to your comments below:\n\n**(1) The three parts of your model are of little novelty. The dataset encoding part is just borrowed and there is no improvement to adapt NAS tasks. Similar graph decoder is proposed in previous NAS works [1] and performance predictor are proposed more times. It seems that the proposed framework is just putting existing models together.**\n\n- Our model consists of a data-to-graph generator and meta-performance predictor (Please refer to concept Figure 2). Both are designed to learn meta-knowledge while to our best knowledge, none of conventional NAS methods design their framework to learn the meta-knowledge using graph encoder/decoder and predictor. Most conventional NAS methods use graph encoder/decoder for graph-to-graph reconstruction, which has a limitation to be re-trained when a target dataset is changed. On the other hand, our data-to-graph generator once learned meta-knowledge with MetaImageNet works for new dataset **without additional training** by taking input dataset. We believe such an **amortized meta-learning framework to learn cross-modal latent space** between datasets and graphs to generate set-suitable architectures is challenging and has novelty. \n\n- To our knowledge, the performance predictors proposed in the existing NAS works need re-training for the new dataset since they have no module to handle multiple datasets (Most of them have encoders only for graphs). On the other hand, our proposed meta-performance predictor consists of dataset encoder and graph encoder to address such a problem with one training. Moreover, the proposed meta-performance predictor learns meta-knowledge accumulated from multiple tasks of MetaImageNet. Thus, the predictor can predict performance for the new unseen dataset. As the number of target datasets increases, the time complexity of other predictors linearly increases while our predictor has a constant time complexity of O(1). In addition, none of the existing works in both NAS or meta-learning can handle the scale of meta-learning (training over **1,679 datasets**, each of which contains **2,000 samples**) as our MetaD2A does.\n\n- Moreover, the dataset encoding part is **new**, although we borrow the components from the Set Transformer [Lee et al. 19] or the hierarchical set encoder from [Lee et al. 20], since the former does not have the notion of hierarchical encoding and the latter resort to simple averaging of the instance-wise representations.\n\n---\n\nWe have also responded to your second and the third question in the responses below, which provide the results of the additional experiments you requested. We hope that the discussions and additional experiments in this response further strengthens our argument and addresses R3's concerns. We include the experiments of (2-1) and (3-2) in the Experiments section and (2-2) in the Appendix of the revision. Please let us know any more things we should provide.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "fn-Vel6Jpwg",
                "reply_to": "iclr_2021_rkQuFUmUOg3",
                "title": "Summary of the updates in the revision",
                "comment": "We thank the reviewers for constructive comments. We appreciate that the reviewers consider the proposed framework to be new attempt (R1, R3), interesting (R1), good direction of both NAS and meta-learning (R3), and the results to be solid (R2). The reviewers also mention that the paper is well written (R2) and has clear structure (R2, R3). We have updated the paper by making the following changes:\n\n---\n\n- We revised the paper to clarify the technical novelty of meta-predictor and included **the experiments compared with the competitive predictors** in Appendix of the paper. (**R1**)\n\n---\n\n- We clarified the ablation study in Experiment Section 4.3. (**R1**)\n\n---\n\n- We added **the discussions for scalability of the proposed method compared with conventional Meta-NAS approaches** in Method Section 3.3. (**R2** and **R1**)\n\n---\n\n- We included **the experiments to show that our model can generate novel architectures** in Experiment Section 4.3. (**R3**)\n\n---\n\n- We included **the experiments for comparing MetaD2A with FNA [1]** which studies fast adaptation by NAS in Table 2 and Table 3. (**R3**)\n\n---\n\n- We added the discussions on FNA[1] in the Related Work Section. (**R3**)\n\n---\n\n- We performed **the experiments to compare our model and Meta-NAS methods** and included the results in Appendix of the paper. (**R1** and **R3**)\n\n---\n\n[1] Fast neural network adaptation via parameter remapping and architecture search. ICLR 20\n\n---\n\nWe strongly believe that the problem we tackle (rapid architecture search without training for new task) and the idea (amortized meta-learning framework to learn cross-modal latent space) we propose to tackle the problem are both highly novel, and provide important contributions to the research in both meta-learning and NAS. We also believe that the new experimental results of the revision effectively address the concerns of the reviewers on comparison with Meta-NAS approaches, and diversity of generated architectures.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2FTxGoDHui5",
                "reply_to": "NRH0FUrwmw3",
                "title": "Initial Response",
                "comment": "Thank you for your constructive comments. We address your comments below:\n\n**Negative: The authors claim that NAS with meta-learning has only been done with small datasets in the past. However, the authors do not really use big datasets as well (see Appendix C; the ImageNet subset considered is small as well, if I understand this correctly)**\n\n- Existing Meta-NAS approaches [Lian et al. 19, Shaw et al. 19, Elsken et al. 20] cannot scale to such a large task with a large number of samples, since they rely on gradient-based meta-learning with rollout gradient steps. For few-shot learning, taking a few gradient steps is more than enough since there are only a few samples from which we can compute the gradients, but for large-scale learning, taking few gradient steps with minibatch sampling will only consider a very small portion of the samples in the task, which will be largely insufficient for the model to converge. Thus the model needs to take a large number of gradient steps with large tasks (with a large number of samples). However, this is extremely costly when we want to train over a large number of tasks (we consider **1,679 tasks**to train the MetaD2A generator, and **3,752 tasks**for the meta-performance predictor), and is almost infeasible when computing second-order derivatives as in MAML.\n\n- On the contrary, MetaD2A can handle the large-way many-shot problem tackled by general classification tasks, since it does not require any rollout gradient steps when inferring the architecture. For example, CIFAR-100 in Table 2 is orders of magnitudes larger than the tasks considered in a few-shot classification (m-way k-shot classification) existing Meta-NAS methods target, where the number of samples is $m\\times{k}$. For conventional settings (5-way 5-shot), the number of samples per task is 25. Contrarily, the number of samples of CIFAR-100 used is **100$\\times$20=2,000**, which is **$80$ times larger**than the tasks in 5-way 5-shot classification. Further, the number of instances(e.g. 20) can be set to a much larger number.\n\nPlease let us know if there is anything else that you want us to clarify or provide.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BBU-UbDhpOa",
                "reply_to": "iclr_2021_rkQuFUmUOg3",
                "title": "The end of the discussion phase approaching",
                "comment": "Dear Reviewers,\n\nCould you please go over our responses and the revision since we can have interactions with you only by this Tuesday (24th)? We have responded to your comments and faithfully reflected them in the revision, and provided additional experimental results that you have requested. We sincerely thank you for your time and efforts in reviewing our paper, and your insightful and constructive comments.\n\nThanks, Authors\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "GVuW-hYIztc",
                "reply_to": "iclr_2021_rkQuFUmUOg3",
                "title": "The authors present a novel framework to learn architecture efficiently from datasets. Even though some components have been proposed before, the way of combination is interesting to me. However, their experiment results are not convincing.",
                "comment": "The authors propose a novel framework named MetaD2A. Their motivation is interesting to me and they clarify their difference compared with traditional metaNAS in Figure 1. There are mainly three components in MetaD2A: a set encoder, a graph decoder and a meta-performance predictor. I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios. In summary, the paper has following drawbacks that need to be further concerned:\n\n1. Since the predictor consists of two linear layers, we cannot take architecture with different nodes as input. Thus, it will limit the generalization ability of the whole algorithm. Moreover, there are also different kinds of performance predictors in NAS field like LSTM and GCN predictor [1]. And I prefer to see the effect of predictor part with different predictors.  \n\n2. In Table 1, MetaD2A is pretrained on Meta-ImageNet while other baselines are trained from the scratch. I think this is unfair to compare them. It's better to compare MetaD2A with some MetaNAS methods listed in related work part such as [2]. In my view, I think MetaD2A pays more emphasis on meta-learning in NAS field.\n\n2. In ablation study section, they only compare MetaD2A with random, which is a somewhat weak baseline. However, the improvement of their method is not significant. I think there are some other perspectives to perform ablation study. For instance, they can replace hierarchical set pooling with flatten set pooling and then see the importance of different component.\n\n[1] Shi, Han, et al. \"Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS.\" Neural Information Processing Systems. 2020.\n\n[2] Lian, Dongze, et al. \"Towards fast adaptation of neural architectures with meta learning.\" International Conference on Learning Representations. 2019.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Xi63zgnTCkk",
                "reply_to": "iclr_2021_rkQuFUmUOg3",
                "title": "This paper proposes a framework to learn meta knowledge which helps to transfer architecture search task among different datasets. This is a new attempt to adopt meta NAS on such a scene. This work encode dataset into embedding space, then sample a vector from the space and decode it into an architecture. A predictor is also used to find the optimal architecture. ",
                "comment": "Pros:\n1.\tThis paper proposes new scene, where a meta model is pre-trained on some datasets, and transfer the learned meta feature onto other datasets to do fast adoption. This scene can be applied in variety of domains.\n2.\tThe experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance. \n3.\tThe paper is well-organized, the paper structure is clear.\n\nCons:\n1.\tThe three parts of your model are of little novelty. The dataset encoding part is just borrowed and there is no improvement to adapt NAS tasks. Similar graph decoder is proposed in previous NAS works [1] and performance predictor are proposed more times. It seems that the proposed framework is just putting existing models together. \n2.\tNo comparing to other methods on fast adaptation by NAS such as [2]. Besides, meta-learning methods may also be compared.\n3.\tAccording to Figure 8 and Figure 9, it is likely that your graph decoder can only generate one type of edge connections. Your graph decoder may fail. Since your framework needs other methods (GDAS / NAS-Bench-201) to provide training material. These materials are all good architectures. It is possible that your framework just gives architectures the same as those good architectures rather than using meta features. Your experiment should prove that your model can generate variety of architectures.\n\nOverall Review:\nThis paper proposes a new scene of fast adaption of NAS, which may be a good direction of NAS & meta-learning. The paper proposes a framework to generate good architectures according to the datasets. However, the model may need to improved and more experiment need to be done to solve the problems mentioned above.\n\n[1]Does unsupervised architecture representation learning help neural architecture search? NeurIPS 20\u2019\n[2]Fast neural network adaptation via parameter remapping and architecture search ICLR 20\u2019 \n",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "NRH0FUrwmw3",
                "reply_to": "iclr_2021_rkQuFUmUOg3",
                "title": "Solid paper, weak accept",
                "comment": "The authors address neural architecture search (NAS) scenarios. In particular, a framework, MetaD2A, is proposed, which yields a neural architecture for a new dataset. In a nutshell, the framework learns a \"dataset-to-neural-network-architecture\" transformation using a database of datasets and architectures. Each dataset is encoded via a \"set encode\" and the architecutres are obtained via a \"graph decoder\". The experiments demonstrate the usefullness of the approach and its improvements over conventual NAS approaches. The approach could be described \n\nPositive:\n- The results look very solid and indiciate improvements (time/prediction performance) over existing approaches\n- The paper is well written and structured\n- Additional details (e.g., implementation details) are provided in the appendix\n\nNegative:\n- The authors claim that NAS with meta learning has only been done with small datasets in the past. However, the authors do not really use big datasets as well (see Appendix C; the ImageNet subset considered is small as well, if I understand this correctly)\n\nTo sum up, I think the work might be a suitable candidate for being accepted at ICLR. I have to admit that this is not precisely an area of my expertise, so I might be missing something.\n",
                "rating": 6,
                "confidence": 2,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The proposed method",
                "Sentiment Expression": "interesting and effective",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "existing pool of good architectures",
                "Sentiment Expression": "requires",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the meta-learning of a 'dataset-conditioned architecture generation' framework",
                "Sentiment Expression": "can enable fast generation of a good architecture",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "experiments on a larger pool of architectures than the NAS-Bench-201 search space",
                "Sentiment Expression": "to include",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "training MetaD2A with pairs of MetaImageNet and randomly sampled graph",
                "Sentiment Expression": "doesn't show that it can successfully meta-learn to produce better architectures",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "sEIl_stzQyB": {
        "paper_id": "iclr_2022_sEIl_stzQyB",
        "paper_title": "Greedy-based Value Representation for Efficient Coordination in Multi-agent Reinforcement Learning",
        "paper_abstract": "Due to the representation limitation of the joint Q value function, multi-agent reinforcement learning (MARL) methods with linear or monotonic value decomposition can not ensure the optimal consistency (i.e. the correspondence between the individual greedy actions and the maximal true Q value), leading to instability and poor coordination. Existing methods focus on addressing the representation limitation through learning the complete expressiveness, which is impractical and may deteriorate the performance in complex tasks. In this paper, we introduce the True-Global-Max (TGM) condition for linear and monotonic value decomposition to achieve the optimal consistency directly, where the TGM condition can be ensured under the unique stability of the optimal greedy action. Therefore, we propose the greedy-based value representation (GVR), which stabilises the optimal greedy action via inferior target shaping and destabilises non-optimal greedy actions via superior experience replay. We conduct experiments on various benchmarks, where GVR significantly outperforms state-of-the-art baselines. Experiment results demonstrate that our method can meet the optimal consistency under sufficient exploration and is more efficient than methods with complete expressiveness capability.",
        "paper_acceptance": "Reject",
        "meta_review": "The paper studies the join-Q value decomposition problem in MARL. Some of the results are interesting, e.g., the True-Global-Max condition and several experiments. However, the majority of the reviews are negative due to the current presentation of the paper. We encourage the authors address all the reviewers' comments and submit a new version to the next conference.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "m4fPoTIrKx2",
                "writer": "official_reviewer",
                "reply_to": "-9bzk8bae9F",
                "title": "Response",
                "comment": " Thank you for the author's responses. \n\n (1) In author's response, author says the joint Q function is modeled by neural network. Then I think the utility funcitons in Eq(3) should also be modeled by neural network so we don't know the utility function. However, the author makes the utility function as expactation of $Q_{ik}-U_k^2$. I can't understand how you can ensure this expression. Furthermore, the proposed target of utility function is not reasonable, and I think the target of true utility function may be considered as expectation of true Q function over other agent's action i.e $\\frac{\\epsilon}{m}\\sum_{k=1}^m Q_{ik} + (1-\\epsilon)Q_{ij^{\\ast}}$\n\n (2) The proof in Appendix D is based on the fact the utility function equals to the expectation on the proposed target of utility function, but as metioned in (1), I can't agree this.\n\n (3) I can understand what the author is trying to say in target of critic, but the proof is not acceptable to me.\n\n (4) In the proposed algorithm, I can't find the usage of the reward function. In the both critic loss and agent loss, the algorithm doesn't use the reward in the target becuase the author makes target as equation (5) and (9). I didn't understand why the proposed algorithm can work without reward.\n\nTo due the above reasons, the paper is still not enough to raise the score.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5stwl1I8KsU",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_sEIl_stzQyB",
                "title": "",
                "comment": "The paper addresses the problem of monotonic value representations for non-monotonic true values in MARL. The authors perform a theoretical analysis of the conditions under which the greedy decentralized policy coincides with the optimal joint policy and derive a novel update scheme to destabilize incorrect fix-points. They also introduce a priority replay buffer that selects transitions with optimal actions more often and thereby stabilizing learning further. The new algorithm GVR is tested on a matrix game, a predator-prey task and StarCraft II micromanagement tasks. In the latter two GVR appears to significantly outperform decentralized baselines.  \n The authors seem to attempt a deep theoretical analysis of the underlying problem and the good results on predator-prey and StarCraft II indicate that they have found an algorithm that should be definitely published. However, although the reviewer has published on this field, he/she was largely unable to follow the text and the derivations. While the text is technically well written, the underlying concepts are very unclear to the reviewer and the formulas are swapping notations. Finally the developed algorithm remained unclear to the reviewer, who is not sure how statements like (eq.6) can actually be implemented (the pseudo-code in the appendix did not help). In detail:\n\n(1) As far as the reviewer understands the field, the problem is that monotonic value function functions cannot represent non-monotonic true values. Rashid et al. (2020) have shown that putting more importance on optimal actions can resolve this issue and Gupta et al. (2021) have linked this to relative overgeneralization, showing that sampling optimal actions more often allows to learn the optimal policy. Both papers use matrix-games similar to Table 1 to motivate their approach. However, the reviewer was not able to connect these insights with the notion of a \"stable greedy action\". Which are those? Definition 3 uses the formulation \"a stable joint Q value function $Q(s,u)$ which has converged to the true Q value $\\mathcal Q^\\pi(s, u)$ under $\\pi(s, u)$\". Does this mean that $Q(s,u) = \\mathcal Q^\\pi(s, u)$? Is $Q$ factored? How are stability and factoredness related? This seems to be the central concept of GVR, but the reviewer has no idea how to connect it to the presented problem or the proposed solution.\n\n(2) The core of the ITS method seems to be (eq.6). How is it computed in practice, when the agent does not know $\\mathcal Q(s, u)$, which the reviewer assumes corresponds to $\\mathcal Q^\\pi(s, u)$? For the current action this could be approximated with the Bellman operator, but (eq.6) requires  the evaluation of $\\mathcal Q(s, u^*)$ as well. It would be generally helpful to differntiate between $u^*$ and $u_{opt}$ throughout the text, but in particularly here, where the agent does not know $u_{opt}$, but uses $u^*$ in (eq.6), which is confusingly defined as \"optimal\".\n\n(3) The formal derivation is at least sloppy. The authors introduce the Dec-POMDP framework where the policy $\\pi(u|\\tau)$ conditions on the action-observation histories, but then define the utilities $\\mathcal U^a(u^a, o^a)$ as conditioning only on the observations $o^a$, and the values $Q(u, s)$ as conditioning on the state. This does not work (see Oliehoek and Amato, 2016). Either work in a Dec-MDP, which ignores many interesting issues, or consistently condition on the histories $\\tau^a$ and not the state or observation.\n\n(4) The paper uses the term \"reward shaping\", but it seems the authors mean \"reward function\" (as in the matrix game experiments). This is very confusing, as GVR \"shapes\" the value-targets, which one could interpret as actual reward shaping, i.e., (here indirectly) changing the given reward function to improve performance.\n\n(5) (eq.8) insinuates that higher exploration noise will \"destabilize\" non-optimal stable greedy policies. How does this fit with the relative overgeneralization example in Gupta et al. (2021)?\n\n(6) The reviewer did not understand the conclusions the authors drew from the matrix game experiment. Both the setup (why are the rewards for most actions randomized? how is this a fair comparison?) and the results in Figure 2 (are there siginificant differences in return?). While Figure 1b seems to support the theoretical statement, it is unclear why the setup justifies this (e.g. what would happen if your random rewards are from another range than (-20,6)?). The paper seems to be onto something that would be of great interest to the community, and the good results indicate that the authors know what they are doing, but the paper is currently almost incomprehensible to this reviewer. Although the authors are welcome to refute or explain the above criticisms, the fact that the reviewer was not able to understand the core idea of the paper during the first reading makes it unlikely that he/she will recommend to publish it in the current form. The reviewer would like to encourage to the authors to rewrite and resubmit the paper, though, as the content seems to be truly significant! \n\n\n**POST-REBUTTAL**\n\nThanks to the authors for their clarifications. However, it was not enough to clarify the paper's main message to the reviewer. The reviewer will therefore not change the score.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "GFhMCkmIqc",
                "writer": "official_reviewer",
                "reply_to": "368D_7DwwGeA",
                "title": "Still not clear enough",
                "comment": " Thanks to the authors for their clarifications. However, it was not enough to clarify the paper's main message to the reviewer. The reviewer will therefore not change the score.\n\nThe reviewer admits that he/she did not spend the same time reading the revised script, but the story-line is still very confusing. In particular the formal side is still unclear. For example, what does \"with target $\\mathcal Q$\" (which should throughout the text be $\\mathcal Q^\\pi$) mean? How is the target approximated? Is that important at all?\n\nFurthermore, there are notational inconsistencies: the true value $\\mathcal Q^\\pi(s,a)$ must also condition on all histories. As the histories can not be deduced by the state, different histories would mean different futures and thus different values.\n\nThe reviewer recommends that the authors take a step back and completely rewrite the argumentation in the paper. This is not just a question of mathematical correctness, but also of intuitive understanding for someone who is familiar with prior work, where the key to learning the optimal policy seemed to be the update distribution.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "368D_7DwwGeA",
                "writer": "author",
                "reply_to": "5stwl1I8KsU",
                "title": "Replies",
                "comment": " Your comments and questions are really helpful to us, and we have thoroughly revised our paper. We list the replies for the questions as follows\n\n1. According to the investigation on LVD and MVD (Section 3.2 of resubmitted paper), the non-optimal stable point is the root cause of non-optimal coordination and relative overgeneralization.\n\nWe connect our conclusion with recent works in the resubmitted version. Both methods the reviewer mentioned (Placing more importance on the optimal action or sampling optimal action more often) increase the proportion of the optimal sample, which helps to eliminate the non-optimal stable points (as proved by Eq.7). However, these methods can not ensure convergence to the optimal coordination (i.e., optimal consistency). More details are provided in Appendix A.\n\n\nDue to the representation limitation of LVD and MVD, the joint Q value function can not fully represent the true Q values. As a result, the converged joint Q values are not equal to the true Q value. Instead, they converge to the situations in Tab1(b) or Tab1(d). $Q$ is linearly (or monotonically) factorized. However, such factorization introduces representation limitation, which causes multiple stable points. We clarify Def.3 and present an example to explain the stability in Section 3.2 of resubmitted paper.\n\n\n2. Firstly, the reviewer is right about the assuming of $\\mathcal{Q}(s,u)$. For Eq.6 (Eq.5 in resubmitted version), we mistyped $Q(u^*,\\tau)$ with $\\mathcal{Q}(s,u^*)$, which has been revised in the resubmitted version. We denote the greed action with $u^*$ and optimal action with $u_{opt}$ throughout the paper, which is defined in the resubmitted preliminary.\n\n\n3-4.\t Thanks for your kind and insightful comments, we have revised these problems. In addition, to clarify our methods and derivations, we have modified the notations in the resubmitted version.\n\n\n5. In UneVEn, the authors analyze a special case (a well-shaped two-agent payoff matrix game) to support the idea that the convergence to the optimal joint action depends on the probability of the other agent's optimal action. The conclusion is consistent with our findings (Eq.7), where we consider a more general situation (n-agents, any reward function).\n\n\n6. We generate the matrices randomly for two reasons. First, our experiments on matrix games are carried out in a larger action space (up to $12^4$) than previous work, where the manual design of the matrix is time-consuming. Second, the non-optimal stable points depend on reward function (as verified in Appendix H), to evaluate whether a method can eliminate the non-optimal stable points (or ensure optimal consistency), we test this method under general (random) reward functions. To guarantee the fairness of comparison, We use the same set of seeds for all methods to generate the matrices (i.e., the generated matrices are the same for all methods).  The difference in return in figure 2 may be not significant, but we mainly concern the stability and optimality of evaluated methods. In figure 2, at the end of the training, our method converges to the optimal (return = 8) with little variance. The range of the random reward is not necessarily (-20,6), it can be any other range. As verified in the experiment on predator-prey, the reward function has little influence on our method.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jqbsRWgDXj5",
                "writer": "author",
                "reply_to": "evImWrLdJAX",
                "title": "Replies",
                "comment": " Thanks for your questions and comments, we list the replies as follows\n\n1.\tTarget of the agents and critic\nWe have further clarified our method in the introduction and Section 4.1. We also provide more theoretical and empirical proves of ITS and SER in Appendix D1, E and F.\n\n(1)\tThe ITS target for $\\mathcal{Q}(s,u)> Q(u^*,\\tau)$ is $\\mathcal{Q}(s,u)$. The error of joint Q value function is $\\mathcal{Q}(s,u)-Q_{\\theta_a}(s,u)$, which is not zero.\n\n(2)\tNotice one of the conditions to update the critic is $u=u^*$, i.e., the critic is updated when receiving a greedy action. If $\\mathcal{Q}(s,u)<V(s)$ in the beginning, $V(s)$ will approximate the true Q value of current greedy action (i.e., $\\mathcal{Q}(s,u^*)$). The critic is designed to distinguish whether an action is better than the current greedy action. Therefore, we let the critic approximate the true Q value of the greedy action. In practice, the critic is also updated when receiving a superior sample. As a result, the output of the critic is marginal above the true Q value of the greedy action, and the superior sample with a slight advantage will be ignored. This is helpful for the stability of training.\n\n2.\tdetails\n\n(1)\tVDN+ITS refers to the method where we simply apply the ITS target to VDN, which is modified to ITS in the resubmitted version. The comparison between VDN and VDN+ITS is used to directly verify the effectiveness of ITS target.\n(2)\tWe have clarified the notations in resubmitted preliminary.\n(3)\tThanks for pointing out this typo, we have revised it in resubmitted version.\n\n3.\tbaselines\n\nWe build connections between efficient exploration based approaches and our method, detailed information can be found in Appendix A. The comparison between these methods and our method is provided in Appendix I.2. However, two baselines (CMAE and EDTI) are not included in our experiments because both methods are count-based, which is impractical to apply them to complex tasks in SMAC.\n\n\n4.\tthe results of WQMIX\n\nWQMIX seems to perform unsteadily under different seeds (e.g., in our experiments on 2c_vs_64zg, the test win rate of WQMIX varies from 0 to 0.8). The authors of WQMIX do not provide the seeds of their curves. Despite we have downloaded the source code from their website, we can not reproduce their experiment results.\n\n\n5.\tSuper hard experiments on SCII\nWe have complemented the experiments on corridor and 3s5z_vs_3s6z in Apendix I.2. Due to the memory limitation of our machine, we can not implement the experiments on 27m_vs_30m.\n\n\n6.\tcode\nKey resources and details (proofs, experimental setup) are sufficiently described to reproduce the main results (especially the results on matrix games). And we will post the code on github as soon as possible.\n\n\nBesides, we notice that the reviewer mentioned in the summary that he/she is not convinced that our method can destabilize the non-optimal greedy action. Therefore, we provide proof in Appendix F in resubmitted version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-9bzk8bae9F",
                "writer": "author",
                "reply_to": "9e3A3rYtvNP",
                "title": "Replies",
                "comment": " Thanks for your questions and comments. We list the replies as follows\n\nTo clarify our methods and derivations, we made great modifications in the resubmitted paper. Therefore, we denote the equations and appendixes of resubmitted version with $(R)$ (e.g., Appendix B(R)) in replies.\n\n1. The joint Q value function is the action-value function of the agent group, which is modeled by neural networks. The true Q value is the actual action-value of the agent group, which is the external criterion of the team's performance defined by the environment. The target of the joint Q value function is just the true Q value. \n\n2. For the question about Eq.3 (i.e., Eq.2 of Appendix B(R)), the utility function equals to the expectation on its target (e.g., $\\mathcal{Q}_{ik} - \\mathcal{U}_k^2$ is a target of $\\mathcal{U}_i^1$ with a probability $\\epsilon/m$). If we replace true Q values with the joint Q values,  according to Eq.1 of Appendix B(R), the equation still holds. In fact, the equation holds for both situations.\n\n3. Eq.13 in Appendix D (i.e., Eq.19 of Appendix D.2(R)) is acquired with the following steps: (1) refer to the expression of the utility function in Eq.2 (Appendix B.1(R)); (2) replace all true Q values of actions except the greedy action and $u_s$ with the target $(1-\\alpha)Q(u^*, \\tau)$; (3) combine the terms containing utility functions and use the mapping $f$ to represent it. The accurate expression of the mapping $f$ is very complicated, which is unnecessary because it would be canceled out by a subtraction (Eq.23, Appendix D.2(R)). We also provide proof without the mapping $f$ in Appendix D.1(R).\n\nFor Eq.18 in appendix E, we update the proof to n-agent situation. The high-level idea is to represent the utility function with the expectation on different kinds of samples (i.e., inferior, current and greedy). Please refer to Appendix E.1(R) for the details. \n\n4. The critic is designed to distinguish whether an action is better than the current greedy action. Therefore, we let the critic approximate the true Q value of the greedy action. In practice, the critic is also updated when receiving a superior sample. As a result, the output of the critic is marginal above the true Q value of the greedy action, and the superior sample with a slight advantage will be ignored. This is helpful for the stability of training.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cbqL0UsK0dB",
                "writer": "author",
                "reply_to": "N6d0A8axSQp",
                "title": "Replies",
                "comment": " Thanks for your questions and comments, the answers are listed as follows\n\n1. Due to the representation limitation of LVD and MVD, the joint Q value function can not fully represent the true Q values. As a result, the converged joint Q values are not equal to the true Q value. Instead, they converge to the situations in Tab1(b) or Tab1(d). We clarify Def.3 and present an example to explain the stability in Section 3.2 of resubmitted paper.\n\n2. Whether the joint interactive policy is fixed or not depends on the situation. When we analyze or evaluate the stability, we fix the joint interactive policy (e.g., Tab.1, Fig.1(a)). When we evaluate the performance, the joint interactive policy is not fixed (e.g., Fig.1(b), Fig.2-Fig.4). For the question about Eq.5 (Eq.4 in the resubmitted paper), please refer to reply 1.\n\n3. Indeed, the high-level idea is to make agents realize that there is a better alternative action. But I don't understand the \"penalty\", would you mean $\\alpha$ (in Eq.5 of the resubmitted paper)?  A small value of $\\alpha$ may lead to confusion between greedy and inferior actions.  Besides, we have proved in the resubmitted paper that a small improvement is insufficient to achieve the optimal consistency, there is always a lower bound for parameters (e.g., $\\epsilon$ in Eq.8 of Section 4.1 and sample weight in Eq.33 of Appendix E.1).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "N6d0A8axSQp",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_sEIl_stzQyB",
                "title": "",
                "comment": "The paper studies the problem of value decomposition, which decomposes the joint-Q function into some linear or monotonic transformations of individual factored Q functions for each of the agents. The paper identifies limitations with previous linear/monotonic forms of the decomposition, in particular that the joint greedy action (which matches the greedy joint action for these decompositions) might not match the maximum true Q value. This condition is termed \"True-Global-Max\", and the paper introduces two techniques (inferior target shaping, superior experience replay) to satisfy this condition and improve upon previous suggested Q-value decompositions in literature. The paper walks through a toy matrix game example, a predator-prey experiment, and the Starcraft Multi-Agent challenge environment. Strengths:\nThe problem that is being studied is of high importance, given the popularity of the value decomposition baselines (VDN and QMIX). The True-Global-Max condition is interesting, and the paper proposes relatively simple techniques to satisfy the condition, which is a plus. The experiments are also interesting and the ablation studies in the supplemental are informative. I found the derivations and the writing easy to read, and overall I think the paper can be a good contribution. \n\nSome questions I have:\n- Def 3 and Tab1: the condition says that the joint Q function has converged to the true Q function under policy \\pi. But in Table 1 it seems that none of the Q functions have converged to the true Q function (Tab 1a)? Could you please clarify Def 3 (I'm not sure why we can't just write Eq5 in terms of argmax Q^\\pi = argmax \\pi\n- Is the joint interactive policy \\pi fixed? Or is it updating as the estimated Q function updates? I assume it is updating, but that also seems to mean Eq5 will always hold?\n- For inferior target shaping (ITS), we are penalizing non-optimal actions. The high-level idea is to have a large enough epsilon\\*penalty so that the agents can realize that there is a better alternative action, right? But it is unclear to me why we don't just need that epsilon\\*penalty > 0, it seems that if there is just a little improvement then over time the value iterations will converge to the better alternative? Overall I found the problem important, and the paper interesting (both technical and experimental parts) and easy to read. I do have some questions about the framework, and I would be more certain of my recommendation if the authors can help me clarify my confusion.",
                "rating": 6,
                "confidence": 2
            },
            {
                "review_id": "evImWrLdJAX",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_sEIl_stzQyB",
                "title": "",
                "comment": "This paper aims to improve value-factorization in cooperative multi-agent reinforcement learning settings under centralized training and decentralized execution (CTDE) framework. The proposed method (GVR) attempts to ensure both Individual-Global-Max(IGM) and True-Global-Max(TGM) conditions without learning a completely expressive (CEC) value function (which can represent all joint- actions). IGM ensures the consistency between joint action selection and local greedy action selection (Individual-Global-Max, IGM), and TGM ensures that the joint-action value function correctly represents optimal values. The GVR method proposed by the authors claims to ensure both IGM and TGM conditions without CEC.\n\nThe main contribution is greedy-value based representation (GVR) which consists of two parts. First, Inferior Target Sampling (ITS) ensures that the greedy joint action is stable such that if the greedy action corresponds to the optimal joint action, then for other joint actions, the gradient will be negative, thereby selectively focusing on representing the unique optimal joint action. If the greedy action does not correspond to the unique optimal joint action, then higher exploration is required in order to destabilize it. However, the lower bound for exploration can become quite high (close to 1) with increasing agents/joint action space, therefore the authors propose a prioritized experience replay buffer. The prioritized buffer assigns higher priority to non-greedy actions which have higher values than a state-based critic.\n\nExperimental results on matrix game, predator-prey and hard/super-hard maps from Starcraft II benchmark show improvements over baseline methods.\n ### Strengths:\n\n* The ideas in the paper are novel, and experimental results validate the equations as well. Moreover, the ITS method is based on a proof and can guarantee the stability of greedy actions.  \n* The experimental results on some super hard SMAC maps like 6h_vs_8z (known to be a hard exploration map) look impressive compared to other state of the art methods like QMIX, QPLEX.\n* The paper is well motivated as both IGM and TGM conditions are essential for stability and learning coordinated policies, and can be especially helpful for non-monotonic tasks. \n\n### Weaknesses:\n\n* ITS can keep the greedy action stable which works great if the greedy action is already optimal. However, the authors rely on superior experience replay to destabilize the non-optimal greedy actions. I have some major concerns about this and would request authors to clarify the following:\n\n\t- Looking at Algorithm 1 in appendix, it is unclear how the agent loss is computed. If the Q value of an action $u$ is greater than the greedy action $u^\\star$ i.e. $Q(s,u) > Q(s,u^\\star)$, then it seems like the error would be zero as $Q^{its}(s,u) = Q(s,u)$. Does this rely on the greedy action always being the optimal joint action through higher exploration rate or superior experience replay? \n\n\t- Similarly, the update for the critic is a bit unclear as well. Why is the critic loss only updated for superior samples? It makes sense to consider the samples with $Q(s,u) > V(s)$ or $u = u^\\star$ as superior samples, but in the beginning of the training, the $Q(s,u) < V(s)$ can happen for samples with true optimal joint actions as well, but the critic loss will not be updated for them. \n \n* The paper is a bit hard to follow in certain areas, especially Section 5.1. It would be great if the authors could take some time to add more details to the paper as follows.\n\n\t- In Section 5.1, the authors can make it easier to understand what VDN+ITS variant refers to. \n\t- The paper sometimes refer to $u^\\star$ as optimal greedy  joint action (in Section 4.1), and then refer to $u^\\star$ as just greedy joint action in section 5.1 which is a bit confusing.\n\t- Should Equation (3) have $Q_{ik}$ and $Q_{kj}$ instead of $Q_{ki}$ and $Q_{jk}$?\n\n* One weakness of this paper is that the experimental section is missing  some important baselines. The destabilization of the non-optimal greedy actions through the superior replay buffer is interesting, but the authors should empirically compare against other efficient exploration based MARL approaches such as [1,2,3,4]. Although these approaches also cannot guarantee both IGM and TGM, they can potentially also alleviate the destability associated with stable non-optimal greedy actions via different exploration mechanisms. \n\n* I have a major concern about the performance of WQMIX reported on the StarCraft II benchmark map 6h_vs_8z. The reported performance is not consistent with that reported in the WQMIX paper (see Figure 4 in [5]). WQMIX paper also uses the same exploration schedule as this paper i.e. $\\epsilon$ annealed over 1e6 time steps. The original paper reports a median test win rate of around 80% with optimistic weighting but this paper only reports around 20%. Also, there seems to be only a single seed used for evaluating GVR on 6h_vs_8z. \n\n* The authors seem to have skipped a few super hard SMAC maps such as corridor, 27m_vs_30m and 3s5z_vs_3s6z which are important to determine the scalability of their approach with more agents.\n\n* The code is missing from the submission for reproducibility.\n\n### References:\n\n[1] Wang, Tonghan, et al. \"Influence-based multi-agent exploration.\" arXiv preprint arXiv:1910.05512 (2019).\n\n[2] Liu, Iou-Jen, et al. \"Cooperative exploration for multi-agent deep reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Mahajan, Anuj, et al. \"Maven: Multi-agent variational exploration.\" arXiv preprint arXiv:1910.07483 (2019).\n\n[4] Gupta, Tarun, et al. \"Uneven: Universal value exploration for multi-agent reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[5] Rashid, Tabish, et al. \"Weighted qmix: Expanding monotonic value function factorisation.\" arXiv e-prints (2020) The paper is well motivated so as to ensure both IGM and TGM conditions, and therefore stable convergence in MARL. The ideas in the paper are novel, and ITS is based on a proof and can guarantee the stability of greedy actions.  However, I am not convinced that the proposed superior experience replay can destabilize the non-optimal greedy actions.  I have asked a clarification question about this and will improve my score based on a satisfactory response. Other than this, although the empirical results are good, the authors have missing baselines from the experimental evaluations, and missing experiments with respect to proving the scalability of GVR with more agents. Finally, I have concerns regarding evaluations reported on 6h_vs_8z. Therefore, I don\u2019t think the paper is ready for publication as is.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "9e3A3rYtvNP",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_sEIl_stzQyB",
                "title": "",
                "comment": "The authors introduced the optimal consistency for value decomposition methods in multi-agent reinforcement learning (MARL) and suggests True-Global-Max (TGM) condition along with Individual-Global-Max (IGM) proposed by QTRAN to achieve the optimal consistency. Then the authors suggested the greedy-based value representation (GVR) through the inferior target shaping and superior experience replay to ensure the TGM condition.\n Strength\n    The paper introduces new condition TGM for value deposition MARL,and the reviewer thinks that this condition should continue to be discussed in the value deposition method. The experiment for ablation study shows their claim well, and the performance of the proposed method GVR is shown to be outperformed than the state-of-the-art value decomposition methods in various experimental environments.\n\n    Weaknesses\n    1. In the paper, the authors used true $Q$ function and joint $Q$ value function, but reviewer doesn't find the definition of joint $Q$ value function. What is the difference between joint $Q$ value function and true $Q$ function?.\n    \n    2. In the equation (3), the authors expressed the utility function as true $Q$ function. If joint $Q$-value function is used for equation (3) instead of true $Q$ function, then the equation is true. However, the author use true $Q$ function and reviewer doesn't understand how to derive the equation (3). Some more explanations are needed\n    \n    3. In Appendix D and E, the authors prove their claim, but I couldn't follow the process of proof. How can the author express the utility function as equation (13) in appendix D? The reviewer doesn't understand how mapping $f$ occurs. Therefore, more explanation is needed to understand equations 13 and 18\n    \n    4. Finally, the authors madeThe author proposes a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the author can solve the above mentioned questions, the reviewer will raise the score. target of critic $V(s)$ as equation (9), but there is no explanation of why that can happen. You need explanation of reason for target of critic. The authors proposed a new additional condition for value decomposition in MARL, True-Global-Max (TGM) condition, which is reasonable in some respects, but the reviewer believe that in the proof of the author's claim, there are lots of explanation to understand. Thus, if the authors can solve the above mentioned questions, the reviewer will raise the score.",
                "rating": 3,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the current presentation of the paper",
                "Sentiment Expression": "the majority of the reviews are negative",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            }
        ]
    },
    "RmcPm9m3tnk": {
        "paper_id": "iclr_2021_RmcPm9m3tnk",
        "paper_title": "Generative Scene Graph Networks",
        "paper_abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.",
        "paper_acceptance": "poster-presentations",
        "meta_review": "This submission generated significant discussion between the reviewers; three of them ended up on the \"accept\" side, but one remained firmly in the \"reject\" camp.\n\nThe main strength of the paper is that it tackles a very hard problem: learning an unsupervised generative model (and accompanying inference model) of scene graph structures given only image data. As one reviewer mentioned, it is remarkable that the authors were able to get their system to work at all, given the seeming intractability of this problem. The work builds upon a clear line of prior work in this area, and the type of data on which it is evaluated (\"toy\" synthetic datasets a la CLEVR) is consistent with prior art.\n\nMultiple reviewers brought up the \"toy\" nature of the dataset as a drawback to the paper, but most agreed that this is not reason to reject the paper. Rather, the paper demonstrates a convincing proof of concept that this kind of model can be built, and improvements in the elements out of which the model is composed (generative and inference networks) should improve its applicability to real-world data.\n\nAnother question mark raised by multiple reviewers: could a simpler, handcrafted inference procedure work just as well or better? The authors included a new experiment against a hand-coded heuristic in their rebuttal, and their method outperforms it. One reviewer noted that more careful tuning might make a heuristic perform as well as the proposed method, but it is still clear that it is not trivial to get a hand-coded solution to perform well (even for this \"toy\" data). Another reviewer pointed out that this is one of the main attractions of variational inference methods: the ability to specific knowledge as simple generative priors rather than complex bottom-up inference procedures.\n\nOne reviewer, R1, remains negative about the paper. His (it is a he; I know this reviewer) main concern is that the scene graphs used are shallow and have a simple structure, and thus (a) it's not clear what value they add, (b) a simple postprocess could reconstruct them, assuming the individual object parts could be detected, and (c) it's not clear whether the method would generalize to deeper/more complex hierarchies. He believes this calls into question the validity of the entire method.\n\nI am sympathetic to this argument, but I think setting the bar this high may prevent progress in this field. For point (a), the authors included an image-manipulation application in their rebuttal--again, a proof of concept, not a directly useful tool. For point (b), the authors did compare against a hand-coded inference baseline and achieved better results, so while this may be possible, it is probably not as easy as the reviewer suggests. (c) remains an open question, to me. But even if this method as presented cannot generalize to more complex scene graphs, it likely paves the way for future work that can.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "zaYaEoYRhV",
                "reply_to": "iclr_2021_RmcPm9m3tnk",
                "title": "Not Sure If the Method is Significant Enough",
                "comment": "=====Post-Rebuttal Comment=====\n\nI thank the authors for the detailed response and the updated results. While my overall opinion of the work is slightly more positive post-rebuttal, I still maintain that this is a clear reject, primarily for the following reason:\n\n- The technical contribution (adding a hierarchical layer to SPAIR and demonstrate the hierarchy can also be learned without supervision) is not significant enough to accept purely based on a \"proof-of-concept\" of a \"new\" direction.\n- For incremental contributions, I expect the experimental results to be more convincing to be acceptable, a few points that I still really expect to see:\n    - comparisons on harder datasets when one doesn't have to go into a specific metric to show an edge over a prior art that is targeting a different application\n    - results on decomposition with more significant overlaps (especially in 2D) and on objects where part boundaries are harder to infer (actual 3D objects is still preferable)\n    - object-level manipulation (row 3&4 in fig 4 did not match description) and latent space interpolation\n\nI would also strongly encourage the authors to highlight the similarity and different between SPAIR and SPACE when introducing the latent code formulation.\n\n=====Summary=====\n\nThis work proposes a method that can learn a three-level (scene, object, part) hierarchical representation of scenes in an unsupervised manner. A variational autoencoder is used here, where the encoder recursively infers the shape and pose of individual objects and parts, and the decoder recomposites the inferred parts are then recomposited to the inferred poses. The proposed method is evaluated on two datasets created for this paper. The results suggest that the proposed method can reliably breaks down scenes into meaningful objects and parts, and performs slightly better than another method designed for a slightly different task in terms of reconstruction quality, learned representation, and data efficiency for downstream tasks.\n\n=====Strengths=====\n\n- The motivation of the need for hierarchy is solid, and the solution proposed seems to me to be a reasonable way to impose some sort of hierarchy.\n- The model seems to be well-tuned, utilizing appropriate training tricks and architectures.\n- Good performance for the evaluations chosen in the paper.\n\n=====Weaknesses=====\n\n- *Very* inadequate attribution of ideas. I am not too familiar with the AIR line of work, but I think quite a few ideas can be traced back to prior works. It would be much better if the authors can, in addition to a brief one sentence mention in related works, add clear discussions for the inspirations of the main design choices.\n- Missing discussions of relevant works that do: 1. unsupervised part decomposition e.g. \u201cUCSG-NET - Unsupervised Discovering of Constructive Solid Geometry Tree\u201d, \"Bae-net: Branched autoencoder for shape co-segmentation\", \u201cCvxNet: Learnable Convex Decomposition\u201d; 2. Learning hierarchical representations e.g. \u201cStructureNet: Hierarchical Graph Networks for 3D Shape Generation\u201d.\n- I think the comparisons in this work are neither adequate nor fair. I am not convinced that the two toy dataset used here can prove the superiority of the method. The authors claim that \u201cother works can\u2019t work on our dataset\u201d, but I think the burden of the proof is on the authors to show that their method is superior, even under a more specific setting. In other words, if the method is indeed \u201cgeneral\u201d and can learn good decompositions, then I would expect it to perform better even under an slightly unfair setting i.e. comparing against metrics/datasets adopted in other works. Furthermore, the datasets used in this paper appears to be way too simple as compared to real world data. The authors argue that dataset with a single shape is easier, but I disagree: datasets like partnet contains much more complicated part structures, as well as joints between parts, than what is used here, even with only a single shape. (And it is pretty evident from the qualitative examples that the challenging part is decomposing objects into parts, not decomposing scenes into objects). Last but not least, I want to see more evidence that the proposed method is actually useful in real applications.\n- The learned representation does not seem to be of very good quality, as seen in Figure 4.\n- A lot of overclaims, to name a few: 1. \u201cGSGN is a general framework for representing and inferring scene graphs of arbitrary depth\u201d: I don\u2019t think a model being able to work a toy setting with three levels will mean that the same framework can be used for more complex settings of arbitrary depth (as an analogy: MLP works for MNIST but not on ImageNet). If the framework can handle more general cases, then show it. 2. \u201cClosely follow the rendering process in graphics engines\u201d: I don\u2019t think applying affine transformations and compositing alone is enough to warrant this claim, it is pretty clear that the learned representation lacks a good sense of \u201cobjectness\u201d, as textures, lighting and etc. are all entangled together (evident in Fig 3). There does not seem to be a straightforward way to extend the method to truly parallel the 3D rendering process, neither. 3. \u201cFirst deep generative model for unsupervised scene-graph discovery\u201d: there are a lot of works that infer structures in an unsupervised way, I don\u2019t think it\u2019s fair to give a very narrow definition of \u201cscene graph\u201d and claim \u201cthe first\u201d. 4. \u201cGSGN has capture many predefined object types in the dataset\u201d: I don\u2019t think one can make this claim when there are only three primitives and ten types of objects\u2026\n\n=====Reasons for Score=====\n\nOverall, I have the impression that this work is cherry picking a very specific setting where the proposed architecture works reasonably well. For a work making a quite big claim of being \u201cthe first deep generative model that learns \u2026\u201d, I would expect much more comprehensive evaluations than what is currently shown here. Furthermore, many ideas in this work are not attributed properly, making the novelty of the method quite unclear. From my limited knowledge of the direct predecessors of this work, I don\u2019t think there is too much novelty in this paper. I would be more than willing to change my score if the authors can 1. Provide more comprehensive evaluations 2. State the novelty / discuss prior works more clearly. But for now, I tend to give a pretty clear reject.\n\n=====Additional Comments & Questions=====\n\n- I am not sure if translation & rotation alone is a good way to handle 2D renders of 3D objects, since any translation & rotation will result in change of perspectives and illumination e.g. rotating the bronze sphere in Figure 3, row 1 will make the specular highlight inaccurate and translating the blue cube in row 3 will make the top surface less visible. Could the authors justify why predicting translation/rotation make sense, when the perspective/illumination of the object already provides a really strong cue?\n- Following previous point: would like to see examples of the same learned object being used in multiple scenes.\n- The quality of the learned primitives, as seen in Figure 4, seems to be pretty underwhelming. If the aim of the work is discovering those primitives, would it make more sense to impose a stronger prior on the properties of the primitives?\n- Table 1 & 2: why are all the ELBO terms the same? I would imagine them to be different, especially for SPACE-O/P, which, if I understand correctly, is a *completely different* model with different architecture and loss formulation? \n- Table 2 & 3: why are the metrics so close between SPACE-P & GSCN in table 2 but so different in Table 3? Does that suggest unbalanced dataset?\n- Still Table 2 & 3: why no comparison between SPACE-O & GSGN for object level occlusion?\n- The paper claims that being able to handle background is a unique advantage as compared to other works, but the background used in the toy dataset is quite simple. Would like to see more complex examples.\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "WS49JqZPDhf",
                "reply_to": "iclr_2021_RmcPm9m3tnk",
                "title": "Technically impressive, but is the complexity necessary?",
                "comment": "**PAPER SUMMARY**\n\nThe paper presents a generative model for scenes that uses tree-structured latent variables to recursively decompose images into objects and parts, without any object or part supervision during training. The model is trained using variational inference. Experiments are performed on two new datasets (2D Shapes and Compositional CLEVR), demonstrating that the model is able to successfully uncover recursive scene/object/part decompositions in an unsupervised setting. The model is compared against prior work (SPACE) that performs non-hierarchical scene modeling.\n\n**STRENGTHS**\n\n- The paper presents a novel generative model that can infer tree-structured latent variables\n- The method is technically impressive, and a clear improvement over the non-hierarchical modeling used in prior work\n- The paper presents two new datasets (2D Shapes and Compositional CLEVR) for studying hierarchical scene decomposition. I hope these can be publicly released!\n\n**WEAKNESSES**\n- The method is quite complex, and though it is technically impressive I wish that it had been compared against very simple baselines\n- No experiments on real-world data\n- Unclear how the model will scale to wider and deeper trees\n- Many implementation details are unclear\n\n**SCALABILITY**\n\nA big selling point of the proposed model is that it can model the hierarchy of scenes into objects and parts, and the tree-based formulation of the latent space used to achieve this is technically impressive. However, how well would the method scale to larger scenes? All experiments used a relatively small three-level hierarchy where all nodes have a fixed out-degree of 4; thus in all experiments the full tree has just 21 nodes total. How well can the method scale to much larger trees, with deeper hierarchies or wider out-degrees?\n\n**PRIOR KNOWLEDGE**\n\nThe method requires choosing a tree depth and node out-degree as hyperparameters. To some extent, these hyperparameters encode very strong prior knowledge about the data being modeled (levels of hierarchy, and number of subparts within each part). In the experiments, these hyperparameters are perfectly matched to the synthetic datasets: you use a two-level tree with out-degree of four, and each image has between 1 and 4 objects, each of which is composed of between 1 and 3 parts. However in more complex real-world scenarios, you may not have such detailed knowledge of the world\u2019s compositional structure. For this reason, I\u2019m curious as to how the method would behave when the structural hyperparameters are mismatched to the underlying statistics of the dataset.\n\n**SIMPLE BASELINES**\n\nThe model is evaluated on two synthetic datasets -- 2D Shapes, and Compositional CLEVR. How difficult is the scene graph inference problem on these datasets? I wish that the authors had compared to very simple baselines in order to give a sense for how difficult the problem really is.\n\nFor example, I suspect that a simple \u201chandcrafted\u201d baseline that oversegmented the image into superpixels, then recursively merged superpixels based on proximity and simple appearance features (e.g. color histograms) could produce very plausible scene graphs for these two datasets.\n\nAt a high level, the promise of an end-to-end learning-based approach compared to a \u201chandcrafted\u201d approach like the above is that a learning-based approach should require less tuning, should be more adaptable to new datasets and tasks, and should scale better to complex real-world datasets where the assumptions of the model don\u2019t perfectly match the statistics of messy data from the real world. However in this case, I\u2019m not convinced that this complex variational method would be any simpler to implement or tune, or even give better results than, a very simple \u201chandcrafted\u201d baseline like the above on these synthetic datasets. There are also no experiments to demonstrate its scalability to real-world datasets.\n\nThis leads to a pointed question: If someone wanted to infer scene graphs from images, why should they prefer your approach over a very simple \u201chandcrafted\u201d approach?\n\n**MANY IMPLEMENTATION DETAILS UNCLEAR**\n\nThere are many implementation details that are unclear from the paper and supplementary material, without which reproducing the results are practically impossible. Even if some of these details do make sense as part of the main text, they should be specified more explicitly in the supplementary material. For example:\n\nWhat is the generative process for the presence variables $z_v^{pres}$? This is not clear from Equations 3 or 4, nor the surrounding discussion. From Equation 9, the encoder predicts presence variables conditioned on image patches; but how can you predict the presence variables when sampling an image from scratch?\n\nWhat is the dimension of the latent variables $z^{appr}_r?$\n\nYou say that the pose variables are Gaussian, but how is the pose parameterized in terms of relative location and scale?\n\nFrom the discussion after Equation 6, the pose variables also include a depth map -- how are these depth maps represented and parameterized? Are they per-pixel Gaussians like the other pose variables?\n\nHow exactly are the transparency maps $\\alpha_v$ computed from the depth maps? The text states that they are computed \u201cby the softmax over negative depth values\u201d, but this is hard to understand -- what are the sets of values being fed to softmax? If the depth map contains a per-pixel depth then a softmax over space wouldn\u2019t make sense, since this would cause the transparency of an object to depend on its spatial size (since it would have more pixels competing in the softmax).\n\nIn Equation 9, the distributions of the latents $z_v^{pres}$, $z_v^{pose}$ are conditioned only on data from the parent node $z_{pa(v)}^{appr}$ and $z_{pa(v)}$. Thus all children of a node will have the same distribution for their pose and appearance. Is this correct? If so, how do you encourage the child nodes to cover all parts of the parent, and not collapse to a single part of the parent?\n\nThere are no details about any of the neural network architectures used to implement the model, nor any details about any training hyperparameters (e.g learning rates, training schedule, regularization strengths etc).\n\n**SUMMARY**\n\nThe model is technically impressive, and a clear improvement over prior work. However on the whole I\u2019m not sure whether the complexity of the method is actually necessary to solve the problem at hand; I wish that the authors had done a better job demonstrating the benefits of the proposed method over very simple baselines. There are also many implementation details that are very unclear, for which reason I fear that the paper as written is utterly unreproducible.\n\nOn the whole I lean slightly toward acceptance, but I hope the authors can address my concerns in their rebuttal.\n\n\n**AFTER REBUTTAL**\n\nThe rebuttal largely addresses my concerns about implementation details.\n\nI am pleased to see the additional experimental results provided by the authors; I think that these do improve the paper. I still feel that some well-tuned handcrafted approach could likely perform on-par with the results of the proposed method, but the comparison with [Wei et al] show that achieving such results is at least not trivial, which does help to better ground the complexity of the task. The additional experiments with a three-level hierarchy show a bit more evidence for scalability than provided in the original paper. While these extra experiments do strengthen the paper, I feel that they don't really address the core issue with the paper, which is whether there is any hope for the proposed method to scale to more complex and realistic datasets.\n\nOverall I think that this is a reasonable paper and I still lean slightly toward acceptance, so I maintain my original rating of 6.",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "lygkHt1HkxM",
                "reply_to": "iclr_2021_RmcPm9m3tnk",
                "title": "auto-encoder for object-part scene graphs in simple environments",
                "comment": "Generative Scene Graph Networks (GSGN) is a variational auto-encoder with the intermediate representation being tree-like scene graphs. The leaf nodes stand for primitive parts and edges stand for poses to compose parts into objects recursively. The experiments are done in two image datasets of single color, simple shape 2D/3D objects: Multi-dSprites and CLEVR, and the model is able to discover objects without supervision.\n\n**Strength**: I find the direction important, and the method well established in a variational inference framework with graphics-inspired designs. Experiment numbers look good in general.\n\n**Weakness**: Perhaps my biggest concern is that the current datasets are a bit weak. First, objects are too simple (single color, simple shape), so we are not sure if GSGN can work with more realistic visual domains where objects have more complex 3D structure or texture. Second, the object/primitive decomposition is slightly weird to me. I would expect hierarchal structure like an object being human body, and parts being legs, arms, head and so on. But in this work a \"part\" is a single-color object, and an \u201cobject\u201d is a bunch of single-color adjacent objects. Based on these two points, I believe the paper will be much stronger with experiments on more complex objects like humans or tables or \n\nAnother concern is the application of learned scene graph. The paper only shows unconditional sampling, but not really how to use the learned scene graph. For example, I'd expect scene graphs to be used for image manipulation, as one can change part of the object (shape, color, pose) without changing the rest. Showing the learned scene graph is useful for any downstream tasks can be a great plus for the current work.\n\nFinally, I wonder how variational the learned scene graphs can be, as the objects in the datasets are fairly simple and the learning might be easy. I'd be happy to see some analysis but this is not my main concern.\n\n---------\n\nAfter rebuttal: I'm glad they added some experiments and analysis I wanted to see, so I raise the score to 6. As the authors said, the paper is a proof-of-concept of unsupervised hierarchal scene graph learning, and the rebuttal to some degree reassured me. For example, modeling a cube on top of another top as two parts of an object (which was weird to me: why not each cube as an object?) helps edit tasks where the top cube is enlarged but the \"on top\" relation is maintained. The downstream representation transfer also makes sense. Of course experiments are still toy from computer vision perspective, but I'm now okay with acceptance.\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "2R26oAjJTID",
                "reply_to": "iclr_2021_RmcPm9m3tnk",
                "title": "A convincing proof-of-concept solution for a difficult new task",
                "comment": "== Update ==\n\nThank you for your response and clarifications. I have left my score as is.\n\n== Original Review ==\n\nThe paper presents an unsupervised method for inferring scene graphs from images. Building upon\nscene-attention methods such as AIR and SPACE, it hierarchically decomposes a scene into objects and\nthose objects into parts, giving rise to a tree structure. It is shown that this model successfully\nrecovers the hierarchies underlying the data on two newly proposed hierarchical variants of the\nSprites and CLEVR datasets.\n\nStrengths:\n 1. The paper is well written, and despite the considerable complexity of the method, its\n    presentation is relatively easy to follow.\n 2. The task of interest is well-defined, and has clearly been effectivly solved on the datasets\n    considered. Both quantitative and qualitative evaluations make it very clear that the model has\n    learned to infer the correct scene graphs as desired. Its ability to infer the appearance of\n    occluded parts is especially impressive.\n 3. While there are no direct competitors on this newly defined task, the paper does a decent job of\n    comparing to the closest available baseline, showing how the additional structure can be\n    beneficial.\n\nWeaknesses:\n 1. One may argue that the datasets have been deliberately constructed to showcase the model. While\n    that is probably true, I think this is a valid approach given the novel nature of the task and\n    the lack of supervision. Despite the clearly helpful structure (limited number of objects and\n    parts), the datasets still appear sufficiently challenging.\n 2. In the experiments, scene graphs are limited to trees of height 2 and degree 4. This is a\n    significant constraint, however, each additional level of hierarchy introduces ambiguities and\n    makes it harder to learn the graph in an unsupervised manner. More complicated structures would\n    likely require supervision.\n 3. As far as I can tell, the object types were chosen once to generate the datasets, and then kept\n    fixed across the experiments. Reporting results for multiple different datasets with randomly\n    chosen object types would be somewhat more convincing.\n 4. As is common for unsupervised scene models, the proposed method likely only works on synthetic\n    images in its current state. However, due to the additional structural assumptions on the data,\n    it seems especially challenging to find suitable real-world use-cases.\n\nOverall, the paper presents an effective new method for the task it sets out to solve. While it is\nquestionable how it would work on real-world data, I believe the paper is of sufficient interest as\na proof of concept, and am therefore leaning towards acceptance.\n\nQuestions:\n 1. It is stated that auxiliary KL terms are added, with the sparseness constraint on $z^{pres}$\n    being one of them. But is not clear if there are others. This would be important to know in\n    order to evaluate how strong the model's inductive biases are.\n 2. The downstream task used for Fig. 5 is not clear to me. If the number of parts is computed for\n    each object, and these numbers are then summed, isn't the result equal to the total number of\n    parts in the scene, which SPACE-P can also infer? If only distinct parts are counted,\n    how is equality of parts defined on the dataset?",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "0PXW5ucde7B",
                "reply_to": "iclr_2021_RmcPm9m3tnk",
                "title": "To All Reviewers",
                "comment": "We thank all reviewers for their time and insightful feedback. In this general response, we would like to address the main concern about the simplicity of our datasets.\n\nWe understand that some prior work on part decomposition has been applied to real-world datasets such as ShapeNet and PartNet. However, these works have three significant differences from ours. First, most of the works require some kind of supervision while ours is fully unsupervised. This usually includes 3D supervision (requiring voxels or point clouds as input) and part-level supervision (providing pre-segmented parts or decomposing into predefined parts like boxes and spheres). Second, they typically focus on geometry (e.g., part occupancy) without learning to represent appearance (e.g., color, material) simultaneously. Third, most of these methods can do either inference or generation, but not both. Lastly, most of these models do not support the controllability of the representation to generate novel out-of-distribution scenes.\n\nA recent line of work on unsupervised object-centric representation learning aims to eliminate the need for supervision in structured scene understanding. These methods learn **a holistic generative model capable of decomposing scenes into objects, learning appearance representations for each object, and generating novel scenes via controllable composition of object representations---all without supervision and in an end-to-end trainable way.** We believe such unsupervised and holistic models are more desirable, albeit more challenging to learn.\n\nAlthough it is a promising approach with great potential in the long run, the unsupervised object-centric representation learning approach has a more ambitious goal. The methods are in its very infancy, and thus currently have some difficulty dealing with real-world data; the most complex datasets used in state-of-the-art models are evaluated on CLEVR-like datasets. Some representatives include IODINE (ICML 2019), SPACE (ICLR 2020), and Slot Attention (NeurIPS 2020). However, we believe future advancements will likely generalize them to more complex data. Thus, **as pointed by Reviewer 1, we hope the reviewers to see our paper as a proof-of-concept paper about a challenging but promising direction.** Accordingly, we will make our claim about proof-of-concept clearer. \n\nIn this paper, we take a step forward in this specific line of research by further decomposing objects into parts. Our model is also unsupervised and holistic, in the sense that it can infer part-object structures, learn appearance representations, perform image manipulation (newly added Figure 4), and generate objects and scenes. **We are not aware of existing part decomposition methods that can do all of the above in a single model without supervision.** \n\nWhile we agree that it would have been better if we made our model work on complex natural images, we believe that our compositional CLEVR dataset, where an object is composed of several CLEVR-like shapes, is still **significantly more complex than the datasets used in previous works in the same line of research.** We showed that the severe occlusion among parts presents some difficulty to SPACE (Table 3). We believe that by introducing hierarchical structures and demonstrating effectiveness on this more challenging dataset, our model makes significant progress in the line of unsupervised object-centric representation learning.\n\nDuring the rebuttal period, we made a slightly more complex version of the Compositional CLEVR dataset, by introducing four new parts. We obtained similar results on this new dataset (Section E). We acknowledge that this is still simple compared to real-world data, and we expect that by upgrading the inference module, our model would benefit from future advancements that can decompose more complex scenes.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kZp2G8ai4FW",
                "reply_to": "WS49JqZPDhf",
                "title": "Response to Reviewer 3 (1/2)",
                "comment": "Thank you for your insightful and very detailed feedback. We are encouraged that you find our method technically impressive and a clear improvement over prior work. We would like to address your concerns and answer your questions in the following.\n\n**IS THE COMPLEXITY NECESSARY?**\n\nThe actual implementation is quite simple, although some complexity is needed for the mathematical formulation. To illustrate this, in the supplementary material we added Algorithm 1 and Algorithm 2 describing foreground inference (implementation of Equation 9), and Figure 9 depicting network architectures. From this, we expect that one will see how easy to implement this model. The main operations performed for decomposition at each level are just one feedforward pass through each of the three CNN submodules.\n\n> Compare to very simple baselines.\n\nThanks for your suggestion. We added qualitative results from a simple baseline called Superpixel Hierarchy (Wei et al., 2018) in Section D. This algorithm hierarchically merges pixels into superpixels based on connectivity and color histograms, until the full image is merged into one superpixel. It can then produce segmentation results when given the desired number of superpixels. We evaluate its ability to perform object and part grouping by providing the groundtruth number of components (the number of objects/parts plus one for background) as the desired number of superpixels. We find that our datasets present some difficulties to this simple baseline. The 2D Shapes dataset contains very tiny shapes, which tend to be merged into the background. Also, the parts within an object do not touch each other, making it hard to merge them into a single 4-connected superpixel. In the Compositional CLEVR dataset, the shadows in the background and the specular highlights on metal materials cause color dissimilarity within components, leading to incorrect decomposition.\n\n> Why should someone prefer your approach over a very simple \u201chandcrafted\u201d approach?\n\nFirst, we showed that the simple baseline we considered was incompetent to infer the scene graph even when given the groundtruth number of objects and parts. Second, our model does more than scene graph inference:\n\n- It infers the appearance of each part, which enables the model to inpaint occluded parts (many examples in Figure 3)\n- It is able to composite new objects by reconfiguring the pose and appearance variables in an inferred scene graph (newly added Figure 4B)\n- It can perform unconditioned scene generation, which even SPACE cannot do (Figure 5)\n- The learned hierarchical representations can improve data efficiency in downstream tasks (Figure 6)\n\nWe believe a model unifying the above capabilities is a meaningful contribution.\n\n**SCALABILITY & MISMATCHED STRUCTURAL HYPERPARAMETERS**\n\nWe implemented GSGN-9, a three-level GSGN where each node has an out-degree of 9, resulting in a tree with 91 nodes. We evaluated its performance on the Compositional CLEVR dataset, and updated the results in Table 2 and Table 3. We observed a slight drop in performance when compared to GSGN. However, GSGN-9 is still much better than SPACE-P at identifying parts that have severe occlusion. The slightly worse part F1 score is caused by slightly inaccurate prediction of center positions. If we increase the error tolerance from 5 pixels to 6 pixles, then the part F1 score of GSGN-9 becomes 99.07%, which is comparable to other models.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HMOqj6VaGH-",
                "reply_to": "zaYaEoYRhV",
                "title": "Response to Reviewer 2 (2/2)",
                "comment": "> A lot of overclaims, to name a few:\n\nThanks for pointing this. We agree that the tone of some of our claims requires some adjustment. We made the adjustment as follows:\n\n> 1. \u201cGSGN is a general framework for representing and inferring scene graphs of arbitrary depth\u201d: I don\u2019t think a model being able to work a toy setting with three levels will mean that the same framework can be used for more complex settings of arbitrary depth (as an analogy: MLP works for MNIST but not on ImageNet).\n\nWe changed it to \"While our formulation of GSGN allows representing and inferring scene graphs of arbitrary depth, in our experiments, we have only investigated the effectiveness of a three-level GSGN\".\n\n> 2. \u201cClosely follow the rendering process in graphics engines\u201d: I don\u2019t think applying affine transformations and compositing alone is enough to warrant this claim, it is pretty clear that the learned representation lacks a good sense of \u201cobjectness\u201d, as textures, lighting and etc. are all entangled together.\n\nWe changed it to \"follow the recursive compositing process in graphics engines\".\n\n> 3. \u201cFirst deep generative model for unsupervised scene-graph discovery\u201d: there are a lot of works that infer structures in an unsupervised way, I don\u2019t think it\u2019s fair to give a very narrow definition of \u201cscene graph\u201d and claim \u201cthe first\u201d.\n\nWe changed it to \"first deep generative model for unsupervised scene-graph discovery from multi-object scenes without knowledge of individual parts\".\n\n> 4. \u201cGSGN has capture many predefined object types in the dataset\u201d: I don\u2019t think one can make this claim when there are only three primitives and ten types of objects\n\nWe changed it to \"GSGN has captured many predefined object types in the two datasets considered\".\n\n**ADDITIONAL QUESTIONS**\n\n> Why predict translation/rotation, when the perspective/illumination of the object already provides a really strong cue?\n\nTranslation/rotation are inherent components of the scene graph. They identify an image region as an object/part, allowing structured understanding of the scene. Our current model learns the 2D appearance of objects/parts in a single rendered image. It does not learn 3D appearance, which would require a set of input images from multiple viewpoints. Hence, we cannot explicitly or accurately model perspective/illumination.\n\n> Examples of the same learned object being used in multiple scenes.\n\nPlease see row 3-4 of the newly added Figure 4A, where we replace an object of the current scene with an object from another scene. As we explained above, we cannot expect to model perspective/illumination changes from a single input image.\n\n> Table 1 & 2: why are all the ELBO terms the same?\n\nELBO is usually dominated by the reconstruction error. Here, we normalized the ELBO by the number of pixels, which is common practice in VAE literature. A similar ELBO in this case indicates similar reconstruction quality. The precise ELBO values are different.\n\n> Table 2 & 3: why are the metrics so close between SPACE-P & GSGN in Table 2 but so different in Table 3? Does that suggest unbalanced dataset?\n\nHere are the statistics over the 12800 test images. We did not have direct control over the minimum number of pixels per part.\n\n| Min Visible Pixels Per Part | <100 | 100~200 | >200 |\n|:---------------------------:|:----:|:-------:|:----:|\n|    Number of Test Images    |  49  |  3772   | 8979 |\n\n> Table 2 & 3: why no comparison between SPACE-O & GSGN for object level occlusion?\n\nThere is no significant difference in overall performance on object-level decomposition, so we thought it would be unnecessary.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YY1qlh4Cohb",
                "reply_to": "2R26oAjJTID",
                "title": "Response to Reviewer 1",
                "comment": "Thank you for your positive and thoughtful comments. We are encouraged that you find our paper well written and the inference of occluded parts impressive. We would like to address your concerns and answer your questions in the following.\n\n> Deliberately constructed datasets with limited number of objects and parts.\n\nWe appreciate that you think this approach is valid and the datasets are sufficiently challenging. While the number of objects and parts are limited, we designed each object to have a different structure in terms of the relative pose of its parts. This helps cover a wide range of scene graphs, of which we added an analysis in Section F.\n\n> Limited tree height and width.\n\nWe implemented GSGN-9, a three-level GSGN where each node has an out-degree of 9. We evaluated its performance on the Compositional CLEVR dataset, and updated the results in Table 2 and Table 3. GSGN-9 is still much better than SPACE-P at identifying parts that have severe occlusion. The slightly worse part F1 score is caused by slightly inaccurate prediction of center positions. If we increase the error tolerance from 5 pixels to 6 pixles, then the part F1 score of GSGN-9 becomes 99.07%, which is comparable to other models. While we did not evaluate GSGN-9 on more complex data, our results demonstrate the effectiveness when structural hyperparameters are mismatched to data (suggested by Reviewer 3), and show some potential for scalability.\n\n> Reporting results for multiple different datasets.\n\nThanks for the suggestion. We made a slightly more complex version of the Compositional CLEVR dataset with four new parts. Results are similar, and we added them in Section E. Although the object types were not randomly chosen, we believe this new result is still meaningful.\n\n> Real-world use cases.\n\nWe agree that our model currently cannot deal with real-world data. However, because we use scene decomposition models as an inference module, our model is likely to benefit from future advancements that generalize scene decomposition models to more complex scenarios.\n\n**ADDITIONAL QUESTIONS**\n\n> Auxiliary KL terms.\n\nWe added description of auxiliary KL terms in Section C.3. We note that the prior distributions in these terms were not chosen to match the actual distributions in the datasets. Also, we used mostly the same terms on both of our datasets.\n\n> How is the number of parts computed in the downstream task?\n\nWe clarified this in our description. Only distinct parts are counted within each object. Two parts are considered the same if they have the same shape, regardless of their pose, color, and material.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "btauxoKwG-",
                "reply_to": "lygkHt1HkxM",
                "title": "Response to Reviewer 4",
                "comment": "We appreciate your constructive comments. We would like to address your concerns below.\n\n> Objects are too simple (single color, simple shape).\n\nPlease see our general response regarding this concern. We also made a slightly more complex version of the Compositional CLEVR dataset with four new parts. We obtained similar results on this new dataset (Section E).\n\n> The object/primitive decomposition is slightly weird.\n\nWe agree that the compositional objects in our datasets do not look realistic. However, given that state-of-the-art unsupervised object-level decomposition models can only deal with simple shapes, we think it is reasonable to use these shapes as primitives. Also, some simple real-world objects like dumbbells and hair dryers have similar structures as those in our datasets, albeit with more complex shapes as primitives.\n\n> Application of learned scene graphs (e.g., image manipulation).\n\nThanks for your suggestion. We added image manipulation results in Figure 4. We showed that one can individually modify the position and scale of objects and parts. In addition, we demonstrated that by reconfiguring the appearance variables, one can add objects from other scenes, and generate novel objects by compositing parts from multiple scenes.\n\n> Analysis of how variational the learned scene graphs can be.\n\nWe added some analysis in Section F, regarding the number of possible scene graphs.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Y7fgJw9fk3",
                "reply_to": "zaYaEoYRhV",
                "title": "Response to Reviewer 2 (1/2)",
                "comment": "Thank you for your very detailed comments and suggestions for improvement. We would like to address your concerns and answer your questions in the following.\n\n> Unclear novelty and simple datasets.\n\nPlease see our general response regarding this concern.\n\n> Very inadequate attribution of ideas.\n\nWe improved the related work section as you suggested. We would like to mention that we did give credit to prior work when introducing our main design choices. For example, we cited relevant prior work when introducing parameter sharing and auxiliary KL loss.\n\n> Missing discussions of relevant works.\n\nThanks for pointing to relevant works. We have added them to the related work section. In general, these works solve different problems than ours:\n\n- The first three methods take 3D voxels as input, and predict part geometry (i.e., occupancy). They only work for single objects, and cannot model appearance (color, material, etc). In contrast, our model takes 2D images of multi-object scenes as input, and infers both the decomposition and appearance of each part.\n- The first two methods have some additional assumptions. UCSG-NET assumes predefined primitive parts, such as box and sphere, while our model can learn to discover the primitive parts purely from data. Bae-net assumes the objects come from the same category, thus having similar part structures. In particular, all objects have the same number of parts. Our model can infer the number of parts, and does not require that all objects have similar structures.\n- In StructureNet, the part hierarchy is provided as input to the model, rather than inferred from the input.\n\n> Comparing against metrics/datasets adopted in other works.\n\nWe have compared with the closest baselines, and we do not think it is fair or suitable to compare with other works. As we mentioned in the experiment section, \"Previous work on hierarchical scene representations assumes single-object scenes (Kosiorek et al., 2019) and requires predefined or pre-segmented parts (Li et al., 2017; Huang et al.,2020)\". We do not think it is fair to compare our model (without access to groundtruth parts) to (Li et al., 2017; Huang et al.,2020). Also, their datasets use voxels/point clouds as input, which is a different problem setting. In (Kosiorek et al., 2019), the main metric is unsupervised classification accuracy on MNIST, CIFAR10, and SVHN. These datasets do not seem suitable for evaluating interpretable part decomposition. And as we discussed above, the papers you mentioned solve different problems than ours, and also require different input formats.\n\n> The authors argue that dataset with a single shape is easier, but I disagree.\n\nWe did not say that dataset with a single shape is easier. What we intended to argue is that methods that work on single-object scenes circumvent the challenge of grouping parts into objects, because they assume that all parts belong to the same object. Clearly this assumption cannot hold when there are multiple objects. Our model directly addresses this challenge by taking a top-down inference approach. This is in itself a contribution. If we took a bottom-up approach, then when objects are close, it would be very hard to determine which part should belong to which object. Also, the fact that SPACE-P cannot separate occluded parts indicates that the bottom-up approach would likely be sub-optimal. We believe that solving this part grouping challenge is orthogonal to learning complicated part structures of a single object.\n\n> Real applications.\n\nAs suggested by Reviewer 4, we added Figure 4 showing an application of the learned scene graph in image manipulation. We demonstrated that by modifying the pose and appearance variables in a learned scene graph, one can individually manipulate the objects and parts in the scene. Also, by combining parts from multiple scenes, one can composite new objects that are never seen during training. While our model currently cannot deal with real-world data, it is likely that our inference module can be upgraded to incorporate future advancements that generalize scene decomposition models to more complex scenarios.\n\n> The learned representation does not seem to be of very good quality, as seen in Figure 4.\n\nWe respectfully disagree. Figure 3 would be more appropriate for investigating the quality of learned representations. Figure 4 (Figure 5 in updated version) shows unconditioned generations. The latent variables are not inferred from a given image, but directly sampled from the prior, i.e., the root node is sampled from $\\mathcal{N}(0,1)$, and others are sampled from conditional priors. Typically in VAEs, we cannot expect unconditioned generations to be of very good quality. However, it is clear from the figure that our model gives better object generations than SPACE. Also, our model can generate meaningful scenes while SPACE can only generate empty scenes.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-rKTowOoQxE",
                "reply_to": "WS49JqZPDhf",
                "title": "Response to Reviewer 3 (2/2)",
                "comment": "**IMPLEMENTATION DETAILS**\n\nWe added implementation details to the supplementary material. We will also release the code and the Compositional CLEVR dataset upon publication.\n\n> What is the generative process for the presence variables $z_v^{pres}$?\n\nIn Equation 4, the term $p(z_v^{pres} \\!\\mid\\! z_{pa(v)}^{appr})$ suggests that during generation, the presence variable of node $v$ is conditioned on the appearance variable of its parent $pa(v)$. This makes sense because $z_{pa(v)}^{appr}$ is expected to summarize lower-level compositions. Note that this conditional distribution is a Bernoulli distribution, and as mentioned in the text right below Equation 4, its distribution parameter is learned by an MLP.\n\n> What is the dimension of the latent variables $z_r^{appr}$?\n\nOn both datasets, the part-, object-, and scene-level appearance variables have dimensions 32, 64, and 128, respectively.\n\n> How is the pose parameterized in terms of relative location and scale?\n\nWe clarified this in Section C.2. In short, the Gaussian variables are first converted to the desired range through nonlinear squashing functions, and then used to calculate the affine transformation matrix in the usual way required by the spatial transformer.\n\n> Depth maps and transparency maps.\n\nWe clarified this in Section C.2. The pose variables are not per-pixel, but per-node variables. Specifically, we use a 1-dimensional depth variable for each node $v$ to represent the relative depth of $v$ with respect to its siblings (e.g., if $v$ is a part of object $u$, then the siblings of $v$ are the other parts of the same object $u$). This depth value is broadcast to all pixels that would belong to node $v$ if there were no occlusion. In regions of occlusion, each pixel will maintain multiple depth values. The transparency map is then computed by a per-pixel softmax over the negative depth values, to select for each pixel the entity that contains it and has the smallest depth.\n\n> In Equation 9, the distributions of the latents $z_v^{pres}$, $z_v^{pose}$ are conditioned only on data from the parent node $z_{pa(v)}^{appr}$ and $x_{pa(v)}$. Thus all children of a node will have the same distribution for their pose and appearance. Is this correct? If so, how do you encourage the child nodes to cover all parts of the parent, and not collapse to a single part of the parent?\n\nGood question. First, the distributions are not the same, because the network parameters are not shared among the children nodes of the same parent. In fact, the children nodes are inferred in parallel through a CNN. Second, similar to SPAIR and SPACE, we divide $x_{pa(v)}$ into grid cells. Each child node is associated with one cell and is encouraged to identify a part that is close to that cell. We made the above clear in the supplementary material by showing network architectures in Figure 9 and describing the parameterization of positions in Equation 15.\n\n> Details about neural network architectures and training hyperparameters.\n\nThanks for the suggestion. We added Section C describing the implementation details.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "tackles a very hard problem: learning an unsupervised generative model (and accompanying inference model) of scene graph structures given only image data",
                "Sentiment Expression": "The main strength of the paper",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the authors were able to get their system to work at all, given the seeming intractability of this problem",
                "Sentiment Expression": "it is remarkable",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The work builds upon a clear line of prior work in this area, and the type of data on which it is evaluated ('toy' synthetic datasets a la CLEVR)",
                "Sentiment Expression": "is consistent with prior art",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper demonstrates a convincing proof of concept that this kind of model can be built, and improvements in the elements out of which the model is composed (generative and inference networks)",
                "Sentiment Expression": "should improve its applicability to real-world data",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "a new experiment against a hand-coded heuristic in their rebuttal, and their method",
                "Sentiment Expression": "outperforms it",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "remains negative",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "the scene graphs used are shallow and have a simple structure, and thus (a) it's not clear what value they add, (b) a simple postprocess could reconstruct them, assuming the individual object parts could be detected, and (c) it's not clear whether the method would generalize to deeper/more complex hierarchies",
                "Sentiment Expression": "main concern",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "WXwg_9eRQ0T": {
        "paper_id": "iclr_2022_WXwg_9eRQ0T",
        "paper_title": "MergeBERT: Program Merge Conflict Resolution via Neural Transformers",
        "paper_abstract": "Collaborative software development is an integral part of the modern software development life cycle, essential to the success of large-scale software projects. When multiple developers make concurrent changes around the same lines of code, a merge conflict may occur. \nSuch conflicts stall pull requests and continuous integration pipelines for hours to several days, seriously hurting developer productivity.\n\nIn this paper, we introduce MergeBERT, a novel neural program merge framework based on the token-level three-way differencing and a transformer encoder model. Exploiting restricted nature of merge conflict resolutions, we reformulate the task of generating the resolution sequence as a classification task over a set of primitive merge patterns extracted from real-world merge commit data.\n\nOur model achieves 63--68\\% accuracy of merge resolution synthesis, yielding nearly a 3$\\times$ performance improvement over existing structured, and 2$\\times$ improvement over neural program merge tools. Finally, we demonstrate that MergeBERT is sufficiently flexible to work with source code files in Java, JavaScript, TypeScript, and C\\# programming languages, and can generalize zero-shot to unseen languages.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper is a fair effort, making some headway on a problem of practical importance. \nThere was some discussion of scoping and whether the contribution was Machine-Learning-y enough. \nI'm kind of ambivalent on that particular question: I think the general rule is that the further out-of-scope the paper seems, the better the results need to be for people to overlook it. \nI think in this case, unfortunately, even the two most positive reviewers did not evince enough excitement about this paper for it to get accepted in light of the scoping concerns. \nGiven the various constraints involved, I don't think I can recommend acceptance.\n\nIn order to get it accepted into a future conference I would recommend either:\na) Submit to a more Software-Engineering focused venue\nb) Really shore up the evaluation such that the reviewers sympathetic to this kind of paper will find it unimpeachable and score it more generously.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "oQOkOfcsh-H",
                "writer": "author",
                "reply_to": "AHus-1usngI",
                "title": "Re: Discussion",
                "comment": " > Where do the `1` and `1000` numbers in your statement come from? I don't know whether you're implying recall or precision by saying that. Of course, the recall is important but the level of importance depends on applications. For conflict resolution, it would easier for a user to resolve the conflict themselves than trying to understand what's wrong with a resolution generated by an imprecise automated tool. \n\nWe agree with the reviewer that both precision and recall metrics are important for the user experience. In the paper, we estimate the F-score, a harmonic mean of precision and recall, but other metrics could be measured during A/B tests or user studies: we elaborate on this below. The \u201c1\u201d and \u201c1000\u201d are metaphoric numbers, referring to a hypothetical tool with a low recall (but possibly high precision). \n\n> That being said, I am not asking for a 100% precision, but there may be a better trade-off between precision and recall by calibrating your model.\n\nAn optimal trade-off between precision and recall would depend on various factors around the deployment and UX. \nHowever, there are several knobs one could use to increase the \u201conline\u201d precision of the tool (\u201conline\u201d as in IDE or a version control system): \n1. Show top-K suggestions: more than 1 merge resolution suggestion could be shown e.g. as a dropdown list in an IDE (MergeBERT top-3 precision is 76%)\n1. Introduce a log-probability threshold to further increase the precision\n\nWe believe for actual deployment, an automatic resolution suggestion tool such as MergeBERT could be paired with oracles (require resolution to pass all unit tests in the project, or even formal verification of semantic conflict freedom) that ensure that the merged code is \"correct\". In such a case, the high recall from MergeBERT (75% top-3) becomes crucial to produce a semantically correct merge.\n\n> About the example, thanks for the derivation. How do you decide where and how many PAD tokens you need to add?\n\nFirst, we utilize difflib\u2019s (https://docs.python.org/3/library/difflib.html) `SequenceMatcher` method to determine the longest contiguous matching subsequence following the Ratcliff-Obershelp algorithm. This algorithm will return a list of 5-tuples of the form `(tag, i1, i2, j1, j2)` describing how to turn one sequence into another. We then iterate over these 5-tuples, and, depending on the `tag`, decide on the placement of the `<pad>` tokens.\n\nLet us walk through the example. Given the tokenized sequences:\n\n`A_tokens`: `['--', 'num_cores', '=', '2']`\n\n`B_tokens`:  `['--', 'max_length', '=', '256', '--', 'num_cores', '=', '2']`\n\nA call to `SequenceMatcher` would give a list of following 5-tuples:\n1. `('+', 0, 0, 0, 4)` # insert 4 tokens in the beginning of A\n1.  `('=', 0, 4, 4, 8)` # unchanged\n\nWe then iterate over the tuples to obtain the resulting diff token sequences for A and B (denoted as `result_A_tokens` and `result_B_tokens`), as well as the edit types. \n\nThe first tuple gives (\"+\" tag, corresponds to insert action):\n```\nedit_types +=  [\"+\"] * (j2 - j1)               # ['+', '+', '+', '+']\nresult_A_tokens += [\"<pad>\"] * (j2 - j1)       # ['<pad>', '<pad>', '<pad>', '<pad>']\nresult_B_tokens += B_tokens[j1:j2]             # ['--', 'max_length', '=', '256']\n```\n\nSince the tag for the second tuple is \u201c=\u201d (equal), there are no padding tokens added:\n```\nedit_types +=  [\"=\"] * (j2 - j1)     # ['=', '=', '=', '=']\nresult_A_tokens += A_tokens[i1:i2]   # ['--', 'num_cores', '=', '2']\nresult_B_tokens += B_tokens[j1:j2]   # ['--', 'num_cores', '=', '2']\n```\n\nFor completeness, we also share the Python code from Yin et. al. We can add pseudocode in the appendix of the final version.\n```\ndef sequence_diff(base, reference):\n\n    matcher = difflib.SequenceMatcher()\n\n    base_tokens = []\n    reference_tokens = []\n    edit_types = []\n    matcher.set_seqs(base, reference)\n\n    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n        if tag == \"=\":\n            change_types.extend([0] * (i2 - i1))  # change type \"=\"\n            base_tokens.extend(base[i1:i2])\n            reference_tokens.extend(reference[j1:j2])\n        elif tag == \"-\":\n            change_types.extend([1] * (i2 - i1))  # change type \"-\"\n            base_tokens.extend(base[i1:i2])\n            reference_tokens.extend([\"<pad>\"] * (i2 - i1))\n        elif tag == \"+\":\n            change_types.extend([2] * (j2 - j1))  # change type \"+\"\n            base_tokens.extend([\"<pad>\"] * (j2 - j1))\n            reference_tokens.extend(reference[j1:j2])\n        elif tag == \"<->\":\n            largest_span_size = max(i2 - i1, j2 - j1)\n            change_types.extend([3] * largest_span_size)  # change type \"<->\"\n            reference_tokens.extend(base[i1:i2] + [\"<pad>\"] * (largest_span_size - (i2 - i1)))\n            reference_tokens.extend(reference[j1:j2] + [\"<pad>\"] * (largest_span_size - (j2 - j1)))\n    assert len(change_types) == len(base_tokens) == len(reference_tokens)\n\n    return change_types, base_tokens, reference_tokens\n```",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "AHus-1usngI",
                "writer": "official_reviewer",
                "reply_to": "ZXBDk9akAo",
                "title": "Discussion",
                "comment": " Thanks the authors for the response.\n\n> Besides precision, the recall metric is crucial for real-world scenarios (a model that is correct 1 out of 1000 merges is not very interesting).\n\nWhere do the `1` and `1000` numbers in your statement come from? I don't know whether you're implying recall or precision by saying that. Of course, the recall is important but the level of importance depends on applications. For conflict resolution, it would easier for a user to resolve the conflict themselves than trying to understand what's wrong with a resolution generated by an imprecise automated tool.\n\nThat being said, I am not asking for a 100% precision, but there may be a better trade-off between precision and recall by calibrating your model. \n\nAbout the example, thanks for the derivation. How do you decide where and how many PAD tokens you need to add?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nKkkYP4w6B",
                "writer": "author",
                "reply_to": "iclr_2022_WXwg_9eRQ0T",
                "title": "General Response",
                "comment": " **General response**\n\nWe thank all the reviewers for their time, feedback, and thoughtful comments. We respond here to questions raised in several reviews and include information about updates in the newest version. Responses to individual comments are posted as replies to each review.\n\n**Syntactic vs Semantic merge conflicts and `diff3` (vcTN & k5j6)**\n\nBoth reviewers bring up excellent points about the `diff3` algorithm. Token-level `diff3` embodies the same intuition as a standard git line-level `diff3`; if one person changes a line, and the other does not, there is no __syntactic__ conflict, and thus `diff3` can merge that edit. Being a syntactic merge algorithm, `diff3` does not guarantee __semantic__ properties of a program. We see this work as a step toward the more general problem of building models that generate semantic free merges. As we point out to reviewer TUUK, we believe this more general semantic conflict-free merge resolution problem is a great task to drive modeling efforts of code because \u201c\u2026 the dataset is plentiful (merges are common and easily mined from Github), the solutions are simpler than a traditional code generation task as most resolutions do not require generating new tokens, and finally, as the reviewer notes, \u201csolving\u201d this problem would have a positive impact in the daily lives of a huge number of people.\u201d \nLastly, our data shows that in most cases (82\\%), MergeBERT matches a user's resolution, which we assume does not have semantic issues.\n\n**Impact of edit type information on the result (TUUK)**\n\nWe conducted an ablation study on the edit type embedding to understand the impact of edit-awareness of encoding on the model performance. As shown in Appendix 11.3, edit type embedding information improves MergeBERT\u2019s F-score by 7\\%. \nWe also included two ablation experiments to carry out a more precise comparison against the DeepMerge baseline, showing that MergeBERT outperforms DeepMerge with token diff by over 30\\% F-score (7\\% higher precision and 60\\% higher recall), and to demonstrate that MergeBERT performance on line interleaving merge type is comparable to that on the entire test set.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "W9p_1doojS2",
                "writer": "author",
                "reply_to": "-8IpHVzxK74",
                "title": "Responses to reviewer TUUK (part 2)",
                "comment": " **Q: How important is the edit type embedding used in finetuning?** \n\nA: We added an ablation study on the edit type embedding that demonstrates the impact of edit-awareness of encoding on the model performance. As shown in below, edit type embedding information could improve MergeBERT's F-score by 7\\%.\n\n| Approach |  Precision | Recall | F-score  |\n|-----|-----|-----|-----|\n| w/o type embedding  | 65.2 | 63.1 | 64.1  |\n| MergeBERT | 69.1 | 68.2 | 68.7 |\n\n**Q: how should it handle the new kind of inputs?** \nA: The underlying BERT model is pretrained on a massive multilingual source code dataset, and the downstream fintuning task of merge conflict resolution is also in the domain of source code. Our BPE vocabulary size is 50000 which is sufficiently large to compose unseen code identifiers not present in the training set. For a more specific answer, we would ask what do you mean by new kinds of inputs (e.g. an unseen language)\n\n**Q: Table 3 is pretty much due to properties of BERT and not of MergeBERT, right? Similar experience reported in CodeBERT?**\nA: Table3 results should be attributed to both the BERT and MergeBERT.\nBERT is a single input single output model. First, it has been shown in the DeepMerge paper that feeding a concatenation of (A,B,O) as a single input leads to poor performance. Second, we have observed that concatenating the embeddings of (A,B,O) and feeding to pre-trained BERT as single input gives results similar to a randomly initialized model. This is why we proposed MergeBERT architecture, which independently encodes the aligned sequences then aggregate them. Finally, edit-awareness aspect of the model is also crucial.\n\n**Q: Primitive Merge Resolution Types: Appendix 10.1 seems to emphasize this, but I do have some comments: \"Our analysis shows that over 99% of all the merge conflicts can be represented using these labels\" But FIgure 3 (left) says that you failed to label 21.64% of the TypeScript examples. Can you please elaborate on this point?**\n \nA: We will made the Appendix 10.1 and the Figure3 more clear in the new version of the paper.\nThe Figure3 left shows the distribution of merge conflicts extracted with the standard (line-level) diff3 algorithm, while the right plot refers to token-level diff3. The corresponding labels do not refer to the same exact cases. For instance, the label \u201cA\u201d in left plot simply corresponds to a merge resolution strategy like \u201ctake ours\u201d or \u201ctake theirs\u201d and is conceptually different from label \u201cA\u201d on the right (which may correspond to a more complex merge cases, including an interleaving). As plot label shows, we failed to assign a label to 21% of line-level conflicts, but not the token-level cases. \n\n**Q: It\u2019s not clear how the colors in Figure 3 are related to your 9 classes. Does \u201cRemove base\u201d correspond to classes 6-9?, is so, what does pink (\u201cOther\u201d) refer to? \nA: We will clarify this in the caption and improve quality of the plot. \nThe \"Remove base\" category combines the four primitive merge patterns that remove the lines present in the base branch from the resolution (e.g. take changes proposed in one program and exclude the lines present in base). The \"Other\" category consists of arbitrary line-interleavings, at the token level (right plot) these merge conflicts are mapped to one of the 9 primitive merge patterns.\n\n**Q: I would emphasize that there are almost 21.64% non-trivial examples (in TypeScript), which the token-level diff method can solve by using a classification approach.**\nA: This statement would be incorrect: Token-level diff is a deterministic algorithm, which does not perform classification. The Figure.3 only shows the statistics about labels assigned to conflicts, which serve as input for supervised learning (finetuning). \nThe unlabeled 21.64\\% of conflicts at the line-level may map to various categories at the token-level, and are not the only cases the MergeBERT solves.\n\n**Q: Figure 2: should all edit operations in the blue squares be `<->`?**\nA: The blue squares represent the edit sequence steps for pairwise alignment between Base and B. Since most of those tokens are unchanged, the edit steps should indeed be `==` (`<>` means a swap), but for two inserted tokens `,` and `z` the edit step should be `+`, which are shown incorrectly. Thanks for pointing this out, we have corrected the figure.\n\n**Q: Figure 7(b) needs to be corrected: `data +=` is also a conflict, and there is a piece of code after the last conflict - ``});`.**\nA: Thanks for pointing it out, this is a typo. We have corrected the figure.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-8IpHVzxK74",
                "writer": "author",
                "reply_to": "BP_cbM4Fzt6",
                "title": "Responses to reviewer TUUK",
                "comment": " Thank you for engaging with our submission.\n\n**Re: Overall, this may be a great software-engineering contribution, but I'm afraid that there's not a lot here for ICLR.**\nA: We respectfully disagree that there is not a lot for ICLR.  Understanding, modeling, and generating code is of huge interest to ICLR.  The \u201cmerge resolution\u201d problem studied in our paper is a great task to elucidate different modeling techniques for code generation.  The dataset is plentiful (merges are common and easily mined from Github), the solutions are simpler than a traditional code generation task as most resolutions do not require generating new tokens, and finally, as the reviewer notes, \u201csolving\u201d this problem would have a positive impact in the daily lives of a huge number of people.  We hope this is the first of many papers on this topic in ICLR.  \n\n**Q: Experimental evaluation partial with respect to most important baseline. What about using BERTMerge with line diff and DeepMerge with token diff? It\u2019s essential for a fair comparison.**\nThe reviewer raises several related issues regarding the baselines, and we address them all here. \n\nFollowing is the DeepMerge performance with token diff (also included in Table 1 of new version):\n\n| Approach | Precision | Recall | F-score |\n|-------|-------|-------|-------|\n|DeepMerge  | 64.5 | 42.7 | 51.4 |\n| MergeBERT  | 69.1 | 68.2 | 68.7 |\n\nThe overall DeepMerge performance improves, resulting in a 13\\% higher F-score, but still falling short of MergeBERT by 34\\% F-score (7\\% lower precision and 60\\% lower recall). \n\nFIgure 3 (left) shows that over 20% of all merges require token-level interleaving. Thus, any line-based approach cannot address this large subset of merges (including DeepMerge). A line-level version of MergeBERT\u2019s recall would drop by at least this much and is also motivation for why MergeBERT\u2019s approach is superior to DeepMerge. In contrast, DeepMerge trained at the token-level granularity is an interesting area to pursue but is not a simple extension of the prior work. When investigating the data, we found almost all token-level merges fell into one of our 9 classes (See Figure 3), and thus a classification-based approach does not give up on recall. The architecture of MergeBERT is designed to exploit this fact. Thus, we respectfully disagree that adding MergeBERT trained on line-level conflicts (clearly has limitations by only operating on lines) or DeepMerge trained on token-level conflicts (a paper in of itself) is a requirement to show that MergeBERT is a contribution over prior art. We can do a better job explaining this in the paper and will update the text to reflect this discussion.\n\nFollowing are the results for TypeScript MergeBERT restricted to the line-interleaving merge type only:\n\n|\t| Top-1 | Top-3 |\n|-----|----|----|\n| Precision | 70.6 | 79.0 |\n| Recall | 69.9 | 78.3 |\n| F-score | 70.2 | 78.6 |\n\nComparing to DeepMerge's top-1 precision of 55.0% and top-3 precision of 59.1%.\n\nLastly, the DeepMerge paper demonstrated a Fairseq based encoder-decoder baseline (on the concatenation of A, B, and O) performed poorly, and was only able to correctly generate resolutions for ~3% of the cases. We can replicate this work for a final version.\n\n**Q: Do you train BERT only for word vectors? If so, what about exploiting the context?**\nA: Can you please clarify what exactly do you refer to as \u201ccontext\u201d? \nInput sequences to MergeBERT cover token-level conflicting regions and surrounding code (i.e., a prefix and suffix surrounding the token-level conflict region). In comparison, DeepMerge only considers code within a line-level conflict as \u201ccontext\u201d. \n\nWe treat the source code data as a sequence of tokens corresponding to the output of a lexical analyzer. In this work, we do not leverage non-terminal nodes of syntax trees, or global file-level context. This is based on an assumption that most of the information needed for developer to resolve a merge conflict is around the conflicting region itself. \n\n**Q: Is the encoder of the aligned sequences the pre-trained BERT?** \nA: All token sequences (4 total, aligned pairwise) are encoded by BERTs. The edit sequences, however, are not encoded by BERT, but rather are passed as embedding layer. This is done because BERT is not pretrained on edit types.  \n\n**Re: finetuning**\nA: Lightweight finetuning has been getting a lot of attention in practice. In this work, we try to promote the re-usability of pretrained transformer models for software engineering tasks. For that reason, we kept most of the parameters frozen and only finetuned type embeddings, aggregation, and output classification layers. Nevertheless, experiments around gradual unfreezing or finetuning end-to-end could yield further improvement, but would be computationally more expensive (we will mention that in the discussion).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZXBDk9akAo",
                "writer": "author",
                "reply_to": "x9pVWP9QzdH",
                "title": "Response to reviewer k5J6",
                "comment": " Thank you for your kind review and detailed questions.\n\n**Re: MergeBERT is not as good as diff3 in terms of precision**\nA: Besides precision, the recall metric is crucial for real-world scenarios (a model that is correct 1 out of 1000 merges is not very interesting).  MergeBERT has significantly better recall as it is invoked when diff3 fails.    \n\n**Q: It is somewhat unintuitive that `let` is not part of the token-level conflict. I understand that `var` is not changed in the version B, but it is important from both A and B perspectives that there is a change that needs an agreement in resolution. In that case, I don\u2019t think merging `let` right away is an indisputable decision.**\nA: This is a good point; however, it can also be applied to the existing git `diff3` algorithm that works on lines. The idea of `diff3` is that if one person changes a line, and the other does not, there is no conflict. We embody this same intuition at the token-level (but of course add a neural component to suggest a resolution when a conflict occurs). \n\nPut another way, suppose we forced B\u2019s edits to happen after A\u2019s edits.  B would only see A\u2019s change to use `let`.  Likewise, suppose B\u2019s edits happen before A\u2019s edits: B would never see A\u2019s change.  In both cases, B does not have the chance to influence A.  Of course, B can later revert A\u2019s edit if they want `var` instead of `let`! \n\n\n**Q: For this change from `--num_cores=2` to `--max_length=256 --num_cores=2`, what is the result of the two-level diff?**\nA: The answer would generally depend on the granularity of diff, and programming language grammar since we use tree-sitter lexical tokenizer to split into tokens. In case of the aligned diff, it would also depend on the order of sequences compared.\nWe use the definition of aligned sequence diff from Yin et. al. (\u201cLearning to Represent Edits\u201d, 2019) paper. \nLet\u2019s denote `A = --num_cores=2` and `B = --max_length=256 --num_cores=2`. Using notations of Figure2, we would get:\n1. diff(A, B)\n\n`A|B`: `['<pad>', '<pad>', '<pad>', '<pad>', '--', 'num_cores', '=', '2']`\n\n`Delta(A, B)`: `[+, +, +, +, =, =, =, =]`\n\n`B|A`: `['--', 'max_length', '=', '256', '--', 'num_cores', '=', '2']`\n\n2. diff(B, A)\n\n`B|A`: `['--', 'max_length', '=', '256', '--', 'num_cores', '=', '2']`\n\n`Delta(B,A)`: `[-, -, -, -, =, =, =, =],`\n\n`A|B`: `['<pad>', '<pad>', '<pad>', '<pad>', '--', 'num_cores', '=', '2']`\n\n**Q: Is treating conflicting regions independently in Eq. (2) is oversimplified?**\nA: Treating token-level conflicts is a simplifying assumption. However, our data shows that only 5% of merge conflicts result in more than 1 token-level conflict per line-level conflict. Our future work is to investigate a more complex model for dependence between token-level conflicts. We add a discussion of these statistics when we introduce Eq. (2).\n\n**Q: How do you split the train and test dataset (Table 4)? Is there a validation dataset?**\nA: The finetuning dataset is split into development and test set in the proportion 80/20 at random at file-level. The development set is further split into train and validation set in 80/20 proportion on conflict level.   \n   \n**Q: How do you deal with conflicts from more than 2 versions?**\nA: Conflicts involving more than 2 versions are not considered in this study. Our dataset only includes commits with 2 parents, that resulted in a merge conflict (we will correct the description in section 6 which says \u201cat least two parents\u201d). \nIn practice, the strategy employed by existing systems when faced with a conflict of more than 2 versions is to decompose the problem into a series of pair-wise merges.  That is, merge the first two versions, then merge the result with the next version, etc.  Such a strategy could easily leverage our approach for merging two branches.\nAlso, we note that such merges are quite rare.  In the Linux kernel (one of the most complex git repositories that exist) only 3% of merges include more than two branches (https://www.destroyallsoftware.com/blog/2017/the-biggest-and-weirdest-commits-in-linux-kernel-git-history).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ze8GNIr1jFR",
                "writer": "author",
                "reply_to": "HAKP6cbvvk",
                "title": "Response to reviewer vcTN",
                "comment": " Thank you for your detailed review and thoughtful questions.\n\n**Q: My biggest concern with your work is that you seem to treat token conflicts as independent of each other, but of course they are most certainly not, in general.**\n\nA: The issue the reviewer raises demonstrates the difference between a semantic merge conflict vs a syntactic one. Indeed, the same argument could be made of the existing git diff3 algorithm: there is no guarantee that a clean git merge is semantically correct (where correct is defined in terms of all unit-tests, say, in the project)! In a semantic merge conflict, a syntactic merge is conflict free \u2013 but introduces a semantic difference (e.g,. compiler break, a bug, assertion violation, etc). This paper is focused on applying neural techniques to syntactic conflicts and demonstrates in most cases (82%), we match the user\u2019s resolution. If we assume a user is unlikely to introduce a semantic conflict, then our results suggest the model is also able to mitigate semantic conflicts. That said, the reviewer is absolutely correct: the ultimate goal is a (semantic) conflict-free merge. This is a hard learning problem, and we see this paper as a step toward it. We will expound on this distinction in a future version.\n\n**Q: I'd be curious if you think the strategies in those earlier tools are incompatible with an encoder like your Transformer-based encoder, too coarse grained, or perhaps just too complicated.**\n\nA: Finding ways to incorporate semantic or syntactic strategies employed by FSTmerge (or other structured techniques) into neural models is a great research problem and part of the reason we hope this paper is accepted to ICLR! We note that our aim is to develop an approach that can be used for many languages; adding a new language requires only a tokenizer (which are freely available) and gathering additional data for that language.  Leveraging syntactic/semantic information similar to FSTMerge requires non-trivial engineering for each language.  For example each language has different semantics around properties of a correct merge such as identifying what structures can be reordered and which cannot. Further, in our experience FSTmerge leads to side effects (e.g. syntactically incorrect code) and cannot resolve various types of merges (for instance when conflict appears on a program statement).  This is a great research direction, but will require substantial innovation and engineering.\n\n**Q: are your classification labels arbitrary? Did you choose them by analyzing actual merge-conflict resolutions?**\nA: We used a data-driven approach to identify these 9 primitive merge resolution patterns based on analysis of the real-world merge conflict resolutions from GitHub. They cover almost all merges in practice.  Appendix 10.1 discusses the primitive merge patterns in more detail. \n\n**Q: Please explain your metrics with specificity.**\nA: Our definition of precision is standard: `Precision@K = correct@k/merged@k`, which is a ratio of correct merge resolutions predictions (true positives) divided by the total merged `TP/(TP+FP)`.\nThe \u201cpositives\u201d here are the cases for which the model was able to make a syntactically correct prediction (as determined by tree-sitter parser). Which is also why the fraction of merged programs is less than 100%. No log probability threshold is applied on neural model predictions. Recall is calculated as a ratio of correct merge resolution predictions divided by the total number of merge conflicts: `Recall@k = correct@k/total@k`. This is not a traditional definition, and we are happy to change the name to accuracy if you suggest to do so. We use this definition for recall instead of the standard `TP/(TP+FN)` because there is no easy way of estimating `FN`s, given all merges we consider had conflicts.  We will make this more clear in the final paper.\n\n**Q: Please explain why BLEU-4 is a good metric for this task, perhaps with an example.**\nA: We agree with the reviewer that BLEU and other n-gram based metrics are generally not a good choice for evaluating source code. We only consider BLEU-4 score as a secondary metric, and not include it in any tables or refer to it in supporting any conclusions. We will remove BLEU results to avoid the confusion.\n\n**Q: How does MergeBERT do if you pre-train on Scala and fine-tune on Scala, and how does that compare to your zero-shot results?**\nA: We have submitted an experiment you requested, and will add the number when available.\nThe main motivation for zero-shot generalization experiments is to evaluate performance on relatively \"unpopular\u201d languages in Github (as in we do not have as much training data). Scala\u2019s grammar and language features are somewhat like that of Java and C#. It provides language interoperability with Java so that libraries written in either language may be referenced directly in Scala or Java code. This partially explains a relatively good top-3 zero-shot performance.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "x9pVWP9QzdH",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_WXwg_9eRQ0T",
                "title": "",
                "comment": "This paper presents MergeBERT, a deep model for resolving program merge conflicts in software development. The authors introduce a new, hierarchical differencing and cast the problem as classifying over a fixed set of merge patterns, instead of generating . The model is pre-trained on a large corpus of GitHub code. Pros:\n\n+ Hierarchical two-level differencing appears to be an intuitive and good idea.\n+ Casting the problem as classification over a set of merge patterns appears advantageous in both learning and computation, since most of the time the resolution comes from either of the two change versions.\n+ Pre-training can be applied directly.\n\nCons:\n\n- This idea is somewhat incremental, largely based on the work of Dinella et al., 2020.\n- MergeBERT is not as good as *diff3* in terms of precision, which is to me a more important measure. \n- Some parts of the techniques and experiments are not clear, for example the class label and data split. I have more detailed questions below.\n\n\nQuestions:\n\n1. It is somewhat unintuitive that `let` is not part of the token-level conflict. I understand that `var` is not changed in the version B, but it is important from both A and B perspectives that there is a change that needs an agreement in resolution. In that case, I don\u2019t think merging `let` right away is an indisputable decision.\n\n2. For this change from `--num_cores=2` to `--max_length=256 --num_cores=2`, what is the result of the two-level diff?\n\n3. Is treating conflicting regions independently in Eq. (2) is oversimplified?\n\n4. How do you split the train and test dataset (Table 4)? Is there a validation dataset?\n\n5. How do you deal with conflicts from more than 2 versions?\n\n Overall, the paper presents several new technical contributions and insights, but some parts are not entirely convincing.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "HAKP6cbvvk",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_WXwg_9eRQ0T",
                "title": "",
                "comment": "This paper addresses merge-conflict resolution in source code repositories. Whereas prior neural work (DeepMerge) can only do line-level resolutions (i.e., predict a resolution that chooses some sequence of entire lines from the original file version or the two divergent branches), this work refines the task further towards token level resolutions, thereby covering more interesting cases. Whereas DeepMerge cast the task as a pointer-sequence prediction (predicted a sequence of lines from the input), MergeBERT casts the task as a classification problem: for each conflict instance (some token-sequence conflict), one of 9 recipes are chosen to merge (e.g., just the original tokens, just one of the branches, some concatenation of the three, and some substitution of tokens in the original by concatenations). Similarly to DeepMerge, the input is encoded as a combination of aligned token sequences of the original snapshots and the divergent branches, as well as an encoding of the token-wise edit \"script\" for the pairwise diffs (same, inserted, deleted, replaced). But instead of encoding this edit sequence explicitly, MergeBERT uses it as a token type embedding in a Transformer encoder, along with a positional embedding and the token embedding itself.\n\nThe Transformer encoder is pre-trained with the masked language model objective, before fine-tuning on merge-conflict resolutions. The resulting tool is shown to resolve more merge conflicts than prior work, and also to generalize to a language unseen during pre-training or fine-tuning. This is an important problem, and I'm excited to see more work in finding a neural solution to it.\n\nHowever, this submission seems more like a refinement of DeepMerge than a novel contribution on its own. The data representation for conflicts is a generalization from DeepMerge, and the model architecture is a straighforward form of a Transformer-based classifier. However, the limited novelty might be offset by the positive progress on this important problem, modulo a number of questions below.\n\nI wonder if there's room to incorporate the strategies of syntactic or lexical merging employed by FSTMerge and its variants and other non-neural baselines into the actions predicted by something like MergeBERT. It seems excessive to throw out the engineering that went into such non-neural tools, if they can be used for something more sophisticated than o, a, b, oa, ob, etc. Q1: I'd be curious if you think the strategies in those earlier tools are incompatible with an encoder like your Transformer-based encoder, too coarse grained, or perhaps just too complicated.\n\nThat said, I like the data-driven design of the task (e.g., the merge resolution \"recipes\" described in section 4). I would have liked to see where the 9 edit patterns came from though (presumably some analysis of real conflict resolutions?), since otherwise they appear arbitrary. Q2: are your classification labels arbitrary? Did you choose them by analyzing actual merge-conflict resolutions?\n\n## Non-conflicting token edits\n\nMy biggest concern with your work is that you seem to treat token conflicts as independent of each other, but of course they are most certainly not, in general. For example, on page 3, with respect to Figure 1, I'd be careful about calling a token with no direct conflict a \"non-conflicting edit\" (e.g., the statement on `var` versus `let`). Consider the following diff:\n```\n<<<<<<<< A\na = func2(y, 10)\n|||||||| O\na = func2(y, 9)\n========\na = func3(y, 10, 12)\n>>>>>>>> B\n```\nwhere `func2` is a function with arity 2, and `func3` is a function with arity 3. Here, although the `func2` to `func3` change on `B` is \"non-conflicting\", it is not independent of the change from `9)` to `10)` or `10, 12)`, since one keeps the arity at 2 and the other changes it to 3. Perhaps a model can learn these interdependencies but your model does not. I'd caution you against calling independent token changes \"non-conflicting\". Maybe there's a better motivating example than this one, or again, perhaps there's some analysis of real conflict resolutions showing that real conflicts are indeed token-independent.\n\n## Evaluation\n\nI don't quite understand your definition of precision and recall. Is precision perfect accuracy modulo whitespace per conflict region? What's recall? Why is it not 100%? Do you have a threshold on classification probability you're using that's causing you to lose some examples? It seems you have one interpretation of recall for diff3 (it cannot resolve the conflict) and possibly another for MergeBERT? Q3: Please explain your metrics with specificity.\n\nIt's not obvious how BLEU-4 is a relevant metric. In cases where token-level conflict resolution kicks in, A or B or even O probably share many n-grams with the resolution. What kind of solution does BLEU-4 allow you to score positively that would be genuinely acceptable for this task? An example would be helpful here. Q4: Please explain why BLEU-4 is a good metric for this task, perhaps with an example.\n\nThe zero-shot generalization to Scala is puzzling. Presumably the value proposition here is to argue that you don't have to pre-train for every language, and a single strong (potentially multilingual) model can do the job. But then I would want to see the comparison of pre-training/fine-tuning on Scala versus the zero-shot version with neither pre-training nor fine-tuning. In the absence of that, it's not clear what the headroom is for this language. Is the result good? Is it bad? Maybe pre-training and fine-tuning on Scala gives super high F1. Q5: How does MergeBERT do if you pre-train on Scala and fine-tune on Scala, and how does that compare to your zero-shot results?\n\nI don't understand the point of Section 8.2. You already do MLM pre-training on your unilingual and multilingual pre-training datasets. What does this section add to what you already do? Q6: Please explain what research question Section 8.2 is answering, and what the answer is.\n\n Important problem, but there are many unanswered questions making the submission feel incomplete.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "BP_cbM4Fzt6",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_WXwg_9eRQ0T",
                "title": "",
                "comment": "This paper is about using BERT for the automatic resolution of merge conflicts. The main idea is to cast the problem of automatic merge resolution as a classification problem with 9 classes (token-level merge patterns). The technique use token-level differencing to represent the input to the merge problem as four aligned token sequences, together with two edit-type embeddings. The input sequences are fed to BERT and the results are aggregated and used for classification. \nThe technique is compared to state-of-the-art automatic-merge tools and is shown to outperform them in all benchmarks. \n Pros:\n+ This is a compelling task, merge conflicts are an important problem\n+ Reducing the problem to using a classifier is valuable practically \n+ The nine primitive merge resolution patterns covering 99% of merge cases is an interesting observation\n\nCons:\n+ The encoding is token-based and rather standard\n+ Missing ablation study\n+ Experimental evaluation partial with respect to most important baseline \n\n* Tackling the merge conflicts problem is interesting. Using a learned model to decide how to resolve it is interesting as well (but not novel), since, it\u2019s not always clear what is the best resolution (which is consistent with the context).\n\n* It\u2019s not clear how the model looks like, Do you train BERT only for word vectors? If so, what about exploiting the context? Is the encoder of the aligned sequences the pre-trained BERT? It looks like you are not finetuning it (based on the blue squares of Figure 2, showing that only the Edit Type Embeddings are finetuned), so how should it handle the new kind of inputs? (token embedding + type embedding + position embeddings)\n\n* How important is the edit type embedding used in finetuning? The paper refers to Figure 5 in the appendix that shows how these are used, but not how important are they for the final result. In general, the paper would benefit from a more extensive ablation study. \n\n* The comparison with DeepMerge includes cases where DeepMerge cannot produce a prediction, as these cases are not line-level merges. This is not an apples-to-apples comparison. What would happen if we compare DeepMerge/MergeBert considering only line-level merges?\n\n* Comparisons to JDime and jsFSTMerge are useful for showing the superiority of your neural approach, but I am more interested in understanding what makes your approach work well.\n\n* Table 3 is pretty much due to properties of BERT and not of MergeBERT, right? Similar experience reported in CodeBERT?\n\n* Primitive Merge Resolution Types: Appendix 10.1 seems to emphasize this, but I do have some comments:\n\"Our analysis shows that over 99% of all the merge conflicts can be represented using these labels\"\nBut FIgure 3 (left) says that you failed to label 21.64% of the TypeScript examples. Can you please elaborate on this point?\nIt\u2019s not clear how the colors in Figure 3 are related to your 9 classes. Does \u201cRemove base\u201d correspond to classes 6-9?, is so, what does pink (\u201cOther\u201d) refer to?\nPlease align the legends on both sides of Figure 3.\nI would emphasize that there are almost 21.64% non-trivial examples (in TypeScript), which the token-level diff method can solve by using a classification approach.\n\n* It would be helpful to add an encoder-decoder baseline - given (A, B, O), generate the resolution.\n\n* Table 1- what is the meaning of the diff3 row? I guess it\u2019s for the examples where the token granularity diff managed to solve conflicts the line-diff couldn\u2019t. Please clarify that. \n\n* Table 1 - what about using BERTMerge with line diff and DeepMerge with token diff? It\u2019s essential for a fair comparison.\n\n* Section 8.2: The impact of pretraining is not disputed, but again it is not a contribution of the technique presented here. \n\nMinor: \n\n* Figure 2: should all edit operations in the blue squares be <->? \n\n* Figure 7(b) needs to be corrected: `data +=` is also a conflict, and there is a piece of code after the last conflict - ``});`.\n The problem of automatic merge resolution is compelling. The authors present a solution that is based on token-level diff (no ML contribution), identify 9 merge patterns (no ML contribution), and finally use BERT with minor adaptations to classify which merge pattern should be used. For this model, some details of what exactly is being fine-tuned are not clear. \n\nThe suggested approach shows improvement compared to existing baselines. \n\nThe experimental evaluation is hard to follow because neither of the existing baselines addresses the exact same kind of merges. The closest baseline in terms of technique is DeepMerge, and the comparison to that baseline is quite partial. \n\nOverall, this may be a great software-engineering contribution, but I'm afraid that there's not a lot here for ICLR.\n\n",
                "rating": 3,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "This paper",
                "Sentiment Expression": "a fair effort, making some headway on a problem of practical importance.",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "did not evince enough excitement for it to get accepted in light of the scoping concerns",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "I don't think I can recommend acceptance",
                "Sentiment Expression": "Given the various constraints involved",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "NgwrhCBPTVk": {
        "paper_id": "nips_2022_NgwrhCBPTVk",
        "paper_title": "(Optimal) Online Bipartite Matching with Degree Information",
        "paper_abstract": "We propose a model for online graph problems where algorithms are given access to an oracle that predicts (e.g., based on modeling assumptions or past data) the degrees of nodes in the graph. Within this model, we study the classic problem of online bipartite matching, and a natural greedy matching algorithm called MinPredictedDegree, which uses predictions of the degrees of offline nodes. For the bipartite version of a stochastic graph model due to Chung, Lu, and Vu where the expected values of the offline degrees are known and used as predictions, we show that MinPredictedDegree stochastically dominates any other online algorithm, i.e., it is optimal for graphs drawn from this model. Since the \"symmetric\" version of the model, where all online nodes are identical, is a special case of the well-studied \"known i.i.d. model\", it follows that the competitive ratio of MinPredictedDegree on such inputs is at least 0.7299. For the special case of graphs with power law degree distributions, we show that MinPredictedDegree frequently produces matchings almost as large as the true maximum matching on such graphs. We complement these results with an extensive empirical evaluation showing that MinPredictedDegree compares favorably to state-of-the-art online algorithms for online matching.  ",
        "paper_acceptance": "Accept",
        "meta_review": "The paper studies online bipartite matching problem under a random graph model, and shows that using the expected mean of the degrees could achieve certain optimal performance under their graph model. The authors complement the theoretical study with empirical evaluation, and demonstrates that estimating the degrees would result in good performance in online bipartite matching.\n\nThe reviewers agree that the paper contains good technical contributions to the problem, and is worth to be published. There is some concern on whether the paper is related to ML/AI or is a purely combinatorial optimization paper. In particular, the reviewers share the concern that the theoretical algorithm takes the expected degrees as the input, and not the predicted degrees as suggesting an estimation with errors. After some discussions among the reviewers, here is my conclusion and recommendation:\n\n1. The technical algorithm is like a combinatorial optimization algorithm, but it has a strong indication that degree estimation could be helpful in algorithm design. This is further complemented by the empirical study of the paper. Therefore, the study fits into the data-driven optimization and algorithm design paradigm that would interest the ML/AI community. \n\n2. The use of the term \"predicted degrees\" in the title/abstract/intro is indeed misleading. It gives a strong impression that the algorithm is using a prediction (or estimation) that contains estimation error, but it actually does not use such predicted degrees, and instead using the expected degrees as input. Of course expected degrees are still not the actual random degrees but they are not in the normal sense the \"prediction\" result. I strongly suggest the authors to properly change the title/abstract/intro to more accurately reflect what they are doing and to reduce confusion (If 3 out of 4 reviewers raise this issue, plus I also has this concern, the authors should expect a significant confusion if they do not revise the presentation). The authors should clearly state that their theoretical algorithm is for expected degrees, and this may indicate that using predicted degrees may be helpful, but the latter is not part of the theoretical result. The following is my try on the title:\n\n(Optimal) Online Bipartite Matching with Known Expected Degrees on Random Graphs --- A Theoretical Justification of the Effectiveness of Algorithms Based on Ordering of Predicted Degrees\n\nIt is certainly not elegant, but hopefully it suggests the authors to spend some effort to give a more accurate title/abstract/intro in their presentation. \n\nAlso, in terms of their theoretical result, their Appendix D does provide some result regarding the noise in the prediction. But it looks like the presented result is not in the normal sense of the prediction error between the prediction and the ground truth. I also suggest the authors to substantiate this part and perhaps move it into the main text so that the paper indeed has some treatment on predicted degrees.\n\nOverall, with the above comments and suggestions, I believe the paper has good contributions to be accepted at NeurIPS.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "DWbI5mriIS",
                "writer": "official_reviewer",
                "reply_to": "ZFHZvMoLIh",
                "title": "To authors",
                "comment": " Thanks for the detailed response. It answers my questions. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZFHZvMoLIh",
                "writer": "author",
                "reply_to": "DOxKRN5-WOt",
                "title": "Follow-up to Reviewer o3xo",
                "comment": " Dear Reviewer o3xo,\n\nThanks again for your review! We wanted to follow up to see if our responses helped to address your concerns.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Pscu9XVrnxH",
                "writer": "author",
                "reply_to": "YUaQ3MOs_B",
                "title": "Reply to reviewer gW3G",
                "comment": " ### 1) Ranking algorithm\n\nIt is not true that our algorithm is the same as the Ranking algorithm. They are related in that they both choose which node to match greedily based on an ordering of the offline nodes. However, in Ranking the ordering is random, and in our algorithm, it is given by a set of predictions. We remark that such alterations can affect the performance of the algorithms significantly. For a related example, if instead of using Ranking, one matches each online node independently to a random offline node in its neighborhood, the competitive ratio drops from 1-1/e to 1/2. Therefore, the analysis of our algorithm would not apply at all to the Ranking algorithm as we are leveraging the fact that the predictions are related to degrees. Likewise, the analysis of Ranking does not seem to apply in our setting. This is validated by our experiments where our algorithm performs much better than Ranking.\n\n### 2) Choice of venue\n\nIn terms of fit with NeurIPS, many papers on algorithms with predictions (see our Related Work section for some examples) have appeared at NeurIPS and other machine learning conferences where the core technical analysis is concerned with combinatorial optimization. We think that this paper would be of interest to that community.\n\nFor this reason as well as because of our extensive experiments\u2013which we think are a key part of this paper, we think that a machine learning conference is the most appropriate venue for this work.\n\n### \"I am a little surprised that the authors claim that many of the results are novel.\"\n\nIf the reviewer is aware of any papers with results that overlap with ours, we would appreciate the references.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ERp6AUyBDl",
                "writer": "official_reviewer",
                "reply_to": "CcmJF9V4PEW",
                "title": "To authors",
                "comment": " Thanks for the response and clarification. My question is well addressed. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YUaQ3MOs_B",
                "writer": "official_reviewer",
                "reply_to": "L_Xt6sVWv0F",
                "title": "To authors",
                "comment": " Thanks for your response. \n\nFirst, I would like to keep my opinion that this paper does not fit very well with Neurips. Except for the experiments, the problem studied in this paper is purely combinatorial optimization. \n\nSecond, I have the following two *optional* questions:\n\n1 The proposed algorithm is the same as the Ranking algorithm of Karp, Vazirani and Vazirani proposed in 1990. Could you compare your optimality results with the one analyzed in the paper of Karp el al.? \n\n2 The online bipartite matching problem has been extensively studied following the seminal work of Karp, and the algorithms and settings in this paper do not differ much from the existing works. I understand that the analysis in this paper may be non-trivial, but I am a little surprised that the authors claim that many of the results are novel. In that sense, the paper seems to be a very strong contribution to theoretical computer science, as online bipartite matching is a fundamental problem. In the authors' opinion, if the paper were submitted to STOC/FOCS/SODA, what is the chance that this paper can be accepted?\n\nThanks,",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "L_Xt6sVWv0F",
                "writer": "author",
                "reply_to": "aUyrlRmFku3",
                "title": "Follow-up to reviewer gW3G",
                "comment": " Dear Reviewer gW3G,\n\nDid we address all your concerns satisfactorily? If your concerns have not been resolved, could you please let us know which were not sufficiently addressed so that we have a chance to respond? Thanks!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "82mwFKJfHUE",
                "writer": "official_reviewer",
                "reply_to": "x7c3hlT9LdE",
                "title": "To authors",
                "comment": " Thanks for the response. My concerns are well addressed. The authors can consider adding the explanations in Q2 into the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aUyrlRmFku3",
                "writer": "author",
                "reply_to": "ipXNQCmCMQG",
                "title": "Response to Reviewer gW3G (2/2)",
                "comment": " Here is a catalogue of all of our theoretical results and their relation to prior work in response to W2.\n- Theorem 5.1, Appendices A, B, C (Optimality of MPD): To our knowledge, this proof idea is novel, and we did not rely on any prior work for this result. At a very high level, the general structure of the argument follows a \u201ccoupling\u201d argument which is a standard technique to prove stochastic dominance.\n- Theorem 6.1, Appendices G, H, J (Competitive ratio analysis via differential equations): As discussed in both the exposition and proof itself, the core tool used in lower bounding the size of the matching returned by MPD is a result on relating continuous differential equation approximations to the asymptotic behavior of random discrete processes (see, for instance, the cited works of Kurtz or Wormald). However, finding the right system of equations to describe our problem on which this technique can be applied as well as finding a solution to those equations was, to our knowledge, original.\n- Appendix D (Analysis of MPD with noisy predictions): This analysis is original, and we don\u2019t know of any closely related prior work.\n- Appendix E (Worst-case bound): The construction of the worst-case example for MPD with true degrees as predictions is a construction which uses the hard example of Karp, Vazirani, and Vazirani. However, this specific construction applied to our problem with degree information is, to our knowledge, novel.\n- Appendices I, J (Upper bound on maximum matching size): We came up with our approach to upper bounding the size of the maximum matching independently of prior work and do not know of any other work which uses this specific idea though it is certainly true that the general technique of finding sets which violate Hall\u2019s theorem is a common idea in matching theory.\n- Appendix K (Analysis in the special case of Erdos-Renyi bipartite graphs): We came up with this analysis independently of prior work by, in part, applying our general analysis from Section 6 to this special case. However, matchings in Erdos-Renyi graphs have been intensively studied, and after extensive search for related work, we have found similar results on greedy matchings in Erdos-Renyi bipartite graphs using a refined upper bound on the maximum matching specific to the special setting where n=m from Mastin and Jaillet 2013: https://www.mit.edu/~jaillet/general/greedy_bip.pdf. This paper gives an improved result on the competitive ratio in this conjectured hard case of 0.837 over 0.831. We will cite this paper as an improvement over our results in the balanced case (n=m) but note that our results generalize to the unbalanced case. In fact, analysis of the unbalanced case was stated as possible future work in that paper.\n- Appendices L and M (Concentration of the quantities from section 6): For these results, the underlying idea is the standard technique of relating the properties of interest to a martingale and applying Azuma\u2019s inequality. The specific application to our problem is our own.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ipXNQCmCMQG",
                "writer": "author",
                "reply_to": "1Ml73XGQI5u",
                "title": "Response to Reviewer gW3G (1/2)",
                "comment": " Thanks for your review! We address several comments below.\n\n**W1:** We respectfully disagree with the characterization that the general degree prediction framework we introduce is misleading in terms of the content of the paper. In particular, our aim is to introduce the idea of general degree predictions for online bipartite matching (within the context of several online bipartite matching with predictions papers appearing in machine learning conferences). Our experiments (Section 7 and Appendix N) are extensive and in a variety of settings, including settings which clearly fall within the realm of data-driven algorithms with no assumptions about the distribution of graphs or the quality of predictions (see Figures 1 and 2) as well as in settings with added error (Figure 3b). Our theoretical results attempt to give some justification for the good empirical performance of MPD. To that end, we have to make some assumptions, and we focus on a constrained, but very natural, random setting of CLV-B graphs with expected degrees as prediction. *But this is by no means a limitation of the applicability of the algorithm in practice*\u2013rather, we have to make some assumptions in order to effectively analyze the algorithm. As a final point, we emphasize that we analyze the case when the algorithm has access to the expected but not the true degrees (these quantities differ non-trivially).\n\n**W2:** In terms of distinguishing our technical contribution, we attribute any prior results used in our theorems while the rest of the math is our own. At a high level, our main results from Section 5 and 6 are non-trivial, and, while these use some known techniques (e.g., coupling random variables in Section 5), do not follow immediately from any prior work we are aware of. Specifically in relation to work on the known i.i.d. model, our analysis differs in that in every step, we make use of the specific structure of the CLV-B model (for instance, independence between edges). For completeness, we will go through all of our theoretical results and discuss their similarities to prior work in the following comment (see below).\n\n**W3:** Due to space, we had to make editorial decisions about what to include in the main text. We felt that having ample space to discuss Theorems 5.1 and 6.1 as well as the experiments was important but would be happy to consider any specific suggestions of what to move into the main text vs. the appendix.\n\n**W4:** Thanks for the writing suggestions, we will update the text accordingly. For the first comment, we indeed mean that the consumers (online nodes) pick their neighbors i.i.d.: each consumer $j$ forms an edge with producer $i$ w.p. $p_i$. On the other hand, the producers edges are not identically distributed (producers with high $p_i$ will have higher probabilities of forming edges). For the third comment, we mean to say that a setting of the parameters $\\mathbf{p}$, $\\mathbf{q}$ that minimizes the performance of MPD is one in which all $p_i$, $q_j$ take on the same value. We will make appropriate updates regarding the second and fourth points, thanks.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "x7c3hlT9LdE",
                "writer": "author",
                "reply_to": "jRzLaMFgRP5",
                "title": "Response to Reviewer FyN6",
                "comment": " Thanks very much for your review! We address several comments and questions below.\n\n**W2:**  As described in the Related Work section, multiple prior papers on online matching with predictions have appeared in machine learning conferences. Furthermore, our paper includes extensive experiments (Section 7 and Appendix N) for a variety of prediction methods. Thus, we believe that a machine learning conference is the most appropriate venue for this work. \n\n**Q1:** While we are not sure of the exact underlying cause, the graph from that specific day is much smaller (around 1/3 the size) than the other graphs and is an outlier in the sequence.\n\n**Q2:** At a high level, the differential equation analysis works as follows. First, we consider the number of offline nodes of a given degree $d$ that are matched at the $t$th timestep (upon seeing the $t$th online node). This is a discrete random process that we can model as a Markov chain, but it is difficult to analyze its behavior. Borrowing from prior work, we show that this discrete process can be asymptotically approximated by a continuous version where the time variable is continuous. In that case, these differences between timesteps can be viewed as differential equations. By getting a closed form solution to the system of differential equations, we can then understand the expected behavior of the algorithm after all $n$ timesteps. Does this help explain these results? Happy to add more details if not.\n\n**Q3:** In general, our results (in particular, Theorems 5.1 and 6.1) are tailored to the specific properties of the CLV-B graphs such as independence between edges and do not follow from prior work in other settings. We are not aware of optimality results similar to Theorem 5.1 on other variants of the matching problem. The differential equation technique is a known technique, but finding the right system of equations to describe our problem on which this technique can be applied as well as finding a solution to those equations is original to our knowledge. See the response to Reviewer gW3G below for a complete description of how our techniques relate to prior work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CcmJF9V4PEW",
                "writer": "author",
                "reply_to": "cDpGBS32rSM",
                "title": "Response to Reviewer Skmv",
                "comment": " Thanks very much for your (second) review! In response to your question on the robustness result, the one place we use CLV-B graphs is in using Lemma 5.2 which states that for MPD with expected degrees, removing an offline node never decreases the matching size by more than one. As you suggest, this statement should hold for MPD run on an arbitrary graph with an arbitrary degree predictor, so the result should generalize to any fixed graph (and therefore any distribution over fixed graphs). In particular, for any graph G and two degree predictors $\\sigma$ and $\\sigma\u2019$, the size of the matchings returned by MPD run using the two degree predictors differ by at most the minimum number of nodes which must be deleted for the resulting permutations induced by $\\sigma$ and $\\sigma\u2019$ to be the same. Thanks for the suggestion, we will update the statement and proof.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DOxKRN5-WOt",
                "writer": "author",
                "reply_to": "JekQ_HTzE2",
                "title": "Response to Reviewer o3xo",
                "comment": " Thanks very much for your review! We address several comments and questions below.\n\n**W1. \u201cThey only cover CLV random graphs where the expected degrees follow zipf\u2019s law.\u201d**\nTo clarify, while our theoretical results are concerned with CLV-B random graphs, we study them in their full generality, not just when the distribution of degrees follows Zipf\u2019s law. For instance, Theorem 5.1 shows optimality of MPD on any CLV-B random graph given access to the expected degrees, regardless of the parameters of the random graph. The equations in Theorem 6.1 apply to MPD run on any symmetric CLV-B random graph with any distribution of parameters. We plug in the specific Zipf\u2019s laws parameters into these equations to demonstrate how this general analysis can be applied to show how MPD performs on this popular and useful specific distribution.\n\n**W2. Prediction error**\nOur experiments (Section 7 and Appendix N) demonstrate that MPD performs very well even when given imperfect predictions from past data which have no guarantees (see Figures 1 and 2), when given much weaker predictions than other algorithms (in the known i.i.d. setting), and in the CLV-B setting with added prediction error (Figure 3b). In our theoretical analysis, we assume access to the expected not actual degrees of the offline nodes and emphasize that these quantities can differ significantly. Finally, we do theoretically address the issue of errors in predicting the expected degrees in Appendix D though we agree that further exploration of this issue would be interesting future work.\n\n**Q1**\nThanks for pointing out this potential point of confusion. The two quantities are complements of each other. We will decide on a single expression (perhaps while mentioning the alternative in passing) in the final text.\n\n**Q2**\nRegarding prediction error, see the response to W2 above. In terms of the overall setting of the problem, our aim is to introduce the idea of degree predictions for online bipartite matching without regard to a specific type of prediction (within the context of several recent online bipartite matching with predictions papers). Our experimental results are carried out in this general setting, and our theoretical results attempt to give some justification for the good empirical performance of MPD. To that end, we have to make some assumptions, and we focus on a constrained, but very natural, random setting with expected degrees as prediction. *But this is by no means a limitation of the applicability of the algorithm in practice*\u2013rather, we have to make some assumptions in order to effectively analyze the algorithm. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JekQ_HTzE2",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_NgwrhCBPTVk",
                "title": "",
                "comment": " This paper considers the online bipartite unweighted matching problem with predicted degrees of offline vertices. The author proposes a simple algorithm MPD which matches the vertices with low degrees first to exploit the predicted degree. This algorithm has a sub-optimal 1/2 competitive ratio in the worst case. But it performs well on some real data set and CLV random graphs, as the experiments in this paper show. The author proves that this algorithm is optimal on CLV-B random graphs when the prediction is perfect. The author also estimates the lower bound of the competitive ratio of MPD on symmetry CLV-B random graphs through solutions of some derivative equations. Strengths: \n\n1.\tOnline bipartite matching problems are very important and have many applications in the real world. The prediction model in the paper is simple and well-motivated. It\u2019s reasonable that we can obtain predicted degrees in real-world applications.\n2.\tThe experimental results are well done and detailed. The author compares MPD and Ranking on real-world data sets, symmetry CLV-B graph and known i.i.d setting. The MPD outperforms Ranking well. And it seems that even when the degrees are estimated very poor, MPD still outperforms Ranking on CLV-B random graphs.\n3.\tThough MPD is sub-optimal in the worst case, the author shows that it is optimal on CLV-B random graphs. This is a nice theoretical result.\n\nWeakness:\n\n1.\tThe theoretical results are not general enough. They only cover the CLV random graphs where the expected degrees obey zipf\u2019s law. \n2.\tMain result on the optimality of MPD assumes that the predictions are perfect. If \u201cprediction\u201d is the main motivation of this paper, the error of the prediction is an important factor to study. However, the paper does not discuss the performance under imperfect prediction thoroughly. This makes the contribution of the paper limited.  \n 1.\tIn the paragraph \u201cOn prediction error\u201d, the author uses the quantity \u201cthe minimum number of offline nodes that needs to be deleted such that \u03c0 and \u03c0_0 induce the same order on the remaining nodes\u201d to measure the prediction error. But in Appendix D, the author uses the quantity \u201cLargest Increasing Subsequence\u201d. It seems that these two are the same thing, but it\u2019s better to use the same expression to avoid confusing.\n2.\tThe optimality result assume that the prediction is perfect in fact. But a \u201cprediction\u201d itself means it is not perfect. If all the theoretic results (except Appendix. D) assume that the prediction is perfect, then it might be better to consider the setting such as \u201cxxx with known expected vertex degrees\u201d but not \u201cxxx with predicted degrees\u201d. \n Yes. I don\u2019t think there are potential negative societal impact of their work.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "cDpGBS32rSM",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_NgwrhCBPTVk",
                "title": "",
                "comment": " This work considers the online bipartite matching problem with an oracle that predicts the degrees of nodes in the graph. They proposed the MinPredictedDegree(MPD) algorithm, which matches each online node to the offline node with the minimum predicted degree. \n\nOn a natural random graph model (CLV-B), the expected degrees of the offline nodes are known and used as predictions. They showed that this MPD algorithm stochastically dominates any other online algorithms. \n\nThey analyze this algorithm on a natural random graph model (CLV-B) with power-law degree distribution. Using the expected degrees as the predictor in the algorithm, they show that the competitive ratio of this algorithm to the maximum offline matching is more than 0.94 for several model parameters. \n\nThe experimental results on multiple random graph models and real-world graphs show that the performance of MinPredictedDegree exceeds state-of-the-art online algorithms. Strengths:\nLearning augmented algorithms is a popular and important topic. Online bipartite matching with degree predictors is a new and interesting setting. \n\nThis work analyzed the MinPredictedDegree algorithm for this problem. They showed the optimality of the MPD algorithm on the CLV-B random graphs. The theoretical analysis of this algorithm on the random graph (CLV-B) model is also very interesting. On CLV-B random graphs with power-law degree distribution and many real-world datasets, they show that the MPD algorithm achieves a substantial improvement. \n\nThis paper is well-written and easy to follow.\n\nI reviewed this paper before, they improved the results by adding the optimality analysis.  1. In the introduction and appendix D, you showed that the MDP is robust to the prediction error. I think your proof is the robustness of the MDP on the CLV-B graphs, right? I am confused about this proof where you used that the graph is a CLV-B random graph. Is a similar analysis of the robustness also hold for other graphs? No",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "jRzLaMFgRP5",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_NgwrhCBPTVk",
                "title": "",
                "comment": " This paper studies the online bipartite matching problem. The graph edges are not known beforehand but arrive in a certain order. Given Graph G(U\\cup V, E), U is known while the nodes in V arrive online. Once a node in V arrives, its incident edges are also known. The paper aims to find the maximum matching in CLV-B model graph. The CLV-B model assigns a probability value to each node, and an edge linking from one node in U and one node in V exists with probability that is the product of the probabilities of the two nodes.\n\nThe main contributions of the paper are new theories and proofs that show a classic MinPredictedDegree (MPD) based method can achieve optimum in the CLV-B model. It also further analyzes the lower-bounds of the competitive ratios in several special cases. \n\nOverall, the contributions look solid to me and the paper is well written. S1. This paper shows a theoretical contribution on a classic problem -- online bipartite matching.\n\nS2. This paper is in general well written.\n\nS3. It also analyzes the bias when the degree predictor has some errors.\n\nS4. It analyzes the lower-bounds of the competitor ratios in some special cases.\n\nS5. It shows empirical competitive ratios.\n\nW1. This paper assumes that there is an oracle that can predict the degree for any node in U. In CLV-B model, the expected degree is used. \n\nW2. The link of this problem to the community of Machine Learning should be made clearer. Is a pure theory conference better for this paper? Q1. In fig 2, why this is a fluctuation in 2007?\n\nQ2. I cannot get the intuition behind Theorem 6.1 (Eq. 4). Please explain the results with more intuition.\n\nQ3. Since the a CLB-V graph is a special case of a general graph, and there are results for the online bipartite matching for the general graphs already. I wonder how difficult are the proofs in addition to the known results.  The results of this paper is very interesting. However, the link of the studied problem to the community of Machine Learning is not that clear. How is the online bipartite matching applied in some ML based applications? If the link not that strong, I am not sure NeurIPS is the best fit for this paper. ",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "1Ml73XGQI5u",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_NgwrhCBPTVk",
                "title": "",
                "comment": " This paper studies the problem of online bipartite matching. A new model called CLV-B is proposed under which the performance of the greedy algorithm is analyzed. For the general case, the paper proves that the greedy algorithm is optimal in terms of the success probability. For the special case, called symmetric CLV-B model, the paper analyzes the competitive ratio and the expected matching size, and further discusses the competitive ratio for power law and Erdos-Renyi graphs. Simulations are conducted to verify the practical performance of the proposed algorithm. \n\n Strengths\n- S1: The considered algorithm is definitely a natural choice for the CLV-B.\n- S2: The paper presents a comprehensive study regarding the considered problem, and the presented results seem to be technically sound.\n- S3: The experimental studies are also comprehensive, with source code available. \n\nWeaknesses\n- W1: The time and introduction suggest that the problem is solved using predicted variables, but the entire paper (module appendix D) focuses on the case where the exact expected degree is given. In this sense, the title, abstract, and introduction are somehow misleading, and could be improved to make it clear that this paper studies classic combinatorial optimization problems, without involving any machine learning or data-driven aspect. In this sense, the paper fits better with STOC/FOCS/SODA, even though NeurIPS has a wide scope. \n\n- W2: My main concern is that the significance of the technical contribution is not very clear. The paper provides a lot of theoretical analysis (15 pages of proofs in the appendix), but it is not clear how easy or hard the presented results can be derived based on the existing works. For theory papers, the presented proofs are often inspired by some existing proofs, instead of being completely new. Such information is crucial for evaluating the significance of a new paper. For example, since the symmetric CLV-B model is a special case of the known iid model, does the analysis of the known iid model apply directly? What are the fundamental challenges caused by the CLV-B model in analyzing the algorithms?\n\n- W3: The paper could be better organized such that all the theoretical results are clearly stated in the main paper. Many results in the appendix are only mentioned in the introduction but not discussed in the rest of the paper, and I am wondering whether such results are significant. \n\n- W4: Some minors\n\t- In the case that q=1, it seems to be the case that the producers (not consumers) select their adjacent edges iid.\n\t- Instead of claiming that the competitive ratio exceeds 0.99 for power-law graphs, it would be better to simply state that the competitive ratio depends on the parameters of the power-law graph.\n\t- What does it mean by \u201cthe worst distribution for MPD\u201d? It would be better to formally state the proposed conjecture.\n\t- P4, it would be better to give a formal definition of p(A, S). \n\n\n To make the contribution clear, it is necessary for the paper to draw proper connections between the presented proofs and the existing proofs, or claim that the presented proofs are not inspired by any existing works.\n\n - The paper does not discuss the limitations.\n- One potential limitation is that some of the presented results can be easily acquired from existing papers.\n",
                "rating": 4,
                "confidence": 3
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "contains good technical contributions",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "some concern",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "use of the term 'predicted degrees'",
                "Sentiment Expression": "is indeed misleading",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "clearly state",
                "Sentiment Expression": "should",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "accurate title/abstract/intro in their presentation",
                "Sentiment Expression": "more",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "their Appendix D",
                "Sentiment Expression": "does provide some result",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "has good contributions",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "H1gax6VtDB": {
        "paper_id": "iclr_2020_H1gax6VtDB",
        "paper_title": "Contrastive Learning of Structured World Models",
        "paper_abstract": "A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.",
        "paper_acceptance": "accept-talk",
        "meta_review": "This paper presents an approach to learn state representations of the scene as well as their action-conditioned transition model, applying contrastive learning on top of a graph neural network. The reviewers unanimously agree that this paper contains a solid research contribution and the authors' response to the reviews further clarified their concerns.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "S1gKxxeZ5B",
                "reply_to": "iclr_2020_H1gax6VtDB",
                "title": "Official Blind Review #1",
                "comment": "This paper aims to learn a structured latent space for images, which is made up of objects and their relations. The method works by (1) extracting object masks via a CNN, (2) turning those masks into feature vectors via an MLP, (3) estimating an action-conditioned delta for each feature via a GNN. Learning happens with contrastive losses, which ask that each feature+delta is close to the true next feature, and far away from other random possibilities. Experiments in simple synthetic environments (e.g., 2D geometric shapes moving on a black background) show encouraging results. \n\nThis paper has a simple, well-motivated method. It is clearly written, and easy to understand. The evaluation is straightforward also: the paper merely shows that this model's nearest neighbors in featurespace are better than the nearest neighbors of World Model (2018) and PAIG (2019). Also, some visualizations indicate that for these simple directional manipulations (up/down/left/right motion), PCA compressions of the model's states have a clean lattice-like structure.\n\nIt is impressive that the model discovers and segments objects so accurately. Perhaps this could actually be evaluated. However, I do not understand why results are so sensitive to the number of object slots (K). This seems like a severe limitation of the model, since in general we have no idea what value to set for this. \n\nAlthough I like the paper, I am not sure that there is sufficient evidence for the method being something useful. Yes, H@1 and MRR are high, but as the paper itself implies, the real goal is to improve performance (or, e.g., sample efficiency) in some downstream task. Given how simple these domains are, and the fact that data is collected with purely random exploration, it is difficult to imagine that there is any significant difference between the training set and the test set. For example, if you make 1000 episodes of 10 steps each in Space Invaders, you practically get 1000 copies of the same 10 frames. I worry that all the evaluation has shown so far is that this model can efficiently represent the state transitions that it has observed.\n\nThe authors note that it was beneficial to only use the hinge on the negative energy term. This seems unusual, since a hinge on the positive term allows some slack, which intuitively makes the objective better-formulated. Can the authors please clarify this result, at least empirically? \n",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "B1l195thiH",
                "reply_to": "B1gr78U2jH",
                "title": "Response to reviewer #1",
                "comment": "Thank you for your follow-up question.\n\nThe following workshop paper is a good reference for an exploration of this issue in the context of representation learning for relational data:\n\nMelanie Weber & Maximilian Nickel, \"Curvature and Representation Learning: Identifying Embedding Spaces for Relational Data\", NeurIPS 2018 Workshop on Relational Representation Learning, https://web.math.princeton.edu/~mw25/project/files/nips_FB.pdf",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1gr78U2jH",
                "reply_to": "B1l07hfwoS",
                "title": "Thank you",
                "comment": "Your point about L2 normalization is interesting. Do you have a reference for the claim that \"hyperspherical latent space is likely less suitable for learning grid-structured representations\"? ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1gu47mniS",
                "reply_to": "ByxR2zlsoH",
                "title": "Response to reviewer #2",
                "comment": "Thank you for your kind response and for taking our rebuttal into account for your assessment of our paper.\n\nRegarding your extra question:\n\nQ10 [PAIG baseline]:\nOur dataset slightly differs from that used in PAIG: we use larger time steps between rendered frames in the physical simulator (as we are only using a total of 10 observations, every observation consists of two consecutive frames) and our frames are of slightly different size (50x50x3 as opposed to PAIG\u2019s 36x36x3) to be in line with the other environments we tested. Lastly, we use only 2 frames for velocity estimation instead of 4 for fair comparison with the World Model baseline and C-SWM (which take pairs of 2 frames at every time step). \n\nNone of these changes, however, should in principle prevent the PAIG model from learning the correct dynamics. We will have a closer look at what causes the PAIG model to fail (due to very little remaining time we will have to do this after the rebuttal period), but looking at the PAIG model predictions, it looks like the model often fails at object identification (predicting the correct mask for an object) in our setting. You can find a video of PAIG model predictions in our anonymous repository (paig_predictions.gif).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ByxR2zlsoH",
                "reply_to": "ryee-pMwiH",
                "title": "Response to authors",
                "comment": "Thank you very much for the extremely thorough rebuttal, and for the extra experiments on such short notice. The improved figures and text are great.\n\nThis improved the manuscript a lot for me and I would recommend it for acceptance.\n\nQ1a+Q4: Great to know, thanks for checking. The text is also clearer now, along with the discussion of when this would help or not.\nQ1b: Very interesting to see that this still works for D>2, thanks a lot for running these.\n\n--\n\nOne extra question I was interested to hear your thoughts on:\n\n10. Why is PAIG doing so poorly on the 3-body problem? Given it has the \"true\" dynamics model built-in, I would have expected it to perform better?\nTheir Figure 3 seems to indicate near-perfect results at 10-steps, however it drops to a third of top-performance in your case?",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1l81UwCYS",
                "reply_to": "iclr_2020_H1gax6VtDB",
                "title": "Official Blind Review #2",
                "comment": "This paper tackles the problem of learning an encoder and transition model of an environment, such that the representation learnt uses an object-centric representation which could favor compositionality and generalisation. This is trained using a contrastive max-margin loss, instead of a generative loss as previously explored. They do not consider RL or follow-up tasks leveraging these representations and transition models yet.\nThey perform an extensive assessment of their model, with many ablations, on 2 gridworld environments, one physical domain, and on Atari.\n\nThe paper is very well motivated, easy to follow, and most of its assumptions and decisions are sensible and well supported. They also provide interesting assessments and insights into the evaluation scheme of such transition models, which would be of interest to many practitioners of this field.\n\nApart from some issues presented below, I feel that this work is of good quality and would recommend it for acceptance.\n\n1.\tThe model is introduced in a very clear way, and most decisions seem particularly fair. I found the presentation of the contrastive loss with margin to be clear, and the GraphNet is also well supported (although see question below). \u2028However, two choices are surprising to me and would deserve some clarification and more space in the main text, instead of the Appendix:\n\ta.\tWhy does the object extractor only output a scalar mask? This was not extremely clear from reading the main text (and confused me when I first saw Figure 1 and 3a), but as explained in the Appendix, the CNN is forced to output a sigmoid logit between [0, 1] per object channel.\u2028\n\tThis seems overly constraining to me, as this restricts the network to only output \u201c1 bit\u201d of information per \u201cobject\u201d.\n\tHowever, maybe being able to represent other factors of these objects might be necessary to make better predictions? \u2028\n\tThis also requires the user to select the number of output channels precisely, or the model might fail. This is visible in the Atari results, where the \u201cobjectness\u201d is much less clear.\n\t\u2028Did you try allowing the encoder to output more features per objects? \n\tObviously this would be more complicated and would place you closer to a setting similar to MONet (Burgess et al. 2019) or IODINE (Greff et al. 2019), but this might help a lot.\n\tb.\tIt was hard to find the dimensionality D of the abstract representation $z_t$. It is only reported in the Appendix, and is set to $D=2$ for the 2D gridworld tasks and $D=4$ for Atari and the physics environments. \u2028These are quite small, and the fact that they exactly coincide with your assumed sufficient statistics is a bit unfortunate.\u2028 \n\tWhat happens if D is larger? Could you find the optimal D by some means?\n2.\tThe GraphNet makes sense to me, but I wondered why you did not provide $a_t^j$ to $e_t^{(i, j)}$ as well? I could imagine situations where one would need the action to know if an interaction between two slots is required.\n3.\tSimilarly, the fact that the action was directly partitioned per object (except in Atari where it was replicated), seemed slightly odd. Would it still work if it was not directly pre-aligned for the network? I.e. provide $a_t$ as conditioning for the global() module of the GraphNet, and let the network learn which nodes/edges it actually affects.\n4.\tIn your multi-object contrastive loss, how is the mapping between slot k in $z_t$ and $\\tilde{z}_t$ performed? Do you assume that a given object (say the red cube) is placed in the same $k$ slot across different scenes/timesteps?\u2028This may actually be harder to enforce by the network than expected (e.g. with MONet, there is no such \u201cslot stability\u201d, see [1] for a discussion).\n5.\tIt was unclear to me if the \u201cgrid\u201d shown in Figure 3 (b) and 5 is \u201creal\u201d? I.e. are you exactly plotting your $z_t$ embeddings, and they happen to lie precisely along this grid? If yes, I feel this is a slightly stronger result as you currently present, given this means that the latent space has mirrored the transition dynamics in a rather impressive fashion.\n6.\tRelated to that point, I found Figure 3 b) to be slightly too hard to understand and parse. The mapping of the colours of the arrows is not provided, and the correspondence between \u201cwhat 3D object is actually moving where\u201d and \u201cwhich of the coloured circles correspond to which other cubes in the image\u201d is hard to do (especially given the arbitrary rotation). \u2028Could you add arrows/annotations to make this clearer? \u2028Alternatively, presenting this as a sequence might help: e.g. show the sequence of real 3D images, along with the trajectory it traces on the 2D grid.\n7.\tFigure 4 a) was also hard to interpret. Seeing these learnt filters did not tell much, and I felt that you were trying too hard to impose meaning on these, or at least it wasn\u2019t clear to me what to take of them directly. I would have left this in the Appendix. Figure 4 b) on the other hand was great, and I would put more emphasis on it.\n8.\tThere are no details on how the actual test data used to generate Table 1 was created, and what \u201cunseen environment instances\u201d would correspond to. It would be good to add this to the Appendix, and point forward to it at the end of the first paragraph of Section 4.6, as if you are claiming that combinatorial generalization is being tested this should be made explicit. I found Table 1 to be great, complete, and easy to parse.\n9.\tIt would be quite interesting to discuss how your work relates to [1], as the principles and goals are quite similar. \u2028On a similar note, if you wanted to extend your 2D shape environment from a gridworld to a continuous one with more factors of variations, their Spriteworld environment [2] might be a good candidate.\n\n\nReferences:\n[1] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P. Burgess, Alexander Lerchner, \u201cCOBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration\u201d, 2019, https://arxiv.org/abs/1905.09275\n[2] Nicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra, Alexander Lerchner, \u201cSpriteworld: A Flexible, Configurable Reinforcement Learning Environment\u201d, https://github.com/deepmind/spriteworld/ \n\n",
                "rating": 8,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SJlwMQTdsS",
                "reply_to": "SkxeBpMDjS",
                "title": "Response",
                "comment": "Thank you for the additional details and experiments, as I believe these are useful bits of knowledge for probing how your model performs. It appears that K is not *overly* sensitive to misspecification, and indeed automating this would would be a great avenue for future research. The modified 2D shapes environment for testing no-ops and stochasticity sounds like a great realisation of my suggestion, and, again, the results are very interesting to know!",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkxeBpMDjS",
                "reply_to": "rJgl-c_P_B",
                "title": "Response to review #3",
                "comment": "Thank you for your valuable feedback.\n\nPlease find our responses to your questions and comments below.\n\n[Number of object slots]:\nWe have carried out an analysis for the 3-body physics environment with misspecification of the number of object slots (all other parameters are left the same), see results table for mean MRR (in %) results over 4 model runs below. The results show that K=1 is not sufficient for good generalization on the 3-body system, whereas K=5 performs comparable or slightly better than K=3 (note that there is some variance in between runs). Looking only at the best 10-step prediction result out of 4 runs, we have 95.4 (for K=3) vs. 93.3 (for K=5).\n\n+----------------------------------------------------------------+\n| Model                       | 1 Step | 5 Steps | 10 Steps |\n+----------------------------------------------------------------+\n| K=1                           |  97.5    |  71.5    |  40.9     |\n+----------------------------------------------------------------+\n| K=3 (default)             | 100      |  98.5    |  85.2     |\n+----------------------------------------------------------------+\n| K=5                           | 100      |  98.7     |  91.0     |\n+----------------------------------------------------------------+\n\nIn terms of increasing K beyond 5 on the Space Invaders benchmark: We have carried out an additional experiment with K=7 with the following mean MRR (in %) results: 71.5 (1 step), 28.3 (5 steps), 22.7 (10 steps) -- i.e., worse in long-term prediction than for K=5 (best setting) but comparable to K=5 in the 1 step prediction setting.\n\nNote that for the block pushing environments, we cannot change the number of object slots without changing the way actions are factorized and presented to our model. To make the model more robust to the number of object slots, an iterative object detection mechanism such as in MONet (Burgess et al., 2019) might be useful which can assign 'empty' slots if all objects are explained by the model already, which would be interesting to explore in future work.\n\n[Synthetic dataset to test the effect of no-op actions & other agents]:\nThis is a great suggestion. We have performed additional experiments on a variant of the block pushing (2D Shapes) environment where a) some actions have no effect (no-op action), and b) one block moves randomly and independent of agent actions. We find that adding a no-op action has little to no effect on the ability of the model to discover objects, learn the underlying grid structure and to generalize to new environment instances. In the other setting (one out of the five objects moves randomly), the model correctly discovers representations for the other 4 objects, but learns a \"blank\" feature map for the randomly moving object -- prediction performance on the test set is negatively affected by the randomly moving object: 93.7 mean MRR (in %) instead of 100 for 10-step prediction. It would be interesting to extend the C-SWM model to explicitly handle uncertainty in environments in future work to address this gap in performance.\n\nWe have posted an updated version of our manuscript. You can find a short summary of our changes in our top-level comment.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryee-pMwiH",
                "reply_to": "BkxrThzwor",
                "title": "Response to review #2 -- part 2",
                "comment": "Q8 [Test set generation]: \nWe generate the test sets in the same way as the training sets (i.e., using a random policy and a random initialization of the environment where possible), but using different random seeds. The state space for the grid world environments is large (~6.4M possible environment configurations), and hence the train/test overlap can be expected to be small (both train and test set contain 100k experience triples, and each test episode is a sequence of 10 such triples). Only for the Atari games the situation is a bit trickier (as episodes can be very similar). We address this by first running a fixed number of random actions before we start populating the experience buffers. We have verified that not a single (full-length) test episode exactly coincides with a training episode, so that doing well on this task requires generalization. We have clarified this in the paper.\n\nQ9 [COBRA/Spriteworld]: \nThanks for the literature suggestion. The COBRA (Watters et al., 2019) paper is something we have overlooked to include in our related work discussion, and it is indeed very related. The main differences between the unsupervised component of COBRA and our work are that we use a GNN transition model to model interactions (instead of modeling each slot independently with an MLP), a contrastive loss (instead of decoding back into pixel space), and a simpler object detection module (COBRA uses MONet, which would be interesting to try in future work in a contrastive setting as well, but this would require solving a matching problem as outlined in the appendix of the COBRA paper). Lastly, our model is trained end-to-end whereas COBRA is demonstrated using a pre-trained vision model. We have included a short discussion in the paper.\n\nThe Spriteworld tasks considered in the COBRA paper will most likely pose some challenges to our simplified object detector/encoder, as we do not perform instance disambiguation (but rather assume that there is a fixed number of objects of specific appearance), but we agree that it would be a very interesting benchmark for testing an extension of C-SWM with a more elaborate encoder.\n\nWe have posted an updated version of our manuscript. You can find a short summary of our changes in our top-level comment.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxrThzwor",
                "reply_to": "H1l81UwCYS",
                "title": "Response to review #2",
                "comment": "Thank you for your extensive review and for your detailed feedback, this is greatly appreciated.\n\nPlease find our responses to your questions and comments below.\n\nQ1a [Scalar mask]: \nThis is indeed a very good point, which we initially did not try experimentally to avoid additional complexity. Our synthetic block pushing environment does not require more than a simple scalar mask, as the model only needs to encode object location. For more complex environments, it could indeed be beneficial to assign more than one output channel per object. Note that the object encoder (which maps from scalar mask to object latent variable) is an MLP with high-dimensional hidden representations, which allows the model to extract, e.g., object position from its mask. We carried out additional experiments on the Space Invaders task with {2, 5, 10} output channels per object slot and we found no significant difference in MRR results compared to using just one output channel. We added these results to the appendix, and we also further clarified this architecture detail in the paper.\n\nQ1b [Dimensionality of latent space]:\nWe ran additional experiments with D>2 on the block pushing experiments (2D shapes) and we found that we get the same results for D in {4, 8, 16}, i.e., MRR=100% (on {1,5,10} prediction steps into the future) and the latent representations lie on a close to perfect 2D grid that is randomly oriented in the higher-dimensional latent space. So it seems like the choice of D does not have a significant influence on results as long as it is not too small. We have improved clarity on this detail in the paper.\n\nQ2 [Actions on edge messages]: \nThanks for this suggestion. We initially designed the model with the example of the synthetic block pushing environments in mind, where it is not necessary to condition the messages on the action, but this could indeed in principle be useful for the Atari game setting. Alternatively, one could perform multiple rounds of message passing as suggested in the paper. We ran an additional experiment on Space Invaders with K=5 object slots, where we also conditioned the edge update on the action, but the results were very similar to our original setting, where we only condition the node update on the action: 26.0\u00b14.1 MRR (in %) 10-step prediction for the original setting vs. 27.5\u00b12.3 MRR (in %) for the setting with actions included in the edge update.\n\nQ3 [Learning action-to-node assignment]: \nThis is a very good point and something we haven't had the chance to try experimentally yet. Extending the GNN model with a global state in the line of GraphNets (Battaglia et al., 2018) would certainly be a good starting point for learning the action-to-node/-edge assignment automatically, but it would likely require some more changes to the model (or to the way actions are encoded) as object slots are fully exchangeable in the current architecture and one would need a way to break this symmetry.\n\nQ4 [Slot stability]: \nOur model is \"slot stable\" as objects are identified with a particular feature map of the CNN. In other words, we can assume that the same object always ends up in the same slot (for a fixed set of model parameters). While this is convenient, this is of course a limitation as it does not allow for instance disambiguation (e.g. two objects with the same appearance), which needs to be overcome in future work (see \"Instance Disambiguation\" in Section 4.7 on Limitations). For encoders that are not \"slot stable\", one could potentially use something like the Sinkhorn distance to compute the energy terms, but this could come with other challenges.\n\nQ5 [Grid-structure of embedding space]: \nYes, the grid is \"real\"! There is no post-processing done to get these plots -- we directly visualize the learned 2D embedding space and plot learned transitions as lines/edges. We found it indeed remarkable that the model learns to uncover this latent structure so precisely. We made this point clearer in the paper.\n\nQ6/7 [Figure clarity]: \nThank you, this is very helpful feedback regarding Figures 3 and 4. We have updated both figures in the paper to improve clarity.\n\n(continued in the next comment due to character limitations)",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1l07hfwoS",
                "reply_to": "S1gKxxeZ5B",
                "title": "Response to review #1",
                "comment": "Thank you for your valuable feedback.\n\nPlease find our responses to your questions and comments below.\n\n[Number of object slots (K)]:\nThis is a very good question. Our results indicate that it is best to choose K based on validation set performance if there is no clear a-priori choice. Generalization to unseen environment instances likely not only depends on how well objects are discovered and represented, but also to what degree the learned transition model (GNN) on this structured latent space generalizes. Hence, it is difficult to a-priori predict which number of object slots would work well on a particular problem, unless the model has some built-in mechanism to assign \"empty\" slots, such as the iterative mechanism in MONet (Burgess et al., 2019), which however relies on pixel-based losses. Despite the dependency on K, we still observe stronger generalization performance across a range of settings compared to unstructured baselines using pixel-based losses.\n\n[Difference between training and test sets]:\nThis is a very good point and we indeed try to control for this issue. For the Atari benchmarks, we populate the experience buffer only after a certain number of frames (which represent fully deterministic opponent transitions) during which we take random actions. We have verified that no episode (i.e., the full 10-step sequence of states/actions) in the test set exactly coincides with an episode from the training set for both Pong and Space Invaders, and hence performing well on this task requires some form of generalization. In the grid world / block pushing environments, there are around 6.4M possible environment configurations, and hence the train/test overlap can be expected to be small (both train and test set contain 100k experience triples). For the physics simulation the state space is continuous and starting positions are sampled at random. We have made this clearer in the paper.\n\n[Hinge loss]:\nWe performed a direct comparison between our loss and the triplet loss from TransE (Bordes et al., 2013), i.e. with the hinge covering both the positive and the negative energy term. The table below summarizes mean MRR (in %) results (from 4 runs with random init.) on the 2D Shapes environment for hinge parameters $\\gamma$ in {1,5,10}.\n\n+----------------------------------------------------------------+\n| Model                            | 1 Step | 5 Steps | 10 Steps |\n+----------------------------------------------------------------+\n| C-SWM (default loss, $\\gamma=1$) | 100    | 100     | 100      |\n+----------------------------------------------------------------+\n| Full hinge, $\\gamma=1$           | 97.8   | 54.4    | 21.8     |\n+----------------------------------------------------------------+\n| Full hinge, $\\gamma=5$           | 98.8   | 65.7    | 26.8     |\n+----------------------------------------------------------------+\n| Full hinge, $\\gamma=10$          | 98.5   | 63.3    | 30.3     |\n+----------------------------------------------------------------+\n\nThis setting performs significantly worse in our case, most likely because we do not force the embeddings to lie on a (hyper-)sphere (i.e., L2 norm = 1). In (Bordes et al., 2013), the authors include this constraint to avoid pathologies in their loss function (trivial minimization by growing the norms of the embeddings), which might be the cause for suboptimal performance in our case. We do not wish to constrain embeddings to a hypersphere in general, however, as this could affect how accurately we can learn certain structures (e.g., a hyperspherical latent space is likely less suitable for learning grid-structured representations and might make it more difficult for the transition model to generalize). Hyperspherical latent spaces could be useful, however, for learning rotational transformations.\n\nWe have posted an updated version of our manuscript. You can find a short summary of our changes in our top-level comment.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1gve3zDjS",
                "reply_to": "iclr_2020_H1gax6VtDB",
                "title": "General response",
                "comment": "We would like to thank the reviewers for their valuable feedback. \n\nWe have updated our manuscript with the following changes:\n- We have added additional experimental results in Appendix A with a) a variant of our loss function that places the 'hinge' over the energy terms of both positive and negative samples (R1), b) multiple feature maps per object slot (R2), and c) variants of our grid world environments with no-op actions and with randomly moving objects (R3).\n- We have improved the clarity of our exposition and writing to address questions and comments by all three reviewers (R1-3).\n- We discuss additional related work (R2).\n- We have improved clarity of Figures 3 and 4 (R2).\n\nThese changes have increased the length of the main part of the paper to close to 9 pages. We believe that this is justified for better coverage of related work and for improved clarity.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgl-c_P_B",
                "reply_to": "iclr_2020_H1gax6VtDB",
                "title": "Official Blind Review #3",
                "comment": "The construction and learning of structured world models is an interesting area of research that could in principle enable better generalisation and interpretability for predictive models. The authors overcome the problem of using pixel-based losses (a common issue being reconstruction of small but potentially important objects) by using a contrastive latent space. The model otherwise makes use of a fixed number of object slots and a GNN transition model, similarly to prior approaches. The authors back up their method with nice results on 3D cubes and 3-body physics domains, and reasonable initial results on two Atari games, with ablations on the different components showing their contributions, so I would give this paper an accept.\n\nThe comparisons to existing literature and related areas is very extensive, with interesting pointers to potential future work - particularly on the transition model and graph embeddings. As expected, the object-factorized action space appears to work well for generalisation, and could be extended/adapted, but setting a fixed number of objects K is a clearly fundamentally limiting hyperparameter, and so showing how the model performs under misspecification of this hyperparameter is useful to know for settings where this is known (2D shapes, 3D blocks, 3-body physics). The fact that K=1 is the best for Pong but K=5 is the best for Space Invaders raises at least two questions: can scaling K > 5 further improve performance on Space Invaders, and is it possible to make the model more robust to a greater-than-needed number of object slots? On a similar note, the data collection procedure for the Atari games seems to indicate that the model is quite sensitive to domains where actions rarely have an impact on the transition dynamics, or the interaction is more complex (e.g. other agents exist in the world) - coming up with a synthetic dataset where the importance of this can be quantified would again aid understanding of the authors' proposed method.",
                "rating": 8,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "contains a solid research contribution",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            }
        ]
    },
    "S0UdquAnr9k": {
        "paper_id": "iclr_2021_S0UdquAnr9k",
        "paper_title": "Locally Free Weight Sharing for Network Width Search",
        "paper_abstract": "Searching for network width is an effective way to slim deep neural networks with hardware budgets. With this aim, a one-shot supernet is usually leveraged as a performance evaluator to rank the performance \\wrt~different width. Nevertheless, current methods mainly follow a manually fixed weight sharing pattern, which is limited to distinguish the performance gap of different width. In this paper, to better evaluate each width, we propose a locally free weight sharing strategy (CafeNet) accordingly. In CafeNet, weights are more freely shared, and each width is jointly indicated by its base channels and free channels, where free channels are supposed to locate freely in a local zone to better represent each width. Besides, we propose to further reduce the search space by leveraging our introduced FLOPs-sensitive bins. As a result, our CafeNet can be trained stochastically and get optimized within a min-min strategy. Extensive experiments on ImageNet, CIFAR-10, CelebA and MS COCO dataset have verified our superiority comparing to other state-of-the-art baselines. For example, our method can further boost the benchmark NAS network EfficientNet-B0 by 0.41\\% via searching its width more delicately.",
        "paper_acceptance": "spotlight-presentations",
        "meta_review": "The paper proposed locally free weight sharing strategy (CafeNet) for searching optimal network width. The proposal is a nice tradeoff between manually fixed weight sharing pattern (too small search space) and completely free weight sharing pattern (too large search space). The *originality* and *significance* are clearly above the bar. The paper is related to the general interests of deep learning research and its *applicability* deserves a spotlight presentation.\n\nIt seems the *clarity* can still be improved, so please carefully revise the paper following the reviews. BTW, I am very curious, why \"locally free weight sharing strategy\" goes to a short name CafeNet? I went over the paper but I didn't find the answer. Perhaps the name of the proposal should also be explained...",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "0HuWFvkH6x-",
                "reply_to": "ARfspQ3zWtj",
                "title": "Response to AnonReviewer2",
                "comment": "Thanks for your positive support and instructive comments. We have revised the presentation issue and typos of this paper in our next version.\n\nQ1: More explanations with the smallest training loss. \\\nA1: Since network width is specified more freely by several sub-networks in CafeNet, and the performance of these sub-networks can be different. Therefore, to directly compare the performance of different network width, we need to specify an indicator for each network width. For example, use the Top-1 accuracy of all sub-networks (CafeNet) or other methods that involve more sub-networks(e.g., the average accuracy of all sub-networks). Although there are countless ways to specify the indicator, these ways will undoubtedly introduce more computation for each width, and also sub-networks with poor performance may not well represent the performance of width. Therefore, to indicate the performance w.r.t network width, we propose to leverage the sub-network with maximum performance, which amounts to the smallest training loss.\n\nQ2: More details of Eq. (7). \\\nA2: Since the FLOPs of a layer is linear to the number of filters(channels), to evaluate the influence of filters, we propose to construct the FLOPs-sensitivity of a layer by examining the real FLOPs variation of reducing a single filter. In detail, a filter influences the output channel for the current layer and the input channel as the next layer, which corresponds to two terms of Eq. (7).\n\nQ3: Selection of layer width with Eq. (2). \\\nA3: In our method, for the given FLOPs constraint, the search is implemented with random or evolutionary search, named CafeNet-R and CafeNet-E in all tables. In detail, for evolutionary search, we implement it with the multi-objective NSGA-II algorithm [1]. As illustrated in Appendix A.2, we set the population and iteration size of evolutionary search to 40 and 50, and we randomly select 40 network width within the FLOPs budget as the initial population. In each generation,  with each width satisfying the FLOPs budget, we specify the sub-network with the strategy of max-max selection to indicate its performance. Then, we assign the validation accuracy (the validation dataset is split from the training dataset) of the specified sub-network for the width to indicate its performance. For those sampled network width with larger FLOPs, we just drop them. Afterward, the network width with the highest score is selected as the optimal width to train from scratch for evaluation.\nWhile for the random search, we uniformly sample the same number of network width as the evolutionary search. Then, for each network width satisfying the FLOPs budget, we specify the sub-network with the strategy of max-max selection and examine its validation accuracy. Afterward, we select the width with the highest accuracy to train from scratch for evaluation. \\\n [1] Deb, Kalyanmoy, et al. \"A fast and elitist multiobjective genetic algorithm: NSGA-II.\" IEEE transactions on evolutionary computation 6.2 (2002): 182-197.\n\nQ4. Zero width issue for a layer. \\\nA4: The current setting of CafeNet cannot reach 0 width for two reasons. First, as described in Eq. (5), we limit the width of each layer to no less than 1. Second, even if 0 width can be selected as a candidate, the performance of the corresponding sub-network will be greatly restricted due to the existence of the disconnected layer, and thus cannot be selected as the optimal width through evolution or random search.\n\nQ5. Presentation issue of Fig. 2(b). \\\nA5: Thanks for pointing out this issue. In fact, $\\lambda$ in the text corresponds to $1 - \\lambda$ in Fig 2(b). Therefore, some descriptions of min-min optimization should be revised as follows:  \nDuring training CafeNet, we only optimize the sub-network with minimum training loss for each sampled width. To investigate the effect of this optimization strategy, suppose we have all $\\tau$ iterations, then we implement min-min optimization only on the last $(1-\\lambda)\\cdot\\tau$ iterations with $\\lambda\\in[0,1]$. For the first $\\lambda\\cdot\\tau$ iterations, we simply optimize one sub-network randomly for each sampled width. As shown in Fig. 2(b), our 0.5$\\times$-FLOPs MobileNetV2 on CIFAR-10 improves 0.92\\% accuracy from $\\lambda = 1$ to $\\lambda = 0$.\n\nQ6. More experiments of searched network width with aligned hyperparameter settings. \\\nA6: Thanks for your advice. As illustrated in Appendix A.10, we retrain the searched network width with the same training recipes of AutoSlim, as shown in Table. 14. Some examples of the comparisons are shown below: \\\n1G FLOPs of ResNet50: \\\nAutoSlim: 74.0\\%, CafeNet: 74.5\\% \\\n207M FLOPs of MobileNetV2: \\\nAutoSlim: 73.0\\%, CafeNet:  73.2\\% \\\nMore details about the retraining results can refer to as Table. 14 of Appendix A.10. With the same training recipes of the baseline method (AutoSlim), the results in Appendix A.10 show the effectiveness of our proposed CafeNet.\n\n\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "odT7dXhAJ_5",
                "reply_to": "km2Sy1SBvZw",
                "title": "Response to AnonReviewer4",
                "comment": "Thanks for your positive opinions and suggestions. We have revised the typos and polished our presentation according to the response below.\n\nQ1: Discussion of related papers. \\\nA1: Thanks for reminding of the related works. OFA proposes to train a supernet that supports diverse architectural settings by decoupling training and search; thus, it can be quickly used to get a specialized sub-network without additional training. TF-NAS proposes a novel method to boost the search with three levels of differentiable NAS (i.e.,  operation-level, depth-level, and width-level).  With this method,  TF-NAS achieves good performance in terms of both classification accuracy and precise latency constraint.  We have cited these papers (i.e., OFA and TF-NAS) in the first paragraph of introduction.\n\n\nQ2: Correlation of weight sharing degree with performance under the same FLOPs. \\\nA2: Indeed, higher degrees of freedom will lead to better search results. As shown in Figure 2(a), with the same FLOPs budget, accuracy performance benefits from the increase of $r$. In detail, when $r$ is set to 1, a large gap is introduced in comparison to $r = 0$ (fixed weight pattern); this is because a better representation for each width is induced with the freedom in selecting channels.  However, when $r$ goes larger, the increase of performance of searched width gradually slows down, which means using a small offset $r$ can already help distinguish the performance of different width, thus helping to select the optimal width.\n\nQ3: Typo of Eq. (9). \\\nA3: The Eq. (9) should be revised to $ b_i = \\beta \\times \\frac{\\max_j \\varepsilon_j}{\\varepsilon_i}$.  Thanks for pointing out this typo. Since we specify the minimum bin size as $\\beta$, and bin size is inverse to the sensitivity $\\varepsilon$ as defined in Eq. (8). Therefore, the minimum bin size should correspond to the maximum sensitivity.  With this revised Eq. (9),  the second term on the right side of Eq. (9)  will always be $\\geq$ 1. As for the definition of $\\beta$, in practical implementation, the number of channels in each bin should be an integer and greater than or equal to 1. Therefore, in the code level, the bin size $b_i$ should be implemented as $b_i = round(\\max(b_i, 1))$. As a result, $\\beta \\leq 1$ means that the bin size of several layers with $\\varepsilon$ close to the maximum sensitivity $\\max_j \\varepsilon_j$ are defined to 1, which induces a larger search space.\n\nQ4: Presentation issue of Fig. 2(b). \\\nA4: Sorry for the inconsistent meanings of lambda. In fact, $\\lambda$ in the text corresponds to $1 - \\lambda$ in Fig 2(b). Therefore, the descriptions of min-min optimization should be revised as follows:   \\\nDuring training CafeNet, we only optimize the sub-network with minimum training loss for each sampled width. To investigate the effect of this optimization strategy, suppose we have all $\\tau$ iterations, then we implement min-min optimization only on the last $(1-\\lambda)\\cdot\\tau$ iterations with $\\lambda\\in[0,1]$. For the first $\\lambda\\cdot\\tau$ iterations, we simply optimize one sub-network randomly for each sampled width. As shown in Fig. 2(b), our 0.5$\\times$-FLOPs MobileNetV2 on CIFAR-10 improves 0.92\\% accuracy from $\\lambda = 1$ to $\\lambda = 0$, which means that min-min optimization does help to better evaluate each width and boost the searching performance accordingly. We further record the performance of 1K sampled width; details refer to Appendix A.13.\n\n\nQ5: Code release and other presentation suggestions: \\\nA5: Thanks for your suggestions. We will release our code after this paper is published, and we have modified the presentation of Table 1 and use $\\mathcal {E}$ (Epochs) to indicate the epochs in algorithm1.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "61OfgEEZRt6",
                "reply_to": "rRx_eGlyhXN",
                "title": "Response to AnonReviewer3",
                "comment": "Thanks for the comments. We have polished our writing, and the following answers have been revised accordingly in our next version.\n\nQ1: The relation between completely free weight and fixed weight. \\\nA1: For fixed weight pattern, it simply assigns the left $c$ channels as the sub-network for the width $c$. However, for the completely free weight pattern, to assign layer width of $c$ from $l$ channels in a layer, there will be $\\mathbb{C}^c_l$ kinds of configurations of weights, which is computationally unaffordable for practical search. Therefore, we bridge these two extreme situations by proposing a locally free weight sharing (CafeNet) strategy. In CafeNet, we split the channels of a layer into two parts, i.e., base channels and free channels, with base channels following the strategy of fixed weight sharing pattern while free channels from the neighborhood of the $c$-th channel with a preset allowed offset of $r$. The search space of our method scales at $\\mathcal{O}(\\mathbb{C}_{2r+1}^{r+1}n)$, which is only a constant time of (e.g., $\\mathcal{O}(3n)$ for $r = 1$) of fixed weight sharing search space.    \n\nQ2: The size of search space. \\\nA2: With the proposed locally free weight pattern, the size of the search space is indeed scaled from  $\\mathcal{O}(n)$ to $\\mathcal{O}(\\mathbb{C}_{2r+1}^{N})$. Instead of randomly sampling and optimizing the sub-networks, we focus on the sub-network with the best performance for a particular width. In other words, each width only corresponds to one sub-network, which largely enhances the efficiency. Details about this strategy of min-min optimization can be found in the Appendix with Eqs. (11-13). A similar strategy is applied during the evaluation of sub-networks as well. \n\nQ3: Analysis of freedom within CafeNet. \\\nA3: It is indeed that higher degrees of freedom will lead to better results. As shown in Figure 2(a), accuracy performance benefits from the increase of $r$ (more freedom). In detail, when $r$ is set to 1, a large gap is introduced in comparison to $r = 0$(fixed weight pattern); this is because a better representation for each width is induced with the free channels. However, when $r$ grows larger, the increase of searched width performance gradually slows down, which means using a small offset $r$ can already help distinguish the performance of different width. Thus a larger offsets $r$ can only lead to a little performance improvement with the selected width.  Nevertheless, the larger $r$ also aggravates the burden of training as Table 6. For a trade-off between performance and time cost, we set $r = 1$ in all experiments.\n\nQ4: Writing issue.  \\\nA4: We carefully proofread this paper and fix the typos.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r7DQcxu60AN",
                "reply_to": "Y7n2WWlQhRl",
                "title": "Response to AnonReviewer1",
                "comment": "Thanks for the reviewer's effort on reviewing our paper. The responses to the reviewer\u2019s questions are as follows:\n\nQ1: Explanation of max-max selection and min-min optimization. \\\nA1:  With the proposed locally free weight sharing pattern, width is specified more freely by several sub-networks, and the performance of these sub-networks can be different. Therefore, to directly compare the performance of different network width, we need to specify an indicator for each width. For example, use the Top-1 accuracy of all sub-networks (CafeNet) or other methods that involve more sub-networks(e.g., the average accuracy of all sub-networks). Although there are countless ways to specify the indicator, they will undoubtedly introduce more computation for each width. Sub-networks with poor performance may not sufficiently represent the performance of width. Therefore, to indicate the performance w.r.t network width, we propose to leverage the sub-network with maximum performance, which amounts to the minimum loss. (min-min optimization)\n\nBesides, during the search, for evaluating each network width $c$, we follow a similar max-max selection strategy by leveraging the sub-network with the highest performance to indicate its performance.\n\nQ2: The assignment of free channels. \\\nA2: Thanks for your valuable idea. In this paper, we propose to leverage a more freely assigned weights pattern for network weights. Suppose we use both the locally free weight pattern and assign channels more freely(i.e., channels on the right side), which may cause an unfair comparison with the current mainstream baseline methods (algorithms with fixed weight pattern). However, with more freely assigned channels, network width search performance may be further increased, which can be researched as innovative work in the future.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Y7n2WWlQhRl",
                "reply_to": "iclr_2021_S0UdquAnr9k",
                "title": "An interesting weight sharing mechanism for network width search",
                "comment": "In this paper, the authors introduce a new weight sharing pattern to search for the width in a network layer. Besides, FLOPs-sensitive bins is proposed to measure the real FLOPs of a single channel at a layer and further reduce the search space. The paper proposes a locally free weight sharing mechanism where the channels in a layer are split into base channels and free channels. Compared with conventional fixed weight sharing pattern where the leftmost channels are assigned as the sub-network, the proposed locally free pattern increases more flexibility while the search space also scales at O(n). The proposed FLOPs-sensitive bins forces the layers with larger FLOPs sensitivity to have fewer channels, thus reducing the search space at a fixed FLOPs. Experimental results on several datasets show that the proposed CafeNet outperforms many other width search algorithms. The searched network experimentally achieves high performance with tiny FLOPs budgets.\n\nWhat I like about this paper in that: \n1.\tThe motivation and intuition are reasonable, which is to design a more flexible weight sharing pattern for network width search. \n2.\tExperiments are sufficient, thorough and carefully designed. Experimental results can support the objective of proposed methods. The searched network achieves remarkable performance with tiny FLOPs budgets.\n3.\tThe paper is well written and organized. The work is easy to follow and be reproduced.\n4.\tThe proposed methods have high generality and might be used on any convolutional network.\n\nSome minor concerns or suggestion about this paper:\n1.\tThe searching and training algorithms (max-max selection and min-min optimization) should be described in more detail.\n2.\tThe free channels are the neighborhood of the c-th channel in this paper, but I think more channels on the right should be included in the zone.",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ARfspQ3zWtj",
                "reply_to": "iclr_2021_S0UdquAnr9k",
                "title": "Official Blind Review #2",
                "comment": "Most existing methods follow a manually \ufb01xed weight sharing pattern, leading to the difficulty that estimates the performance of networks with different widths. To address this issue, this paper proposes a locally free weight sharing strategy (CafeNet) to share weights more freely. Moreover, this paper further proposes FLOPs-sensitive bins to reduces the size of the search space. Specifically, this paper divides channels into several groups/bins that have the same FLOPs-sensitivity and searches for promising architectures based on the divided groups. Extensive experiments on several benchmark datasets demonstrate superiority over the considered methods. However, some important details regarding the proposed method are missing. My detailed comments are as follows.\n\nPositive points:\n1. Compared with the manually \ufb01xed weight sharing pattern, this paper proposes a locally free weight sharing strategy (CafeNet), which allows more freedom in the channel assignment of a sub-network.\n\n2. To reduce the size of the search space, this paper proposes to divide channels into several groups/bins (also called minimum searching unit) that have the same FLOPs-sensitivity.\n\n3. The experimental results on image classification and object detection tasks show that the proposed method outperforms the existing methods by a large margin.\n\nNegative points:\n1. When training the super network, why the authors optimize the sub-network with the smallest training loss? More explanations are required.\n\n2. Why the sensitivity of a layer should be calculated as Eqn. (7)? It would be better to provide more details about that.\n\n3. Given a FLOPs constraint in Eq. (2), how to select a suitable width for each layer? Please discuss more and make it clearer.\n\n4. Is it possible to find a sub-network with zero width ($c=0$) for a layer? If so, how to deal with this case when evaluating the sub-network?\n\n5. The experimental results are inconsistent with the descriptions. In Figure 2(b), the performance of the proposed method goes worse with the decreasing of the $\\lambda$. However, the authors state that \u201cMobileNetV2 on CIFAR-10 improves 0.92% accuracy from $\\lambda$ =0 to $\\lambda$=1\u201d.\n\n6. The experimental comparisons in Table 1 are unfair. Compared with other methods (e.g. AutoSlim), the proposed method trains the models on ImageNet for more epochs (100 v.s. 300). More experiments under the same settings are required.\n\nMinor issues:\n1. In appendix A.13, \u201c\u2026 and the bin evolving speed \u03b1 in Section 3.4\u201d should be \u201c\u2026 and the bin evolving speed \u03b1 in Section 3.3\u201d.",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "km2Sy1SBvZw",
                "reply_to": "iclr_2021_S0UdquAnr9k",
                "title": "An interesting idea with convincing results",
                "comment": "This paper explores the weight sharing schema in one-shot width search and proposes a locally free weight sharing strategy (CafeNet). By splitting each width candidate into base channels and free channels, CafeNet makes a compromise between fixed weight pattern and full freedom pattern. Such strategy can reduce the search complexity and improve the performance ranking, w.r.t. different width, in the supernet. Experiments on various tasks, including classification, detection and attribute recognition, are well provided to support the effectiveness of the proposed method. The final results are quite promising.\n\nStrengths:\n1) The paper is well written and easy to follow. The motivation is clearly explained by an example and the problem formulation.\n2) The idea of locally free weight sharing is interesting. Such a solution for the previous fixed weight sharing seems sound.\n3) Experiments with additional analyses are well provided.\n\nI have the following concerns and suggestions:\n1) Missing some relevant papers. OFA[1] and TF-NAS[2] introduce width search by dynamically choosing the channels. The authors should cite and explain the differences.\n2) Is there any correlation between the degree in Eq. (4) and the searched accuracy under a fixed FLOPs?\n3) The bin size of Eq. (9) makes me confusing. As shown in experiments, \u03b2 can be less than 1 and the second term in the right side of Eq. (9) is also less than 1. Thus, the bin size bi (i.e., number of channels in a bin) is less than 1 channel. Please explain it in detail.\n4) Why lambda=0 achieves the best accuracy in Fig. 2(b)? It is in conflict with the statement \u201cAs shown in Fig. 2(b), our 0.5-FLOPs MobileNetV2 on CIFAR-10 improves 0.92% accuracy from lambda=0 to lambda=1\u201d.\n5) I suggest the authors to split 1G group in Table 1, as done in Table 11.\n6) In Algorithm1, both the supernet and the total number of epoch are defined as N.\n\nAlthough some details make me a little confusing, the experimental analyses and the intuitive solutions of locally free weight sharing in one-shot width search are quite informative and helpful to the NAS community. I suggests the authors to release their code and would like to see the authors\u2019 responses. \n\n\n[1] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han. Once-for-All: Train One Network and Specialize it for Efficient Deployment. ICLR, 2020.\n\n[2] Yibo Hu, Xiang Wu, Ran He. TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search. ECCV, 2020.\n",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rRx_eGlyhXN",
                "reply_to": "iclr_2021_S0UdquAnr9k",
                "title": "Interesting Attempt on Network Width Search",
                "comment": "The authors introduce in this submission a locally-free weight sharing strategy for selecting effective network width. The rationale and intuition behind are well-grounded. Experiments on various datasets and pruning setups prove the validity. \n\nStrength:\n+ The approach is well motivated and makes sense. The problem studied here is also important and could be of interest to a large audience.\n+ Experiments are sufficient. The results are promising and well support the claim.\n+ FLOPs-sensitivity bin considers factors including feature size and kernel size and seems to be independent of the total channel number, which, without douts, brings values.\n\nWeakness:\n- The proposed approach seems to be a  compromise between completely free weight and fixed weight, right? As a result, it would be good if the authors could elaborate the relation between the two.\n- By utilizing the methods, my understanding is that the search space scales from O(N) all the way up to O(C_{2r+1}^{N}), no? This is a considerable amount of time required as compared to the single network width. Please provide some discussion along this line.\n- The influence of the super network should be detailed. Intuitively, higher degrees of freedom will lead to better results. The authors should provide more analysis along this line.\n- The writing can be enhanced. Please go over the manuscript and make sure all the grammar errors have been taken care of. \n\n\n\n\n\n\n\n\n",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper proposed locally free weight sharing strategy (CafeNet) for searching optimal network width",
                "Sentiment Expression": "proposed",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The proposal",
                "Sentiment Expression": "is a nice tradeoff",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The originality and significance",
                "Sentiment Expression": "are clearly above the bar",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The paper's relation to the general interests of deep learning research and its applicability",
                "Sentiment Expression": "deserves a spotlight presentation",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The clarity of paper",
                "Sentiment Expression": "can still be improved",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "iaqgio-pOv": {
        "paper_id": "iclr_2022_iaqgio-pOv",
        "paper_title": "Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners",
        "paper_abstract": "Post-hoc explanations for black box models have been studied extensively in classification and regression settings. However, explanations for models that output similarity between two inputs have received comparatively lesser attention. In this paper, we provide model agnostic local explanations for similarity learners applicable to tabular and text data. We first propose a method that provides feature attributions to explain the similarity between a pair of inputs as determined by a black box similarity learner. We then propose analogies as a new form of explanation in machine learning. Here the goal is to identify diverse analogous pairs of examples that share the same level of similarity as the input pair and provide insight into (latent) factors underlying the model's prediction. The selection of analogies can optionally leverage feature attributions, thus connecting the two forms of explanation while still maintaining complementarity. We prove that our analogy objective function is submodular, making the search for good-quality analogies efficient. We apply the proposed approaches to explain similarities between sentences as predicted by a state-of-the-art sentence encoder, and between patients in a healthcare utilization application. Efficacy is measured through quantitative evaluations, a careful user study, and examples of explanations.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper presents two novel approaches to provide explanations for the similarity between two samples based on 1) the importance measure of individual features and 2) some of the other pairs of examples used as analogies.  The proposed approach to explain similarity prediction is a relatively less explored area, which makes the problem addressed and the proposed method unique. However, reviewers expressed concerns about evaluation methods and there were some concerns about the design choices that were not well motivated. The major issue is, as pointed out by the majority of the reviewers, the evaluation methods. Given the paper, reviews, and responses of the authors and the reviewers, it appears that there is certainly room for improvement for more convincing evaluation methodologies to convince a cross-section of machine learning researchers that the proposed approach advances the field. Overall, this is a good paper, but appears to be borderline to marginally below the threshold for the acceptance.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "5lgTObKOx-k",
                "writer": "author",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Any more clarifications needed?",
                "comment": " Thanks to all reviewers for their critical reviews and also for engaging with us. We believe we have addressed most of your concerns. Please let us know if any more clarifications or explanations are required. Thank you.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DsgzDmXfRPV",
                "writer": "author",
                "reply_to": "Vzs9a1uxaxy",
                "title": "Thank you for raising your score and the interesting reference. More clarifications...",
                "comment": " 1) **Evaluating different explanation techniques:** We would like to stress that the methods used for quantitative evaluations are a *superset* of what we evaluated in the user study and not some arbitrary intersection as the reviewer initially thought (e.g. ProtoDash not present in quantitative evaluations, which it is as indicated by us). The reasons for including the additional techniques in the quantitative evaluations was to show that straightforward application of popular methods such as LIME and methods that are not obvious baselines because of the settings they are used in (search and query) such as JSLIME underperform in our setting even from a quantitative standpoint.  Not to mention that interpretation of these methods for our setup (i.e. explaining pairwise similarity for tabular and text data) is not straightforward. This is also the reason for not including them in the user study, where the most natural competitors are compared with. We believe this makes our findings generalizable.\n\n2) **Regarding Hummel et. al (2014):** First, thank you so much for sharing the interesting reference. We will refer to this in the final version. Although, the paper brings out the importance of analogies as explanations (which further motivates our work) it also states that analogies are *not sufficient* for a good explanation and additional aspects such as providing causal factors and integrating information from diverse sources is essential. This key observation in (Hummel et. al, 2014), we believe, further corroborates our results that feature based explanations using a powerful method such as FbFull could provide complementary information (viz. uncovering robust/causal factors) that leads to users performing reasonably well on them. We thus believe that our results do *not* violate the surmise made in the shared reference, but rather support it.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Vzs9a1uxaxy",
                "writer": "official_reviewer",
                "reply_to": "BOg0Uyf9rLc",
                "title": "Thank you",
                "comment": " * Evaluating different explanation techniques in human and systematic evaluation is not advisable. One can include fewer techniques in both studies but the conclusions should be generalizable. \n\n* See Hummel et. al (2014) with regards to my statement about analogies\n\nHummel, J. E., Licato, J., & Bringsjord, S. (2014). Analogy, explanation, and proof. Frontiers in human neuroscience, 8, 867.\n\nNote: I upgraded my score to 6 because of the inclusion of the ablation study and the analysis.  ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Nqv1cfm4OMs",
                "writer": "author",
                "reply_to": "i0x1QMIkYSI",
                "title": "Checking in...",
                "comment": " Thank you for being so responsive. We hope the above clarifications address your concerns. We would be happy to answer more questions in case you have any. Thanks again.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZCDYNZBlUZ3",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "",
                "comment": "The paper proposes a new technique for explaining models that predict the similarities of an input pair. The authors propose two forms of explanations for such models: feature and analogy-based. Feature-based explanations highlight the important features of a predicted similarity for an input pair. For the explained pair, analogy-based explanations provide a new input pair that has a similar relationship to one another. The proposed technique outperforms other similar techniques in human and functionally grounded empirical experiments.\n I have summarized the main review into the following pros and cons: \n\nPros: \n\n* The proposed technique is flexible as it can provide two forms of explanations: feature and analogy-based. Moreover, explanations in the form of analogies are intuitive for human users. \n* The study includes human and functionally grounded evaluation experiments to show the usefulness of the proposed explanation technique.\n\nCons:\n\n* Many important design choices behind the proposed method in sections 4.1 and 4.2 are not well motivated.\n* Some of the methods in functionally-grounded evaluation are not included in the human grounded evaluation experiments and vice versa. This makes it difficult to draw a general conclusion in favor of the proposed approach across both types of evaluation methods. \n Overall, I vote for rejecting the paper. Although the proposed technique performs well in both human and functionally grounded evaluation experiments, many important design choices are not well motivated. Overall, I believe that the study needs some further refinements before it can be accepted to ICLR 2022. \n\n\nI have divided my detailed feedback into two categories: \u201cmajor concerns\u201d and \u201cminor improvements\u201d. I am willing to improve my current score in case the authors can address points raised in the major concerns section.\n\nMajor Concern\n\n* What are the reasons that LIME and JSLIME are performing relatively similar in comparison to the proposed FBFull and FBDiag methods on MEPS dataset (Table 1)? Does that mean that the problem at hand can be solved with LIME and JSLIME formulation as well? If so, what are the benefits and limitations of the proposed explanation techniques in this paper? \n\n* How can the usefulness of the analogy-based explanations be argued for when the result of user studies show that users can get nearly similar accuracies using AbE or FBFull (Figure 3)? \n\n* Can authors provide explanations on the effect of each of five additive components in Equation 2? \n\n* What are the reasons for not performing the human and functionally grounded evaluations on the same set of techniques? In addition, how can this affect the generalized statements about which explanation techniques perform best across both evaluation experiments? (For example, LIME and JSLIME are missing in human studies in Figure 3 whereas PDash is missing in the functionally grounded evaluation in Table 1)\n\n* Why lambdas and alphas are not tuned per example and what is the effect of this on the fidelity of \u201clocal\u201d explanations (section 5.1 - AbE hyper-parameters)? \n\nMinor Improvement\n\n* Can authors provide a more detailed explanation for the problems that hinder the extension or use the work of [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021] for the problem at hand? \n\n* I see a potential problem in the additive definition of w_{x_i, y_i} (section 4.1). In the current definition, the loss cannot differentiate between these two cases:  perturbations x_i s are close to x and many y_i points are further away from y and vice versa. This can be problematic since removing and adding terms to the explained pair of instances changes the Mahalanobis distance asymmetrically (see Example 1-3 in Figure 2). Can authors confirm this and provide an analysis on the possible effect this can have on the quality of explanations?\n",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "BOg0Uyf9rLc",
                "writer": "author",
                "reply_to": "tujvNHHBhcZ",
                "title": "Thank you for responding. Hope you are feeling better. Further clarifications... [2/2]",
                "comment": " We see that using the full objective, we are able to obtain analogies that have all the three desired properties - high fidelity to black-box, meaningful analogousness, and sufficient diversity. However, as we turn off the black-box fidelity term, the chosen pairs seem to have no fidelity in terms of $\\delta_{BB}$ values, and this also qualitatively leads to choosing analogies that are quite dissimilar such as in the second pair, given that the input pair had high similarity (low $\\delta_{BB}(x,y)$). Without the analogy closeness term, the essential sense of analogousness in the input pair (people performing some activity) is lost in the second chosen pair. Finally without the diversity term, the second and third pairs chosen are the same, just with the order flipped. This example clearly demonstrates the usefulness of each term in the objective. We will include more such examples in the final paper.\n\n\n3) **Doing human evaluation of LIME analogous to how functional evaluation was done:** We do not agree that LIME could be used in the same manner in the human evaluation. The latter requires methods to actually produce coherent explanations (feature-based in the case of LIME). The problem with LIME, as we mentioned in point 4 above, is that there is no principled (i.e. non-controversial) way of doing so due to having two copies of the same features. On the other hand, we *are* able to include LIME in the functional evaluation because it only considers the outputs of LIME in assessing Generalized infidelity and Infidelity. Interpretation of the model is not evaluated there. \nWe thus do not agree that not including LIME in the user studies is a major flaw. In fact, we think that including these additional baselines in the functional evaluations (which we could have simply dropped) leads to a fairer positioning of our work.\n\n\n4) **Advantage over related work [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021]:** As mentioned above, all these works have been proposed for the image domain and require white-box access. The latter is not a minor hindrance for at least two reasons: i) In today's cloud-driven world, it is common for models to exist in different cloud platforms where explainability is provided as a service (Dhurandhar et. al, 2019). In such scenarios one does not have access to the model, other than being able to query it. ii) More importantly, requiring white-box access imposes certain constraints on the type of models that can be explained. For example, the cited methods need the model to be differentiable (to perform backpropagation), which restricts their usage to models such as neural networks. However, in real industrial applications, models are many times ensembles which may be a combination of business rules, trees, and neural networks making the overall model non-differentiable and hence these methods will not apply. However, our methods will. In fact, even with simpler ensembles such as random forests or boosted trees, one cannot use the cited methods, but our methods could readily be applied. We will clarify these points in the final version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tujvNHHBhcZ",
                "writer": "author",
                "reply_to": "tIEMF_46IMY",
                "title": "Thank you for responding. Hope you are feeling better. Further clarifications... [1/2]",
                "comment": " 1) **Analogies vs feature based explanations:** First, it would be great if you could provide a reference for the hypothesis you mentioned regarding analogies being preferred. Thanks in advance. With that said, we do not think there is a problem with such a hypothesis being violated in some cases, nor is there a problem in letting users gravitate toward different techniques based on their preference. Since explanations are ultimately consumed by human users who have different backgrounds and different requirements, one cannot expect a single type of explanation to best satisfy everyone.\nRegarding our user study specifically, as indicated in Hullermeier (MDAI 2020), analogies could be viewed as one more level of indirection over feature based explanations as the latter are a function of the feature embedding they utilize. In our user study, FbFull and AbE being close indicates that for the problem we studied, features, which essentially were words in the sentence pairs, had a critical role in determining a pair's similarity. However, certain latent factors (viz. context) also played a role which possibly led to the slightly improved performance of AbE. This type of setup seems natural, where an interpretable representation (viz. bag of words) for a feature based explanation method will carry a reasonable amount of information about the similarity but is not complete. The analogies might capture this additional information and it is still up to the user to be able to exploit this latent information. This may not be possible for or suited to all individuals. Our results (and comments in Appendix N) in the user study corroborate this argument, where some users were able to utilize this latent information, while others were not.\n\n2) **Ablations:** We performed ablations by removing each of the three terms in eq. 2 while obtaining analogous pairs. We report the results for one representative example here.\n\n*Original input pair:*\n(a) A group of men play soccer on the beach.\n(b): A group of boys are playing soccer on the beach.\nThe black-box distance, $\\delta_{BB}(x,y)$ for this input pair is $0.111$.\n\n*Analogies with the full objective:*\n1. (a) Two people in snowsuits laying in the snow making snow angels. (b) Two children lying in the snow making snow angels. $\\delta_{BB}(x, y) = 0.104$.\n2. (a) A sad man is jumping over a small stream to meet his companion on the other side. (b) A man is jumping over a stream to meet his companion on the other side. $\\delta_{BB}(x, y) = 0.114$.\n3. (a) A woman puts flour on a piece of meat. (b) A woman is putting flour onto some meat. $\\delta_{BB}(x, y) = 0.133$.\n\n*Analogies without the black-box fidelity term (1st term in eq. 2):*\n1. (a) A woman is bungee jumping. (b) A girl is bungee jumping. $\\delta_{BB}(x, y) = 0.045$.\n2. (a) The man is aiming a gun. (b) A boy is playing on a toy phone. $\\delta_{BB}(x, y) = 0.726$.\n3. (a) The religious people are enjoying the outdoors. (b) The group of people are enjoying the outdoors. $\\delta_{BB}(x, y) = 0.291$.\n\n*Analogies without the analogy closeness term (2nd term in eq. 2):*\n1. (a) A woman paints a picture of a large building which can be seen in the background. (b) A person paints a picture of a large building which can be seen in the background. $\\delta_{BB}(x, y) = 0.011$.\n2. (a) The company claims it's the largest single Apple VAR Xserve sale to date. (b) The company claimed it is the largest sale of Xserves by an Apple retailer. $\\delta_{BB}(x, y) = 0.363$.\n3. (a) A boy is at school taking a test. (b) The boy is taking a test at school. $\\delta_{BB}(x, y) = 0.111$.\n\n*Analogies without the diversity term (3rd term in eq. 2):*\n1. (a) Two people in snowsuits laying in the snow making snow angels. (b) Two children lying in the snow making snow angels. $\\delta_{BB}(x, y) = 0.104$.\n2. (a) A sad man is jumping over a small stream to meet his companion on the other side. (b) A man is jumping over a stream to meet his companion on the other side. $\\delta_{BB}(x, y) = 0.114$.\n3. (a) A man is jumping over a stream to meet his companion on the other side. (b) A sad man is jumping over a small stream to meet his companion on the other side. $\\delta_{BB}(x, y) = 0.114$.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tIEMF_46IMY",
                "writer": "official_reviewer",
                "reply_to": "BNWxWhH5SxE",
                "title": "Thank you for your thorough feedback",
                "comment": " I am sorry for the late reply. Unfortunately, I caught Covid and I was not in good health to engage with you until today.  \n\nI am very happy that the authors engaged with my review in the rebuttal phase. \n\n#2. One possible hypothesis in the literature is that users find analogies much more explainable because that is how humans provide explanations. Your study somewhat violates that hypothesis without a proper explanation. In addition, the result of choosing analogy or feature-based techniques lead to two completely different understanding of the explained scenario. It is not really convincing to me that you propose two techniques in the same paper and conclude that users can pick each one based on their own preference. \n\n#3.  It is difficult to see what terms in Equation 2 contribute to what exactly. There are many ways to tackle this, e.g. ablation studies where one removes one term at a time and sees how the quality of explanations change. Based on this, you can conclude if the added term is effective or not. I highly recommend you to perform such a study.\n\n#4. Am I right to believe that you still could use the same assumptions you had for LIME in the functional evaluations and include LIME in the human evaluation? I think this is a major flaw in your paper that different techniques are compared across the human and systematic evaluations. \n\n#6 It would have helped if you made us understand the challenges of using LIME or extending other cited related work for the problem in more details. Even after reading your paper as a reviewer, I still have a hard time positioning your work relative to other cited related work.  Remember that as a user of the explanation, I might not think having access to the trained model is a hinder if a technique from your cited related work can work well for the problem at hand. So in that case, should the user still prefer your approach? If so, why. \n\nFor me #",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "twQlSowjLDC",
                "writer": "author",
                "reply_to": "sT3nZIKjPzr",
                "title": "UBTb: Thank you for suggesting alternate designs, however our user study design is still valid...",
                "comment": " Thank you for engaging with us deeply and providing your thoughts.\n\n**Consensus/majorities have little effect by themselves:**\nFirst, for the cases where the three example-based methods (AbE, ProtoDash, DirSim) return a consensus (all three example pairs have the same label), participants agree with the consensus only 43.2\\% of the time. However, users agreed with AbE when there was consensus 71.2\\% of the times. There were 6 consensus questions overall and 2 for AbE out of the overall 30 questions. Since the methods are not known to participants, this indicates AbE explanations made more sense to the participants even in the case of label consensus. And as a reminder from our original response, if a participant simply accepted the majority label (at least 2 out of 3), their accuracy would be 40\\%, which is significantly less than not only AbE's performance (> 80 \\%) but also those of the other methods. We take these to be strong evidence that the participants were not overly swayed by consensus or majorities in the returned examples, and that they indeed used their judgement guided by the explanations, which was the goal of providing AbE explanations.\n\nThe SHAP example provided by the reviewer does not apply here since it is a *theoretical property* of SHAP explanations that summing up the attributions along with the baseline will result in the black-box model prediction. Our AbE explanations do not have such any such guarantee, both theoretically as well as when seen empirically.\n\n**Purpose of analogies:**\nWe feel that analogous examples do not need to share common words, content, or sentence structure. What is important is that they *point to latent factors* that may be responsible for the model's output. This is where analogy-based and exemplar-based explanations are different from other types of explanations. With this in mind, the examples that the reviewer provided for single-instance sentiment classification *could be good* analogies, since positive words (like \"best\" vs. \"impresses\") are present in both sentences and could have been picked up by the model (i.e. positive words are latent factors here). Moreover, while the reviewer may not see the value in such analogies, the user study participant comments in Appendix N suggest that many others do.\n\n> This knowledge is a characterization of the model prediction (i.e. for all sentence pairs with this same-type-ness present, does the model always produce a high similarity prediction?), and thus can be quantitively evaluated from only the input and output pairs of the model.\n\nWe disagree that this can be evaluated objectively as pairs that have the same-type-ness present are not annotated and it is then up to the subjective assessment of the user. More importantly, we are explaining a black-box model and stress that the black-box does not have to match human judgement, for example of same-type-ness, and hence a good explanation does not either. The potential mismatch is also why participants were given analogous pairs together with their predictions in asking them to guess what the model's prediction would be for an input pair. We believe that the provided analogies can help users subtly understand what the model \"thinks\" about the input pair locally.\n\nOverall, we agree that there could be other experimental designs which are worthy of investigation. However, as we have argued, our current design is meaningful for the stated goal (revealing latent factors that are responsible for the model prediction) and does *not* suffer from major loopholes that the reviewer indicated.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sT3nZIKjPzr",
                "writer": "official_reviewer",
                "reply_to": "Fk_pJTB7CLm",
                "title": "Response to authors",
                "comment": " I appreciate the authors for their detailed response. However, I am not convinced on the technical quality of the paper, for the reasons below. \n\nI think there is a misunderstanding by the authors on my point of the participant being convinced in the situation that all AbE instances to have the same label, which is also the same as the pair in the question. The authors mentioned that the majority vote accuracy is 40%, but presumably a concensus is not very frequent. What is the probability that the participants agree with the AbE instance labels when there is a concensus among them? In general, I still believe that giving the model explanation when asking the participant to predict the model output is fundamentally flawed, since a trivial post-hoc explainer can trivially produce an explanation that maximally correlate the prediction, but does not really reveal how the prediction is produced. Theoretical reasons are argued in the student-teacher distillation paper that referenced in the original review. As a \"natural\" and practical example, the SHAP explanation is defined to add up to a known baseline (typically 0.5 for binary classification). So the participant can deduce the model prediction by summing up all the attribution values, along with the baseline. \n\nI understand that the distance similarity is a factor in the explanation selection. However, without clear semantic relations, I am not sure how useful that would be. The equivalence for single instance classification (say sentiment classification from movie reviews) would be like this: in order to explain why the model makes a positive prediction on the sentence \"This movie is the best I have seen in many years\", the explanation being produced is another positively predicted sentence \"Director Smith impresses his audience for another time with his innovation\". Yes, the predictions are the same, but what else? They have different sentence structure, no common words, and are content-wise very different. So it is not clear how this explanation could be related to the sentence that it is intended to explain. Indeed, I find it hard to identify any semantic relations for some of the examples that the authors provided. \n\n> While it would be ideal if we could know with certainty whether the model recognizes concepts such as same-type-ness, we respectfully think that the reviewer's ask is somewhat unfair, given the state-of-the-art in the area and the complexity of the black-box sentence encoder in question. \n\n\"Know(ing) this with certainty\" does not require understanding the black-box complexity of the model. This knowledge is a characterization of the model prediction (i.e. for all sentence pairs with this same-type-ness present, does the model always produce a high similarity prediction?), and thus can be quantitively evaluated from only the input and output pairs of the model. As another way to evaluate this, a user study could be set up with a two-person cooperative game, where the first person receives raw model explanation, summarizes some high-level findings (such as the same-type-ness) and passes them to the second person, and the second person tries to simulate/predict the model prediction on a different (i.e. held-out) set of explanations. If the second person can achieve high performance, then that means the high-level findings, and thus the low-level explanations, are useful. Otherwise, it would be hard to argue for their usefulness in helping people to understand model. Note that if there is only one person but the evaluation is done on the held-out data, then this setting corresponds to the teacher-student evaluation setting referenced above. \n\nLast, as another way of automating such experiment, a ground-truth based experiment could be carried out, as also mentioned in the original review. Specifically, the authors could define some similarity rules; for example, similarity is correlated with two factors: the number of common words or the length difference of the sentence. Then if the model performs really well, then it must have picked up both cues. Then we can see if the extracted AbE instances share the same \"ground truth factor\" as the instance to be explained. For more details I would refer to the papers referenced in the original review. \n\nOverall, I believe that the experiments need to be made more \"bullet-proof\" and convincing in order to truly establish the usefulness of the proposed model. As such, I would maintain my assessment. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i0x1QMIkYSI",
                "writer": "author",
                "reply_to": "M5HpmdhBbfo",
                "title": "Further evidence of trustworthiness of user study results...",
                "comment": " In addition to the justification provided in bullet 3 above, to further verify our findings we evaluated what the accuracies would be considering *only* the questions in the survey where an input pair is seen by the subject for the first time (i.e. we ignored 2$^\\text{nd}$ and 3$^\\text{rd}$ repetition). In this case the number of questions per subject reduces to 10 (as there are 10 distinct input pairs), but there is for sure no risk of multiple interference (viz. order bias etc.) as every input pair has only a single explanation that the user has seen. We found the resultant (percentage) accuracies to be as follows: AbE-> 82.3, DirSim-> 55.2, Pdash-> 54.9, FbFull-> 77.8, FbDiag-> 56.1. These numbers are very similar to those seen in Figure 3 (left), where the advantage of our methods (AbE and FbFull) is maintained. This we believe further corroborates the fact that the results in Figure 3 based on our user study can be trusted.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "M5HpmdhBbfo",
                "writer": "author",
                "reply_to": "17vX_KiBbK",
                "title": "Thank you for your response. Further clarifications...",
                "comment": " **1. Equivalence of diagonal A with running LIME using the vector (x-y)**\n\nTo make LIME equivalent to diagonal $A$ (using our FbDiag method), the vector would have to be $(x - y)^2$, not $(x - y)$. More importantly, our diagonal $A$ is constrained to be non-negative so that the quadratic form is positive semidefinite, whereas LIME does not have this constraint. \n\n**2. Hyperparameter search**\n\nWe believe that preserving fidelity while ensuring the intuitiveness of analogies is a good scheme since ultimately we want the users to be able to consume these. Also by setting these hyperparameters only once per dataset, we ensure we do not overfit to any one input pair with the human effort being not unreasonable.\n\n**3. Current user study design**\n\nThanks for your comments. However, we would like to point out that our study also follows a standard experimental design, which can be trusted, and which makes more efficient use of the number of subjects.\n\nOur current design for the user study follows an \"alternating treatment design\" (https://www.sciencedirect.com/topics/psychology/alternating-treatments-design), where the treatments are alternated randomly even within a single subject. In our case, the treatments correspond to the different explanation methods, and the subjects correspond to the $41$ individuals who participated in the study. While the randomized treatment assignment that the reviewer pointed out is standard, in the presence of five different treatments, each treatment would be limited to $\\approx 8$ subjects, which limits statistical power. As we wrote in Sec. 5.2, Setup subsection, we wished to focus on people with certain backgrounds and so our number of subjects was smaller.  By using an alternating treatment design, we are able to make more efficient use of the number of subjects to understand the relative benefits of the explanation methods. Note that such designs are common in psychology (Barlow & Hayes, 1979). There is indeed a risk of \"multiple interference\" (viz. order effect/bias which probably is the reviewer's main concern) in these designs, but they can be mitigated by *randomizing* the order of the treatments as we have done. Given that our results clearly point to the superiority of our proposed explanation methods (i.e., the effect is large), we believe that these findings are thus generalizable and valid.\n\nFurthermore, well-known online survey platforms such as SurveyMonkey and QuestionPro also suggest randomization as a way to mitigate order bias (https://www.surveymonkey.com/curiosity/eliminate-order-bias-to-improve-your-survey-responses,  https://www.questionpro.com/blog/eliminate-order-bias-in-surveys-with-question-randomization/).\n\n- Barlow, D. H., & Hayes, S. C. (1979). Alternating treatments design: One strategy for comparing the effects of two treatments in a single subject. Journal of Applied Behavior Analysis, 12(2), 199-210.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "17vX_KiBbK",
                "writer": "official_reviewer",
                "reply_to": "gba_cupxoum",
                "title": "Response to author ",
                "comment": " Thank you for your detailed response.\n\n1. I understand. But then if we run LIME with the vector (x-y), then LIME and having A diagonal would be equivalent or no?\n\n2. Thank you for the clarification about the hyperparameter search procedure. I think with 3 hyperparameters, it is quite hard to manually search over them especially without a clear objective. \n\n5. I don't believe randomizing the order is sufficient. It will remove the bias, but now the quantity we are evaluating is different. With the current study design, I don't think they can be trusted.\n\nIt is hard for me to increase my score given that the only valid systematic evaluation is the fidelity score which is not a stand-alone metric to evaluate an interpretability method. \n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xpdig0VlWH0",
                "writer": "author",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Checking in ...",
                "comment": " Thanks again to the reviewers for their valuable comments. We would be happy to provide any further clarifications before the discussion phase ends. Looking forward...",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ezA4pc9VIL",
                "writer": "author",
                "reply_to": "-w2o4Ox4mXw",
                "title": "Response to  UBTb: Thank you for the references, but we believe our user study design is kosher",
                "comment": " 1) **User study/method gameable:** \n\nThank you for your comments and the references. We have carefully gone through them and we believe our setup is kosher. Regarding Trojan explanations, we do not believe that would have been possible in our case as it requires humans to be trained before doing the task so as to pick up on spurious associations to make a decision. In our study, we never provided the users with the true predictions of the input sentences that we tasked them to predict. Given this, there would presumably be no way in which they would be able to *cheat* the intended goal. In fact, similar setups have been used in multiple prior works where one is tasked to guess the prediction based on an explanation without training the user (Luss et al. 2021, Madaan et al. 2021, Wu et al. 2021).\n\nRegarding our method simply picking analogies that replicate the distance but not the semantics, yes it is possible, but that is why we conducted a user study wherein for users to guess the correct prediction of the input there would have to be some semantic connection in what we provided. Given that users were able to predict more accurately using our method, we believe that the analogies did have semantics that the users could exploit. In general, of course it is difficult to conclusively state that a method considers semantics and to outline the exact reasoning or thought process it respects, but that is precisely the reason user studies are conducted, where each user can reason for themselves based on the provided evidence. In aggregate, if the performance of explanations over such users is good, then the method should have some merit.\n\nRegarding your latest comment on showing the qualitative ranges (i.e. similar, somewhat similar, dissimilar) for the analogies: We understand your concern. We thus evaluated what would happen if a user simply picked the majority range amongst those provided (i.e. if two or more analogies are \"similar\" predict \"similar\" for the input pair). The resulting accuracy is 40\\%, which is significantly less than not only AbE's performance ($>80$\\%) but also those of the other methods. This strongly indicates that the users did reason about the explanations and that the effect of being swayed by the majority was minimal. Moreover, without providing the qualitative ranges corresponding to the analogies, there is no way for humans to calibrate their intuitive understanding to these ranges as they have to predict the black-box's output (mentioned in Section 5.2).\n\nLastly, note that our method has a tuning parameter $\\alpha$ in Eq. 3 which can control in a qualitative sense how much we want the analogies to respect certain feature attributions that may come out of a feature based explanation method or based on domain knowledge. This flexibility in design choice can further ensure that we return semantically meaningful analogies over and above the impact that *direction similarity* might have in this regard.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3eo6Dd8fA9",
                "writer": "author",
                "reply_to": "rS2uA3ypdgi",
                "title": "Response to  Un2j",
                "comment": " 1) **how can one compare doing LIME over the concatenated (x,z) to having A be diagonal?**\n\nGiven the two interpretable representations $\\bar{x}$ and $\\bar{y}$, LIME uses the concatenated vectors $(\\bar{x}, \\bar{y})$ as features to compute a linear explanation, whereas the proposed feature-based explanation with diagonal $A$ (FbDiag) is computed by taking the squared elementwise differences $(\\bar{x}_i- \\bar{y}_i)^2$ as features (See Section 4.1). They cannot be compared directly since the feature dimensions are not comparable ($(\\bar{x}, \\bar{y})$ has twice the feature dimension).\n\n2) **Optimization over $\\lambda_1$, $\\lambda_2$ is unclear, how can one systematically search over them to get intuitive analogies? Furthermore, why is $\\alpha$ set to 0 in all the experiments? What is the effect of  both quantitatively and qualitatively ?** \n\nFor setting $\\lambda_1$ and $\\lambda_2$, we first note that too high a value of $\\lambda_2$ may result in analogous pairs that do not have similarities close to the input. So we set it to a small value ($0.01$) in all cases and search around that range. Next, when we set $\\lambda_1$ we want to give somewhat equal priority to the first and second terms in equation 2. Hence, we search between $0.1$ and $1.0$. Again, we would like to have good fidelity between the input and the analogous pairs, and this guided our decision. Finally, we also consider how intuitive the analogies are for a randomly chosen set of inputs. At least for STS dataset, this consideration also guided our choice when setting these two hyperparameters. Such a human-in-the-loop process to tune explanations is also seen in prior works (Luss et al. 2021, Madaan et al. 2021, Wu et al. 2021).\n\n$\\alpha$ is set to $0$ because we wanted to evaluate independently the benefit of analogy-based explanations without any influence of feature-based explanations. Qualitatively, higher values of $\\alpha$ (along with high values of $\\lambda_1$) will mean that pairs with close predictions from feature-based explanations will be prioritized.\n\nOur feature-based explanations have high fidelity / low infidelity to the black-box (in-sample) (see Tables 2 and 3 rows named *Infidelity* and *Fidelity* respectively), so high values of $\\alpha$ could mean that the weights for first, second, and third terms in equation 2 are approximately $1+\\lambda_1 \\alpha$, $\\lambda_2$ and $\\lambda_3$, essentially providing higher weight to the black-box fidelity term.\n\n3) **Figure 2: why are the words on the x and y axis shuffled from their original order? Also this kind of visualization is a bit hard to parse, is there a better way to visualize the cross weights (off diagonal elements of A) ?** \n\nWe have provided updated Figures in Section O (Appendix) where we order the words in descending order of their contributions (top left to bottom right). We hope this representation is easier for the reviewer to parse, since you can focus more easily on the top left corner of each figure. We will replace the corresponding figure in the main paper with this one, if the reviewer is satisfied. We dabbled with other visualizations such as showing a list of univariate attributions followed by interaction terms, however, it got unwieldy fast and so we decided on the one we report now.\n\n4) **using google forms and non-paid participants raises questions on the effort:** \n\nThere are many published/established works that have performed unpaid user studies\n(Ramamurthy et al. 2020, Ribeiro et al. 2016, Lundberg et al. 2017, Kim et al. 2017). In fact, since the survey was done by folks willing to do it for free and with backgrounds in data science, engineering and business analytics, as mentioned in the paper (page 7, 3$^\\text{rd}$ paragraph), we expect to have received quality feedback (see Appendix N for the feedback), as opposed to people with unknown backgrounds taking the survey on platforms such as Amazon Turk where the main motive may be to earn the promised payment. An indication of the sincerity is the number of *optional* comments that people taking the survey decided to leave us (Appendix N).\n\n5) **showing participants the same example with different explanation methods:** \n\nThe design you proposed where different examples satisfying a condition are shown to the users is valid. However, our design was also valid since the order in which the explanations from the different methods appeared for different examples was randomized. This will break any systematic bias that could otherwise occur. For example, suppose A, B and C were three explanation methods used to show explanations (in that order) for a specific example. Then for a different example using the same methods, it is equally likely that the order could be any of the six possibilities such as B, C and A, or A, C and B, etc.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gba_cupxoum",
                "writer": "author",
                "reply_to": "3eo6Dd8fA9",
                "title": "Response to Un2j - continued",
                "comment": " 6) **no alignment between objective of user study (replicate BB model scores) and practical use cases:**\n\nThe setup of using explanations to guess the black-box (BB) model's prediction is an accepted procedure to evaluate explanations (Ramamurthy et al. 2020, Ribeiro et al. 2016, Luss et al. 2021, Dhurandhar et al. 2019). This tests *simulatability* of the model by a human (Lipton 2016, Doshi et al. 2017), where the model can be used for varied tasks. Incorporating a specific use case may bias the user study results towards that application, since some explanations may be innately more suited to an application than others. We thus adopted the accepted procedure to maintain generality.\n\n7) **why fidelity?** \n\nFidelity is one of the standard metrics (Ribeiro et al. 2016, Lunberg et al. 2017, Ramamurthy et al. 2020)\nused to evaluate quality of post hoc explanation models. The Black Box Invariance assumption (Sundarajan et al. 2017), which states that post hoc explanations should be the same if the black-box model behavior/outputs are the same, motivates the fidelity metric, as we would ideally want the post hoc explanation model to exactly mimic the behavior of the BB model.\n\n- Sundararajan, M., Taly, A. and Yan, Q., Axiomatic attribution for deep networks. ICML 2017.\n\n8) **implications of a low generalized infidelity score and a high score:** \n\nGeneralized infidelity (Table 1) measures how applicable an explanation for an example is to its neighboring examples. Hence, low values of generalized infidelity imply higher applicability, meaning that the explanation is robust and stable enough to explain not just the example in question but also examples near it which is what we would also intuitively expect in practice. High generalized infidelity means, the explanation for an example is not applicable to its neighboring examples.\n\n9) **(Ramamurthy et al., 2020) also relies on comparing the feature importance weights, , is it possible to do something here that is similar?** \n\nRamamurthy et al., 2020 perform comparisons of the global feature importances obtained from different local post-hoc explanation methods of classification models with the feature importances directly outputted by a black-box model. They perform this analysis only for black-box models that can output the global feature importance scores directly (such as random forests). However, in this paper, none of the black-box similarity models can directly output feature importances. The black-box models used in our case are, a Siamese neural network for the Iris dataset, the distance between leaf embeddings of random forests for MEPS dataset (note that this is different from a simple random forest regressor that can output feature importances), and cosine distance between embeddings of text obtained using a universal sentence encoder for the STS dataset. None of these are able to output feature importances directly, and hence the analysis mentioned in Ramamurthy et al., 2020 is not applicable in our case.\n\n10) **Your comment below on gameability:** Regarding your latest comment in response to Reviewer UBTb. Thank you for realizing that our explanations are doing something non-trivial. Their concern about showing the qualitative ranges (i.e. similar, somewhat similar, dissimilar) for the analogies, we believe had minimal effect. We evaluated what would happen if a user simply picked the majority range amongst those provided (i.e. if two or more analogies are `similar` predict `similar` for the input pair). The resulting accuracy is 40\\%, which is significantly less than not only AbE's performance ($>80$\\%) but also those of the other methods. This strongly indicates that the users did reason about the explanations and that the effect of being swayed by the majority was minimal. Moreover, without providing the qualitative ranges corresponding to the analogies, there is no way for humans to calibrate their intuitive understanding to these ranges as they have to predict the black-box's output (mentioned in Section 5.2). We hope this relieves your concern about this aspect.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "m6eZBZpzI5e",
                "writer": "author",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "Thanks to reviewers. All clarifications included in responses and paper updated.",
                "comment": " We thank the reviewers for their diligent reviewing and comments. We have now provided a complete set of responses to their comments including those on user study. The latest version of our paper has changes updated in blue.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "48uwGO3g0k",
                "writer": "author",
                "reply_to": "ezA4pc9VIL",
                "title": "Continued response to UBTb",
                "comment": " 2) **Qualitative examples:**\n\n> For the qualitative explanation, and AbE in particular, none of the three examples in Sec. 5.1 are convincing, and they feel more like post-hoc over-explanation on the authors' part based on the analogy pairs produced.\n\n> [Example 1:] Yes, but does this show that the model is recognizing the same-type-ness of the entity (musical instrument or sport) when making the similarity prediction?\n\nWhile it would be ideal if we could know with certainty whether the model recognizes concepts such as same-type-ness, we respectfully think that the reviewer's ask is somewhat unfair, given the state-of-the-art in the area and the complexity of the *black-box* sentence encoder in question. For analogies in particular, the idea is to provide a human expert with example pairs satisfying certain mathematical properties in equation 2 (closeness in degree of similarity, within-pair relationship, diversity) and then allow the human to interpret the results and decide whether the example pairs are useful. As we wrote in the first paragraph of Section 4.2, AbE does *require human judgement.* Our discussion in Section 5.1 attempts to portray what a real user might observe. Exemplar-based explanations (Gurumoorthy et al. 2019, Kim et al. 2016) have the same requirement in the unsupervised and standard supervised learning (prediction) settings: the algorithm provides the user with mathematically similar exemplars that the user then interprets. Even for post hoc explanations that approximate the black-box model with a simpler model, we may understand the mechanism of the simpler model and can ensure that it is reasonably close to the black-box model in an input-output sense, but we cannot be certain that the mechanism of the black-box model is similar to what we understand.\n\nIn fact, \"Black-box Invariance\" is stated as a desirable property for a model agnostic black-box explanation method in seminal works on XAI (Sundararajan et al. 2017). The property states that the explanation should be completely determined by the input-output behavior of the model as no further information is available about the model in such settings. In other words, if two black-box models produce the same outputs for the corresponding inputs, a ``good'' explanation method will produce the same explanations. As one can see it is possible that the mechanisms of the two black-box models to arrive at the same output can be different, nevertheless in such a setting there is no way to distinguish the models. Our proposed approaches are consistent with this property.\n\n- Sundararajan, M., Taly, A. and Yan, Q., Axiomatic attribution for deep networks. ICML 2017.\n\n> This statement is not about explanation, but rather merely about model prediction.\n\nSince closeness in model output is one criterion in equation 2 (the first term), we feel that it is relevant to comment on it.\n\n> I could not understand what a \"dolphin scheme\" is. Is it a particular way for economic fraud/exploitation (like \"pyramid scheme\")? As a result, I could not understand the author-provided explanations for this example, and I do not think it is a good opening example for the same reason.\n\nTo the best of our knowledge a \"dolphin scheme\" is also a type of fraud (now mentioned in the introduction). Nevertheless, we do not think that it is crucial to understanding why the model believes that the two sentences are similar, as the function of the phrase is simply to provide more context in one of the sentences.\n\n3) **\"Both sentences express the same idea (second half faster than first half) but in different ways, similar to the input pair.\" -- This is a very loose assertion. In fact, if both sentences truly express the same idea, I would expect the similarity to be much higher...:**\n\nIt is important to note that we are explaining a black-box model whose behavior may not align in all cases with human intuition. Hence, in the example we are explaining why we think it is a good analogy given the black-box's predictions and are not justifying the black-box models behavior in absolute terms or relative to what a human might expect. Keeping this in mind the reason we have stated for why the analogy is similar to the input pair is in our opinion valid.\n\n4) **More appropriate citations:**\n\nThank you for suggesting Simonyan et al. (2013). We have now cited it at the indicated location and have used `citet` as appropriate.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "i25GBjKrMmD",
                "writer": "author",
                "reply_to": "kd-5M6neydC",
                "title": "Response to  F17R",
                "comment": " 1) **Extension to vision tasks:**\n\nCurrently we show results with both tabular and text data. However, this is an interesting idea for future work, which requires some modifications and extensions to our approach. This is because our feature based explanations approach requires computation of $\\bar{x}-\\bar{y}$, $\\bar{x}$ and $\\bar{y}$ are the interpretable representations of the original data $x$ and $y$. For images, this means that we have to set $\\bar{x} = x$ and $\\bar{y} = y$ (use the original pixel representation as the interpretable representation) or use some joint super-pixel segmentation, as having different superpixel segmentations for each image will not directly apply. In general, the current instantiation of the method is naturally suitable for data where all examples can be encoded using the same feature vocabulary.\n\n2) **MEPS analogies might be more relatable:**\n\nAppendix L discusses analogies from the MEPS dataset. We included more examples from the STS dataset as we thought it is better known and showing the applicability of our method for unstructured data was a stronger testament for it. If you believe that some of the examples in the appendix might bring out our message more strongly we would be happy to move them to the main paper.\n\n3) **It took me a while to understand Figure 1 when it is first mentioned. Consider using shorter samples or/and expanding the description in the introduction:** \n\nWe have now added a sentence in the introduction explaining why the analogy makes sense based on your suggestion.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WKIDZTnpGhv",
                "writer": "author",
                "reply_to": "BNWxWhH5SxE",
                "title": "Response to QDef - continued",
                "comment": " 7) **I see a potential problem in the additive definition of $w_{x_i, y_i}$ (section 4.1). In the current definition, the loss cannot differentiate between these two cases: perturbations $x_i$ s are close to x and many $y_i$ points are further away from y and vice versa:** \n\nIn the feature-based part of this work, our focus was on extending the idea of LIME to similarity functions, which led to our proposing a different proxy model, namely Mahalanobis distance. Weighting of neighbors for LIME-like methods is in general an open question and mostly orthogonal to our focus. We thus kept the method of weighing neighbors close to that of LIME so that we could more fairly compare our main innovation FBFull to other derivatives of LIME such as concatenated LIME and JSLIME. There is no obstacle to combining the $x$ and $y$ components of $w_{x_i, y_i}$ in a different manner if that improves fidelity in a specific application.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BNWxWhH5SxE",
                "writer": "author",
                "reply_to": "ZCDYNZBlUZ3",
                "title": "Response to  QDef",
                "comment": " 1) **What are the reasons that LIME and JSLIME are performing relatively similar in comparison to the proposed FBFull and FBDiag methods on MEPS dataset (Table 1)? what are the benefits and limitations of the proposed explanation techniques?**\n\nWe note that for MEPS, our FbDiag method performs better than LIME and JSLIME in Table 1. MEPS is of course only one dataset and for the other datasets, our methods (FbFull, FbDiag) have wider gaps with the baselines. The smaller gap for MEPS could be due to its features being largely categorical and sparse.\n\n2) **AbE and FBFull (Figure 3) have similar accuracies so why prefer analogies?** \n\nAlthough AbE achieves slightly higher accuracy than FBFull in the user study, the difference is small, and so our recommendation would be to use one or the other based on individual preference. Comments from the user study (see Appendix N) show that some people preferred AbE while others preferred FBFull. Also, since both AbE and FbFull are our contributions, we do not believe that the similar accuracies diminish the significance of either.\n\n3) **Can authors provide explanations on the effect of each of five additive components in Equation 2?**\n\nThe components in equation 2 are already discussed in the paragraph below it and below equations 3, 4. We would be happy to further explain any specific aspect of this that remains unclear to the reviewer.\n\n4) **What are the reasons for not performing the human and functionally grounded evaluations on the same set of techniques?**\n\nFirst, regarding PDash, quantitative results are provided in Figures 4 and 5.\n\nJSLIME: One reason why we did not include JSLIME in the user study is that it did not standout as a natural baseline in our setup as it was proposed primarily for images in the context where a query image is provided to a search engine in order to retrieve similar images (not pairs of inputs provided to a BB as in our case). Moreover, the work is contemporaneous (not published yet in a peer-reviewed venue) and we became aware of it only recently. As a consequence, their code is also not publicly available so we had to re-implement their method based on their description. We thus included this additional baseline for the quantitative evaluation as we thought the comparison would be informative to readers more in there.\n\nLIME: We included LIME (applied to the concatenation of $(\\bar{x}, \\bar{y})$) in the quantitative studies because we wanted to show the performance of this simplest adaptation of LIME to our setting, as a basic baseline. The problem with it is that there is no principled way of deriving the importance of a feature as there are two copies of each feature that may be assigned drastically different coefficients, possibly with the same sign. Merely summing the two coefficients does not seem like the right thing to do as the similarity may be governed by some function of their difference. FBDiag can be seen as a version of LIME that does not have this problem with interpretation, and it is included in the user study.\n\n5) **Why lambdas and alphas are not tuned per example and what is the effect of this on the fidelity of \u201clocal\u201d explanations (section 5.1 - AbE hyper-parameters)?**\n\nWe honestly do not think tuning $\\lambda$'s and $\\alpha$ per example is advisable. First, it would be computationally burdensome, introducing significant latency in the explanation of every example. Second, it may result in ``overfitting'' to the example in the sense that fidelity for the example is high but *generalized* fidelity for nearby examples is poor. This is why tuning was done only once per dataset as is also done in prior works (Luss et al. 2021, Madaan et al. 2021, Wu et al. 2021).\n\n6) **Can authors provide a more detailed explanation for the problems that hinder the extension or use the work of [Zheng et al., 2020; Plummer et al., 2020; Zhu et al., 2021] for the problem at hand?**\n\nZheng et al., 2020, Plummer et al., 2020 and Zhu et al., 2021 explain image similarity models, and require *white box* access to the internals of the model, e.g., gradients, activations, or feature embeddings. In contrast, our method is designed for black-box models, where all we need is a model that can take in two inputs and output a similarity/distance score. Our method is also geared towards tabular and text data. Hence the problem setup and requirements between our work and the works mentioned by the reviewer are entirely different. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZkK-jj65Lao",
                "writer": "author",
                "reply_to": "9m8sSlCtpmi",
                "title": "Response to  Lu2q",
                "comment": " Thanks to the reviewer for their comments. We have provided detailed responses below.\n\n1) **Applicability to Image Datasets:** experiments on them?\n\nCurrently we show results with both tabular and text data. However, this is an interesting idea for future work, which requires some modifications and extensions to our approach. This is because our feature based explanations approach requires computation of $\\bar{x}-\\bar{y}$, $\\bar{x}$ and $\\bar{y}$ are the interpretable representations of the original data $x$ and $y$. For images, this means that we have to set $\\bar{x} = x$ and $\\bar{y} = y$ (use the original pixel representation as the interpretable representation) or use some joint super-pixel segmentation, as having different superpixel segmentations for each image will not directly apply. In general, the current instantiation of the method is naturally suitable for data where all examples can be encoded using the same feature vocabulary.\n\n2) **Attributes used for explanations:**\n\nIn Section G (Appendix), we mention what the interpretable representations for the different datasets are: *The interpretable representation ($\\bar{x}, \\bar{y})$ is the same as the original features in Iris; for MEPS it involves dummy coding the categorical features, and with STS, we create a vectorized binary representation indicating just the presence or absence of words in the pair of sentences considered.*\n\n3) **Perturbations in the text domain:**\n\nIn  Section G (Appendix), we discuss how the perturbations are performed for the text data: *For STS, the perturbations were generated following the LIME codebase by randomly removing words from sentences.*\n\n4) **Compare results with the SHAP[3], LOO[4], RISE, and Occlusion-based method for input perturbation and U-CAM method for logit perturbation:** \n\nWe have compared our feature-based local explanation approaches (FbFull, FbDiag) with the most relevant competing methods adopting their perturbation schemes for a fair comparison. These are LIME and JSLIME as discussed in the paper. SHAP does not use any input perturbations. We also perform erasure of words when computing perturbations for text (one of the methods suggested in [4]) and the U-CAM method (which we think is Patro et al., U-CAM: Visual Explanation using Uncertainty based Class Activation Maps) does not seem to suggest any input perturbations that we can incorporate. In addition, our perturbation method incorporates random-masking-like strategy used in the RISE method (Petsiuk et al., RISE: Randomized Input Sampling for Explanation of Black-box Models). The reviewer has not provided any references to the Occlusion-based method, but we guess it also uses occlusion of portions of data to create perturbations, and we do this as well for perturbations with text data.\n\n5) **Difference between analogies and paraphrasing:**\n\nAnalogies for a pair of sentences are other pairs chosen from the same dataset. Paraphrasing is more about altering a given sentence so that it still implies the same thing. We choose our analogous pairs from a given dataset itself, whereas paraphrasing implies we need to actually modify the input sentence pair, which may result in data samples lying outside the dataset.\n\n6) **Similarity between inputs measured at word/phrase/sentence level?** \n\nFor the STS (text) dataset, the blackbox model uses sentence embeddings from the well-known [universal sentence encoder](https://tfhub.dev/google/universal-sentence-encoder/4). The explanation model uses a bag-of-words representation with $0$ indicating the absence of a word in a vocabulary and $1$ indicating its presence.\n\n7) **Provision of algorithm/pseudo code** \n\nWe have updated Section P (Appendix) of the paper with pseudo codes for the methods developed in this work.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gatLX34ylrY",
                "writer": "official_reviewer",
                "reply_to": "-w2o4Ox4mXw",
                "title": "reviewer discussion",
                "comment": " I agree with reviewer UBTb that it is very easy to game the user study by having the explanation leak the model prediction, but the approach is doing something non-trivial and subjective questions about how participants rate the explanations can reveal whether the study is gamed. \n\nHowever, the authors could have avoided this problem while still showing explanations at test time. What they could have done is split the experiment into two phases: one where they show explanations at test time to \"train\" the participants about the model and one without the explanations to test the participants.\n\nThe big issue though about the user study (in my review) is showing participants the same example with different explanation methods: as I understand, there are 10 test examples, and you show each participant the same 10 with 3 different random methods to explain. Thus the participant has access to 3 explanations when they get to third time they see a given example, this clearly biases the results. A correct methodology is to assign each participant to a condition ( an explanation baseline) and only show them 10 examples with explanations from that condition. Then you compare between conditions.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-w2o4Ox4mXw",
                "writer": "official_reviewer",
                "reply_to": "Fk_pJTB7CLm",
                "title": "Update on my review",
                "comment": " I just took a closer look at the user study form in the supplementary material. In Fig. 9(a) on Page 22, the issue with the human study is actually more severe than I was expecting: the model prediction on the analogous pairs (i.e. AbE) are given. In the particular case shown in this figure, even if the explanation is completely useless or unfathomable, the user would still very likely to select \"Similar\" simply because all the AbE pairs have the same prediction of \"similar\". \n\nThis shows how easy a completely non-informative explainer could game the system (again see https://arxiv.org/abs/2012.00893 and especially the related work section): just select three random pairs for which the model prediction is the same. It seems highly likely that the participants will be swayed to select the same label despite not being able to understand the explanation in any meaningful way. The essence to prevent such loophole, as the above paper argues, is that the explanation has to be *not* present when the user is asked \n to simulate the model's behavior. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9m8sSlCtpmi",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "",
                "comment": "The author addresses the problem of post hoc explanation for the black box model. In this paper, the author discusses the task of explanation for two inputs, and the model provides a similarity score as output. The author proposes a model agonistic local explanation method for tabular and structure data. In the proposed method, the author uses feature attributes to explain the similarity between two inputs. Finally, the author proposed an analogy-based explanation to select diverse analogous pairs of examples for the same similarity. The author claims that using the proposed method can explain state-of-the-art models in healthcare utilization applications. Strengths:\nThe main reason to accept this paper is empirical results, showing performance on the various methods. The author has done plenty of case studies to verify the explanation and with many examples of the proposed explanation method. \n\nWeaknesses: \nGeneralizability of the proposed method. The author shows results on language tasks.  Can it be applied to other tasks? If yes, the author should show results on vision tasks and shows and compare results with state-of-the-art methods.\n\nTo find better similar and contrasting examples in the vision domain, people use exemplar [1,2] theory to find supporting and opposing examples.\n\nWhat sort of feature attribute did the author consider for the explanation? Do you have a section discussing feature attributes?\n\n\u201cset of perturbations (x _i, y_ i ) in the neighborhood,\u201d what kinds of perturbation is used in the text domain?\n\nThe author should compare results with the SHAPE[3], LOO[4], RISE, and Occlusion-based method for input perturbation and U-CAM method for logit perturbation.\n\nHow is it(analogies) different from paraphrasing a sentence? The author could motivate the paper on why analogies help to improve the explanation.\n\n\nHow did the author measure similarity between two inputs (word or phrase or sentence level)? Do you have an analysis of this?\n\nHowever, the paper misses one of the core aspects of machine learning practice: readability and reproducibility of results. What are the various critical components in the proposed method? The author should provide an algorithm or pseudocode to reproduce the results missing in this paper.\n\n\nRef:\n1. J\u00e4kel, Frank, Bernhard Sch\u00f6lkopf, and Felix A. Wichmann. \"Generalization and similarity in exemplar models of categorization: Insights from machine learning.\" Psychonomic Bulletin & Review 15, no. 2 (2008): 256-271.\n\n2. Patro, Badri, and Vinay P. Namboodiri. \"Differential attention for visual question answering.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7680-7688. 2018.\n\n3. Lundberg, S. M.; and Lee, S.-I. 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems, 4765\u20134774.\n\n4. Li, J.; Monroe, W.; and Jurafsky, D. 2016. Understanding neural networks through representation erasure. arXivpreprint arXiv:1612.08220.\n\n I have worked in this field and published related to this work.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "rS2uA3ypdgi",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "",
                "comment": "Goal: provide local explanations for black box (BB) models that assign similarity scores to two input examples. \n\nApproach: Two explanations generated: 1) feature attribution and 2) similar pair of examples that serve as analogies. \n\nApproach for 1) is:\n\nApproximate the BB model on the instance as if it was a quadratic model of the pair of inputs and learn the weighting matrix A by sampling pairs of points around the input and solving the resulting SDP for the matrix A. The weights of A provide a way to assign value to each element of the inputs. \n\nApproach for 2 is:\n\nObtain K different pairs of examples that serve as analogies by solving a subset selection problem from the data that balances three terms:\n\n1) the analogy pair must have similar score to the query according to the BB model\n\n2) the analogy pair and query should have similar features highlighted (weighted by a HP $\\lambda_1$\n\n3) the K pairs should be diverse\n\nEvaluation: The authors show three different kind of evaluations\n\nQualitative examples: on text data from STS, the authors show three examples with the results of their method\n\nA user study on the STS dataset where participants have to predict if the BB predicted the sentences to be similar given the two kinds of explanations. They show that their method outperforms the baselines\n\nQuantitative results on 3 datasets: STS, UCI Iris and MEPS where authors show that their method outperforms LIME and other baselines in terms of a metric called \u201cgeneralized infidelity\u201d \n Strengths:\n\n- Novel formalization of objective function for finding analogies and feature attribution for BB similarity learners\n\n- Diverse evaluation of approach using both objective metrics and a user study\n\nWeaknesses/questions:\n\n- On objective 1: how can one compare doing LIME over the concatenated (x,z) to having A be diagonal?\n\n- On objective 2: Optimization over $\\lambda_1, \\lambda_2$ is unclear, how can one systematically search over them to get intuitive analogies? Furthermore, why is $\\alpha$ set to 0 in all the experiments? What is the effect of $\\alpha$ both quantitatively and qualitatively ?\n\n- Figure 2: why are the words on the x and y axis shuffled from their original order? Also this kind of visualization is a bit hard to parse, is there a better way to visualize the cross weights (off diagonal elements of A) ?\n\n- Issues with user study: 1) using google forms and non-paid participants raises questions on the effort each put into performing the user study.  2) showing participants the same example with different explanation methods: as I understand, there are 10 test examples, and you show each participant the same 10 with 3 different random methods to explain. Thus the participant has access to 3 explanations when they get to third time they see a given example, this clearly biases the results. A correct methodology is to assign each participant to a condition ( an explanation baseline) and only show them 10 examples with explanations from that condition. Then you compare between conditions. 3) no alignment between objective of user study (replicate BB model scores) and practical use cases: supposedly the explanations are to check if the BB is correct or not, why wasn\u2019t that the use case for the user study? I expect it\u2019s because humans are perfect at judging similarity, then it might have been more interesting to introduce an end task where judging similarity is used. \n\n -  I am not sure why is fidelity the \u201cmetric\u201d to compare things across for judging similarity.  Furthermore, it would have been helpful to understand the implications of a low generalized infidelity score and a high score. (Ramamurthy et al., 2020) also relies on comparing the feature importance weights, is it possible to do something here that is similar? \n The paper presents a novel approach for obtaining explanations from a black box measure. The method appears sound, however, the evaluation is lacking in certain aspects. The user study has some flaws and the quantitative experiments rely on a single metric. My recommendation is a borderline reject that can be improved if authors can better argue their evaluation approach.\n",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "kd-5M6neydC",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "",
                "comment": "This paper introduces a novel form of explanations for similarity-based models. The authors present two methods to generate explanations, called Feature-based similarity explanations and analogy-based similarity explanations, where the latter one is a novel type of explanations explicitly developed to explain similarity learners. However, the authors state that both can be used simultaneously, i.e. the latter can use the output of the first.\nThe authors conduct a user study as well as a quantitative evaluation of the proposed methods with a comparison to previous approaches. Since the proposed setting (analogies) is novel, previous approaches compared with needed adaptations to fit the setting.  The paper\u2019s proposed explanation form seems to be very interesting. After the problem and the explanations methods are well motivated and introduced, the authors first illustrate them on selected examples. This shows nicely the methods\u2019 purpose. However, the STS dataset's task and the selected examples do not well support the quality of the generated explanations and the benefit of analogies-based explanations. I could imagine that the MEP dataset would be more relatable.\n\nAfter this illustration, the methods are extensively evaluated with a designed user study and quantitative evaluations both taking (adapted) previous methods into account. The results demonstrate the purpose and the benefits of the proposed methods. Summarized, the approaches seem to be very interesting, especially since similarity learners became more and more popular in recent years, even beyond the text and tabular data domain. Applying, evaluating and extending these methods for e.g. the vision domain seems to be interesting.\nTherefore, I'm tending to accept the present work.\n\nMinor comments:\n\nIt took me a while to understand Figure 1 when it is first mentioned. Consider using shorter samples or/and expanding the description in the introduction. After reading section 5.1 it became more clear.\n I'm tending to accept this paper as it is well written and provides interesting and novel approaches to explain similarity-based methods.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "Fk_pJTB7CLm",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_iaqgio-pOv",
                "title": "",
                "comment": "This paper studies the problem of explanation for similarity prediction models. Given a pair of inputs (x1, x2), the model to be explained assigns a distance (or similarity) score. The task is then to explain the model prediction on individual inputs. Two methods are proposed. First, a feature-attribution style explanation is computed by learning a local distance approximation, similar to the LIME objective. Second, an analogy-based-explanation is used, in which a set of existing pairs of data are selected, and the pairs are encouraged to both be semantically diverse and share similar model reasoning process at the same time. In the experiments, qualitative and quantitative results are presented. Qualitative results are delivered mainly for the STS dataset. Quantitative results are provided via a user study demonstrating that the users can better predict the model prediction when given the proposed explanation compared to baselines, and an automatic evaluation showing that a global version of proposed method perform better than existing approaches on the fidelity metric.  ## Strengths: \n\nThis paper studies an under-studied problem of explanation for similarity models. Due to the particular natures of the similarity prediction task, methods that do not focus on interaction effects (i.e. pretty much all local explainers for the classification setting) understandably could not perform well. The authors solved this problem by the use of a learned local distance matrix, in which interaction effects are clearly shown. In addition, the proposed method of analogy-based-explanation seems novel. The explicit treatment of diversity sets it apart from other explanation methods that also use whole data point for explanation, such as counterfactuals. \n\n## Weaknesses: \n\nDespite the strengths, I do have serious concerns about the experimental evaluation, which fails to convince me of the quality of the explanation. \n\n### Qualitative analysis\n\nFor the qualitative explanation, and AbE in particular, none of the three examples in Sec. 5.1 are convincing, and they feel more like post-hoc over-explanation on the auhors' part based on the analogy pairs produced. As a concrete example, consider the (author-provided) explanation on the analogy-based explanation provided in example 1: \n\n\"The first analogy is very similar except that hackysack is a sport rather than a musical instrument.\" -- Yes, but does this show that the model is recognizing the same-type-ness of the entity (musical instrument or sport) when making the similarity prediction? \n\n\"The sentences in the second pair are more similar than the input pair as reflected in the corresponding BB distance.\" -- This statement is not about explanation, but rather merely about model prediction. \n\n\"The third analogy is less related (both sentences are about cricket player selection) with a larger BB distance.\" -- Again, this statement is only about model prediction. \n\nFor example 2, despite some Internet search, I could not understand what a \"dolphin scheme\" is. Is it a particular way for economic fraud/exploitation (like \"pyramid scheme\")? As a result, I could not understand the author-provided explanations for this example, and I do not think it is a good opening example for the same reason. \n\nFor example 3, \"Both sentences express the same idea (second half faster than first half) but in different ways, similar to the input pair.\" -- This is a very loose assertion. In fact, if both sentences truly express the same idea, I would expect the similarity to be much higher (i.e. distance much smaller), but this pair is actually the most dissimilar pair among the three. \n\n### Quantitative analysis\n\nI have serious concerns about the simulatability user study in the paper. In summary, this design is easily game-able. Since the users have access to the explanation at \"test time\", a simple AbE explanation for achieving such correct prediction would simply be to produce pairs with similar predicted distances, regardless of any similarity in the reasoning process. To make it even worse, if the users could be \"trained\" for a bit, a \"Trojan explanation\" could easily lead to very high user performance, without the users understanding the model at all. For more details about the \"game-ability\" of this approach and the \"Trojan explanation\" definition, see https://arxiv.org/abs/2012.00893 and https://arxiv.org/pdf/2006.01067.pdf. The authors are suggested to consult an earlier proposal for user study design https://arxiv.org/abs/2006.14779, which (in my opinion) avoids this loophole. \n\nIn addition, a synthetic task with known groundtruths could objectively evaluate various properties of the proposed methods, such as whether the highlighted words are indeed important, or whether the analogical pairs also use the same reasoning pattern. Some ideas are discussed in https://arxiv.org/abs/2104.14403 and https://arxiv.org/abs/2105.06506. \n\nMinor: \n\nThe authors could use \\citet in places such as \"Smith (2019) proposed\", rather than the current \"(Smith, 2019) proposed\". \n\nFor gradient-based explanation, the authors cite GradCAM, but I view it more as an adaptation of the original CAM to non-fully-convolutional architectures, and GradCAM are fundamentally about visualizing maximally activating input regions for certain convolution layers/filters. Instead, I would recommend the original Gradient saliency paper by Simonyan et al. (2013) or its SmoothGrad/IntegratedGradient successor.  Unfortunately, I do not believe that this paper meets the standard for publication. While I like the proposed theory, I am really unconvinced by the experimental evaluations. In fact, the qualitative AbE examples raise more questions than they answer, and make me doubtful that the method is really working as intended. A more careful experimental investigation, perhaps with some revision to the theoretical approach based on the problems found, would make this paper a much better one. ",
                "rating": 1,
                "confidence": 5
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "two novel approaches",
                "Sentiment Expression": "novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "evaluation methods and design choices",
                "Sentiment Expression": "concerns",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the evaluation methods",
                "Sentiment Expression": "major issue",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "evaluation methodologies",
                "Sentiment Expression": "room for improvement",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "paper",
                "Sentiment Expression": "good but appears to be borderline to marginally below the threshold for the acceptance",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "B1eWOJHKvB": {
        "paper_id": "iclr_2020_B1eWOJHKvB",
        "paper_title": "Kernel of CycleGAN as a principal homogeneous space",
        "paper_abstract": "Unpaired image-to-image translation has attracted significant interest due to the invention of CycleGAN, a method which utilizes a combination of adversarial and cycle consistency losses to avoid the need for paired data. It is known that the CycleGAN problem might admit multiple solutions, and our goal in this paper is to analyze the space of exact solutions and to give perturbation bounds for approximate solutions. We show theoretically that the exact solution space is invariant with respect to automorphisms of the underlying probability spaces, and, furthermore, that the group of automorphisms acts freely and transitively on the space of exact solutions. We examine the case of zero pure CycleGAN loss first in its generality, and, subsequently, expand our analysis to approximate solutions for extended CycleGAN loss where identity loss term is included. In order to demonstrate that these results are applicable, we show that under mild conditions nontrivial smooth automorphisms exist. Furthermore, we provide empirical evidence that neural networks can learn these automorphisms with unexpected and unwanted results. We conclude that finding optimal solutions to the CycleGAN loss does not necessarily lead to the envisioned result in image-to-image translation tasks and that underlying hidden symmetries can render the result useless.",
        "paper_acceptance": "accept-poster",
        "meta_review": "This paper theoretically studied one of the fundamental issue in CycleGAN (recently gained much attention for image-to-image translation). The authors analyze the space of exact and approximated solutions under automorphisms.\n\nReviewers mostly agree with theoretical value of the paper. Some concerns on practical values are also raised, e.g., limited or no-surprising experimental results. In overall, I think this is a boarderline paper. But, I am a bit toward acceptance as the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation.\n\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "H1e26UE6tr",
                "reply_to": "iclr_2020_B1eWOJHKvB",
                "title": "Official Blind Review #2",
                "comment": "I have read the rebuttal of the authors . Thank you for you answer and for addressing some concerns.  While the question addressed is important, the theory presented here does not seem to hint to a solution, hence I am keeping my score.    \n\n###\nSummary of the paper: \n\nThis paper shows that the cycle GAN loss suffers from the presence of lot of symmetries that make the existence of a unique solution not possible , and moreover adding a regularizer that uses the identity loss is not enough to make the problem less prone to those invariances. \n\nReview of the paper: \n\nThe notations and the formalism  in the paper are heavy and cumbersome and don't come with any surprising result, since the transforms between unpaired spaces will be found always up to   symmetries since we have the composition of one map with another. The use of the identity loss is also shown to not to help either in fixing this invariance issue. \n\nExperiments are not interesting since without any structure on the map of F and G , the source domain and the target domain, one is expected to get permutations.\n\nThe paper points in the conclusion  that the use of skipconnection in F and G has the major influence.\n\n The study of cycle GAN might need some assessment of what is the mutual information between the domains , as on what  information needs be preserved , and information needs to match , skip connection maintain the content in image generation as the information is kept from lower layer and its modified to target the style of the target images. \n\nAn information theoretic analysis of cycle gan is needed using for example the objective of \"MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation\". \nor by using a radically new approach  for cycle gan such as the Gromov Wasserstein distance as done in \" Learning Generative Models Across Incomparable Spaces\"\n",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rylknOkRFr",
                "reply_to": "iclr_2020_B1eWOJHKvB",
                "title": "Official Blind Review #1",
                "comment": "This paper focuses on CycleGAN method to show theoretically when the exact solution space is invariant with respect to authomorphisms of the underlying probability spaces for unpaired image-to-image translation. \n\n- The paper provides interesting theoretical results on identifying conditions under which CycleGAN admits nontrivial symmetries and has a natural structure of a principal homogeneous space.  Proposition 2.1 provides interesting insights into the invariance of the kernel space. \n\n- Propositions 2.5 and 2.6 are interesting in that they show that the existence of authomorphisms can worsen the performance of CycleGAN, however, it is unclear that in practice, how could one verify the conditions efficiently before applying CycleGAN.\n\n- The experimental results are interesting, however, they are very limited. Having a toy experiment is a good sanity check, but it would be more interesting to see the performance on a real-world applications, such as medical images or other use-cases brought in the introduction. Also, more discussion on the results provided in Fig 3, confusion matrices, would be very helpful. Are there any intuitions behind the large and low values in the table? It could be interesting to see what are the effects of other parameters such as alpha in producing the results. \n\n- Overall this paper presents interesting results regarding the theory of CycleGANs, however, the numerical results are very limited, and do not justify the motivations discussed in the introduction and the abstract. Moreover, although the paper introduces novel attempts and theoretically analyzing the CycleGAN, the scope of the work seems to be limited, and thus, it does not have a sufficient significance to be published in ICLR. I strongly suggest the authors to expand and provide more experimental evaluations.\n\n** update:\nThanks for your comments! I found the additional experiment useful and better aligned with the purpose of the model. The discussion added clarified the confusion about the automorphism. That is why I decided to change my score. ",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "B1l_X-GJcS",
                "reply_to": "iclr_2020_B1eWOJHKvB",
                "title": "Official Blind Review #3",
                "comment": "This is an interesting, timely study.  CycleGAN has attracted a lot of attention in unpaired image-to-image translation. Although the basic idea of CycleGAN seem sensible, its precise behavior is not totally clear--can one really avoid mismatch with CycleGAN? Do we need additional constraints? This paper provides a nice answer the the first question.\n\nOverall I enjoyed reading this paper. The addressed issue is important, the investigation is reasonable, and the results are intuitive and plausible, with clear practical implications. I think it is a good paper.\n\n I acknowledge I read the authors' response and other reviews and would like to keep my original rating.",
                "rating": 8,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HygDhZNnjH",
                "reply_to": "H1e26UE6tr",
                "title": "Thank you for your feedback!",
                "comment": "Thank you very much for your feedback and additional references, this is very interesting!\n\nWe opted to use the measure theory language because the language of probability distributions is not flexible enough to accomodate commonly occurring distributions, e.g. those (strictly) supported on lower dimensional manifolds. Unless we require the PDF explicitly to have some symmetries, it is not clear why the corresponding probability space would have any smooth automorphisms at all. Thus we show that the existence of automorphisms is a very general property, and in the setup of e.g. latent space with spherical Gaussian PDF we show that there are smooth automorphisms as well.\n\nWe would like to point out that the goal of the paper is to provide a well-grounded and mostly self-contained analysis for the basic CycleGAN approach and to analyze theoretically the effect of the commonly used identity loss, along with some experiments to justify the claims. While the problem of multiple solutions for the CycleGAN is commonly realised, a good theoretical explanation for this is lacking in ML literature. \n\nNaturally, there are other approaches to unsupervised image-to-image translation with different losses and architectures. While analyzing all of them in a single paper is not realistic, we think that the philosophy we suggest in this paper can help researchers better understand the potential and the limitations of these newer image-to-image translation models. The underlying automorphisms can always pose a problem, and the question then becomes if a new loss/new architecture explicitly restricts this set.\n\nWe have added some additional experiments on BRATS 2015 dataset. In this set of experiments we will show how the loss and PSNR (since we have a ground truth) change when we vary the weight for identity loss. We introduce a (approximate) probability automorphism in the form of left/right flips and show that this highly unwanted transformation still obtains low loss values unless an identity loss is used.\n\nWe have also added a discussion about the MISO paper you suggested, where we hypothesize that the MISO approach does not 'solve' the issue of unwanted automorphisms, but rather restricts the set of these automorphisms to those that leave the style of the image fixed. Therefore the amount of uncertainty in this solution is connected to the capacity of the style encoder, and should ideally be quantified. When some important style content is present - e.g., anatomical landmarks - it seems reasonable that one should make sure that the style encoder learns this information. We think it is a an interesting question for future work. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkelelV3sB",
                "reply_to": "rylknOkRFr",
                "title": "Thank you for your feedback!",
                "comment": "Thank you very much for your feedback!\n\nAs for the MNIST2MNIST task, we observed that adding identity loss here forces the network to preserve the original image quite easily. This is in line with our expectations since the domains are identical and the identity loss should trivially remove the ambiguity. The high values in the confusion matrix correspond to the digit class which is very definitive, and the smaller ones correspond to the cases when the digit class is somewhat ambiguous. It can be for instance digit \u20182\u2019 which is written a bit like \u20186\u2019 with a closed loop in the bottom, and it happens with digits '3', '5', '8' as well. \n\nFollowing your feedback, we have added some additional experiments on the BRATS 2015 dataset with medical images. In this set of experiments we show how the loss and PSNR (since we have a ground truth available) change when we vary the weight for identity loss, and we compare these values with flipped version of the image. We see that in the absence of identity loss the final CycleGAN loss is very similar for both original and flipped network output, while the PSNR drops significantly for the flipped version. Increasing the identity loss weight does not always result in improved performance in terms of PSNR.\n\nWe have  also added additional discussion of some newer image-to-image translation models from the 'automorphism point of view', and we hope that some of the questions we pose can be answered in future work.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1exJyVnoB",
                "reply_to": "B1l_X-GJcS",
                "title": "Thank you for your feedback!",
                "comment": "We kindly thank for your feedback! \n\nWe have added some additional experiments on BRATS 2015 dataset to expand the experimental section, and provided an additional discussion of some newer multimodal image-to-image translation models from the 'automorphism point of view'.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "The authors analyze the space of exact and approximated solutions under automorphisms",
                "Sentiment Expression": "analyze",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "theoretical value of the paper",
                "Sentiment Expression": "agree",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "concerns on practical values, e.g., limited or no-surprising experimental results",
                "Sentiment Expression": "raised",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "this is a boarderline paper",
                "Sentiment Expression": "think",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the theoretical contribution is solid, and potentially beneficial to many future works on unpaired image-to-image translation",
                "Sentiment Expression": "a bit toward acceptance",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "r1xMnCNYvB": {
        "paper_id": "iclr_2020_r1xMnCNYvB",
        "paper_title": "JAX MD: End-to-End Differentiable, Hardware Accelerated, Molecular Dynamics in Pure Python",
        "paper_abstract": "A large fraction of computational science involves simulating the dynamics of particles that interact via pairwise or many-body interactions. These simulations, called Molecular Dynamics (MD), span a vast range of subjects from physics and materials science to biochemistry and drug discovery. Most MD software involves significant use of handwritten derivatives and code reuse across C++, FORTRAN, and CUDA. This is reminiscent of the state of machine learning before automatic differentiation became popular. In this work we bring the substantial advances in software that have taken place in machine learning to MD with JAX, M.D. (JAX MD). JAX MD is an end-to-end differentiable MD package written entirely in Python that can be just-in-time compiled to CPU, GPU, or TPU. JAX MD allows researchers to iterate extremely quickly and lets researchers easily incorporate machine learning models into their workflows. Finally, since all of the simulation code is written in Python, researchers can have unprecedented flexibility in setting up experiments without having to edit any low-level C++ or CUDA code. In addition to making existing workloads easier, JAX MD allows researchers to take derivatives through whole-simulations as well as seamlessly incorporate neural networks into simulations. This paper explores the architecture of JAX MD and its capabilities through several vignettes. Code is available at github.com/jaxmd/jax-md along with an interactive Colab notebook.",
        "paper_acceptance": "reject",
        "meta_review": "The paper is about a software library that allows for relatively easy simulation of molecular dynamics. The library is based on JAX and draws heavily from its benefits.\n\nTo be honest, this is a difficult paper to evaluate for everyone involved in this discussion. The reason for this is that it is an unconventional paper (software) whose target application centered around molecular dynamics. While the package seems to be useful for this purpose (and some ML-related purposes), the paper does not expose which of the benefits come from JAX and which ones the authors added in JAX MD. It looks like that most of the benefits are built-in benefits in JAX. Furthermore, I am missing a detailed analysis of computation speed (the authors do mention this in the discussion below and in a sentence in the paper, but this insufficient). Currently, it seems that the package is relatively slow compared to existing alternatives. \n\nHere are some recommendations:\n1. It would be good if the authors focused more on ML-related problems in the paper, because this would also make sure that the package is not considered a specialized package that overfits to molecular dynamics.\n2. Please work out the contribution/delta of JAX MD compared to JAX.\n3. Provide a thorough analysis of the computation speed\n4. Make a better case, why JAX MD should be the go-to method for practitioners.\n\nOverall, I recommend rejection of this paper. A potential re-submission venue could be JMLR, which has an explicit software track.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SkxoFBTijr",
                "reply_to": "Hylmq1Ik9H",
                "title": "Reply",
                "comment": "Thank you for your careful review of our work and useful suggestions!\n\n> Description of the elements of the design of JAX which are useful here are presented, and appear distinct from other \n> AD libraries like Tensorflow or PyTorch, although the authors stop short of explicitly stating which functionality \n> would be more difficult/impossible to support with the possible alternatives (automatic vectorization of the \n> simulations seems like one?).\n\nWe agree with your assessment that automatic vectorization would probably be the largest pain point associated with implementing JAX MD in a different AD library. Indeed, automatic vectorization is deeply integrated with JAX MD since a number of quantities are defined per-particle pair and then vectorized across systems of particles. Having said this, we believe that a TensorFlow version is probably possible since TF2 now supports \u201cvectorize\u201d. Despite our name, we have contemplated building a TF backend in a similar manner to Pyro.\n\n> Limitations of the library and drawbacks of any design decisions (something must be traded at some point?) are not explicitly mentioned.  \n\nThis is a great question and we have added a discussion of this point to the text in section 4. The main tradeoff that we experience is that the primitives that XLA exposes are sometimes at odds with the most efficient primitives for a molecular dynamics simulation. This is particularly important for spatial partitioning where more complicated data structures are often used that are challenging to implement using XLA. We do have an implementation of a cell-list but it is complicated by the fact that shapes must be static in XLA. Our cell-list implementation uses a sort which scales like O(Nlog^2N) on GPU while standard cell-list implementations scale like O(N) when coded directly in CUDA. We have contemplated writing some custom code to circumvent these issues.\n\n> Despite mentioning numerous existing MD libraries, no performance comparison is drawn against any other.  \n\nThis is also a great question (and related to the above). JAX MD is certainly slower than production quality / custom CUDA MD systems. We benchmarked JAX MD against HOOMD Blue on a very standard physical system using a V100 GPU in each case. We found that JAX MD took ~3200 microseconds / step while HOOMD Blue took ~112 microseconds / step. Thus, JAX MD appears to be around 25x slower. We believe that this performance gap will narrow as we improve infrastructure on our end along with improvements to XLA (and possibly MLIR). Having said this, JAX MD is fast enough to do many kinds of research with and where appropriate, we believe that the improvements to research productivity are worth the reduction in performance. We have added a discussion of this point to the text.\n\n> Could also show some demonstration of running an experiment which has complexity on par with state of the art\n> research?  The bubble raft example is great for illustrative purposes, but it could be better to save some of that for a \n> tutorial and use space to exercise this library on a relevant problem and show performance there.\n\nGreat idea! We have added a short discussion to the text cooling a liquid to form a glass. This represents an experiment of similar complexity to research being published today. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1loEHaiiH",
                "reply_to": "SklDy3jycr",
                "title": "Reply",
                "comment": "Thank you for taking the time to review our work.\n\nWe would like to discuss the applicability of this work to the ML community. While it is true that JAX MD will be of use to Physicists, we note that there has been significant research among ML practitioners that would be aided by JAX MD. In particular, we note the following (non-exhaustive) list of papers [1-7] that were published recently in Machine Learning Conferences. In each case, these papers leverage physical simulation; however, they were hindered since the simulations used were not integrated with deep learning libraries. This is precisely the gap that JAX MD hopes to fill. We would like to draw particular attention to [1], \u201cLearning Protein Structure with a Differentiable Simulator\u201d that could have been implemented out-of-the-box using JAX MD and received an oral at ICLR last year. \n\nApart from this, we believe (though there has been less work in this direction so far) that physical systems are an ideal environment to explore meta-optimization since the inner-loop is much better understood than neural networks in deep learning.\n\n[1] Learning Protein Structure with a Differentiable Simulator\nIngraham et al.; ICLR 2019\n\n[2] Interaction Networks for Learning about Objects, Relations and Physics\nBattaglia et al.; NeurIPS 2016\n\n[3] Visual Interaction Networks: Learning a Physics Simulator from Video\nWatters et al.; NeurIPS 2017\n\n[4] SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\nSch\u00fctt et al.; NeurIPS 2017\n\n[5] Learning Invariant Representations of Molecules for Atomization Energy Prediction\nMontavon et al.; NeurIPS 2012\n\n[6] A Compositional Object-Based Approach to Learning Physical Dynamics\nChang et al.; ICLR 2017\n\n[7] End-to-End Differentiable Physics for Learning and Control\nBelbute-Peres et al.; NeurIPS 2018\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rkeCer6iiB",
                "reply_to": "Hkga6iGecB",
                "title": "Reply",
                "comment": "Thank you for taking the time to review our work and we appreciate your advice about the writing. We will fix the sentence you noted and generally make the writing more formal.\n\nWe would like to discuss the applicability of this work to the ML community. While it is true that JAX MD will be of use to Physicists, we note that there has been significant research among ML practitioners that would be aided by JAX MD. In particular, we note the following (non-exhaustive) list of papers [1-7] that were published recently in Machine Learning Conferences. In each case, these papers leverage physical simulation; however, they were hindred since the simulations used were not integrated with deep learning libraries. This is precisely the gap that JAX MD hopes to fill. We would like to draw particular attention to [1], \u201cLearning Protein Structure with a Differentiable Simulator\u201d that could have been implemented out-of-the-box using JAX MD and received an oral at ICLR last year. \n\nApart from this, we believe (though there has been less work in this direction so far) that physical systems are an ideal environment to explore meta-optimization since the inner-loop is much better understood than neural networks in deep learning.\n\n[1] Learning Protein Structure with a Differentiable Simulator\nIngraham et al.; ICLR 2019\n\n[2] Interaction Networks for Learning about Objects, Relations and Physics\nBattaglia et al.; NeurIPS 2016\n\n[3] Visual Interaction Networks: Learning a Physics Simulator from Video\nWatters et al.; NeurIPS 2017\n\n[4] SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\nSch\u00fctt et al.; NeurIPS 2017\n\n[5] Learning Invariant Representations of Molecules for Atomization Energy Prediction\nMontavon et al.; NeurIPS 2012\n\n[6] A Compositional Object-Based Approach to Learning Physical Dynamics\nChang et al.; ICLR 2017\n\n[7] End-to-End Differentiable Physics for Learning and Control\nBelbute-Peres et al.; NeurIPS 2018\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Hylmq1Ik9H",
                "reply_to": "iclr_2020_r1xMnCNYvB",
                "title": "Official Blind Review #3",
                "comment": "This paper announces a new software package for simulating molecular dynamics which includes close integration with a neural network / machine learning library--the first to do so.  Straightforward access to hardware acceleration (e.g. GPU) is provided for both the simulation and machine learning.\n\nI lean toward accepting this submission.  If it were only about simulation molecular dynamics using hardware accelerators, I would question the appropriateness of the venue, but because it is explicitly intended to support training and usage of learned potential functions, it seems suitable.  Still might be better placed in a physics/chemistry venue, as where most of the references come from and likely where users would, too. The application area is no doubt an important research technique.  The paper is clearly written, with enough specific examples to contrast previous pain points in this line of work against its smoother interface.  \n\nAll of these points are fine for a package-release/tutorial paper, but for a conference paper, might hope to see these addressed:\nDescription of the elements of the design of JAX which are useful here are presented, and appear distinct from other AD libraries like Tensorflow or PyTorch, although the authors stop short of explicitly stating which functionality would be more difficult/impossible to support with the possible alternatives (automatic vectorization of the simulations seems like one?).\nLimitations of the library and drawbacks of any design decisions (something must be traded at some point?) are not explicitly mentioned.  \nDespite mentioning numerous existing MD libraries, no performance comparison is drawn against any other.  \nCould also show some demonstration of running an experiment which has complexity on par with state of the art research?  The bubble raft example is great for illustrative purposes, but it could be better to save some of that for a tutorial and use space to exercise this library on a relevant problem and show performance there.\n\nI took a quick glance at the code on github; it is substantial but not huge, cleanly organized, and is well-documented.  ",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SklDy3jycr",
                "reply_to": "iclr_2020_r1xMnCNYvB",
                "title": "Official Blind Review #1",
                "comment": "The paper presents a python package, called JAX MD for simulating molecular dynamics (MD). JAX MD provides automatic derivations and allows to easily incorporate machine learning models in the MD workflow.\n\nThe paper is clearly written and seems technically correct. However, given that I am a specialist of neither package implementation nor physics, I can not really asses that all the details are correct/useful.\n\nFurthermore, even if this work will surely be of great use for the physics community, I am not not sure that the contribution of this paper is sufficient for ICLR. ",
                "rating": 3,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Hkga6iGecB",
                "reply_to": "iclr_2020_r1xMnCNYvB",
                "title": "Official Blind Review #2",
                "comment": "\n\nThis paper describes a general purpose differentiable molecular dynamics physics package, JAX MD. It shows several instances, where it simplifies the research process and enables new avenues of work. \n\nThe Github link is provided for reproducible research and future development. It should be encouraged.\n\nI am sure whether this paper fit the ICLR or not, or how deep learning community can benefit from it.\n\nThe writing does not feel academic enough sometime. For example,  \"Please let us know if there are features that you would find interesting. We are always seeking contributions!\" Please consider the rephrase it.",
                "rating": 3,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "does not expose which of the benefits come from JAX and which ones the authors added in JAX MD",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "most of the benefits",
                "Sentiment Expression": "are built-in benefits in JAX",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "a detailed analysis of computation speed",
                "Sentiment Expression": "missing",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the package",
                "Sentiment Expression": "is relatively slow compared to existing alternatives",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the authors focused more on ML-related problems in the paper",
                "Sentiment Expression": "would be good",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "recommend rejection",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "uFORMPcA_b": {
        "paper_id": "nips_2021_uFORMPcA_b",
        "paper_title": "Pipeline Combinators for Gradual AutoML",
        "paper_abstract": "Automated machine learning (AutoML) can make data scientists more productive.  But if machine learning is totally automated, that leaves no room for data scientists to apply their intuition.  Hence, data scientists often prefer not total but gradual automation, where they control certain choices and AutoML explores the rest.  Unfortunately, gradual AutoML is cumbersome with state-of-the-art tools, requiring large non-compositional code changes.  More concise compositional code can be achieved with combinators, a powerful concept from functional programming.  This paper introduces a small set of orthogonal combinators for composing machine-learning operators into pipelines.  It describes a translation scheme from pipelines and associated hyperparameter schemas to search spaces for AutoML optimizers.  On that foundation, this paper presents Lale, an open-source sklearn-compatible AutoML library, and evaluates it with a user study.\n",
        "paper_acceptance": "accept",
        "meta_review": "Overall there is not enough support from reviewers for me to recommend acceptance.\n\nReviewers agreed on some real strengths in the paper, including (1) that is well-written and well-organized, and (2) that it tackles an important problem.\n\nOn #1:\n* Reviewer G6BM: \"I really enjoyed reading this paper! The paper is extremely well-written and well-organized\"\n* Reviewer cjD3: \"The authors introduced their AutoML library in a sorted, logical way\"\n\nOn #2:\n* Reviewer G6BM: \"The paper considers a very important problem and provides a very satisfactory solution both in technical and practical point of view\"\n* Reviewer D3TC: \"AutoML is an important application, and if successful, can greatly reduce data scientists' effort\"\n* Reviewer H7JW: \"AutoML is a useful tool [...] and efficient autoML system is an interesting research topic\"\n\nBut the weight of opinion was that the paper (1) doesn't present a clear and significant scientific contribution, (2) shows limited empirical validation, and (3) doesn't concern a software package with large enough practical impact on the NeurIPS community.\n\nOn #1:\n* Reviewer cjD3: \"Furthermore, I don\u2019t see a clear path for scientific future work building on top of Lale\" and \"I'm not convinced that the new operators can express substantially more than the formats of already existing AutoML tools [...] If the expressiveness of the format would be larger than prior work, I would have expected that we need also specialized optimizers for this [...] I have the impression that Lale could be a convenient package for new AutoML users, but this alone does not justify a NeurIPS paper\"\n* Reviewer H7JW: \"be more clear about the scientific contribution from the very beginning of the paper, e.g., a more expressive formalization of the AutoML system.\"\n* Reviewer D3TC: \"The high-level scientific contribution is missing. The paper provides too many low level details without describing what are the main challenges to implement combinators.\" and \"It is not clear why the proposed method is contributing to AutoML\"\n\nOn #2:\n* Reviewer cjD3: \"the user studies only include 9 participants, making statements not very meaningful. Additionally, the survey might even be biased towards Lale\"\n* Reviewer H7JW: \"organize the presentation of the empirical study following the general scientific principles; and to recruit more participants if possible so that one could draw some statistically significant conclusions.\"\n\nOn #3:\n* Reviewer cjD3: \"Overall, I\u2019m not fully convinced that there will be many users of Lale at the end of the day.\" and \"Lale is not even close to the level of pytorch. It has 225 stars on github and was forked 52 times. If that would be the level of impact we expect from a NeurIPS software paper, we will have thousands of papers of those each year.\"\n\nReviewer G6BM disagreed on #1 and #3, and felt the paper does clear the bar, but offered this follow-up commentary: \"I suggest the authors address those concerns in the next version of the paper to make the paper stronger, e.g., by highlighting the first point more and providing concrete/quantitative evidence on how much useful the Lale library is (and will be) for the NeurIPS community.\"\n\n\nThe other reviewers offered several suggestions for the authors to improve the paper, in addition to those quoted above:\n* Reviewer cjD3: \"I liked Section 3 regarding the gradual automation which is a real problem for new (Auto)ML practitioners and I believe that there is a lot of untouched potential here. Unfortunately, this is not really the main focus of the paper.\"\n* Reviewer cjD3: \"Maybe JMLR MLOSS would be a better fit for this paper\"\n\n\nOverall, we weren't able to justify acceptance based either on the significance of the scientific contribution or on the practical impact of the software described here within the NeurIPS community. For that reason I recommend rejecting the paper, but I hope that the reviewers' feedback is helpful to the authors in improving it.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "GNuZexsVC3",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "",
                "comment": "This paper aims at constructing a system for gradual AutoML that is concise, modular (or compositional), and easy-to-use. To this end, the paper introduces three orthogonal combinators (i.e., higher-order functions) which enable compositional code for gradual AutoML, and hyperparameter schemas which describe search spaces of hyperparameters. To support various backend AutoML optimizers, the paper proposes a translation scheme which translates pipelines, described by combinators and hyperparameter schemas, into search spaces for those optimizers. The paper implements these ideas into a Python library (called Lale) which includes a new execution mode (called AutoML search) for running AutoML searches. Through user studies and experiments, the paper shows that Lale is easy-to-use, and can express various pipelines and support various optimizers.  I really enjoyed reading this paper! The paper is extremely well-written and well-organized---it was easy to understand both low-level details and high-level ideas clearly. In particular, the paper succeeds at abstracting out unimportant details and explaining core ideas succinctly. Moreover, the technical contents of the paper look novel and sound to me.\n\nThe paper considers a very important problem and provides a very satisfactory solution both in technical and practical point of view. Technically, the paper introduces three combinators important for compositional gradual AutoML, among which the or combinator is particularly novel. Also the entire system (including combinators, hyperparameter schema, and translation scheme) seems to be designed and engineered in a principled way, and the paper describes some important design principles. Practically, learning and using Lale look easy to me (which is confirmed by user studies) in that it is implemented in Python and based on popular frameworks (sklearn and JSON Schema). Also Lale supports various pipeline structures and optimizer backends. For these reasons, I think Lale will have a huge positive impact on the NeurIPS community.\n\nQuestions & Comments:\n- Line 357 describes \u201cdataset schemas\u201d but I think it is not explained elsewhere in the paper. What is it in detail?\n- Line 291. \u201cparticipants where able\u201d --> \u201cparticipants were able\u201d.\n\n-----\n**Updates after the author response.** Thank the authors for answering my questions. I am still fond of the paper and will keep the same score. \n\n----\n**Updates after internal discussions.** I like the paper for two reasons (which I wrote in the internal discussion):\n\n* First, I do think the paper has scientific contributions. Most importantly, the paper identifies three combinators (pipe, and, or) for the domain of AutoML, and shows that they are enough to express most tasks in AutoML and further enable \"gradual\" AutoML by making a programming system for AutoML \"modular\"---note that the combinators are the key to the modularity. I think this is an important scientific contribution, and would be useful for the NeurIPS community as it enables gradual AutoML (which has not been possible in existing AutoML systems).\n* Second, I have made the following assumption: engineering software that is useful to the NeurIPS community is considered as one of the contributions NeurIPS appreciates. In this context, the paper makes an another contribution by providing the Lale library which I think would be quite useful at least for beginners of AutoML. Of course, my above assumption could be wrong and I totally understand if others disagree with it.\n\nHowever, during internal discussions, other reviewers expressed different thoughts and raised concerns on each of the two points. Given this, I suggest the authors address those concerns in the next version of the paper to make the paper stronger, e.g., by highlighting the first point more and providing concrete/quantitative evidence on how much useful the Lale library is (and will be) for the NeurIPS community. Limitations and societal impact are discussed in the paper.",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "nQMS0UiH_3G",
                "writer": "author",
                "reply_to": "5ahnEpsJ71",
                "title": "Author response to updates towards the end of the discussion period.",
                "comment": " Thank you for updating your review after our author response!\n\nYou wrote:\n\n> I hope during this procedure, the author would also feel that both\n> my comments and comments from other reviewers could help to make it\n> a better paper.\n\nWe appreciate the time, effort, and constructive comments from you and\nthe other reviewers and will try to use them to make our paper better.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5ahnEpsJ71",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "",
                "comment": "This paper propose a new autoML system with the formalization of pipeline combinators. At the high-level, machine learning can be automatically used by novices without any expertise; at the low-level, a formalization is proposed to facilitate the search of optimal hyper parameters. User studies and experiments are included to justify the designs.    AutoML is a useful tool to help novices apply machine learning in their domains, thus, to design easy-to-use and efficient autoML system is an interesting research topic. This paper proposes a system with some new formalizations under its software layout. The motivation is clear: to make the utilization of the system as easy as possible while enabling efficient optimization of the hyper-parameters demanded by different machine learning pipelines.\n\nTechnically, I have a few comments:\n+ I am confused by the design philosophy  of the formalization of combinator in Section 2.  Essentially, why do we need these formalizations? The author keeps mentioning that there are some roughly equivalent operators within the sklearn toolkit. Does this section just offer some more formal definition of a language, or is the syntax defined in this section more expressive than the existing systems? It would be better to have a more explicit description about the motivation of the design.     \n+ W.r.t the grammar of \"pipeline\", I am wondering if this could make the formalization be able to be used for neural network architecture search? It seems there is no such discussion.\n\nThe presentation of the empirical study should be organized better: \n+ For RQ1, even though there is limited number of participants (which is fine due to the difficulty of organizing the user study), there still should be some formal description about the null hypothesis in this user study, then there should be some statistical analysis about the statistical significance of the result, e.g., one can consider bootstrap based methods to report the p-value w.r.t the hypothesis. Further, this section should be self-contained, there should be some brief introduction about the tasks.\n\n+ For RQ2 and RQ3, it seems that only the end-to-end performance boost is reported, on the other hand, it is important to understand where the performance gain comes from (perhaps based on some micro-benchmarks). In other words, the section should answer the question why the formalization of the combinators is effective for the black-box optimizer.\n\nPost rebuttal updates:\n\nI really appreciate the great effort the author has made to address my concerns. On the other hand, I hope during this procedure, the author would also feel that both my comments and comments from other reviewers could help to make it a better paper. Here are some follow-up suggestions:\n\n+ To be more clear about the scientific contribution from the very beginning of the paper, e.g., a more expressive formalization of the AutoML system. \n\n+ To organize the presentation of the empirical study following the general scientific principles; and to recruit more participants if possible so that one could draw some statistically significant conclusions.\n\n+ To provide some analysis about the performance gain and to discuss how the gain relates to the proposed design---at the end of the day, as scientists, we not only want to know if something works, we want to know why it works as well.\n \n\n Yes.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "c34WxydEJQb",
                "writer": "author",
                "reply_to": "n7ivTK7ITDY",
                "title": "Author Response to Reviewer cjD3",
                "comment": " > The novelty part can be narrowed down to the unified translation\n> schema, which is the first work in the domain of AutoML to my\n> knowledge. However, it is not clear to me whether this is more\n> novelty in the sense of engineering some useful software or whether\n> this sufficient scientific novelty in the method directly.\n\nWe argue that the novelty of our paper is the programming model\ntogether with the translation scheme that makes the programming model\npossible.  In your words, one way to view this is as \"engineering some\nuseful software\", which is well-aligned with the call-for-papers\ncategory \"Infrastructure (e.g., datasets, competitions,\nimplementations, libraries)\". A more scientific way to phrase this is\nthe hypothesis \"combinators and JSON schemas can effectively specify\nsearch spaces for AutoML\". This hypothesis felt radical at the outset\nof our project, but in retrospect, our answer is a resounding \"yes\".\n\n> experts in ML and AutoML tend to work as closely to the optimizer\n> interfaces as possible to have full control over the optimizer.\n\nThis may be true for people who are experts in both ML and AutoML.\nHowever, that is a small population, and there are far more people who\nmay have hands-on experience in ML and data science but at most a\nworking knowledge in AutoML. Lale is designed for this broader\npopulation.\n\n> Table 2: It seems like Auto-Sklearn outperforms Lale-Auto. What are\n> the reasons for that?\n\nIf you look at all datasets, then Lale outperforms auto-sklearn.\nIf you exclude the dataset on which auto-sklearn performs worst\n(shuttle), then on the remaining datasets, the average accuracy with\nLale is 0.3% worse than with auto-sklearn. Note that unlike Lale,\nauto-sklearn benefits from warm-start via meta-learning.\nThat said, given that 0.3% is less than the standard deviation on most\ndatasets, and given the shuttle result, we did not deem the difference\nimportant enough to investigate further.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "hNOvO7qHDwY",
                "writer": "author",
                "reply_to": "5ahnEpsJ71",
                "title": "Author Response to Reviewer H7JW",
                "comment": " > a formalization is proposed to facilitate the search of optimal\n> hyper parameters\n\nThe goal of our library goes beyond hyperparameter search, but also\ncovers the whole spectrum of \"gradual AutoML\" where users can access\nany point in the spectrum (full automation to partial automation to\ndefining arbitrary complex search spaces for directed acyclic\nmachine-learning pipelines) with a consistent programming interface.\nAnd even for expert users who would define complex search spaces\n(which can be done succintly using the syntax facilitated by the\nproposed combinators), the library provides various backends\n(Hyperopt, SMAC) to explore these search spaces regardless of their\ncomplexity.\n\n> Does this section [Section 2] just offer some more formal definition\n> of a language, or is the syntax defined in this section more\n> expressive than the existing systems?\n\nThe syntax is more expressive than existing systems. Specifically, the\nsyntax is more expressive than sklearn, since sklearn lacks a choice\ncombinator (|), and since Lale allows omitting hyperparameters from\nindividual operators. Also, the syntax is more expressive than that of\nmost existing AutoML tools, since it supports higher-order operators,\nwhere hyperparameters passed to an outer operator are themselves inner\npipelines with optimizable operator choices and hyperparameters of\ntheir own.\n\n> W.r.t the grammar of \"pipeline\", I am wondering if this could make\n> the formalization be able to used for neural network architecture\n> search?\n\nYes, the formalization could also be used for neural network\narchitecture search. In fact, we have already created prototypes and\nrun experiments using Lale for simple topologies of neural network\ncomponentry including back-propagation. However, these results are\nstill preliminary and too premature to include in this paper.\n\n> For RQ1, even though there is limited number of participants (which\n> is fine due to the difficulty of organizing the user study), there\n> still should be some formal description about the null hypothesis in\n> this user study, then there should be some statistical analysis\n> about the statistical significance of the result (...)\n\nAs the reviewer points out, the limited number of participants makes meaningful\nquantitative analysis difficult. However, we will provide the following null hypotheses\nto test: 1) T1 correct is the same between Lale and Sklearn, 2) T4 correct is the same, \n3) T4 lines of code (LoC) is the same, and 4) total time taken is the same. The \nMann-Whitney test T4 LoC rejects the third null hypothesis with \na p-value of 0.0039. We are unable to reject the other null hypotheses. This is probably\ndue to the small sample size that is common for laboratory-style user studies. When\nbootstrapping the user study data to 100 observations for each task, all four null \nhypotheses are rejected.\n\n> Further, this section should be self-contained, there\n> should be some brief introduction about the tasks.\n\nWe agree that there should be brief introductions about the tasks and note our\ndescriptions for the tasks starting on line 257. Given the page limit, more detailed\ndescriptions would be difficult to include in the main body but note that detailed \ndescriptions along with the actual test notebooks are available in the supplemental\nmaterial.\n\n> For RQ2 and RQ3, it seems that only the end-to-end performance boost\n> is reported, on the other hand, it is important to understand where\n> the performance gain comes from (perhaps based on some\n> micro-benchmarks). \n\nGiven the page limit, such detailed experiments would be difficult to\ninclude in the main body of the paper. In fact, earlier versions of\nthis paper did include additional performance results, for instance\nwith other data modalities besides tabular data.  Based on your\nfeedback, we may add more drill-down experiments to the supplemental\nmaterial.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oAThYj8H5e",
                "writer": "author",
                "reply_to": "GNuZexsVC3",
                "title": "Author Response to Reviewer G6BM",
                "comment": " Thank you very much for your review! We hope you can convince the\nother reviewers with your positive opinion of our paper.\n\n> Line 357 describes \"dataset schemas\" but I think it is not explained\n> elsewhere in the paper. What is it in detail?\n\nLale uses JSON schema to describe datasets, including the schema of\ndata that an operator expects as input to various methods (fit,\npredict, predict_proba, etc.) or produces as output from various methods.\nTypically, this ends up being a (possibly nested) array schema, sometimes with\ndifferent per-item schemas for columns with different types.\nFurthermore, Lale includes functionality to automatically deduce JSON\nschemas from ARFF files, numpy ndarrays, pandas dataframes, etc.\nThen, Lale performs subschema checking: the schema of data passed\nneeds to be a subschema of the schema of data expected, starting from\nthe input and proceeding along all edges in a pipeline's dataflow graph.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qqg-u4YmyOl",
                "writer": "author",
                "reply_to": "ogiUYIbxm2k",
                "title": "Author Response to Reviewer D3TC",
                "comment": " > It is not clear why the proposed method is contributing to\n> AutoML. It seems like it is just combining existing AutoML\n> approaches with the combinators.\n\nThe contribution of our paper is a library that offers a more\nconvenient programming interface for AutoML.  This is well-aligned\nwith the NeurIPS call for papers, which includes a category on\n\"Infrastructure (e.g., datasets, competitions, implementations,\nlibraries)\". Also, NeurIPS has published papers on innovative\nprogramming interfaces for machine learning in the past.\n\n> The high-level scientific contribution is missing. The paper\n> provides too many low level details without describing what are the\n> main challenges to implement combinators.\n\nLines 52-54 in the introduction crisply state the contributions of\nthis paper. The main challenge is to support the modular\ncombinator-based syntax by rewriting it to a form suitable for a given\noptimizer. This challenge is stated in Lines 215-217 of the paper.\n\n> All the datasets used in this paper seems rather small\n\nWe chose OpenML datasets that allow for meaningful optimization (as\nopposed to just the initial few trials) with the default 1 hour\nsetting of auto-sklearn. Our datasets are drawn from the AutoML\nBenchmark [1] and four of them were also used in the auto-sklearn\nevaluation. Also note that the goal of our evaluation is orthogonal to\nthe sizes of the datasets.\n\n[1] Pieter Gijsbers, Erin LeDell, Janek Thomas, Sebastien Poirier, \nBernd Bischl, and Joaquin Vanschoren, \"An Open Source AutoML Benchmark\".\nICML Workshop on Automated Machine Learning (AutoML@ICML), 2019.\n\n> it is not clear to whether the underlying framework can support\n> diverse model architectures.\n\nSince Lale supports arbitrary nesting of simple core constructs, the\ndiversity of supported architectures is one of its strengths. For\ninstance, Table 2 presents results for four significantly different\nmodel architectures (Lale-Auto, Lale-TPOT, Lale-AD3M, Lale-ADB), and\nSection A.2 in the supplemental material shows details for them.\nWe have also successfully used Lale on other data modalities, such as\nimages, text, and time series. Furthermore, Lale also works with\ndeep-learning operators, such as BERT embeddings.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ogiUYIbxm2k",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "",
                "comment": "This paper proposes a combinators that can combine two functions without naming datasets. They implement these combinators as a part of sklearn pipeline---both prediction and training. Finally, it also adds a hyperparameter search using AutoML to optimize the models.   Strength\n\n+ AutoML is an important application, and if successful, can greatly reduce data scientists' effort.\n+ It looks like a significant engineering effort and mature tool. They support many backend for the optimizers. \n+ Besides reporting testing accuracies for 14 OpenML classification tasks, they also reported  results of an user study demonstrating usefulness of the tool. \n\nWeakness\n- The high-level scientific contribution is missing. The paper provides too many low level details  without describing what are the main challenges to implement combinators.\n- All the datasets used in this paper seems rather small, and it is not clear to whether the underlying framework can support diverse model architectures.\n- It is not clear why the proposed method is contributing to AutoML. It seems like it is just combining existing AutoML approaches with the combinators.  N/A",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "n7ivTK7ITDY",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_uFORMPcA_b",
                "title": "",
                "comment": "The authors introduced a wrapper for hyperparameter optimization, named Lale, to create and verify search spaces more easily. To this end, instructions with combinators are used, which further are translated to fit the underlying optimizer (like SMAC, Hyperopt, etc.). Lale is meant to be an interface between machine and human s.t. parts of the pipeline can be easily changed, if desired.  # Novelty\nThe novelty part can be narrowed down to the unified translation schema, which is the first work in the domain of AutoML to my knowledge. However, it is not clear to me whether this is more novelty in the sense of engineering some useful software or whether this sufficient scientific novelty in the method directly.\n\n# Significance / Impact\nThe advantages for Lale over existing AutoML frameworks are especially advantageous for inexperienced users due to two reasons. (i) The unified interface makes it easy to quickly set up an AutoML pipeline for any implemented optimizer. (ii) Graph overviews are a helpful and easy way to check whether the search space is defined correctly. \n\nHowever, experts in ML and AutoML tend to work as closely to the optimizer interfaces as possible to have full control over the optimizer. Since defining search spaces is transparent and easily understandable in several frameworks too, e.g. SMAC and Hyperopt, Lale would not give many advantages over these. Full AutoML systems such as AutoSklearn, on the other hand, would definitely benefit from Lale. Overall, I\u2019m not fully convinced that there will be many users of Lale at the end of the day.\n\nFurthermore, I don\u2019t see a clear path for scientific future work building on top of Lale.\n\nOverall, it is uncertain whether Lale will have a great impact in the community. \n\n# Soundness (method and experimental setup)\n\nThe authors described their method and experiments detailedly and the concept is sound. \n\nI appreciate the overall idea of having several API levels which allow to gradually build up expertise in using AutoML tools. Depending also on the level of the user\u2019s expertise, this should allow to find an appropriate API without switching between different packages. However, I have the impression that conceptually this is a very high-level idea of software engineering and unclear how this scientifically contributes to the NeurIPS community.\n\n# Scholarship\nRelated work covers both combinators in general and in the AutoML domain. The domain is sufficiently covered.\n\n\n# Clarity\nThe authors introduced their AutoML library in a sorted, logical way.\n\n# Minor Comments\n* Using Lale still requires a decent background on machine learning. Calling methods and Interpreting graphs (Figure 11) may not be straight forward for inexperienced users.\n* As already mentioned in the author\u2019s limitations, the user studies only include 9 participants, making statements not very meaningful. Additionally, the survey might even be  biased towards Lale.\n* Figure 6 shows a default routine with the only difference of \u201cencode search space\u201d. There\u2019s no novelty aspect at all.\n* Table 1 would not convince me to use Lale over Auto-Sklearn.\n\n# Questions for Rebuttal\n* Table 2: It seems like Auto-Sklearn outperforms Lale-Auto. What are the reasons for that?\n Yes.",
                "rating": 4,
                "confidence": 5
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "support from reviewers for me to recommend acceptance",
                "Sentiment Expression": "not enough",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "(1) doesn't present a clear and significant scientific contribution, (2) shows limited empirical validation, and (3) doesn't concern a software package with large enough practical impact on the NeurIPS community",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "a clear path for scientific future work building on top of Lale",
                "Sentiment Expression": "I don\u2019t see",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The high-level scientific contribution",
                "Sentiment Expression": "is missing",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the user studies",
                "Sentiment Expression": "only include 9 participants, making statements not very meaningful",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "acceptance",
                "Sentiment Expression": "weren't able to justify",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "BJlowyHYPr": {
        "paper_id": "iclr_2020_BJlowyHYPr",
        "paper_title": "CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point-cloud Stream Forecasting",
        "paper_abstract": "This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (D-Conv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The D-Conv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms.\n          We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e. mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of neural network models.",
        "paper_acceptance": "reject",
        "meta_review": "The paper presents an approach to forecasting over temporal streams of permutation-invariant data such as point clouds. The approach is based on an operator (DConv) that is related to continuous convolution operators such as X-Conv and others. The reviews are split. After the authors' responses, concerns remain and two ratings remain \"3\". The AC agrees with the concerns and recommends against accepting the paper.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "rJxT5QF55S",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Official Blind Review #3",
                "comment": "=========== Update after rebuttal\n\nThanks for the clarifications and the update. I recommend acceptance of the paper and updated to 8.\n\nLast comment: please still improve the appearance of Figure 4 by using a more diverse set of marker shapes as well as overlay and offset tricks -- see https://www.cs.ubc.ca/~schmidtm/Software/prettyPlot.html for an example. \n\n============\n\nThis paper introduces a new convolution operator (D-conv) specifically tailored to model point-cloud data evolving over time, i.e. a set of n points with features and localization-coordinates that can evolve over time. The main idea is to use the k-nearest neighbor structure for each point to get a fixed size k window to use in the convolution to determine the new location and feature values of a point (and a permutation-invariant operation). The D-Conv operator is included in a LSTM architecture (CloudLSTM) to enable the spatio-temporal modeling of point-cloud data, and can be combined in standard neural network architectures such as a Seq2Seq with attention. This is in contrast to previous approaches which modeled the data on a grid through preprocessing, or did not include the temporal component for cloud data. Experiments is conducted on 4 benchmark datasets, covering two point-cloud stream forecasting applications, showing how CloudLSTM give lower prediction error than numerous baselines and alternatives.\n\nWhile the D-Conv idea seems fairly simple and natural, it is novel AFAIK and fairly appropriate to model point-cloud data streams. The approach is well situated in the literature, and the experiments are indicative that this method can improve on the current approaches. I am thus leaning towards accept.\n\nThe paper is fairly clear, though the notation is a bit confusing and somewhat sloppy (see detailed comment below).\n\nImportant clarification requested: the current notation suggests that each channel could have a different location for a point p_n, the K nearest points seem to be defined irrespective on the channel. So is the location fixed across channels; or does this paper allow the neighborhood structures to vary across channel?\n\n== Other detailed comments ==\n\n- p.3 Q_n^K -- it seems it would be more appropriate to define it as an ordered list of k points (rather than a set, as this would loose all information about the order); unless you append a new dimension to each point where you put the ordering information there for the purpose of defining the k points in Q_n^K.\n\n- (2) the notation is a bit weird and overloaded for the summation (without being defined). Examples include \"i in U1\" (when U1 is an integer, not a set); \"p_n^k in Q_n^K\" when p_n^k does not appear in the summation (a clearer alternative would be using the notation v(p_n^k)_i^h for the h^th value of channel i of point p_n^k, e.g.; now p_n^k would indeed appear in the expressoin); \"v_n^h in v_n\" -> why not just summing over h as it is really doing? Etc.!\n\n- (2) S_out^j: each p_n^' should be a *tuple* (not a set like currently written).\n\n- Figure 4: the lines are really hard to distinguish just by the similar colors -- please use different markers for the different lines (and offset the marker so that they can be seen)\n\n- Several neighborhood sizes are experimented with. Note though that smaller neighborhood sizes are just *special cases* of bigger neighborhood sizes (by using zero weight on the last few neighbors in the convolution). Wouldn't it make sense to use a big neighborhood size and regularize in some way the weights for the further neighbors?\n\n- Table 2: for SSIM, there are two rows with 0.69 +/- 0.07 (minimal value) -- they could be both bolded.\n\n- Appendix B, they claim that the complexity of finding the K nearest neighbors (in dimension L for n points) is close to O(K L log(n)) if using KD trees. I vaguely recall issues in high dimension though (in particular that the above complexity is only valid for specific distributions of points in low dimension). E.g. see https://en.wikipedia.org/wiki/K-d_tree#High-dimensional_data where it is mentioned that L << log(n) is normally needed to guarantee efficiency. The claim should properly be nuanced.",
                "rating": 8,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HkgqkZFBjB",
                "reply_to": "ByeeBVfN9S",
                "title": "Response to Blind Review #2",
                "comment": "Thank you for your feedback and for indicating which aspects can be improved on. We note that some of the comments made are already addressed in the original manuscript.\n\nQ: Given that the proposed convolution operator use KNN to choose the nearest neighbors. It would be good to empirically to study how K would affect the performance, does it data-dependent?\n\nA: The impact of K (ranging from 3 to 9) is analyzed in Tables 1 and 2, which suggest that K only affects marginally the prediction performance. Regarding other baseline models based on k-nearest-neighbours (i.e., MLP and LSTM), we test with K ranging from 1 to 100 and the results obtained give a similar conclusion. These results can be found in Table 6 of Appendix L in the revised manuscript.\n\nQ: 2. Is it possible to study the time complexity for various models?\n\nA: We gave a detailed complexity analysis of our D-Conv in Appendix B, which suggests that compared to the convolution operator whose inputs, outputs, and filters have the same size, the D-Conv only introduces additional complexity by searching the $\\mathcal{K}$ nearest neighbors for each point $O(\\mathcal{K}\\cdot L\\log N)$. Such complexity does not increase much even with higher dimensional point-clouds.\n\nQ: It might be good to do further oblation test to study which mechanism actually contribute to the performance. The choice of RNN, attention, or the new operator? \n\nA: We conducted our experiments using strict variable-controlling methodology, i.e., only changing one factor while keep the remaining the same. Therefore, it is easy to study the effects of each factor. For example, taking a look at the performance of LSTM, ConvLSTM, PredRNN++, PointLSTM and CloudLSTM, which employ dense layers, and CNN, PointCNN and D-Conv as core operators but using LSTM as the RNN structure, it is clear that the D-Conv contributes significantly to the performance improvements. Further, by comparing CloudRNN, CloudGRU and CloudLSTM, it appears that CloudRNN $\\ll$ CloudGRU $<$ CloudLSTM. Similarly, by comparing the CloudLSTM and Attention CloudLSTM, we see that the effects of the attention mechanism are not very significant. Therefore, we believe the core operator $>$ RNN structure $>$ attention, ranked by their contribution. \n\nQ: Furthermore, the std is quite large, which makes one wonder if the improvement is statistically significant.\n\nA: We note that the large std exists only in the Cluster A of the air quality dataset. We checked this dataset more carefully, and found that the level of noise therein is more severe than in other case studies. We believe that is the root cause of the larger std. In addition, we note that all models achieve similar std in this dataset (except for MLP), while our proposal obtains the best mean for all metrics, which proves that the improvement is statistically significant.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1goK1W3jS",
                "reply_to": "H1gC3MZcsr",
                "title": "Thanks for your reply, more experiments were added",
                "comment": "Thanks very much for taking time to read our response. We have updated the paper to address your concerns, as follows:\n\nQ: I find it a bit hard to believe that storing (sparser) traffic data long enough to capture day of week effects wouldn't be worth it.  I think at least showing some concrete comparison of the current window vs a 7 day window would be needed.\n\nA: We wish to further clarify that in our previous response, we mean that using shorter input to perform long-term forecasting is more desirable, given the overhead incurred by data measurements, as suggested by [1]. That is not to say that exploiting periodic information may not help improve the forecasting performance. However, recall that a 7-day window corresponds to a 2016-long sequence as input, since the data is sampled every 5 minutes. It is very difficult for RNN-based models to handle such long sequences. In addition, by considering the number of mobile services (38) and antennas (792), the input for 7 days would have 60,673,536 data points. This would make any forecasting model extremely large and therefore impractical for real deployment.\n\nTo capture seasonal information more efficiently, we concatenate the 30 minute-long sequences (sampled every 5 minutes) with a sub-sampled 7-day window (sampled every 2h). This forms an input with length 90 (6 + 84). We conduct experiments on a subset of the mobile traffic dataset (City 1) and show the results in the Appendix M of the revised paper. The results suggest that the performance can indeed be improved by introducing seasonal information, and we believe this is a promising direction that we can further explore in future work.\n\nHowever, and more importantly to the present study, we also remark that the improvement determined by considering a richer input is fairly uniform across models (see Table~7 in the paper): therefore, our conclusion that CloudLSTM outperforms state-of-the-art benchmarks in the mobile traffic forecasting task still holds.\n\n[1] Chaoyun Zhang and Paul Patras. \"Long-term mobile traffic forecasting using deep spatio-temporal neural networks.\" Proc. ACM MobiHoc. 2018.\n\nQ: It seems weird to use K=3 for your method (seems best), but only let the baseline use K=1 or K=9 or higher. \n\nA: K=3 was not shown for the baselines, because we did not see clear performance improvements in their performance with different K values. To eliminate any doubt, we added results with K = {3 ,6} for each baselines and updated Table 6 in the revised paper (note the numbering has changed).\n\nQ: Even if there is some lone data point way far from the existing \"normal\" data, your approach will still force its predictions to be based off (potentially very unrelated) neighbors, rather than say falling back on some simpler heuristic. I am happy that there is some experimental evidence that the method is somewhat robust to outliers, though.\n\nA: Thanks for acknowledging our efforts to how the robustness of our CloudLSTM to outliers. We want to emphasize that the DConv is only an operator of the model and it relies on neighbors of each point. Therefore, only taking one DConv layer for the forecasting might indeed force its predictions over outliers to be based on unrelated neighbors. This is because one DConv will not have sufficient representability to learn proper weights individually for inliers and outliers. However, the DConv is only a component of the entire architecture. By stacking multiple such components (DConv) via dedicated structure (LSTM), the CloudLSTM has much stronger representability and therefore it will be able to learn appropriate weights for outliers between target points and their neighbors. \n\nThis is similar to convolutional operator over images. Though one Conv operator only sums the shared weights over each anchor point and its neighbors, a deep stack of CNN can learn very complex correlations within pixels, and therefore performs remarkably in many Computer Vision applications. Our experiments in Appendix K clearly show that the performance of outliers will not be affected by their \"lone\" positions.\n\nQ: I think I'd like to see a more controlled experiment, where there is some outlier with opposite response to any neighbor, and we see how its prediction changes as its distance grows.\n\nA: To further show that our CloudLSTM is robust to outliers, we re-run the experiments on the air quality dataset, where we push some weather stations away from the center with different distances, so as to construct artificial outliers. Our experiments show that our CloudLSTM performs equally well when forecasting over inliers and outliers, irrespective of the moving distance of the outliers. Importantly, CloudLSTM achieves significantly better performance over its counterpart PointLSTM (PointCNN + LSTM). New results can be found in Appendix K of the revised paper, and the synthetic dataset will be also released publicly for the sake of reproducibility.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1gC3MZcsr",
                "reply_to": "rJgfGGKrsB",
                "title": "Thanks for a thorough rebuttal; some concerns remain",
                "comment": "Overall, I'm still a bit borderline on this paper, but I'm leaning more positive than before based on the thorough rebuttal. If others are voting for acceptance I won't stand strongly against it. I'm glad authors are thinking about a reproducible version of the urban-air dataset and were willing to implement a few baselines I suggested.\n\nRE experiments not considering day-of-week / seasonality effects\n\nI am not quite satisfied with this explanation. I find it a bit hard to believe that storing (sparser) traffic data long enough to capture day of week effects wouldn't be worth it. Surely there is more Saturday activity in downloading game apps than weekday activity, why throw this away? Plus, I think the dataset you have would let you quantify the value of using longer windows, etc. I think at least showing some concrete comparison of the current window vs a 7 day window would be needed. \n\nI looked thru the plots in J, I'm not too convinced the current solution is \"good enough\" and would not be substantially improved with day-of-week effects.\n\nRE baselines with nearest neighbors\n\nOne lingering issue: it seems weird to use K=3 for your method (which seems to consistently do best), but only let the baseline use K=1 or K=9 or higher. \n\nRE outliers\n\nI don't think just using the sigmoid to rescale things solves my concern... even if there is some lone data point way far from the existing \"normal\" data, your approach will still force its predictions to be based off (potentially very unrelated) neighbors, rather than say falling back on some simpler heuristic.\n\nI am happy that there is some experimental evidence that the method is somewhat robust to outliers, though.\n\nI think I'd like to see a more controlled experiment, where there is some outlier with opposite response to any neighbor, and we see how its prediction changes as its distance grows (and how this differs from simpler approaches). ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1eRY1JvjS",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Paper has been revised",
                "comment": "We appreciate the valuable feedback from all reviewers.  We have revised the manuscript to address the concerns ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rJgfGGKrsB",
                "reply_to": "BkgVBUphtH",
                "title": "Response to Blind Review #1 (1)",
                "comment": "Thank you for acknowledging the contribution of our paper and offering insightful comments. We address the concerns raised below.\n\nQ: The present paper's new D-Conv operator appears new, though it looks really like a simplification of the PointCNN's \"X-Conv\" operator rather than a brand new operator. The most similar work seems to be the PointCNN (Li et al NeurIPS 2018). This work's contribution was a new X-Conv operator, which also consumes point-clouds and produces learned representations. X-Conv, like the present paper's D-Conv, computes K-nearest neighbors of each point p, but performs first an embedding of each neighbor to a learned \"local\" feature space and then performs convolution on this embedding. Perhaps the biggest practical difference is that D-Conv has fewer parameters (does not perform the embedding) and does not reduce dimensionality from input to output. \n\nA: Our D-Conv is inspired by convolution over grids, which computes summations over small regions, as we also do but over point-clouds. This may of course introduce some problems, such as the permutation of the points. X-Conv addresses this by an X-transformation, while our work solves the problem by keeping the order of the k-nearest-neighbor weights. Therefore, we believe this is a different way of solving the problem, rather than a simplification of the X-Conv. Further, different from X-Conv, our D-Conv treats the value and coordinate features differently, by regularizing the coordinate features using a sigmoid function to avoid outliers in space. By looking at the performance of PointLSTM (X-Conv + LSTM, which is also a new model we derive) and  CloudLSTM (D-Conv + LSTM), our CloudLSTM performs much better, which proves that D-Conv is more suitable for the forecasting task.\n\nImportantly, we want to note that the D-Conv is not the only contribution of our paper. Combining a convolutional operator dedicated for point-cloud with an RNN structure for forecasting is also an important contribution and, to the best of our knowledge, we are the first to do this.\n\n\nQ: My biggest concerns are that the D-Conv has a strong reliance on nearest neighbors. This means the D-Conv has not much accommodation for \"outlier\" points that are far from others.  I would imagine that data with outliers (whose values are unlike most others) would dramatically hurt performance, as the weights of D-Conv would need to be shared equally by outliers and inliers.\n\nA: In Eq 2, we regularize the coordinate features of each point using a Sigmoid function, such that those points which are far from others will move closer to each other and will be more involved in the computation. This is exactly how the D-Conv handles outliers. Further, though the weights are shared by outliers and inliers, the final forecasts are made by multiple stacks of CloudLSTM and through multiple steps of computation. This means that the model would have sufficient representability to move those outliers to positions where they are best to be, thus the performance will not be compromised.\n\nFor demonstration, we add a new section in Appendix K in the revised paper, to test the performance of each model especially with outlier points found by DBSCAN in the air quality dataset. Observe that our CloudLSTM remains the best when performing forecasting with outliers. Compared with the full forecasting in Table 2, our proposals achieve even better performance, which proves that the CloudLSTM remains reliable when forecasting over outliers.\n\nQ: Is there a good reason to not try to compare on publicly available datasets like those used in the PointCNN paper (focusing only on the non-temporal versions of the model)? Using proprietary datasets makes following up on this work a bit hard, would be nice to have some reproducible experiment.\n\nA: The datasets in the PointCNN paper are dedicated to point-cloud classification and segmentation, while our work focuses on point-cloud stream forecasting, hence the scope is different. Since the D-Conv, along with the CloudLSTM, are designed for the task of temporal forecasting, we do not test the model on those datasets.\n\nWe fully agree however that it would be good to have a reproducible experiment with publicly available datasets. While the mobile traffic dataset can not be released, the air quality datasets are publicly available and can be found at https://www.microsoft.com/en-us/research/project/urban-air/.\nWe will release the code and processed dataset upon final decision, so as to support the reproducibility of our results.\n\nto be continued...\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1eohztSjH",
                "reply_to": "BkgVBUphtH",
                "title": "Response to Blind Review #1 (2)",
                "comment": "Continues  from the `````````````\u201cResponse to Blind Review #1 (1)\u201d\n\nQ: It's not clear to me that the experiments here consider realistic scenarios.  Certainly there are time-of-day, day-of-week, and seasonal effects that are all important. At a minimum, I'd think that for the mobile traffic case you could at least look at consuming the last 48 hr of data and predicting the next 30-90 minutes. I suspect that would make even simpler models do much better.\n\nA: There are several reasons for not using long sequences as input: (1) Data measurements are expensive; while measuring air quality is easier, mobile data collection is not straightforward, as it relies on dedicated hardware and involves substantial data processing overhead. Therefore, mobile traffic collection is not activated all the time and thus we may not obtain full 24-48 hours of data for making predictions [1]. Therefore, relying on short-term input to make long-term performance predictions is in fact more realistic [2]; (2) The time complexity for RNN-based models grows linearly with the length of the input. In our case, the mobile traffic dataset is sampled every 5 minutes and a 48-hour data trace would correspond to 576-long sequence, which will hard for RNN-based models to handle; (3) By looking at the heat map of the forecasting in Figs 11 and 12 in Appendix J.2, we see that the CloudLSTM is already performing well, given short time series as input; (4) We agree that including periodic information may improve performance; instead of increasing the length of the input, a smarter way is to mix that with the predictions made [2] or taking those as different channels of the input, as they do not increase complexity significantly. This is an avenue we will pursue as part of future work.\n\n[1] Chaoyun Zhang, Xi Ouyang, and Paul Patras. \"ZipNet-GAN: Inferring fine-grained mobile traffic patterns via a generative adversarial neural network.\" Proc. ACM International Conference on emerging Networking EXperiments and Technologies. 2017.\n\n[2] Chaoyun Zhang and Paul Patras. \"Long-term mobile traffic forecasting using deep spatio-temporal neural networks.\" Proc. ACM International Symposium on Mobile Ad Hoc Networking and Computing. 2018.\\\\\n\n\nQ: I think the experiments are missing some key simple baselines (or I misunderstand something). For example, rather than the complicated CNN/LSTM architectures, why not try to directly see how much value there is in \"neighbors\" in this 2d space? At each point, you can make predictions using only the K nearest neighbors' data, with K swept from 1 to 100 or something. I would expect with these features, using just a simple MLP or RNN would do quite well. I'd like to see a stronger qualitative case made for why we expect the complicated D-Conv weighting operator here to do better than this baselines.\n\nA: We agree such baselines should be used for comparison, therefore in the revised manuscript we consider them in Appendix L, where we show the performance of MLP and LSTM (which only take a sequence of k-nearest neighbors of each point as input). We test with K in \\{1, 9, 25, 50, 100\\}, and find that K does not affect the forecasting performance significantly. Importantly, our CloudLSTM achieves much better performance than these baselines.\n\n\nQ: Overall, the results tables appear promising (for app traffic forecasting in Table 1, the proposed CloudLSTM achieves 3.66 MAE compared to 4.95 for PointCNN and 4.8 for an MLP). However, it's not clear why and I'd like to understand why. Is it that the other approaches are overfitting? \n\nA: Our CloudLSTM is able to learn dynamic spatial correlations in point-clouds through different time steps, while the others does not have this nice property. According to our reply to Reviewer 2, it appears that the D-Conv contributes most to the performance, followed by the RNN, and the attention mechanism. This is not because other approaches are overfitting. We add such a discussion in Sec. 4.4 of the revised paper.\n\n\nQ: I would suggest avoiding calling the method \"$\\mathcal{D}$-Conv\", and instead use just \"DConv\", since this is easier to type into search engines and easier to search for in a PDF document\n\nRelated: Point clouds could be represented as graphs, and then use graph embeddings as feature representations\n\nA: Thanks for the suggestion. We change $\\mathcal{D}$-Conv to DConv in the revised paper. Regarding the graph embedding, we agree this will have potential and will consider this for future work.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r1l5J0OSsS",
                "reply_to": "rJxT5QF55S",
                "title": "Response to Blind Review #3",
                "comment": "We appreciate the detailed and positive comments, which clearly reflect many of the essential contributions of our work. We answer the points raised in what follows.\n\nQ: Important clarification requested: the current notation suggests that each channel could have a different location for a point $p_n$, the K nearest points seem to be defined irrespective on the channel. So is the location fixed across channels; or does this paper allow the neighborhood structures to vary across channel?\n\nA: The K nearest points can indeed vary for each channel at each location. The reason is that the channels in the point-cloud dataset may represent different types of measurements. For example, channels in the mobile traffic dataset are related to the traffic consumption of different mobile apps, while those in the air quality dataset are different air quality indicators (SO2, CO, etc.). The spatial correlations will vary between different measurements (channels), due to human mobility. For instance, more people may use Facebook at a social event, but YouTube traffic may be less significant in this case. This will be reflected by the data consumption of each app. The same applies to air quality indicators affected by vehicle movement and factory working times. We want these spatial correlations to be learnable, so we do not fix the K nearest neighbors across channels, but encourage each channel to find the best neighbor set. This is also a contribution of the CloudLSTM, which helps improve the forecasting performance. We add this discussion in Sec. 3.2 of the revised paper.\n\nQ:- p.3 $Q_n^K$ -- it seems it would be more appropriate to define it as an ordered list of k points (rather than a set, as this would loose all information about the order); unless you append a new dimension for the defining the k points in $Q_n^K$.\n\n- (2) the notation is a bit weird and overloaded for the summation (without being defined). Examples include \"i in U1\" (when U1 is an integer, not a set); \"$p_n^k$ in $Q_n^K$\" when $p_n^k$ does not appear in the summation (a clearer alternative would be using the notation $v(p_n^k)_i^h$ for the $h^{th}$ value of channel i of point $p_n^k$, e.g.; now $p_n^k$ would indeed appear in the expression); \"$v_n^h$ in $v_n$\"  why not just summing over h as it is really doing? Etc.!\n\n- (2) $S_{out}^j$: each $p_n'$ should be a tuple (not a set like currently written).\n\nA: We very much appreciate the detailed comments on these issues. We have update notation accordingly in the revised manuscript, which is available online via OpenReview. We trust the revision eliminates any confusion. \n\nQ:  Figure 4: the lines are really hard to distinguish just by the similar colors -- please use different markers for the different lines.\n\nA: Thank you for raising this issue. We acknowledge the original figure had readability issues and have updated it (and Fig. 9) to address this problem.\n\nQ:  Several neighborhood sizes are experimented with. Note though that smaller neighborhood sizes are just special cases of bigger neighborhood sizes. Wouldn't it make sense to use a big neighborhood size and regularize in some way the weights for the further neighbors?\n\nA: This is an excellent point. We agree that smaller neighborhood sizes are special cases of bigger neighborhood sizes. The reasons we do not use a large K are because (1) as analyzed in Appendix B, the complexity of D-Conv grow linearly with K; (2) While testing with K ranging between 3 and 9, we do not see clear performance improvements with higher K; (3) The K is equivalent to the receptive field of a normal CNN kernel (e.g., K=9 is is equivalent to a 3*3 CNN kernel), and a small CNN kernel has been proven effective in imaging applications. To answer Reviewer 1's question, we test with K between 1 and 100 for k-nearest-neighbor based MLP and LSTM in Appendix L; the results also suggest a large K does not improve performance. At the same time, we agree that using a big neighborhood size and regularizing in some way may be appropriate, as it may reduce overfitting caused by large K. We will consider this approach for future work.\n\nQ: Table 2: for SSIM, there are two rows with 0.69 +/- 0.07 (minimal value) -- they could be both bolded.\n\nA: This has been revised. Thanks.\n\nQ: Appendix B, they claim that the complexity of finding the K nearest neighbors (in dimension L for n points) is close to O(K L log(n)) if using KD trees. I vaguely recall issues in high dimension though (in particular that the above complexity is only valid for specific distributions of points in low dimension).  where it is mentioned that L $<<$ log(n) is normally needed to guarantee efficiency. The claim should properly be nuanced.\n\nA: This is correct, and, in fact, in real life the dimensions of a point-cloud dataset are normally 2 or 3. Also, we usually have much more than 3 points in the dataset. Therefore, L $<<$ log(n) should hold for most applications. We clarified this aspect as a footnote in Appendix B.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkgVBUphtH",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Official Blind Review #1",
                "comment": "Review Summary\n--------------\nOverall this is almost above the bar for me to accept, but I think there's enough concerns about the method and experiments that I'm hesitant. Strengths include the invariance to point cloud order and the relative simplicity of the architecture (compared to PointCNN). Weaknesses include a vulnerability to outliers, experiments that don't seem to think about practical effects like day-of-week in forecasting, and experiments that leave out baselines to help directly assess the impact of neighbors.\n\n\nPaper Summary\n-------------\nThe paper develops a new neural net architecture for processing data structured as spatial point clouds that vary over time (e.g. hourly traffic at several antennas spread throughout a city).\n\nThe core of the approach is a new neural net unit: the \"D-Conv\" operator (See Eq. 2). The output value at each point is obtained via a weighted combination of nearby coordinates and features, using only the K-nearest neighbors (stored in ranked order) to maintain invariance to the original order of points. This layer can be included in modern convolutional (CloudCNN) or recurrent (CloudLSTM) or attention-based architectures in a straightforward way.\n\nUnlike many previous methods that require converting point clouds to quantized regular grids, the present approach directly consumes point cloud data. Unlike some existing methods like PointCNN, it avoids information loss (does not reduce dimension from input to output layer).\n\nTwo experimental evaluations are conducted: forecasting mobile app traffic across 2 European cities (given past 30 min, predict next 30 min), and air quality across several regions in China (given last 12 hrs, predict next 12 hrs). In both experiments, the locations of the sensors are fixed across time. Fig 4 further looks at traffic forecasting as a function of the lookahead time, from 0-3 hours ahead.\n\n\nNovelty & Significance\n-----------------------\n\nThe paper definitely tackles an important problem (point cloud forecasting). \n\nThe present paper's new \"D-Conv\" operator appears new, though it looks really like a simplification of the PointCNN's \"X-Conv\" operator rather than a brand new operator. \n\nThe most similar work seems to be the PointCNN (Li et al NeurIPS 2018). This work's contribution was a new \"X-Conv\" operator, which also consumes point clouds and produces learned representations. X-Conv, like the present paper's D-Conv, computes K-nearest neighbors of each point p, but performs first an embedding of each neighbor to a learned \"local\" feature space and then performs convolution on this embedding. Perhaps the biggest practical difference is that D-Conv has fewer parameters (does not perform the embedding) and does not reduce dimensionality from input to output. \n\nTechnical Concerns\n------------------\n\nMy biggest concerns are that the D-Conv has a strong reliance on nearest neighbors. This means the D-Conv has not much accomodation for \"outlier\" points that are far from others. The X-Conv operator has some nice properties in this regard (it changes coordinate systems so neighbor locations are centered around the current point), but I don't see this in the D-Conv operator, as in Eq. 2, where the coordinate locations are fed directly into the weighted sum after global rescaling to (0,1). I would imagine that data with outliers (whose values are unlike most others) would dramatically hurt performance, as the weights of D-Conv would need to be shared equally by outliers and inliers.\n\n\nExperimental Concerns\n---------------------\n\nIs there a good reason to not try to compare on publicly available datasets like those used in the PointCNN paper (focusing only on the non-temporal versions of the model)? Using proprietary datasets makes following up on this work a bit hard, would be nice to have some reproducible experiment.\n\nIt's not clear to me that the experiments here consider realistic scenarios. Why would I predict mobile app traffic using only the past 30 minutes of data? Why predict air quality using only the last 12 hours? Certainly there are time-of-day, day-of-week, and seasonal effects that are all important. At a minimum, I'd think that for the mobile traffic case you could at least look at consuming the last 48 hr of data and predicting the next 30-90 minutes. I suspect that would make even simpler models do much better.  \n\nFurther, I think the experiments are missing some key simple baselines (or I misunderstand something). For example, rather than the complicated CNN/LSTM architectures, why not try to directly see how much value there is in \"neighbors\" in this 2d space? At each point, you can make predictions using only the K nearest neighbors' data, with K swept from 1 to 100 or something. I would expect with these features, using just a simple MLP or RNN would do quite well. I'd like to see a stronger qualitative case made for why we expect the complicated DConv weighting operator here to do better than this baselines.\n\nOverall, the results tables appear promising (for app traffic forecasting in Table 1, the proposed CloudLSTM achieves 3.66 MAE compared to 4.95 for PointCNN and 4.8 for an MLP). However, it's not clear why and I'd like to understand why. Is it that the other approaches are overfitting? \n\n\nMinor Concerns\n--------------\nI would suggest avoiding calling the method \"\\mathcal{D}-Conv\", and instead use just \"DConv\", since this is easier to type into search engines and easier to search for in a PDF document\n\nRelated: Point clouds could be represented as graphs, and then use graph embeddings as feature representations",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ByeeBVfN9S",
                "reply_to": "iclr_2020_BJlowyHYPr",
                "title": "Official Blind Review #2",
                "comment": "The paper proposed a new convolution operator, named dynamic post-cloud convention over spatiotemporal data, and the convolution operator can be embedded in different neural network architectures, like recurrent neural networks. In order to achieve the convolution over point-clouds by using both value features and the spatial-features, given a data point, the convolution is conducted over its k-nearest neighbors generated by CNN.  They compared the proposed convolution method by embedding it into RNN, GRN, and LSTM against a number of existing methods on two datasets, in terms of MAE, RMSE, PSNR, and SSIM. Overall, this paper is interesting but needs some clarifications on \n\n1. Given that the proposed convolution operator use KNN to choose the nearest neighbors. It would be good to empirically to study how K would affect the performance, does it data-dependent\n2. Is it possible to study the time complexity for various models?\n3. Table 1 and table 2 seem to show that the proposed convolution operator contributes to the performance in terms of the mean of each metric. It might be good to do further oblation test to study which mechanism actually contribute to the performance. The choice of RNN, attention, or the new operator? Furthermore, the std is quite large, which makes one wonder if the improvement is statistically significant.",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "recommends against accepting",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "HkldyTNYwH": {
        "paper_id": "iclr_2020_HkldyTNYwH",
        "paper_title": "AE-OT: A NEW GENERATIVE MODEL BASED ON EXTENDED SEMI-DISCRETE OPTIMAL TRANSPORT",
        "paper_abstract": "Generative adversarial networks (GANs) have attracted huge attention due to\n      its capability to generate visual realistic images. However, most of the existing\n      models suffer from the mode collapse or mode mixture problems. In this work, we\n      give a theoretic explanation of the both problems by Figalli\u2019s regularity theory of\n      optimal transportation maps. Basically, the generator compute the transportation\n      maps between the white noise distributions and the data distributions, which are\n      in general discontinuous. However, DNNs can only represent continuous maps.\n      This intrinsic conflict induces mode collapse and mode mixture. In order to\n      tackle the both problems, we explicitly separate the manifold embedding and the\n      optimal transportation; the first part is carried out using an autoencoder to map the\n      images onto the latent space; the second part is accomplished using a GPU-based\n      convex optimization to find the discontinuous transportation maps. Composing the\n      extended OT map and the decoder, we can finally generate new images from the\n      white noise. This AE-OT model avoids representing discontinuous maps by DNNs,\n      therefore effectively prevents mode collapse and mode mixture.",
        "paper_acceptance": "accept-poster",
        "meta_review": "The authors present a different perspective on the mode collapse and mode mixture problems in GAN based on some recent theoretical results. \n\nThis is an interesting work. However, two reviewers have raised some concerns about the results and hence given a low rating of the paper. After reading the reviews and the rebuttal carefully I feel that the authors have addressed all the concerns of the reviewers. In particular, at least for one reviewer I felt that there was a slight misunderstanding on the reviewer's part which was clarified in the rebuttal. The concerns of R1 about a simpler baseline have also been addressed by the authors with the help of additional experiments. I am convinced that the original concerns of the reviewers are addressed. Hence, I recommend that this paper be accepted. \n\nHaving said that, I strongly recommend that in the final version, the authors should be a bit more clear in motivating the problem. In particular, please make it clear that you are only dealing with the generator and do not have an adversarial component in the training. Also, as suggested by R3 add more intuitive descriptions to make the paper accessible to a wider audience.\n\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "S1ew3XnmsS",
                "reply_to": "SylWjepycS",
                "title": "Response to Reviewer #1",
                "comment": "\n\n----------------------------\nQ1: My concern is whether the proposed method is overkill because the singular point detection can\nbe very tricky and relies on heavy linear programming.\n\nAnswer: The detection of singularities is direct and simple, for the convex polyhedron of the Brenier\npotential, just compute the inner product of the normals to each pair of adjacent facets. If the inner\nproduct is too big, then the projection of the intersection between the facets is in the singularity set.\nSo this work doesn\u2019t involve any linear programming at all.\n\n----------------------------\nQ2: Could you explain why not using the following substitute: Step 1. Fit an auto-encoder just as\nyou did in the paper and get an empirical distribution \u03bd. Step 2. Fit a Gaussian mixture model on \u03bd\nand do model selection over # clusters. Step 3. Sample from the Gaussian mixture model to generate\nfresh images.\n\nAnswer: We thank Reviewer #1 for the suggestions. The proposed approach is inspiring, but it has\npotential drawbacks:\n\u2022 If the empirical distribution has only one mode, but the support is concave, then the proposed\nmethod still can not avoid generating unrealistic samples.\n\u2022 If the empirical distribution has multiple modes, the resulting Gaussian mixture will fill\nthe gaps among the modes, therefore the proposed method still can not avoid generating\nunrealistic samples (mode mixture).\n\u2022 Fitting Gaussian mixture itself is expensive and without further assumptions, the convergence\nof the GMM fitting cannot be guaranteed.\n\nIn order to show the above claims, we did the following experiments: firstly we fit the 60K latent code\nof MNIST dataset by GMM, with the number of modes set to be 10, 30, 100. Then t-SNE is used to\nvisualize the data. The blue crosses are the generated data by the GMM model and the green circles\nare the training data. In the anonymous website https://drive.google.com/file/d/12HbiQNAoTpxnk-h10LY0O90j8QhSIKqw/view?usp=sharing, we provide the results: Fig. (a)(b)(c) show the generation results of GMM,\nfrom which we can see that there are huge number of generated samples in the regions among the\nmodes. While for the proposed method, as shown in Fig. (d), nearly no generated samples fall into\nthe gaps.\n\n----------------------------\nQ3: Since this method relies on a high-quality auto-encoder model, it is hard to say this paper\nmakes progress in fixing the GAN\u2019s mode collapsed problem. Besides, the paper does not involve an\nadversarial training module. So I will not treat it as a satisfactory improvement over GAN. Overall,\nthe proposed problem in GAN indeed exists. But the solution seems to deviate from the goal the\npaper aim to achieve.\n\nAnswer: The real goal of this work is to tackle mode collapse and mode mixture problems in general generative models, not only for GANs. Our work targets at analysis and improvement of generators in generic generative models, including VAEs and GANs. In fact, generators in these models tend to map a unimodal Gaussian to the complex data distribution, which will inevitably encounter the singularity problem proposed in our work. We thank the reviewer #1 for pointing out the ambiguity of our motivation. We have revised our abstract and introduction parts, which illustrate that the proposed AE-OT model solves the discontinuity problems encountered by both GANs and VAEs. Actually, in the original version of our paper, we have reviewed all the DNN based generative models in the related work part, and made comparisons with GANs, VAEs and other generated models in the experiment part. \nAccording to Figali\u2019s Fields medal work, it shows the intrinsic reason for mode collapse is the discontinuity of transportation map, caused by the concavity of the support of the data distributions. Based on this theoretic discovery, the AE-OT model is proposed. This model is not a conventional GAN model, but a novel generative model that exactly solves the main problems we are targeting at.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxxhH_wjS",
                "reply_to": "ryetcUV7iH",
                "title": "Response to Reviewer #4",
                "comment": "\n----------------------------\nQ1:  I have some doubts about moving from the \"semi-discrete OT map\" to the piece-wise linear extension.   The illustration in Fig. 3,  and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper.\n\nAnswer: Here we want to clarify that singular set detection is *piece-wise linear* separation, rather than *linear* separation. In Fig. 3(a), the singular set (shown in red lines) is illustrated by a piece-wise linear curve. Also, Fig. 6 of the appendix shows another example with the numerically computed singular set (also piece-wise linear) by our method, and it is much more curved and complicated.\n\n----------------------------\nQ2: Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better.\n\nAnswer:  As shown in the the last paragraph of Section 4.1, we conducted experiments of mode collapse on real images like stacked MNIST and CelebA on Section C.3 and C.4 of the appendix.\n\n----------------------------\nQ3:  Singular set detection seems to be the most tricky part in this paper, the Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies.\n\nAnswer: (1) There is no simplex projection assumption in our paper. In fact, Fig.3(a) illustrates the Brenier potential and the corresponding power diagram. The upper hyperplane envelope in top of Fig.3(a) is the graph of Brenier potential, and the bottom of Fig. 3(a) shows the source domain of the Brenier potential, expressed as a cell decomposition structure. Each facet in the image of Brenier potential corresponds to a cell in the source domain (\u2126), and the ridges on the image of Brenier potential corresponds to edges of cells in the source domain.\n\n(2) In the image of the Brenier potential, the \"sharp ridges\" are composed of the edges where the angles between the corresponding pairs of adjacent facets are large (as shown in Fig.3(a)). In fact, the normal of a facet n= (p_1, p_2,..., p_d, \u22121) actually corresponds to a latent code y= (p_1, p_2,..., p_d). And the large angle between two adjacent facets means that the distance between the corresponding latent codes is large.  This often happens when the codes come from different modes.  Thus, the singular set, or equivalently the \"sharp ridges\" gives the information about different modes.\n\n(3) Singular set detection is proposed in our paper for the following reason. Firstly, the singular set is totally decided by the semi-discrete OT map, or equivalently, the Brenier potential (Fig.  3(a)). Secondly, the image of the semi-discrete OT map itself is the given discrete latent code, thus we extend it with a piece-wise linear manner, so that the extended OT map can be used to *generate new codes* (Fig. 3(b)). Thirdly, the samples around the singular set will be mapped to the gaps among the modes by our extended OT map and cause the mode mixture problem, thus the singular detection is needed. Finally, given a sample x, if it falls around the singular set (checked by Alg. 2), we just don\u2019t use it to generate new latent code.\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkgkludPiH",
                "reply_to": "SJx3_vOPiH",
                "title": "Response to Reviewer #4 ",
                "comment": "\n----------------------------\nQ5: The authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view.\n\nAnswer: The main goal of this work is to tackle mode collapse and mode mixture problems in general generative models, not only for GANs. Our work targets at analysis and improvement of generators in generic generative models, including VAEs and GANs. In fact, generators in these models tend to map a unimodal Gaussian to the complex data distribution, which will inevitably encounter the singularity problem proposed in our work. We thank the reviewer #4 for pointing out the ambiguity of our motivation. We have revised our abstract and introduction parts, which analyze the discontinuity problems encountered by GANs and VAEs. Then we propose a new generative model called AE-OT. Actually, in the original version of our paper, we have reviewed all the DNN based generative models in the related work part, and made comparisons with GANs, VAEs and other generated models in the experiment part.\n\nBecause our main focus is to solve mode collapse/mixture problems, we didn\u2019t apply the most\nadvanced auto-encoder (AE). If the capacity of AE is insufficient, the result is not satisfying, such as\nthe celebA dataset noticed by the reviewer. But, as we explained in section 4.2, the 3rd paragraph, if\nthe capacity of AE is sufficient, our model outperform others.\n\nAs recent GAN improvements mostly focus on the discriminator, our work complements these\nworks by critically analyzing and making improvement on the generator. Future research on adding\nadversarial loss to our current model is also intriguing.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SJx3_vOPiH",
                "reply_to": "BkxxhH_wjS",
                "title": "Response to Reviewer #4 ",
                "comment": "\n----------------------------\nQ4: Singular set detection seems to be the most tricky part in this paper, which should have been explained further.\n\nAnswer: In the following, we justify our algorithm using the theoretic works summarized in the following book:\nFigalli, A. (2017). The Monge\u2013Amp\u00e8re equation and its applications.\n\nAccording to Brenier\u2019s theorem, the optimal transportation map T is the gradient of the convex Brenier potential u, and u satisfies the Monge-Amp\u00e9re equation.\nIn his book, the Fields medalist Figalli proved the existence and the uniqueness of the solution to the\nMonge-Ampere equation in Chapter 2, where he used Alexandrov\u2019s approach:\n1. Approximate the data distribution \u03bd to a sequence Dirac distributions \u03bdn, such that the sequence of {\u03bd_n} weakly converges to \u03bd;\n2. For each Dirac measure \u03bd_n, there exists an Alexandrov\u2019s solution u_n, which is exactly the discrete Brenier potential in our paper;\n3. The weak solutions {u_n} converges to the real solution u, u is C^1 almost everywhere, except at the singular set.\n\nOur Semi-Discrete OT algorithm is completely equivalent to Alexandrov\u2019s solution. In fact, the proof\nin Figalli\u2019s book is not constructive, (Alexandrov\u2019s original proof is based on Algebraic topology), which doesn\u2019t induce an computational algorithm. Therefore, the theorem 2 in the Appendix gives a variational framework to explicitly compute the discrete Brenier potential. By Figalli\u2019s work, the discrete Brenier potential {u_n} converges to the smooth Brenier potential, which is C^1 except at the singular set. The piece-wise linear map in Fig.3(a) converges to the real optimal transportation map.\n\nThe singular set is the non-differentiable points (only C^0 but not C^1) of the Brenier potentials, namely the ridges of the graph of u. This ridge structure becomes prominent and well-preserved in\nthe process of approximating u by piece-wise linear polyhedra {u_n} in Fig.3(a).\n\nCompared to Fig.3, Fig. 6 and Fig. 7 in the appendix gives better illustration for the singularity. The\noriginal version of Fig. 6 is given by Figalli as the Fig. 3.2 in the following article,\nFigalli, A. (2010). Regularity properties of optimal maps between nonconvex domains in the plane.\nCommunications in Partial Differential Equations, 35(3), 465-479.\n\nWe can see that the singular set has complicated geometric and topological structures, which can not\nbe captured by linear separation, but still can be found by *piece-wise linear approximation*. In\nfact, the optimal transport map shown in Fig. 6 is numerically computed by our algorithm, and the\nsingular set is piece-wise linear, approximating the singular set in the smooth case (shown as Fig. 3.2\nin the above mentioned article).\n\nNext we show that the singular set structure of the smooth Brenier potential is well preserved by\nour SDOT map. From chapter 2 in Figalli\u2019s book, we know that the piece-wise linear functions un\n(discrete Brenier potential) converges to the real smooth Brenier potential u, which is C^1\neverywhere except at the singular points. Therefore the graph of the smooth Brenier potential has ridges, these ridge structure are well preserved during the piece-wise linear approximation by the discrete Brenier\npotential. \n\nTherefore, singularity detection boils down to locate the ride structure of the graph of the discrete\nBrenier potential, which is a convex polyhedron. The ridge on a convex polyhedron can be easily\nfound by computing the angles between each pair of adjacent facets (dihedral angles for 2D case).\nBecause the discrete Brenier potential is convex, its projection induces a power diagram, each cell is\nconvex. The dual of this power diagram gives the power Delaunay triangulation of training samples\n(y_i\u2019s) (Fig. 2 of the following article). This geometric interpretation of semi-discrete OT doesn\u2019t\nrequire the linear separation assumption. The relation among discrete Brenier potential, power\ndiagram and power Delaunay triangulation is explained in details in\nGu, X., Luo, F., Sun, J., & Yau, S. T. (2016). Variational principles for Minkowski type problems,\ndiscrete optimal transport, and discrete Monge\u2013Amp\u00e8re equations. Asian Journal of Mathematics,\n20(2), 383-398. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Syeh0W3Xor",
                "reply_to": "r1e5aB-z9r",
                "title": "Response to Reviewer #3",
                "comment": "\n\n----------------------------\nQ1: Although this paper brings a new perspective, based on optimal transport theory, as far as I can\nunderstand this paper does not establish formal new results. Thus I think some strong claims about\nproviding deep theoretical explanation should be more moderate. In essence, it seems that the paper\nverifies *numerically* (in section B.3) that Figalli\u2019s theorem (stated in Appendix B) holds in this\ncontext.\n\nAnswer: This work focuses on using Figalli\u2019s regularity theory of Optimal Transportation Map to\nexplain mode collapse/mixture in generative models and propose a novel model to tackle it, not to\ndevelop the new regularity theorems. We will follow the reviewer\u2019s suggestion to make our claims\nmore moderate.\n\n----------------------------\nQ2: This is just a suggestion. I think in some parts a lighter notation and a more intuitive explanation\ncould help.\n\nAnswer: We will follow reviewer\u2019s suggestion to add more intuitive explanations and simplify the\nnotations.\n\n----------------------------\nQ3: After Eq. (5) in the Appendix the authors mention Newton\u2019s method, and Thm 3 is also specific\nto Newton\u2019s method. Then they mention that *Gradient Descent* is used (and in the main part of the\npaper they mentioned Adam). This is confusing. All these algorithms are different, and Newton\u2019s\nmethod does not imply convergence results for gradient descent. I don\u2019t see how Thm 3 is relevant.\n\nAnswer: According to the variational framework of semi-discrete optimal transportation map,\ntheorem 2, the computation of OT map is reduced to a convex optimization. Hence both gradient\ndescend and Newton\u2019s method converge. We will modify Thm 3 accordingly. Furthermore, this work\nfocuses on gradient descend method, in the future work, we will explore Newton\u2019s method as well.\n\n----------------------------\nQ4: This is a simple doubt. To avoid non-differentiability of the gradient, the OT step computes the\nBrenier potential and is able to locate the singularities. I wonder if using a simpler approach through\noptimization for nosmooth problems (such as Moreau envelopes or proximal methods) could resolve\nthis issue? In the negative case, why not?\n\nAnswer: Although the OT map is discontinuous, and the Brenier potential is non-differentiable, the \nenergy to be optimized is C^2 in terms of h. Therefore, in the current work, for the optimization\npurpose it is unnecessary to use Moreau envelope or proximal methods. Specifically, the convex\nenergy E(h) we aim to optimize is differentiable with respect to h. With the optimal h, the OT map\ncan be induced. Therefore for the optimization, we actually do not need smoothing techniques to carry\nout the optimization. Secondly, the non-differentiability of Brenier\u2019s potiential uh(x) is considered\nwith respect to x given the optimal h. This is independent of the optimization process.\n\n----------------------------\nQ5: Some Minor comments: 1. Define OT in the abstract (Optimal Transportation?) 2. What is AE?\n(not defined also; Auto Encoder?) 3. There are lots of typos through the text, such as missing \"the\",\n\"a\", etc. and a couple mispelled words. I suggest the authors proofread the draft more carefully. 4.\npp. 4 ... what is a \"PL convex function\". PL is not defined.\n\nAnswer: We thank the reviewer 3 for the comments. In the paper, OT represents optimal transport,\nAE means autoencoder and PL is the abbreviation of piece-wise linear. We will add more explains to\nthe abbreviations and find native speakers to help proofread the updated manuscript.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SylGfl3XiH",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Thanks for your careful comments",
                "comment": "We thank reviewers for carefully examine our work in such a short time.  Since our work involvesnon-trivial theories from optimal transportation, such as the brand new theorems of Figalli, andregularity theorems for Monge-Ampere equation, the review requires huge amount of efforts.  Wedeeply appreciate all reviewers from deep of our hearts. Since all the reviews and rebuttals will bepublic online, we prepared our rebuttal with great caution, and addressed all the questions raised byreviewers carefully",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ryetcUV7iH",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Official Blind Review #4",
                "comment": "This paper deals with an important problem of mode collapse and mode mixture. In order to\ntackle the both problems, the paper proposes to separate the manifold embedding and the\noptimal transportation problems; the first part being carried out using an autoencoder to map the\nimages onto the latent space and the second part is accomplished using a GPU-based\nconvex optimization to find the discontinuous transportation maps.\n\nI have some doubts about moving from the \"semi-discrete OT map\" to the piece-wise linear extension. The illustration in Fig. 3, and implicit in all the explanation charts is the fact that discontinuity can be found by a linear separation. This seems to be an extremely simplifying assumption, which leads to not so great visual results from the paper. Although the numerical results seems promising, I feel that fewer images, but larger in size, and analysis of mode collapse phenomenon in real images would have been much better.\n\nSingular set detection seems to be the most tricky part in this paper, which should have been explained further. The Simplex projection assumption, renders this part not that tricky, but that is where I feel the biggest doubt about this paper lies.\n\nThe authors themselves mention the need for a high quality auto encoder model to encode celebA dataset, which has been improved upon by numerous other papers, the claims seems not too strong. Also, the method does not have any adversarial training and hence, it studies the GAN idea from only fixing the generator point of view. ",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "SylWjepycS",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Official Blind Review #1",
                "comment": "Contributions:\n1. This paper proposes a new problem in GAN distribution mapping: the concavity of support problem.\n2. This paper provides a solution to the concave support together with mode collapsed problem in GAN, via a discrete-continuous optimal transport model, given some post-processing techniques to rule out \"singular points\".\n3. Empirical results show the effectiveness of the proposed method.\n\nTo summarize their method. First, they fit a good auto-encoder model to get embeddings for the observed data as an empirical distribution \\nu on space Z. Second, they use a semi-discrete OT to map a noise distribution \\mu to \\nv. Since OT will be aware of all modes in \\nu, singular points can be detected by checking the angle between \"shards\" and those points that are around the \"ridge\" should be rejected. Thus, the proposed method could handle both the concave support problem and the mode collapse problem.\n\nMy concern is whether the proposed method is overkill because the singular point detection can be very tricky and relies on heavy linear programming. Could you explain why not using the following substitute: \nStep 1. Fit an auto-encoder just as you did in the paper and get an empirical distribution \\nu.\nStep 2. Fit a Gaussian mixture model on \\nu and do model selection over # clusters.\nStep 3. Sample from the Gaussian mixture model to generate fresh images.\n\nSince this method relies on a high-quality auto-encoder model, it is hard to say this paper makes progress in fixing the GAN's mode collapsed problem. Besides, the paper does not involve an adversarial training module. So I will not treat it as a satisfactory improvement over GAN. Overall, the proposed problem in GAN indeed exists. But the solution seems to deviate from the goal the paper aim to achieve.",
                "rating": 3,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1e5aB-z9r",
                "reply_to": "iclr_2020_HkldyTNYwH",
                "title": "Official Blind Review #3",
                "comment": "General Comments:  The generator in Generative Adversarial Networks (GANS) computes an optimal transportation from the noise distribution to the data distribution.  However, such maps are in general discontinuous.  Since deep neural networks can only represent continuous maps, this brings two problems: mode collapse and mode mixture. This paper approaches both problems using Figalli's regularity theory. They separate the manifold embedding (here an autoencoder maps input data to a latent space) from the optimal transportation (this map is found by convex optimization). Composing these two steps yields the proposed method. Their method basically avoids representing discontinuous maps by the generator. Empirically, the proposed method performs similar or better than state-of-the-art.\n\nI think the idea of the paper is nice, and an interesting perspective  on GANs is presented. A new method is proposed. The numerical contributions are certainly significant. Therefore, I believe the paper deserves publication.\n\nNevertheless, I have some comments below.\n\n1) Although this paper brings a new perspective, based on optimal transport theory, as far as I can understand this paper does not establish formal new results. Thus I think some strong claims about providing deep theoretical explanation should be more moderate. In essence, it seems that the paper verifies *numerically* (in section B.3) that Figalli's theorem (stated in Appendix B) holds in this context.\n\n2) This is just a suggestion. I think in some parts a lighter notation and a more intuitive explanation could help.\n\n3) After Eq. (5) in the Appendix the authors mention Newton's method, and Thm 3 is also specific to Newton's method. Then they mention that *Gradient Descent* is used (and in the main part of the paper they mentioned Adam). This is confusing. All these algorithms are different, and Newton's method does not imply convergence results for gradient descent. I don't see how Thm 3 is relevant.\n\n4) This is a simple doubt. To avoid non-differentiability of the gradient, the OT step computes the Brenier potential and is able to locate the singularities. I wonder if using a simpler approach through optimization for nosmooth problems (such as Moreau envelopes or proximal methods) could resolve this issue? In the negative case, why not?\n\n5) Some Minor comments:\n1. Define OT in the abstract (Optimal Transportation?) \n2. What is AE? (not defined also; Auto Encoder?)\n3. There are lots of typos through the text, such as missing \"the\", \"a\", etc. \nand a couple mispelled words. I suggest the authors proofread the draft\nmore carefully.\n4. pp. 4 ... what is a \"PL convex function\". PL is not defined.\n",
                "rating": 8,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "This",
                "Sentiment Expression": "interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the results",
                "Sentiment Expression": "low rating",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the concerns of the reviewers",
                "Sentiment Expression": "addressed",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the original concerns of the reviewers",
                "Sentiment Expression": "addressed",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "be accepted",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "motivating the problem",
                "Sentiment Expression": "should be a bit more clear",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "accessible to a wider audience",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "BJzuKiC9KX": {
        "paper_id": "iclr_2019_BJzuKiC9KX",
        "paper_title": "Revisiting Reweighted Wake-Sleep",
        "paper_abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow. \nThe work is well-written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables.\nThe reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger-scale real-world data and model would provide a more complete and convincing evidence. \nThe paper should be made stronger on at least one (and ideally both) of these accounts.\n\n",
        "meta_review_title": "Interesting empirical observations of the advantage of RWS, but lacking formal theoretical analysis, and larger scale experiments",
        "reviews": [
            {
                "review_id": "SJxOX9_p14",
                "reply_to": "SyeZgH_T1E",
                "title": "Experiments",
                "comment": "Note that our claim is not based only on the GMM experiment. It is also backed up by results from training (i) a VAE with continuous latent variable on MNIST data (compared against IWAE since VIMCO is not needed) and (ii) the AIR model on moving MNIST data (compared against VIMCO; VQ-VAE not applicable).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SyeZgH_T1E",
                "reply_to": "Skx2_YQ6kV",
                "title": "Real world experiments",
                "comment": "I second the reviewer suggestion of real-world experiments. Improvements on toy data-sets like GMMs do not necessarily transfer over to real world data. And if the authors make the claim that  \n\"Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models\", then they should back it up with results that match or improve state-of-the-art generative models like VQ-VAE/VIMCO in bits/dim on large scale, real data-sets.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Skx2_YQ6kV",
                "reply_to": "r1gDQE7nJ4",
                "title": "Thank you for reconsidering our paper",
                "comment": "There are two reasons why we think RWS should be used based on solid (although simple) theoretical reasoning. First, in discrete latent variable models, we don\u2019t need reparameterization, continuous relaxation or control variates. Second, in all models, RWS can be helpful in light of the \u201ctighter variational bound is not better\u201d effect. The GMM and AIR experiments support both points. The continuous VAE (on MNIST data) gives further evidence for the second point.\n\nWhy not compare to RBM and DVAE? We agree that even more evidence would be good. However:\n- For RBMs, this is an entirely different class of models (the joint density p(z, x) can be evaluated only up to a normalizing constant, instead of directly), which is not learnable using RWS or other ELBO-maximizing approaches (learning RBMs requires contrastive divergence or similar). \n- For DVAE, in addition to being slightly different in focus due to branching, it is also orthogonal in another way. DVAE can be used in conjunction with IWAE to tighten the bound. We show through the continuous VAE experiment that RWS can help. It might be interesting to see whether RWS can be used on the continuously relaxed model defined in DVAE in order to improve DVAE further.\n\nWhy not test on real-world data? We agree that the transfer from synthetic to real-world data is difficult. Our paper is methodological and RWS is in its core a statistical method for maximizing the log marginal likelihood and minimizing the KL divergence from p to q. Whether a model works on a real dataset is not a function of the learning and inference algorithms, but rather the particular generative model and inference network. This is also true for other learning and inference algorithms (like VAE, IWAE, REBAR, RELAX, DVAE, etc.).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJxxyGwinm",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "An interesting experimental paper but more insights are expected",
                "comment": "Main idea:\nThis paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by  Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models.   \nTo support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM.\n\nClarity issues:\n1. \"branching\" has been used many times, but AFAIK, this seems not a standard terminology. What do \"branching on the samples\", \"conditional branching\", \"branching paths\" mean?\n2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence \n\"the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x\". \nHowever, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. \n\nQuestions:\n1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))?\n2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards?  \n3. In Fig 4, why WS doesn't achieve a better performance when K increasing?\n\nExperiments:\n1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. \n2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. \n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1gDQE7nJ4",
                "reply_to": "rJxb9BFYaQ",
                "title": "Somehow convinced but still expect more experiments",
                "comment": "\" the resulting discreteness cannot be used for directing the control flow\"\n\nAt least RBM doesn't need any continuous relaxation in training. \nTo test on those discrete applications which can be continuously relaxed during training is also important. It offers us a better understanding of when RWS should be applied.  I don't think your reasons are valid to not compare with RBM, DVAE etc. \n\nI still insist on real dataset/tasks, e.g. semantic segmentation, since there is always a gap between synthetic world and real world.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SylgzBTohX",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "Revisiting Reweighted Wake-Sleep",
                "comment": "This manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS.\n\nWhile this paper investigates an important problem, and also offers interesting observations, it lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. More precisely, the propositions should be stated in more formal language and they should be accompanied with a minimal rigorous justification.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rJxb9BFYaQ",
                "reply_to": "HJxxyGwinm",
                "title": "Response to AnonReviewer2",
                "comment": "Clarity issues:\n\nBy stochastic branching we refer to the evaluation of generative models where discrete latent variables are used to select which part of the model is going to be evaluated next. For example, in AIR, this decides when the program halts, in GMM the cluster index decides which likelihood function is evaluated. Another example of this are probabilistic context-free grammars (PCFGs) where discrete variables are used to describe which production rule is used (for example https://arxiv.org/abs/1806.07832). This is in contrast with modeling approaches where the discrete latent variable is merely an input to a neural network that doesn\u2019t distinguish it from a non-discrete latent variable since it does not explicitly use the discreteness to model distinct modes of the data. (also see our general comment)\n\nWe will clarify the \u201czero-forcing\u201d failure mode and delta-WW in the updated manuscript.\n\nQuestions:\n\nTo estimate KL(q(z|x) || p(z|x)), we take the difference of the log likelihood estimated by a 5000-particle IWAE bound and the ELBO estimated by 5000 Monte Carlo samples.\n\nThe statement that \u201cIWAE learns a better model only up to a point\u201d is justified by the IWAE curve in the middle of Figure 2: the decreasing slope indicates that improvements in marginal log probability decrease with increasing numbers of particles. This is even more pronounced in Figure 1, where IWAE performance decreases for k > 10.\n\nIn Fig 4, WS actually does achieve better performance as K increases - the final value of the learning curve goes down, although only very slightly.\n\nExperiments:\n\nRegarding experiments, RBM/DVAE/++/# and VQ-VAE allow learning models with discrete latent variables in general; however, the resulting discreteness cannot be used for directing the control flow of a generative model (see also response to AnonReviewer3 and our general comment).\n    - In the DVAE family of algorithms, learning in discrete latent variable models is achieved by a continuous relaxation. This prevents using these variables as hard branching conditions.\n    - In the VQ-VAE algorithm, the discrete latent variable is explicitly designed to be used to select an embedding and it is deterministic. This limits the use of a discrete latent variable (cannot be used to model a cluster identity or stopping of a while loop). \n\nEven though we do not have experiments on large-scale real-world datasets, AIR is a non-trivial model, and using it can be seen as a large-scale experiment - taking several days (and several GPUs) to obtain results summarized in Figure 1. Similarly, to the best of our knowledge, ours is the first reported result of an MNIST model trained with IWAE with 512 particles.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJeBGBtFaQ",
                "reply_to": "SylgzBTohX",
                "title": "Response to AnonReviewer1",
                "comment": "The key formal justification is relatively straightforward: RWS, unlike IWAE, does not suffer from the \u201ctighter bounds\u201d problem. On the contrary, RWS uses self-normalized importance sampling to estimate the gradient with respect to \\phi. Both the asymptotic bias and variance of a self-normalized importance sampling estimator decrease linearly in number of particles. This means that increasing number of particles improves our gradient estimator and thus the optimization procedure.\n\nWe will explain this in more detail in the updated manuscript.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BylkLVYKa7",
                "reply_to": "BJepNja0hQ",
                "title": "Response to AnonReviewer3",
                "comment": "Baselines: As set out in our overall response, we aim to show that RWS is a better choice for inference in models that have stochastic control flow, where the choice from the discrete latent variables matters explicitly. In our GMM example, the cluster identity is such a choice, and in AIR, the stopping condition for the loop is another such choice. The work done in DVAE++, DVAE#, and other such approaches do not really handle this general class of problems well---by typically requiring enumeration of all possible branches and choices.\n\nGMM: We will include a more detailed description of how defensive sampling ameliorates issues discovered in the GMM experiments in the updated manuscript.\n\nTheoretical Rigour: We will include a more comprehensive discussion of the theoretical basis of why RWS is better than IWAE in the updated manuscript. Briefly, the justification for why RWS does not suffer from the \"tighter bounds\" problem is due to RWS's use of self-normalised importance sampling to compute the gradient of proposal parameters---resulting in both the asymptotic bias and variance decreasing linearly with number of samples.\n\nEmpirical Rigour: Our experiments strongly support our hypotheses:\n  a. Unlike IWAE, RWS performs better with more particles, both in terms of the generative model and inference network, and\n  b. It allows for effective and easy application to models where the choice from the discrete random variables affects model expansion or computation---something that requires expensive enumeration with continuous relaxations, or extremely finicky and unreliable construction with control-variate methods.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Syg9R7FF6m",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "Discrete latent variables, stochastic control flow and probabilistic programming",
                "comment": "We thank the reviewers for their time and for appreciating our arguments about why RWS is preferable to IWAE-based approaches as we increase number of particles.\n\nWe would like to re-emphasize an important implication from our paper: RWS is a good inference algorithm for models that have _stochastic control flow_: i.e. models where latent variables can be instantiated dynamically, and different branching paths explored, based on the choice from a random variable[4-7]. Note that this is orthogonal to a large number of recent work that employ discrete latent variables in deep generative models[8-10]---where either the discrete variable is transformed via continuous relaxations, or, marginalized out entirely. The crucial difference is that none of these approaches address models where the model execution itself is determined via the discrete choices (if-statements or for-loops), as opposed to simply passing it through to a neural network---what is done with the discrete choice matters.\n\nThis is best illustrated in the domain of universal probabilistic programs (like Pyro [1]) which can contain arbitrary continuous and discrete latent variables, and where latent variables are instantiated dynamically and defined by running the program (or generative model). Stochastic control flow is a feature of such models [2] and allows the definition of expressive models, with potentially infinite number of latent variables [3], as mentioned in the discussion.\n\nUniversal (or higher-order) probabilistic programs form the largest family of samplable distributions and thus are a powerful tool to model data. Amortized inference and model parameter learning in such probabilistic programs, however, is typically only done using variational methods in the VAE/IWAE family of algorithms (as summarized in our manuscript). We\u2019re trying to say: RWS is a simple and often superior algorithm to use in this model family.\n\nOur point about probabilistic programs, hard selection and stochastic branching is illustrated by our choice of experiments (GMM and AIR). However, we will more strongly emphasize this point in the updated manuscript.\n\n[1] http://pyro.ai/\n[2] http://pyro.ai/examples/intro_part_i.html#Universality:-Stochastic-Recursion,-Higher-order-Stochastic-Functions,-and-Random-Control-Flow\n[3] Quote from [2]\n    \"For example, we can construct recursive functions that terminate their recursion\n     nondeterministically, provided we take care to pass pyro.sample unique sample names\n     whenever it\u2019s called.\"\n[4] GMM models in this work\n[5] Tree-structured latent variables in https://arxiv.org/abs/1806.07832\n[6] Memory-based models in https://arxiv.org/abs/1709.07116\n[7] AIR-like models in this work and https://arxiv.org/abs/1806.01794\n[8] DVAE++ - https://arxiv.org/abs/1802.04920\n[9] DVAE#  - https://arxiv.org/abs/1805.07445\n[10] VQ-VAE - https://arxiv.org/abs/1711.00937\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJepNja0hQ",
                "reply_to": "iclr_2019_BJzuKiC9KX",
                "title": "Nice experimental discoveries",
                "comment": "This paper conducts an extensive set of experiments on RWS and compares it against a set of benchmarks such as GMM and IWAE. The main contribution of the paper is the fact revealed by these experiments, that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well. The performance of RWS will increase significantly if we increase the number of particles. \n\nThe experimental part is written in an inspiring way, and I enjoyed reading it. However, there should be stronger baselines incorporated. for example, https://arxiv.org/abs/1805.07445. Also, I think the authors could try to emphasize more on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. There are several other parts in the paper that indicates interesting facts, diving deeper into it could possibly lead to more interesting findings.\n\nIn all, I would consider these comparison results important to be somewhere in the literature, but because its lack of rigorous analysis and explanation for the observations, I personally think these observations alone are not novel enough to be an ICLR paper. \n",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "empirical study of the Reweighted Wake Sleep (RWS) algorithm",
                "Sentiment Expression": "well conducted",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "it",
                "Sentiment Expression": "performs consistently better",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The work",
                "Sentiment Expression": "is well-written and extracts valuable insights",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "weaknesses of the work",
                "Sentiment Expression": "note",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "3R--2TdxMps": {
        "paper_id": "iclr_2021_3R--2TdxMps",
        "paper_title": "Defuse: Debugging Classifiers Through Distilling Unrestricted Adversarial Examples",
        "paper_abstract": "With the greater proliferation of machine learning models, the imperative of diagnosing and correcting bugs in models has become increasingly clear. As a route to better discover and fix model bugs, we propose failure scenarios: regions on the data manifold that are incorrectly classified by a model. We propose an end-to-end debugging framework called Defuse to use these regions for fixing faulty classifier predictions. The Defuse framework works in three steps. First, Defuse identifies many unrestricted adversarial examples--naturally occurring instances that are misclassified--using a generative model. Next, the procedure distills the misclassified data using clustering into failure scenarios. Last, the method corrects model behavior on the distilled scenarios through an optimization based approach. We illustrate the utility of our framework on a variety of image data sets. We find that Defuse identifies and resolves concerning predictions while maintaining model generalization.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The manuscript describes a method for identifying and correcting classifier performance when labels are assigned incorrectly. The identification is based on clustering classification failure regions in a VAE latent space and the correction phase is based on fine-tuning the classifier with additional synthetic samples from the VAE.\n\nReviewers agreed that the manuscript is not ready for publication. The main issue is that the suggested training method is similar to adversarial training methods used to gain adversarial robustness. The method does not help in debugging and fixing failures in general.\n",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "-oKqABtZVlB",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "The paper describes a technique for debugging classifiers through distilling unrestricted adversarial examples. ",
                "comment": "The technique is described in sufficient detail and the paper is easy to read. Experimental results involving three datasets: MNIST, street view house numbers, and German traffic signs. The experimental results show that the proposed technique finds significant failures in all datasets, including critical failure scenarios. After correction, the performance of the method improves. \nAn interesting aspect of the method, which distinguishes it from similar techniques, is involvement of users/experts in the training process to indicate the classification errors in order to improve the performance of the method in the future. Engaging users in the training of classifiers has its advantages and disadvantages. For example, it can make easier to create \u201cpersonalised\u201d classification models that could be applied, e.g. in recommender system or information retrieval, where finding a perfect item depends on user\u2019s subjective perception of certain qualities. At the same time, user involvement in the training process can be tricky if it requires expert judgment as they may not always be available (as the authors demonstrated in the case of their third dataset consisting of German traffic signs). Further, involving user generated assessments requires well defined procedures in terms of requirement of assessors, determining the appropriate number of assessors, resolving disagreements between assessors, to ensure robustness of the final classifier. In the examples provided in the paper, the authors state that they used 5 workers (annotators) and the majority vote was used to decide the final label. What was the inter-annotator agreement? Since using human labellers is a crucial part of the proposed method, I would like to see more discussion of this aspect.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rZKvxsvSJdt",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "An interesting idea, but experiments and analysis do not support it as a significant contribution",
                "comment": "The paper proposes a method to identify and correct regions on the data manifold in which a trained classifier fails. The *identification* phase is based on clustering classification failure regions in a GAN latent space and the *correction* phase is based on fine-tuning the classifier with additional synthetic samples from the GAN.\n\nThe proposed method is strongly based on Zhao et al 2018 (Generating Natural Adversarial Examples), a method to generate on-manifold black-box adversarial examples using a GAN. The authors of the current paper describe some differences of their identification step from Zhao et al (end of section 3.2.1), but in my opinion they are minor.\n\nThe main contribution of the current paper over Zhao et al seems to be clustering the adversarial examples (using GMM) and using them to fine-tune the classifier. This, in my opinion, is potentially an interesting idea, however, the authors do not show sufficient evidence of its success. Specifically, the authors claim to \"achieve near perfect failure scenario accuracy with minimal change in test set accuracy\", but they do not provide any details (e.g. table of accuracy values on the train, test and adversarial sets before and after the fine-tuning). I would also expect to see an ablation study comparing the proposed method to simply including the adversarial examples found using Zhao et al (w/o GMM fitting and sampling) as additional training example - a standard adversarial defense approach (see e.g. [1]).\n\nPerhaps more importantly, the objective of the proposed method is not, in my opinion, clear. The title and abstract describe the goal as \"debugging\" a classifier and correcting fail regions, however the described method seems like a defense against on-manifold adversarial attack. If the method, as claimed, helps debugging and correcting the classifier, I would expect to see an improved accuracy on the (natural) unseen test set - not just on the synthetically generated adversarial examples.\n\nThe quality and clarity of the writing can be improved as well. A lot of space is allocated to describing well-known methods (e.g. VAE, GMM), however, critical information about the experimental results are missing. I'm also not sure all the formally defined algorithms and equations actually help in the understanding (e.g. algorithm 1, equation 2). Some of the mathematical notations are not standard.\n\nMinor comment: The norm in definition 3.1 is a regular vector norm (l2?) and not a matrix norm.\n\nTo summarize:\n\npros:\n- interesting idea (clustering on-manifold failures, labeling them and then using them to improve the classifier)\n\ncons:\n- contribution over Zhao et al not well established\n- insufficient and inaccurate experimental results\n- general quality of writing\n- not sure actual work and experiments match the stated objective\n- significance\n\n*Update:* Following the authors' response, I upgraded my rating, but I still think there are critical issues with the paper. The most problematic point, in my opinion, is the only-marginal improvement on the test data, indicating that the suggested training method only improves the specific \"failure scenarios\", making it is similar to adversarial training methods used to gain adversarial robustness. However, the abstract and introduction indicates that the paper helps in debugging in fixing failures in general, which, I think should have been evident in improved test accuracy.\n\n[1] Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" ICML 2019",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "R6Z858ZdVcb",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "Author response",
                "comment": "We thank all the reviewers for their useful comments.  We have responded individually to the reviewers below and made substantial changes to the paper. In summary we:\n\n1. Include more Defuse experimental details into the paper in section 4.1.  We additionally provide greater justification for our parameter and model choices in section 4.1 and in regards to the GMM in section 3.2.2.\n2. Reduce the description of known methods (VAE+GMM) and focus the writing more on our contributions.\n3. Provide more samples from the failure scenarios in figure 3 in the main text.  We emphasize one of our main contributions is the identification of failure scenarios.  The failure scenarios are useful because they summarize the unrestricted adversarial examples into key failure trends in the model.  We bring greater emphasis to this in the paper by highlighting more examples.\n3. Compare Defuse to fine tuning on the unrestricted adversarial examples as a baseline (per Reviewer 3's recommendation).  We highlight the Defuse finetuning and baseline results in tabular form in figure 4.  We see that Defuse improves the accuracy on the failure scenarios considerably compared to before finetuning and the baseline.\n4. Provide analysis of the annotator agreement in section 4.4.  We see the annotators voted for the majority label on average 85.2% and 82.1% of the time for MNIST and SVHN respectively across the annotated unrestricted adversarial examples. \n\nWe also note we had an issue with data storage that effected the MNIST experiments.  We thus reran these experiments with minimal change to our final results.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SUnCTtW-RMY",
                "reply_to": "rZKvxsvSJdt",
                "title": "Response",
                "comment": "We thank the reviewer for their useful comments and interest in the work.  In response to the reviewer\u2019s comments we\u2019ve revised a number of aspects of the paper. \n\n\u201c..but they do not provide any details (e.g. table of accuracy values on the train, test and adversarial sets before and after the fine-tuning)\u201d\n \n- While we previously provided these values in graphical form (what is now figure 5), we provide such a table in figure 4 of the updated paper to more easily parse the results.\n\n\u201cI would also expect to see an ablation study comparing the proposed method to simply including the adversarial examples found using Zhao et al\u201d\n\n- This is a useful point of comparison and thank the reviewer for the suggestion.  We add results finetuning only on the unrestricted adversarial examples.  The results can be found in figure 4.  We find the accuracy on the failure scenario testing data is considerably higher using Defuse than finetuning on the unrestricted adversarial examples.\n\n\u201cPerhaps more importantly, the objective of the proposed method is not, in my opinion, clear.\u201d \n\n- The objective of our work is to systematically find and correct model bugs. Defuse helps to do this through both identifying trends in misclassified data and offers a route to correct the predictions on such data.  See for instance figure 3 in our paper. In the upper right hand corner, a certain style of skinny 6\u2019s are misclassified as 1\u2019s.  This result is insightful for a model designer because it indicates the model struggles with very skinny numbers. Further, our finetuning results demonstrate we can adequate correct the model predictions on these mistakenly classified data indicating Defuse also successfully corrects the fault predictions.\n\n\u201cI would expect to see an improved accuracy on the (natural) unseen test set - not just on the synthetically generated adversarial examples.\u201d\n\n- The test set accuracy marginally improves for MNIST and marginally decreases for SVHN and German signs (figure 4).  We point out however that the important aspect of our work is that accuracy on the failure scenarios (which we have confirmed are model bugs through human verification) are corrected.  We see this is the case with Defuse.\n\n\u201cThe quality and clarity of the writing can be improved as well..\u201d\n\n- The reviewer is right to point out there a number of places to improve.  We have reduced the emphasis on VAE + GMM background and added more experimental details.  We have additionally moved the psuedo code for the algorithms to the appendix.\n\nAs a minor note, we use VAE\u2019s to perform all our experiments and do not use GANs as the reviewer indicates.  We would appreciate any further response the reviewer has to the above comments and revisions. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "L7JwiOoyX65",
                "reply_to": "-oKqABtZVlB",
                "title": "Reponse",
                "comment": "We thank the reviewer for their comments. The reviewer is right to point out that inter annotator agreement is an important aspect to consider.  We add additional details to our annotation process in section 4.1.  Further, we add section 4.4 describing the annotator agreement.  Please see the response to the first reviewer in regards to these details. \n\nWe would appreciate any further questions or comments on the new annotator results.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "JSLF3PgD8QZ",
                "reply_to": "EDYhz-YIQb-",
                "title": "Response",
                "comment": "We thank the reviewer for their response and comments and appreciate the interest in the work. In response to the reviewer\u2019s points, we\u2019ve significantly improved the experimental detail in the paper. \n\nThe choice of certain algorithms and their parameters needs to be justified clearly:\n\n- We better justify the use of the Dirichlet process GMM in section 3.2.2. We point out there are two main requirements with our approach.  First, we must be able to infer the number of clusters from the data.  Second, we must be able to sample new instances from each of the clusters.  Both these requirements greatly limit the clustering approaches we can use. We use the Dirichlet process GMM because the dirichlet process nicely models the clustering problem where the number of clusters in unknown ahead of time satisfying our first criteria.  Additionally, we can sample new instances from the clusters satisfying our second criteria.  Though it could be possible to propose other bayesian clustering methods that meet both these criteria, we focus on evaluating Defuse with this particular choice of clustering method and leave evaluating other clustering methods up to future work.\n\nThe paper should be rewritten to have sufficient details of experiments in the text rather than delegating them to the Appendix A:\n\n- We\u2019ve moved many of the Defuse details from the appendix to section 4.1.  We additionally add further justification to our parameter choices.\n\nThe motivation of why certain parameters are chosen for experiments should be discussed. For example, \"we sample 10 instances from each cluster in the distillation step. We ask 5 workers to label the instance\" -- Why are these choices appropriate?\n\n- We\u2019ve added additional justification for our annotation choices in section 4.1.  In addition, we\u2019ve better motivated our parameter choices throughout section 4 in general. In response to this specific question, it is usually apparent the classifier disagrees with many of the ground truth labels within 10 instances, and thus, it is appropriate to label the cluster as a failure scenario.  For example, in figure 3 it is generally clear the classifier incorrectly predicts the data within only a few examples. Thus, 10 instances is a reasonable choice.  We ask 5 workers to label the instance in order to reduce the noise in the annotation process. \n\nDescription of the annotation task ought to be more detailed -- \"labeling them ourselves\" -- Who constitutes \"ourselves\"? \n\n- For MNIST and SVHN, we use annotator labels.  For German signs, we, the authors reviewed and assigned the failure scenarios.  Though this is less rigorous than the MNIST and SVHN experiments, it is still useful to see the classifier bugs exposed with Defuse.\n\nWhat is the agreement between the annotators?\n\n- We add section 4.4 describing the annotator agreement during labeling.  We generally find the annotators were in agreement about the labels.  For MNIST, the annotators voted for the majority label on average 85.2% of the time and 82.1% for SVHN over all the unrestricted adversarial examples.\n\nWe would appreciate any further reviewer comments or questions from the reviewer to the above responses and revisions. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EDYhz-YIQb-",
                "reply_to": "iclr_2021_3R--2TdxMps",
                "title": "The authors present DEFUSE a system for debugging classifiers using adversarial examples",
                "comment": "The authors present a system DEFUSE which is geared towards identifying and correcting classifier performance when labels are assigned incorrectly. There are three phases that are used to design DEFUSE: (1) Identify unrestricted adversarial examples using Variational Auto Encoders (2) Use a clustering approach to distill the above examples into failure scenarios and (3) Correct the classifier predictions.\n\nOverall, the idea of using adversarial examples to correct incorrect classifications is very interesting. \n\nThe choice of certain algorithms and their parameters needs to be justified clearly. While it is understandable that a non-parametric model be used for the clustering step, it it not clear why a dirichlet process is the best fit. How does this choice compare with other clustering approaches? Do the results generalize?\n\nThe paper should be rewritten to have sufficient details of experiments in the text rather than delegating them to the Appendix A. \n\nThe motivation of why certain parameters are chosen for experiments should be discussed. For example, \"we sample 10 instances from each cluster in the distillation step. We ask 5 workers to label the instance\" -- Why are these choices appropriate? \nDescription of the annotation task ought to be more detailed -- \"labeling them ourselves\" -- Who constitutes \"ourselves\"? What is the agreement between the annotators?\n\nMinor comments:\n\n1. Section 3.2: how we identity -> how we identify \n2. Section 3.2.3: The paragraph ends with \"For instance.\" The sentence needs to be completed and an example provided.\n3. Section 4.1: 32x32 should be replaced with 32X32. Similarly 128x128 should be replaced with 128X128",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the manuscript",
                "Sentiment Expression": "is not ready for publication",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the suggested training method",
                "Sentiment Expression": "is similar to adversarial training methods used to gain adversarial robustness",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "The method",
                "Sentiment Expression": "does not help in debugging and fixing failures in general",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "HJe6uANtwH": {
        "paper_id": "iclr_2020_HJe6uANtwH",
        "paper_title": "Capsules with Inverted Dot-Product Attention Routing",
        "paper_abstract": "We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. \n      The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing.\n      When compared to previously proposed routing algorithms, our method improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100, and it performs at-par with a powerful CNN (ResNet-18) with 4x fewer parameters.\n      On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given the same number of layers and neurons per layer.  We believe that our work raises the possibility of applying capsule networks to complex real-world tasks.",
        "paper_acceptance": "accept-poster",
        "meta_review": "This work presents a routing algorithm for capsule networks, and demonstrates empirical evaluation on CIFAR-10 and CIFAR-100. The results outperform existing capsule networks and are at-par with CNNs. Reviewers appreciated the novelty, introducing a new simpler routing mechanism, and achieving good performance on real world datasets. In particular, removing the squash function and experimenting with concurrent routing was highlighted as significant progress. There were some concerns (e.g. claiming novelty for inverted dot-product attention) and clarification questions (e.g. same learning rate schedule for all models). The authors provided a response and revised the submission , which addresses most of these concerns. At the end, majority of reviewers recommended accept. Alongside with them, I acknowledge the novelty of using layer norm and parallel execution, and recommend accept.\n",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SklNZ1259S",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Official Blind Review #4",
                "comment": "Authors improve upon dynamic routing between capsules by removing the squash function (norm normalization) and apply a layerNorm normalization instead. Furthermore, they experiment with concurrent routing rather than sequential routing (route all caps layers once, then all layers concurrently again and again). This is an interesting development since provides better gradient in conjunction with layerNorm. They report results on Cifar10 and Cifar100 and achieve similar to CNN (resnet) performance.\n\nFirst, I want to point out that inverted attention is exactly what happens in dynamic routing (sabour et al 2017), proc. 1 line 4,5, and 7. In dynamic routing the dot product with the next layer capsule is calculated and then normalized over all next layer capsules. The only difference that I notice between alg. 1 here and proc. 1 there is replacement of squash with layer norm. There is no \"reconstructing the layer bellow\" in Dynamic routing as authors suggest in intro. \n\nSecond, the Capsules are promised to have better viewpoint generalizability than CNNs while having comparable performance. Replacing the 1 convolution layer with a ResNet backbone and replacing the activation with a classifier on top seems reducing the proposed CapsNet to the level of CNNs in terms of Viewpoint Generalization. Why should someone use this network rather than the ResNet itself? Fewer number of parameters by itself is not interesting, the reason it is reported usually is that it indicates lower memory consumption or fewer flops. Is that the case when comparing the baseline ResNet with the proposed CapsNet? Otherwise, a set of experiments showcasing the viewpoint generalizability of proposed CapsuleNetworks might only justify the switch between resnets to the proposed capsnets.\n\nThirdly, Fig. 4 top images seems to indicate all 3 routing procedures are following the same Learning Rate schedule. In the text it is said that optimization hyperparameters are tuned individually. Did authors tune learning rate schedule individually?\n\nForth, the proper baseline for the current study is the dynamic routing CapsNet. Why the multiMNIST experiment lacks comparison with dynamic routing capsnet?\n\nFor the reasons above, the manuscript in its current format is not ready for publication.\n\n------------------------------------------------------rebuttal\nThank you for your response. I acknowledged the novel contributions of this work. My comment was that some claims in the paper are not right. i.e. \"inverted dot-product attention\" is not new and \"reconstructing the layer bellow\" does not happen in Sabour et al . Parallel execution + layer norm definitely is novel and significant.\n\nRegarding the LR-schedule, I am not sure how fair it is to use same hyper-params tuned for the proposed method on the baselines. \n\nRegarding the viewpoint, the diverseMultiMNIST is two over lapping MNIST digits shifted 6 pixels. There is no rotation or scale in this dataset. An example experiment verifying the viewpoint generalizability of the proposed model is training on MNIST testing on AFFNIST. \n",
                "rating": 3,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BkelrkX5sB",
                "reply_to": "H1x0kqZJ5S",
                "title": "Reponse to Review #1",
                "comment": "We thank the Reviewer for their valuable feedback. \n\n[Code] \nWe will release the code. Reproducibility is our priority.\n\n[Higher Memory Usage than CNNs]\nTwo main reasons result in higher memory usage than CNNs. \n\nThe first reason is that we perform iterative routing, which means that we perform routing multiple times. Since CapsNet is a weight-tied architecture, the memory usage scales linearly with the number of iterations. The details can be found in the illustration in Figure 3 and the bar plot of memory usage in Figure 4. Note that this phenomenon is also observed in Deep Equilibrium Models (DEQs) [1]. Inspired by DEQs, a potential solution is to refer to a fixed-point optimization for finding equilibrium points of the routing updates. Then, we can enjoy the benefits of constant memory. \n\nThe second reason is that CapsNet uses the routing mechanism. As compared to the operations in CNNs, the routing mechanism performs agreement calculation between layers. This calculation introduces additional memory usage.However, we note that the routing mechanism is a dense operation, which means that we need to perform routing between all lower-layer capsules and all higher-layer capsules. We can instead randomly sample the capsules for routing, making the routing mechanism a sparse operation. We leave this dense-to-sparse modification as our future work.\n\n\n[1] \u201cDeep equilibrium models\u201d, S Bai, JZ Kolter, and V Koltun. NeurIPS 2019.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HyxmfJXqir",
                "reply_to": "rygSM4-q9r",
                "title": "Reponse to Review #5",
                "comment": "We thank the Reviewer for the valuable feedback.\n\n[Learning Rate Scheduler]\nWe use the same learning rate scheduler for all three models. The learning rate degrades by 0.1 on the 150th and 250th epochs. There are two distinct points for both the EM and Dynamic routing models, yet the second point is more noticeable when zooming in the convergence plot in Figure 4.\n\nThe difference between all three models is the type of optimization method used. We use SGD for our model, and we use Adam for Dynamic and EM routing models. The type of the optimization method is selected to reach the best performance for each model. For example, SGD leads to worse performance than Adam for the Dynamic routing model. \n\nIn the original submission, we have included these details in Section A.1 in Supplementary.\n\n[Uniform Prediction in Table in Figure 4]\nFor Inverted Dot-Product Attention-A, when routing iteration increases to 5, we observe NaN values in neural network parameters. The prediction result becomes uniform across all classes. Since we consider a 10-class classification, the prediction accuracy becomes 10.00%. We rephrase the \u201crandom guess\u201d to \u201cuniform prediction\u201d in the revised manuscript. \n\n[Non-zero Pose Initilization]\nWe thank the Reviewer for raising the discussion about the capsule's initialization. As compared to 0 initialization, we observe that a random initialization leads to a similar converged performance but slower convergence speed. On the other hand, learning biases for capsules results in similar converged performance and same convergence speed. As a summary, we initialize the capsule's value to 0 for simplicity. We include the discussion in the revised manuscript.\n\n[Sudden Performance Jump in Convergence Plot in Figure 4]\nThe phenomenon of the performance jump is due to applying LayerNorm on the low-dimensional pose. To be more precise, the dimension of the pose used in the convergence plot is 16, and we apply LayerNorm to these 16 units. When increasing the pose\u2019s dimension, the jittering no longer existed. Nevertheless, we empirically find that it does not affect the model\u2019s prediction result once the model converges. \n\nIn the original submission, we have included these details in the last few sentences of the Convergence Analysis in Section 5.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1xGyJm9sH",
                "reply_to": "SklNZ1259S",
                "title": "Reponse to Review #4",
                "comment": "We thank the Reviewer for constructive feedback. We hope the following response will address the concerns of the Reviewer.  \n\n[Remarks on Inverted Dot-Product Attention Routing]\nWe agree that our routing method has similar components to Dynamic Routing (Sabour et al 2017), and we would like to emphasize their differences : 1) Sequential iterative routing is replaced with concurrent iterative routing, 2) Squash activation is replaced with Layer Normalization, and 3) We use cross-entropy loss instead of margin loss. The comparison is summarized in Section 4.3.\n\nWe humbly argue that these modifications are not trivial and stabilize the training, which leads to improved performance. For example, we observe that only our model has improved performance when the routing iteration number increases (CIFAR10 classification Table of Figure 4).\n============================================\nMethod   | Iteration=1 | Iteration = 3 | Iteration = 5\nDynamic  |    84.08%     |    82.88%       |    82.11%\nEM            |    58.08%     |    78.43%       |    31.41%\nOurs         |    84.24%     |    84.83%       |    85.09%\n============================================\n\n[Remarks on using ResNet]\nWe agree that using a deeper CNN such as a ResNet (vs. a single convolutional layer) to produce primary capsules makes our model inherit the disadvantages of CNNs (such as less view-point generalizability) and blunts the potential impact of capsules. However, at this stage, our intent is not to replace CNNs completely with CapsNets, but take a meaningful step towards building a routing mechanism that can at least do the job of the higher layers of a CNN. Previously proposed routing algorithms fail to do so and perform worse than their baseline CNNs.\n==================================================\nMethod                                                                      |  Accuracy\n-----------------------------------------------------------------------------\nDynamic routing with DenseNet backbone [1]  |   89.71%\n-----------------------------------------------------------------------------\nDynamic routing with ResNet backbone             |   92.65%\nEM routing with ResNet backbone                       |   92.15%\nOur routing with ResNet backbone                      |   95.14%\n-----------------------------------------------------------------------------\nOriginal ResNet                                                        |   95.11%\n==================================================\n[1] Phaye et al. \u201cDense and Diverse Capsule Networks: Making the Capsules Learn Better.\u201d Arxiv 2018.05.\n\n[Remarks on Memory Consumption]\n\n\uff37e agree with the Reviewer that reporting only the number of parameters may not be satisfying. Therefore, in Figure 5, we report the memory consumption comparisons between CapsNets and CNNs given the same model architecture. Please see the response to Reviewer #1, where we outline the reasons for why CapsNets consume more memory compared to CNNs even with fewer parameters, and we also suggest some possible solutions on reducing memory consumption, which we leave as our future work. \n\nWe also like to point out that the networks with fewer model parameters but larger runtime memory footprint may still be preferable for certain IC designs, where the L1 cache can store all the parameters.  \n\n[Learning Rate Scheduler]\nWe use the same learning rate scheduler for all three models. The learning rate degrades by 0.1 on the 150th and 250th epochs. In the original submission, we have included these details in Section A.1 in Supplementary.\n\n[Dynamic Routing Methods for DiverseMultiMNIST]\nWe provide the results for the Dynamic routing method by applying it on the DiverseMultiMNIST dataset. For a fair comparison, we consider the same optimizer, the same number of layers, and the same number of neurons per layer for the Dynamic routing method and the other methods. \n\nThe results are highlighted below (CapsNet denotes our routing method):\n================================================\nMethod    |  Pose Structure  | Test Acc.  |   # params.\n-----------------------------------------------------------------------------\nDynamic  |      vector              |  83.39%    |   42.48M \n-----------------------------------------------------------------------------\nCapsNet   |      matrix             |  80.59%    |   9.96M \nCapsNet   |      vector              |  85.74%    |   42.48M \n-----------------------------------------------------------------------------\n            BaselineCNN                |  79.81%    |   19.55M\n================================================\n\nCompared to the Baseline CNN, both our routing method and the Dynamic routing method achieve better performance on the DiverseMultiMNIST dataset. This result suggests a better viewpoint generalization from CNNs to the Capsule networks. Furthermore, our routing method outperforms the Dynamic routing one. We have updated our manuscript with these results. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "S1xoapM5iS",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Manuscript Revision",
                "comment": "We have updated the manuscript, and we highlight the additional results/ discussions in red.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1x0kqZJ5S",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Official Blind Review #1",
                "comment": "In this paper, the authors propose a simple and effective routing algorithm for capsule networks. The paper is well written. A nice analysis of the proposed routing algorithm is provided. Experiments of varying the routing iterations demonstrate the stableability of proposed routing algorithm compared to others.\n\nHere are some issues:\n1. Would the authors release the code for reproducing the results in the paper? It will be helpful for future research in this area.\n\n2. In Fig.5, it would be better to give some brief explanations about why CasNet (Matrix) occupies much more memory while possessing less parameters.",
                "rating": 6,
                "confidence": 1,
                "writer": "official_reviewer"
            },
            {
                "review_id": "rygSM4-q9r",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Official Blind Review #5",
                "comment": "\nThis paper presents a new simpler routing mechanism for capsule networks and achieves good performance on real world data sets making use of this new capsule structure along with a restnet backbone. Strong performance on the cifar10 and cifar100 datasets are presented and the network outperforms earlier versions of capsule networks. This new structure also performs well on an augmented MNIST dataset of overlapping digits (similar to the one used by Sabour et al 2017). \n\nOverall the paper is well written and presents solid results. The paper also presents a thorough comparison of two earlier versions of capsules which is a worthwhile contribution in its own right.\n\nThe paper could be improved by clearing up a few ambiguities:\n\n- is the learning rate schedule the same for all three models? in figure 4 it looks like the learning rate is decayed at two distinct points for your model, but only one distinct point for both the EM and Dynamic routing models.  \n\n-\"Notably, the prediction becomes random guess when the iteration number increases to 5.\" this sentence is a little confusing. Do you mean when the iteration number the performance is equivalent to not random assignments?  \n\n- This new algorithm requires that the capsules in L+1 have initialized poses with which to compare agreement between the poses in L. This is initial value seems like it may greatly effect the performance of the model. In the paper it is set to 0 and not expanded upon. It would be interesting to see if randomizing this value, or learning a bias for it would effect performance.   \n\n-unlike the two previous versions of capsules, the inverted dot product capsules show in figure 4 sudden huge decreases in test accuracy while training. These moments seem to be overcome quite quickly and the model ends up outperforming the other two. But it would be worth mentioning this behavior and perhaps attempting to explain it.\n",
                "rating": 8,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "HylYeJdU_r",
                "reply_to": "BkxFhbtE_B",
                "title": "Response to Code Release",
                "comment": "We are cleaning the code, and planning to release it once it is ready.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkxFhbtE_B",
                "reply_to": "iclr_2020_HJe6uANtwH",
                "title": "Request for code",
                "comment": "Hi.\nI feel that this is a good step forward getting capsules closer to state-of-the-art on complicated datasets like CIFAR10.\nCould you please release the code ASAP.\nThanks.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "a routing algorithm for capsule networks",
                "Sentiment Expression": "presents",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The results",
                "Sentiment Expression": "outperform existing capsule networks and are at-par with CNNs",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the novelty, introducing a new simpler routing mechanism, and achieving good performance on real world datasets",
                "Sentiment Expression": "appreciated",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "removing the squash function and experimenting with concurrent routing",
                "Sentiment Expression": "was highlighted as significant progress",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "claiming novelty for inverted dot-product attention",
                "Sentiment Expression": "were some concerns",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the authors provided a response and revised the submission",
                "Sentiment Expression": "which addresses most of these concerns",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "majority of reviewers",
                "Sentiment Expression": "recommended accept",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the novelty of using layer norm and parallel execution",
                "Sentiment Expression": "recommend accept",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "r1lUl6NFDH": {
        "paper_id": "iclr_2020_r1lUl6NFDH",
        "paper_title": "Mirror Descent View For Neural Network Quantization",
        "paper_abstract": "Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. NN quantization is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones, we introduce a Mirror Descent (MD) framework (Bubeck (2015)) for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we discuss a numerically stable implementation of MD by storing an additional set of auxiliary dual variables (continuous). This update is strikingly analogous to the popular Straight Through Estimator (STE) based method which is typically viewed as a \u201ctrick\u201d to avoid vanishing gradients issue but here we show that it is an implementation method for MD for certain projections. Our experiments on standard classification datasets (CIFAR-10/100, TinyImageNet) with convolutional and residual architectures show that our MD variants obtain fully-quantized networks with accuracies very close to the floating-point networks.",
        "paper_acceptance": "reject",
        "meta_review": "The paper proposes to use the mirror descent algorithm for the binary network. It is easy to read. However, novelty over ProxQuant is somehow limited. The theoretical analysis is weak, in that there is no analysis on the convergence and neither how to choose the projection for mirror mapping construction. Experimental results can also be made more convincing, by adding comparisons with bigger datasets, STOA networks, and ablation study to demonstrate why mirror descent is better than proximal gradient descent in this application.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SJlXGL9j2B",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #4",
                "comment": "The paper proposes to use the mirror descent algorithm for the binary network. The key point is Theorem 3.1, which enables the mirror map. The paper is easy to read and follow, and the main contributions are clearly stated.\n\nHowever, I suggest a weak rejection of this paper. The reasons are\n\nQ1. As Review #3, it is better for authors to provide more theoretical analysis, which better includes the nonconvex objective function and the effect of annealing. \n\nQ2. It is not clear to me, why mirror descent is better than proximal gradient descent, i.e., proxQuant, in this application. The authors repeatedly claim \"MD allows gradient descent to be performed on a more general non-Euclidean space\". This cannot be told by Table 1, which is just overall performance. So, it is better to empirically show this point by an ablation study.\n\nQ3. Since the technical contributions are not enough, I expect more experimental comparisons.\n- Could the authors perform experiments on ImageNet?\n- While VGG and ResNet are taken as a protocol for experimental comparison, it is better to do an extra comparison with STOA networks. VGG and ResNet are too old and easy to be compressed, compression these networks are of little practical values. EfficientNet [1], Mobilenets [2], and Shufflenet [3] can be good ones. The paper will be more convincing with these methods.\n\n[1]. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n[2]. Mobilenets: Efficient convolutional neural networks for mobile vision applications\n[3]. Shufflenet: An extremely efficient convolutional neural network for mobile devices",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "ryg5FaBptB",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #2",
                "comment": "This paper proposes a Mirror Descent (MD) framework for the quantization of neural networks, which, different with previous quantization methods, enables us to derive valid mirror maps and the respective MD updates. Moreover, the authors also provide a stable implementation of MD by storing an additional set of auxiliary dual variables. Experiments on CIFAR-10/100 and TinyImageNet with convolutional and residual architectures show the effective of the proposed model. \n\nOverall, this paper is well-written and provide sufficient material, both theoretical and experimental evidence to support the proposed method. Although the novelty of this work is somehow limited, i.e. appling MD from convex optimization to NN quantization, the authors provides sufficient effort to explore how to success to adopted it the literature. Hence, I lean to make an accept suggestion at this point. \n\nConcern: it would better to provide the code to validate the soundness of the model.\n\n##post comments\nThe rebuttal addresses my concerns and I will not change my score. Thanks.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Bkem9Mo9iB",
                "reply_to": "ryg5FaBptB",
                "title": "Thank you for the positive feedback",
                "comment": "We appreciate that the reviewer finds that our paper has sufficient material on both theoretical and experimental aspects. Below we address the reviewer\u2019s concerns.\n\n# Novelty\n- We appreciate that the reviewer acknowledges adopting MD for NN quantization has some challenges and our paper addresses them successfully (eg, time-varying mirror maps, deriving mirror maps from projections, numerically stable implementation using STE) and introduces the first practical MD based algorithm for NN quantization and demonstrate superior empirical performance against directly comparable baselines.\n\n# Code\n- We will provide the code upon publication and we have provided it for the reviewers and ACs in a separate confidential comment.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkgjviP5sH",
                "reply_to": "r1gG6JwAYS",
                "title": "Thank you for the positive feedback",
                "comment": "We appreciate that the reviewer finds that our method is novel and interesting. Below we address the reviewer\u2019s concerns.\n\n# Writing suggestions\n- We agree with the reviewer\u2019s suggestions and we have appropriately revised the paper. \n- Regarding $y^0$, we would like to clarify that the first iterate of $y$ is $y^1$ and it is obtained using Eq. 4 where $x^0$ is initialized as discussed in the paper.\n\n# Experiment setup\n- As discussed in the paper, not all NN quantization algorithms are directly comparable to each other due to variations in the experimental protocol and considered quantization levels [1]. Since it is impossible to evaluate on all different experimental setups, following the recent publications [2,3], we compare against directly comparable baselines and we consider the extreme case of fully-quantized networks (ie, all learnable parameters are quantized). \n\n[1] Guo, Yunhui. \"A survey on methods and theories of quantized neural networks.\" CoRR (2018).\n[2] Bai, Yu, Yu-Xiang Wang, and Edo Liberty. \"Proxquant: Quantized neural networks via proximal operators.\" ICLR (2019).\n[3] Ajanthan, Thalaiyasingam, Puneet K. Dokania, Richard Hartley, and Philip HS Torr. \"Proximal Mean-field for Neural Network Quantization.\" ICCV (2019).\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rke-y25fiB",
                "reply_to": "H1li8fEhYH",
                "title": "Summary of contributions and response to other comments {Response to R3 [2/2]}",
                "comment": "Below, we first summarize our contributions and then address the comments regarding convergence analysis and choice of projection.\n\n# Main contributions of our MD method\n- We would like to clarify that the main focus of the paper is to show that MD is a suitable framework for NN quantization and introduce a practical MD algorithm for NN quantization. To this end, our main contributions are summarized below:\n- We introduce the first practical MD based algorithm for NN quantization with time-varying mirror maps and demonstrate superior empirical performance against directly comparable baselines.\n- As MD updates are prone to numerical instability, we introduce a numerically stable version of MD and show that the popular STE method is an implementation method for MD under certain conditions on the projection.\n\n# Convergence of MD in the nonconvex setting\n- As mentioned in our submission, convergence analysis of MD in the nonconvex setting is an active research area [3,4] and MD has been recently shown to converge in the nonconvex stochastic setting under certain conditions [5]. This, together with empirical convergence plots (in Fig. 2) justifies the use of MD for NN quantization. We have appropriately cited [5] in the revised version and we believe convergence analysis of MD for NNs could be a completely new theoretical paper in itself.\n\n# Choice of projection\n- As stated in Theorem 3.1, if the projection is invertible and monotonically increasing, a valid mirror map exists and the corresponding MD algorithm can be derived. Moreover, to ensure fully-quantized networks we require the projections to be parameterized by an annealing hyperparameter $\\beta$ (Example 3.1). Nevertheless, choosing a projection that is guaranteed to yield improved quantization performance is an open problem.\n\n[3] Zhou, Zhengyuan, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter Glynn. \"Mirror descent in non-convex stochastic programming.\" CoRR (2017).\n[4] Zhou, Zhengyuan, Panayotis Mertikopoulos, Nicholas Bambos, Stephen Boyd, and Peter W. Glynn. \"Stochastic mirror descent in variationally coherent optimization problems.\" NeurIPS (2017).\n[5] Zhang, Siqi, and Niao He. \"On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization.\" CoRR (2018).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BkgAEhcMjH",
                "reply_to": "H1li8fEhYH",
                "title": "PQ is not based on MD and our MD method significantly outperforms PQ {Response to R3 [1/2]}",
                "comment": "Thank you for the feedback and we appreciate that the reviewer finds that our MD method is suitable for NN quantization.\n\nIn this reply, we clarify the novelty and significance of our MD method compared to ProxQuant (PQ) [1]. Meanwhile, responses to other comments will be provided in a subsequent reply.\n\n# Summary\n- Our main contribution of the paper is to show that MD is a suitable framework for NN quantization and introduce a numerically stable MD algorithm for NN quantization with superior performance compared to directly comparable baselines.\n- In this regard, we find the statement that our MD method is a \u201cnatural extension of PQ\u201d (ie, proximal gradient method or in general gradient descent where the $L_2$ norm is used) to be misleading and the differences are as follows. \n\n# MD vs PQ\n- The main and important difference between our MD method and PQ is that MD allows gradient descent to be performed on a more general non-Euclidean space (refer to Sec. 2) whereas PQ does not. To see this, we first give the update equations of PQ and MD below:\n- PQ: $\\tilde{x}^{k+1} \\gets x^k - \\eta g^k$ where $x^k = \\text{prox}(\\tilde{x}^k)$ and $g^k = \\nabla f(x)|_{x = x^k}$. Here, $x^k, \\tilde{x}^k \\in R$. (refer to Alg. 1 in [1]) \n- MD: $\\tilde{x}^{k+1} \\gets \\tilde{x}^k - \\eta g^k$ where $x^k = P(\\tilde{x}^k)$ and $g^k = \\nabla f(x)|_{x = x^k}$. Here, $x^k \\in B$ and $\\tilde{x}^k \\in B^*$, where $B^*$ is the dual space of $B$. (refer to Eq. 22 in the paper) \n- Notice that, PQ assumes the point $x^k$ and gradient $g^k$ are in the same space. Then only the formula $x^k - \\eta g^k$ is valid. This would only be true for the Euclidean space [2]. However, MD allows gradient descent to be performed on a more general non-Euclidean space by first mapping a primal point $x^k\\in B$ to a point $\\tilde{x}^k \\in B^*$ in the dual space via the mirror map. Such an ability is extremely beneficial in many problems (eg, simple constrained optimization) and it enabled theoretical and practical research on MD for the past three decades. Therefore, as mentioned in the paper (page 7) PQ is not based on MD.\n- Furthermore, it is clear from our experiments that MD significantly outperforms PQ (up to 20% in some cases when fully-quantized, refer to Table 1) demonstrating the importance of optimizing on a non-Euclidean space based on our MD framework.\n- Even though PQ hinted at the connection to the dual averaging version of MD and STE, it does not analyze the conditions on the projections under which corresponding valid mirror maps exist. This is important to show STE as a numerically stable implementation method for MD and such a link was previously lacking in the literature.\n- We have added this discussion in the revised version of the paper (page 7) to improve clarity.\n\n[1] Bai, Yu, Yu-Xiang Wang, and Edo Liberty. \"Proxquant: Quantized neural networks via proximal operators.\" ICLR (2019).\n[2] Bubeck, S\u00e9bastien. \"Convex optimization: Algorithms and complexity.\" Foundations and Trends\u00ae in Machine Learning (2015).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H1li8fEhYH",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #3",
                "comment": "This paper proposes a neural network (NN) quantization based on Mirror Descent (MD) framework. The core of the proposal is the construction of the mirror map from the unconstrained auxiliary variables to the quantized space. Building on that core, the authors derive some mapping functions from the corresponding projection, i.e. tanh, softmax and shifted tanh. The experimental result on benchmark datasets (CIFAR & TinyImageNet) and basic architectures (VGG & ResNet-18) showed that the proposed method is suitable for quantization. The proposed method is a natural extension of ProxQuant, which adopted the proximal gradient descent to quantize NN (a.k.a $\\ell_2$ norm in MD). Different projections in NN quantization lead to different Bregman divergences in MD. \n\nHowever, the authors do not analyze the convergence of the MD with nonconvex objective function in NN quantization neither how to choose the projection for mirror mapping construction. Moreover, it is better to discuss with [Bai et al, 2019] to clarify the novelty of the proposed method. So I concern about the novelty and the theoretical contributions \n\nYu Bai, Yu-Xiang Wang, Edo Liberty. \nProxQuant: Quantized Neural Networks via Proximal Operators. ICLR 2019.",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "r1gG6JwAYS",
                "reply_to": "iclr_2020_r1lUl6NFDH",
                "title": "Official Blind Review #1",
                "comment": "A good paper that uses the Mirror Descent paradigm for learning quantized networks.  \nThough Mirror Descent is not their original idea, but using it in the context of learning quantized network is novel and interesting.  \nEmpirically, they showed better results than existing method, with comparisons with reasonable baselines including using relaxed projected gradient descent.  \n\nOverall, I don\u2019t have much concerns, but here are some more specific comment/questions (most relates to writing)\n\nIn the intro, it would be great to mention some past success on using MD, as opposed to just saying it\u2019s well-known. Also you mention MD can be used for more than quantization, but compression in general, it\u2019d be better to add that discussion, or remove this sentence. \n\nIn the beginning of Section 2.1, it'd be easier for the readers to make clear that the primal space corresponds to the quantized weights and the dual space corresponds to the unconstrained space in the rest of the paper.\n\nAt the top of page 3 you describe MD for the first time, but it\u2019s unclear to me how y^0 is handled.\n\nThe end of section 3 and section 4 talk quite a bit about STE, maybe it'd be clear if the authors can provide a concise description.\n\nAs someone not super familiar with NN quantization, this work seems like a good contribution.  My only possible concerns would be somehow comparisons to existing methods are not comprehensive enough (if this will be pointed out by the other reviewers)\n\n",
                "rating": 8,
                "confidence": 1,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "proposes to use the mirror descent algorithm for the binary network",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "It",
                "Sentiment Expression": "is easy to read",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "novelty over ProxQuant",
                "Sentiment Expression": "is somehow limited",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "The theoretical analysis",
                "Sentiment Expression": "is weak",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Experimental results",
                "Sentiment Expression": "can also be made more convincing",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "0cn6LSqwjUv": {
        "paper_id": "nips_2022_0cn6LSqwjUv",
        "paper_title": "RainNet: A Large-Scale Imagery Dataset and Benchmark for Spatial Precipitation Downscaling",
        "paper_abstract": "AI-for-science approaches have been applied to solve scientific problems (e.g., nuclear fusion, ecology, genomics, meteorology) and have achieved highly promising results. Spatial precipitation downscaling is one of the most important meteorological problem and urgently requires the participation of AI. However, the lack of a well-organized and annotated large-scale dataset hinders the training and verification of more effective and advancing deep-learning models for precipitation downscaling. To alleviate these obstacles, we present the first large-scale spatial precipitation downscaling dataset named RainNet, which contains more than 62,400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, ready to help the evolution of deep learning models in precipitation downscaling. Specifically, the precipitation maps carefully collected in RainNet cover various meteorological phenomena (e.g., hurricane, squall), which is of great help to improve the model generalization ability. In addition, the map pairs in RainNet are organized in the form of image sequences (720 maps per month or 1 map/hour), showing complex physical properties, e.g., temporal misalignment, temporal sparse, and fluid properties. Furthermore, two deep-learning-oriented metrics are specifically introduced to evaluate or verify the comprehensive performance of the trained model (e.g., prediction maps reconstruction accuracy). To illustrate the applications of RainNet, 14 state-of-the-art models, including deep models and traditional approaches, are evaluated. To fully explore potential downscaling solutions, we propose an implicit physical estimation benchmark framework to learn the above characteristics. Extensive experiments demonstrate the value of RainNet in training and evaluating downscaling models. Our dataset is available at https://neuralchen.github.io/RainNet/.",
        "paper_acceptance": "Accept",
        "meta_review": "This paper describes SPDNet, a dataset for spatial precipitation downscaling. \n\nExperiments are provided using a fairly wide set of alternative methods - 14 models (including Kriging which is a widely used standard method in the meteorological community) - as well as a novel architecture proposed by the authors. The authors also extended SRGAN, EDSR, ESRGAN from Single Image Super Resolution (SISR) methods to Video Super Resolution (VSR) methods. While the level of innovation on the neural architecture side of the work is not extreme, clear value is provided in terms of contributions to neural architecture development. Reviewers felt that the dataset itself, the wide variety of models examined and the large set of evaluation metrics offers value to the community and that this dataset could help bring more interest to the problem domain. \n\nDuring the discussion period it was made clear that \"All relevant codes and datasets are open-source for research purposes\" and that \n\"The dataset and the code are not proprietary. We will build a dedicated github repository and website for users to easily use our datasets and codes.\" It is important that this is indeed is fully executed by the authors. \n\nThree of four reviewers recommended acceptance. \n\nFor all these reasons the AC recommends acceptance.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "7hoTfc2gAyo",
                "writer": "author",
                "reply_to": "lnVp28VEsCi",
                "title": "To Reviewer WVYT",
                "comment": " We sincerely thank you for the review and comments.\n\nAlso thank you for acknowledging the value of our dataset.\n\nAfter discussions in our team, we thought that we should reduce the discussion of metrics and focus on metric that are very familiar to the computer field (such as RMSE).\nWe will add more content to introduce the dataset and benchmark itself so that this paper focuses on our propsed dataset and model.\nMore dataset-related details, as well as model design and training details, will be covered in the main text.\n\nBest, Authors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "lnVp28VEsCi",
                "writer": "official_reviewer",
                "reply_to": "iyh5uAQH1sE",
                "title": "Thanks for your response",
                "comment": " I'm sorry for the late response. The authors addressed some of my questions given the limited time. Thanks.\n\nIn the response to Q1, the authors also provide detailed explanation on the necessity of real low-res data and empirical study on the corresponding improvement. The response seems reasonable to me.\n\nAs for Q3 and Q6, the evaluation of performance on precipitation related tasks is still an open problem. E.g., DeepMind's Nature paper resorted to meteorologists for human evaluations due to the discrepancy between evaluations from experts and scores. It's not appropriate to include the intuitive designs of PEM/PDEM as one of the major contributions in this paper.\n\nOverall, the dataset and the corresponding benchmark are valuable. I suggest that the authors focus on them and remove the PEM/PDEM part in the paper.  The value of the dataset and benchmark will not be diminished by not proposing \"novel\" metrics. The proposed method does not necessarily have to outperform baselines in all concerned metrics.\n\n[1] Ravuri, Suman, et al. \"Skilfull precipitation nowcasting using deep generative models of radar.\" Nature 597.7878 (2021): 672-677.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Smp8psilatv",
                "writer": "author",
                "reply_to": "jjMUDMTFaxK",
                "title": "To reviewer 5jsP",
                "comment": " Dear reviewer 5jsP:\n\nWe sincerely thank you for the review and comments. We have provided corresponding responses, which we believe have covered your concerns. We hope to further discuss with you whether or not your concerns have been addressed. Please let us know if you still have any unclear parts of our work.\n\nBest, Authors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5wwckMiMQ2N",
                "writer": "author",
                "reply_to": "1mMfRqp4pty",
                "title": "To reviewer WVYT",
                "comment": " Dear reviewer WVYT:\n\nWe sincerely thank you for the review and comments. We have provided corresponding responses and results, which we believe have covered your concerns. We hope to further discuss with you whether or not your concerns have been addressed. Please let us know if you still have any unclear parts of our work.\n\nBest, Authors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ndu1tUMJVku",
                "writer": "author",
                "reply_to": "WBmpyaytXL",
                "title": "To reviewer 9mB6",
                "comment": " Dear reviewer 9mB6:\n\nWe sincerely thank you for the review and comments. We have provided corresponding responses, which we believe have covered your concerns. We hope to further discuss with you whether or not your concerns have been addressed. Please let us know if you still have any unclear parts of our work.\n\nBest,\nAuthors of Paper 690",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4N-CL_8pR09",
                "writer": "author",
                "reply_to": "9vOAx1JCX0r",
                "title": "To reviewer aP2Q",
                "comment": " Thanks for your valuable reply!\n\nQ1: Can we get a model that is better at domain specific scores by selecting the candidate models with PEM/PDEM and not MSE?\n\nA1: Thank you for your question. In spatial precipitation downscaling tasks, domain researchers typically use the metrics introduced in our paper to evaluate/select models instead of directly using RMSE [1]. The RMSE here is the pixel-level average error over all frames (i.e. 720 frames in a month), this averaging loses structural and dynamic information [1], which are properties of most interest to domain researchers. \"We could expect better PEM/PDEM performance when better RMSE performance is observed, but it might not always be the case.\"  For example, the model can reconstruct some frames very well and others very poorly (this often happens in heavy rain situations, e.g., hurricanes, continuous heavy rain, they occur almost every year), and the model can also get decent RMSE values, in this case, the model exhibits poor temporal consistency and dynamics. However, these issues can be captured by PDE/PDEM, which are more fine-grained metrics.\nIn other words, similar RMSEs may have different PDEs and PDEMs, for example, RCAN (RMSE\\times 100:0.325, PEM: 0.227, PDEM: 0.558) and EDVR (RMSE X100:0.329, PEM: 0.180, PDEM: 0.476) ). So simply using RMSE may cause models selection to fail.\nIn fact, CPMSE has similar functionality to RMSE.\nTherefore, it is entirely feasible to use PDE/PDEM directly for model selection or evaluation.\n\n[1]. Ekstr\u00f6m, Marie. \"Metrics to identify meaningful downscaling skill in WRF simulations of intense rainfall events.\" Environmental Modelling & Software 79 (2016): 267-284.\n\nQ2: For example, there might be other domain-specific scores that are missing in the benchmark, how should we incorporate these scores in the PEM / PDEM? \n\nA2: Thank you for your question. It can be added to PEM/PDEM by first normalizing and then weighting the metric to be added.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9vOAx1JCX0r",
                "writer": "official_reviewer",
                "reply_to": "nf3SWnPfY5",
                "title": "Thanks for the rebuttal",
                "comment": " Thanks for the rebuttal.\n\nRegarding Q1, I can understand the necessity for including a few domain-specific score functions such as  MPPE, HRRE, CPMSE, AMMD, HRTS, CMD. However, it is not clear why we need PEM / PDEM at this stage given that they are very consistent with the simpler MSE score. For example, there might be other domain-specific scores that are missing in the benchmark, how should we incorporate these scores in the PEM / PDEM? In fact, people may later adopt these metrics for model selection. Can we get a model that is better at domain specific scores by selecting the candidate models with PEM/PDEM and not MSE?\n\nRegarding Q2, thanks for agreeing to add vision transformers in the benchmark.\n\nRegarding Q3, thanks for the reply. The event types can be useful for further analyzing whether the SR algorithms are robust for different domains (i.e., meteorological events).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZIIO_BtU8bA",
                "writer": "author",
                "reply_to": "1mMfRqp4pty",
                "title": "To Reviewer WVYT part2",
                "comment": " Q3: There are no clues about the effectiveness of proposed novel compound metrics PEM and PDEM. It would be much more convincing to conduct empirical studies to prove that models achieving better PEM/PDEM demonstrate better ability on addressing some concerned issues.\n\nA3: Thanks for the comment. Here we are trying to make the metrics more applicable to meteorology society while also containing variables (e.g., RMSE) that are familiar to the computer science society. The metrics Precipitation Error Measure (PEM) and Precipitation Dynamics Error Measure (PDEM) are weighted over a series of metrics with clear physical meaning (reconstruction metrics: MPPE, HRRE, CPMSE, AMMD, and dynamic metrics: HRTS and CMD) and have been applied in downscaling research in meteorology society for a long time (may not in the same abbreviation). We\u2019ve mentioned these in the supplementary (section 3. Metrics) and have added an explanation to the main text. In supplementary section 3, we also discussed how each metric is calculated and what other literature employs the metric. This explains why better PEM/PDEM demonstrates a better ability to address downscaling problems (from a meteorology sense). For example (line 30-33 in supplementary), \u201cThe mesoscale peak precipitation error (MPPE; mm/hour) is calculated as the difference of top quantile between the generated/real rainfall dataset which considering both spatial and temporal property of mesoscale meteorological systems, e.g., hurricane, squall. This metric is used in most of these papers (for example [15, 10, 2, 6, 11, 14] (refs in our paper) suggest the quantile analysis to evaluate the downscaling quality)\u201d. To be noticed, in meteorology society, researchers tend to evaluate one downscaling algorithm with not a single variable but multiple variables together. However, it is always important to condense the information when bridging two fields. Here we weighted these variables into two to make comparing machine learning models easier for computer science society. \n\nQ4: Missing training details: Selected baselines are not designed for precipitation data. It is necessary to (at least slightly) modify and tune the models for fair comparison. However, there is no information about these details except for a single statement \"we also adjust the hype parameters of these models for better performance\" in Supl. Sec.5.2.\n\nA4:  Thanks for the question. The parts that need to be adjusted for these models include two parts\uff1a\n1. The hyperparameters required for training, which we have explained in line 309\\~313 of our paper. It is worth pointing out that typically SR models use a learning rate of 1e-4\\~5e-4, but we found that using 1e-3 is better for our task.\nWe have added more detailed instructions in Section 6.1 of the new version of our paper;\n2. The adjustment of the model, including the adjustment of the input channel and upsampling rate. We adjust the number of channels of input data for SRCNN, SRGAN, EDSR, ESRGAN, DBPN, RCAN, EDVR, RBPN, and our model to 1. We set the input data channel to 5 for SRGAN-V, EDSR-V, and ESRGAN-V. We set the upsampling rate to 3 for all models. \n\n\nQ5: Formatting errors.\n\nA5:  Thank you for your thoughtful suggestions. We had corrected \"e.t.c\" to \"etc.\". Careful corrections have been made to the language of our paper. \"L\" denots low-resolution and \"H\" denots the high-resolution. \"T\" represents the frame number. A detailed explanation has been added to Section 5 of the new version of our paper. We have revised the writing of Eq.1. We had highlighted 6 commonly used metrics in Table 1 of the new version of our paper.\n\nQ6: The qualitative results shown in Figure 4 are hard to distinguish. Could the authors provide more evidence to demonstrate that the models achieving better PEM and PDEM generate better predictions?\n\nA6: Thank you for your thoughtful suggestion. For this task, visualization is only an auxiliary means. The field of meteorology directly uses the quantitative metrics mentioned in our paper to measure the quality of model predictions, as described in A3. The two metrics PEM and PDEM describe overall performance over a period of time, while PDEM describes dynamic properties that are difficult to capture in static pictures. PEM and RMSE are usually positively correlated. More reflected in the visualization is the level of RMSE. It is worth mentioning that PSNR/SSIM/LPIPS visual effects are often indistinguishable in image super-resolution tasks. To improve the distinguishability of qualitative analysis results, we marked the PEM and RMSE corresponding to the visualization results to facilitate readers to distinguish. Furthermore, the discriminative regions in the visualization are marked with red boxes.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iyh5uAQH1sE",
                "writer": "author",
                "reply_to": "1mMfRqp4pty",
                "title": "To Reviewer WVYT part 1",
                "comment": " Thank you very much for your interest in our work and for your golden suggestions. \n\nQ1: What are the advantages of using the real data collected by two different systems for training super resolution models than using simple downsampling algorithms.\n\nA1: Thanks for the question. When considering real-world meteorological problems, the downscaling algorithm trained on data collected from two different systems will be more helpful. As we mentioned (lines 57-59), \u201cContrary to image data, the proposed real precipitation dataset \u2026 shows the physical characters (e.g., temporal misalignment, temporal sparse and fluid properties, etc., that challenge the downscaling algorithms.\u201d The down-sampled dataset doesn\u2019t reflect these real-world problems. It is necessary to emphasize that the difference between high-resolution observation data and low-resolution observation data in the real downscaling task [1] is not simply the difference in resolution, but the difference in observation methods (e.g., satellite and radar). This situation is like different degenerate kernels (e.g., unknown and bicubic) in image super-resolution. SR models trained on bicubic degenerate datasets (e.g., DIV2K-bicubic) suffer severe performance degradation on the in-the-wild raw data [2,3,4]. On the other hand, many parts of the world are covered by multiple-resolution observations of metrological variables. How to unify them and how to organize them become an important question. When it comes to the two systems mentioned in this dataset, NLDAS (lower-resolution) covers 1980-now, and StageIV (higher-resolution) covers 2002-now. Developing a downscaling algorithm to transfer NLDAS to StageIV allows researchers to extend higher-resolution observations of metrological variables to a longer period, which helps to understand the climate change effect on precipitation. We\u2019ve added further explanations to the main text to explain the advantages of using the real data collected by two different systems.\n\n[1]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\n[2]. Ji, Xiaozhong, et al. \"Real-world super-resolution via kernel estimation and noise injection.\" proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020.\n\n[3]. Hussein, et al. \"Correction filter for single image super-resolution: Robustifying off-the-shelf deep super-resolvers.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[4]. Xu, Yu-Syuan, et al. \"Unified dynamic convolutional network for super-resolution with variational degradations.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\nQ2: The author should compare these two approaches empirically, e.g., demonstrate that models trained with real data are able to reconstruct better high resolution data and hence boost the performance on downstream tasks.\n\nA2: Thank you for your constructive comments. We use the bicubic method (Widely used to synthesize data) to downsample the high-resolution data (624*999) from 2002.7 to 2016.11 to low-resolution data (208 \u00d7 333), so that we generate a synthetic dataset. We employ this dataset to train our model from scratch, and use the original data from 2017.7\\~2017.11 as the test set. We report the test results in the table below:\n\n|  Approach   | MPPE &darr;  | HRRE &darr;  | AMMD &darr;  | CPMSE &darr;  | HRTS &darr;  | CMD &darr; | PEM &darr; | PDEM &darr; | RMSE X100 &darr; |\n|  ----  | ----  | ----  | ----  |----  | ----  | ----  | ----  | ----  | ----  |\n| Ours (real data)  | 4.198 | 221.859 | 0.191 | 1.890 | 7.723 | 9.568 | 0.197 | 0.441 | 0.312 |\n| Ours (synthetic data)  | 5.187 | 311.212 | 0.232 | 3.121 | 9.953 | 12.282 | 0.259 | 0.568 | 0.399 |\n\nIt can be seen from the table that the performance of the model trained on the bicubic synthetic dataset (row #3) is severely degraded. Therefore, the model trained with the real collected data has a great advantage in the task of spatial precipitation downscaling, and also confirms \"A1\". We have added the above experiment to the Sec.6.1 of the new version of our paper.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nf3SWnPfY5",
                "writer": "author",
                "reply_to": "yXth6737L-j",
                "title": "To Reviewer aP2Q",
                "comment": " Thank you very much for your interest in our work and for your valuable comments.  This would be an important work bridging meteorology and computer science. In this paper, we propose the first large-scale dataset for precipitation downscaling that is based on real measured data while the previous models are usually evaluated on synthetic datasets (downsampling the radar maps to generate the synthetic low/high-resolution pairs) and no formal dataset released previously. Under the general trend of the times, it is always good to extend from AI to AI+X. Alphafold's success is such a good example, which tells that deep and well-communicated interaction between AI and other fields could stimulate large scientific breakthroughs. Downscaling is one of the most important tasks in current meteorological research, and the combination with deep learning is also the main research trend [a]. We believe this paper is also a meaningful and successful one and time proves it. To accomplish this work, great and difficult communications between computer science and meteorology side have been done to ensure this precipitation down-scaling is the most important and cutting-edge meteorological task that could be handle by computer science.\n\n[a]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\nQ1: It seems that RMSE is itself a very good summary metric. Thus, it is not clear why we will still need PEM / PDEM.\n\nA1: Thanks for the comment. We agree RMSE is an excellent summary metric and which is also very familiar to computer science society. For meteorology society, researchers usually use metrics that consider many kinds of meteorological phenomena. The metrics Precipitation Error Measure (PEM) and Precipitation Dynamics Error Measure (PDEM) are weighted over a series of metrics with clear physical meaning (reconstruction metrics: MPPE, HRRE, CPMSE, AMMD, and dynamic metrics: HRTS and CMD) and have been applied in downscaling research in meteorology society for a long time (may not in the same abbreviation). We could expect better PEM/PDEM performance when better RMSE performance is observed, but it might not always be the case. To make this dataset practical for computer scientists and meteorologists, here we introduce both PEM/PDEM and RMSE systems to benchmark the algorithms. \nFor details on calculating PEM/PDEM, we\u2019ve mentioned these in the supplementary (Section 3. Metrics) and have added an explanation to the main text. In supplementary Section 3, we also discussed how each metric is calculated and what other literature employs the metric. This explains why better PEM/PDEM demonstrates a better ability to address downscaling problems (from a meteorology sense). For example (line 30-33 in supplementary), \u201cThe mesoscale peak precipitation error (MPPE; mm/hour) is calculated as the difference of top quantile between the generated/real rainfall dataset which considering both spatial and temporal property of mesoscale meteorological systems, e.g., hurricane, squall. This metric is used in most of these papers (for example [15, 10, 2, 6, 11, 14] (refs in our paper) suggest the quantile analysis to evaluate the downscaling quality)\u201d. To be noticed, in meteorology society, researchers tend to evaluate one downscaling algorithm with not a single variable but multiple variables together. However, it is always important to condense the information when bridging two fields. Here we weighted these variables to two to make it easier to compare machine learning models and easier for computer science society to follow.\n\n\nQ2: Currently, the state-of-the-art image super-resolution model is based on vision Transformers (e.g., SwinIR) and the author need to reference the latest progress in this area.\n\nA2: Thank you for your constructive comments. We will definitely add the state-of-the-art transformer-based SR models (e.g., SwinIR) trained on our dataset to the benchmark models.\n\nQ3: The author mentioned that the dataset contains lots of different events such as hurricane, squall. Are the sequences in the dataset marked with the event type?\n\nA3: Thanks for the comment. Yes, we have provided event annotations such as hurricanes, squall lines for relevant frames.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "bc26pvVJeJA",
                "writer": "author",
                "reply_to": "jjMUDMTFaxK",
                "title": "To Reviewer 5jsP",
                "comment": " Thank you very much for your interest in our work and for your valuable comments. This would be an important work bridging meteorology and computer science. In this paper, we propose the ***first*** large-scale dataset for precipitation downscaling that is based on real measured data while the previous models are usually evaluated on synthetic datasets (downsampling the radar maps to generate the synthetic low/high-resolution pairs) and no formal dataset released previously. Under the general trend of the times, it is always good to extend from AI to AI+X. Alphafold's success is such a good example, which tells that deep and well-communicated interaction between AI and other fields could stimulate large scientific breakthroughs. Downscaling is one of the most important tasks in current meteorological research, and the combination with deep learning is also the main research trend [1]. We believe this paper is also a meaningful and successful one and time proves it. To accomplish this work, great and difficult communications between computer science and meteorology side have been done to ensure this precipitation down-scaling is the most important and cutting-edge meteorological task that could be handle by computer science.\n\n[1]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\nFor the technical novelty concern. \n\nOur work demonstrates its novelty in two aspects: \n\n1.The first large-scale open-source dataset for precipitation downscaling that is based on real measured data as described above, which will greatly help bridge the DL/ML community with meteorological science, while promoting the development of AI-for-Science.\n\n2.Novel benchmark model structure design and performance. Existing VSR methods generally include motion estimation modules, which are composed of modules (e.g., PCD in EDVR, Projection Module in RBPN, etc.) with strong video dynamics assumptions. As mentioned in our paper, the assumptions do not match precipitation downscaling. Unlike them, our implicit dynamic estimation module (IDEM) is a low inductive-bias module (e.g., transformers outperform CNNs), it only contains N-2 (N is the input adjacent frames, 5 frames in our model setting) weight-sharing small networks, so that IDEM can explore the inherent laws in the precipitation data without constraints/assumptions. In addition, self-attention, as a low inductive-bias operator, has achieved huge performance improvements in computer vision tasks (e.g., image classification, object detection, etc.). The low inductive-bias setting allows self-attention to fully explore the inherent laws within the data without being constrained by data assumptions [2]. At the same time, the self-attention operator also exhibits stronger generalization ability. Analogously, this is also the potential reason why our IDEM works better on the precipitation dataset. The results in Table 1 (in our paper) show the superiority of our model. Furthermore, our IDEM module also shows very competitive performance on the VSR data set: Vid4(4\u00d7) Average RGB PSNR 25.85 (EDVR Average RGB PSNR 25.83, DUF Average RGB PSNR 25.79). We will add more details (novelty analysis and performance analysis in VSR task) about the proposed model in Sec.5 and Sec.6.\n\n[2]. Esser P, Rombach R, Ommer B. Taming transformers for high-resolution image synthesis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 12873-12883.\n\nQ1: Eastern coast of US has been selected for data collections. What about other regions ?\n\nA1: Thank you for your constructive comments. There several reasons for selecting the eastern coast of US. \n1. Compared with other regions in the world, the US has systematic and complete observational data (NLDAS (lower-resolution) covers 1980-now, and StageIV (higher-resolution) covers 2002-now) of various resolutions from different observational systems (e.g., satellite, weather radar, etc.).\n2. Compared to the eastern US, the West Coast has very little precipitation, which is not helpful for our task, so we discarded the West Coast to reduce the redundancy of the dataset.\n3. In our future work, we will expand to more regions of the world.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8DMtDBXurHN",
                "writer": "author",
                "reply_to": "WBmpyaytXL",
                "title": "To Reviewer 9mB6",
                "comment": " Thank you very much for your interest in our work and for your golden suggestions. This would be an important work bridging meteorology and computer science. In this paper, we propose the first large-scale dataset for precipitation downscaling that is based on real measured data while the previous models are usually evaluated on synthetic datasets (downsampling the radar maps to generate the synthetic low/high-resolution pairs) and no formal dataset released previously. Under the general trend of the times, it is always good to extend from AI to AI+X. Alphafold's success is such a good example, which tells that deep and well-communicated interaction between AI and other fields could stimulate large scientific breakthroughs. Downscaling is one of the most important tasks in current meteorological research, and the combination with deep learning is also the main research trend [1]. We believe this paper is also a meaningful and successful one and time proves it. To accomplish this work, great and difficult communications between computer science and meteorology side have been done to ensure this precipitation down-scaling is the most important and cutting-edge meteorological task that could be handle by computer science.\n\n[1]. Reichstein M, Camps-Valls G, Stevens B, et al. Deep learning and process understanding for data-driven Earth system science[J]. Nature, 2019, 566(7743): 195-204.\n\nQ1: SPDnet is not a very good name\u2026\n\nA1: Thank you for your constructive comments. SPDNet is a straightforward name derived from shorthand for \"Spatial Precipitation Downscaling\".\nWe believe a good name is very important to our work, so we will look for a better one.\n\nQ2:  \"It says in the checklist that the code and the data are proprietary. This needs to be clarified. What\u2019s the point of publishing a dataset if it cannot be used by others?\"\n\nA2:\nThanks for the comment. \n***All relevant codes and datasets are open-source for research purposes.***\nWe apologize for the error in filling out the checklist, we have changed the item \"Do you include licenses for code and datasets? [No] Code and data are proprietary\" to \"Do you include licenses for code and datasets?[ Yes] see Section 4.2\".\nThe dataset and the code are not proprietary.\nWe will build a dedicated github repository and website for users to easily use our datasets and codes.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WBmpyaytXL",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "",
                "comment": " There is not much to say here. The paper organizes a large precipitation dataset from both high res and low res sources and illustrates some baselines. The problem addressed is both interesting to ML (esp. to those interested in mixing physics and ml), and important. The question then, is; given that this data can be obtained directly from the original sources, is a new organization of it necessary and does it warrant a publication in an ML conference. Although I expect other reviewers will disagree, I have a positive view. Publishing this paper will probably increase interest in ML in this set of problems, and some in the community will find the dataset useful. See above SPDnet is not a very good name\u2026 It says in the checklist that the code and the data are proprietary. This needs to be clarified. What\u2019s the point of publishing a dataset if it cannot be used by others? ",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "jjMUDMTFaxK",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "",
                "comment": " This paper presents a large scale dataset for spatial precipitation downscaling which contains more than 62, 400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, ready to help the evolution of deep learning models in precipitation\ndownscaling. The precipitation maps  collected in the dataset  cover various meteorological phenomena such as hurricane and  squall. The data are organised in time series of maps, with 720 maps/month. Comprehensive metrics are also provided to evaluate the performances of models.  This paper is well written and brings comprehensive dataset for spatial precipitation. However  the technical novelty is low.\n Eastern coast of US has been selected for data collections. What about other regions ? The dataset lacks precipitation maps for several regions",
                "rating": 6,
                "confidence": 2
            },
            {
                "review_id": "yXth6737L-j",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "",
                "comment": " The paper proposed a large-scale spatial precipitation downscaling dataset named SPDNet. The dataset contains more than 62400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, and covers more than 9 million squre kilometers of land area. The author also introduced 6 metrics that evaluate different aspects of the downscaling models, and 2 summary metrics that combine these 6 individual metrics. The author viewed the task as a video super-resolution problem and compared 14 methods. From the experimental results, the overall performance of the video super-resolution (VSR) models are better than Single-Image Super-Resolution (SISR) models.\n Stengths:\n\n1. The paper proposed the first large-scale precipitation downscaling dataset. This is an important scientific problem and a large-scale dataset can help move the area forward. In addition, the author pointed out the unique characteristics of the task such as temporal misalignment, temporal sparse, and fluid properties.\n2. The paper proposed 6 metrics for evaluating the models, including 4 reconstruction metrics that focuses on evaluating if the predicted high-resolution precipitation map matches the ground-truth, and 2 dynamic metrics that evaluate the dynamics of the predicted precipitation (via first order dynamics).\n3. The paper compared 14 models, including the Kriging method that has been widely used in the geospatial community, and other SISR and VSR methods. The author also extended SRGAN, EDSR, ESRGAN to be VSR methods.\n\nWeaknesses\n1. It seems that RMSE is itself a very good summary metric. Thus, it is not clear why we will still need PEM / PDEM.\n2. Currently, the state-of-the-art image super-resolution model is based on vision Transformers (e.g., SwinIR) and the author need to reference the latest progress in this area.\n 1. The author mentioned that the dataset contains lots of different events such as hurricane, squall. Are the sequences in the dataset marked with the event type?\n 1. The paper will be limited regarding the coverage of the baseline methods. However, it is difficult to cover all the latest image super-resolution methods so it is acceptable.\n\n",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "1mMfRqp4pty",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_0cn6LSqwjUv",
                "title": "",
                "comment": " This paper proposed a dataset consists of precipitation image sequences named SPDNet for spatial precipitation downscaling as well as a novel implicit dynamics estimation driven model. The proposed model as well as baseline models are evaluated on SPDNet with task specific metrics.  **Strengths**\n1. **Valuable dataset**: The large scale dataset SPDNet is in high resolution and in sequence, which is a valuable contribution to data-driven meteorological research.\n2. **Benchmark**: The authors have evaluate SOTA super resolution models on the proposed dataset ,and thus provide a good benchmark.\n\n**Weaknesses**\n1. **Necessity of low resolution data**: The authors claim in Introduction that the data obtained by simulated degradation is different from the real data collected by two different systems. However, there is no further discussion on it. It is insufficient to argue that their approach is better than obtaining low resolution data by simply downsampling the high resolution data. The author should compare these two approaches empirically, e.g., demonstrate that models trained with real data are able to reconstruct better high resolution data and hence boost the performance on downstream tasks.\n2. **Unconvincing evaluation**: There are no clues about the effectiveness of proposed novel compound metrics PEM and PDEM. It would be much more convincing to conduct empirical studies to prove that models achieving better PEM/PDEM demonstrate better ability on addressing some concerned issues.\n3. **Missing training details**: Selected baselines are not designed for precipitation data. It is necessary to (at least slightly) modify and tune the models for fair comparison. However, there is no information about these details except for a single statement \"we also adjust the hype parameters of these models for better performance\" in Supl. Sec.5.2.\n4. **Formatting errors**: There are some language errors like \"e.t.c.\" $\\rightarrow$ \"etc.\", and misleading notations in mathematical expressions such as missing brackets in Eqn.1, missing description of \"L\", \"H\", \"T\" in line 250. Scores of 6 commonly used metrics in Table 1 should also be highlighted. 1. What are the advantages of using the real data collected by two different systems for training super resolution models than using simple downsampling algorithms? If the two domains need to be similar, directly downsampling the high resolution data is both cheap and effective. If the two domains need to differ a lot from each other, it would be better to categorize the task as domain transferring instead of \"spatial precipitation downscaling\".\n2. The qualitative results shown in Figure 4 are hard to distinguish. Could the authors provide more evidence to demonstrate that the models achieving better PEM and PDEM generate better predictions?  Listed in **Weaknesses**",
                "rating": 4,
                "confidence": 4
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "innovation on the neural architecture side of the work",
                "Sentiment Expression": "not extreme",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "codes and datasets",
                "Sentiment Expression": "open-source for research purposes",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the dataset itself, the wide variety of models examined and the large set of evaluation metrics",
                "Sentiment Expression": "offers value to the community",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "SRGAN, EDSR, ESRGAN from Single Image Super Resolution (SISR) methods to Video Super Resolution (VSR) methods",
                "Sentiment Expression": "extended",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "ygWoT6hOc28": {
        "paper_id": "iclr_2021_ygWoT6hOc28",
        "paper_title": "Regression Prior Networks",
        "paper_abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD2), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD2 to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "This paper presents a useful contribution to the growing literature on uncertainty estimation with deep learning. The review process has significantly helped with strengthening this paper, specifically with the concerns about novelty and sufficient comparisons to existing work. I hope you will continue to improve this work for submission to a future venue.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "awTxN3m2fOi",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "A simple extension of Prior networks models to regression. ",
                "comment": "This paper extends Prior networks models, previously introduced for classification, to regression problems.  Prior networks are neural networks whose main target is to \"modelling uncertainty in classification tasks by emulating an ensemble using a single model\".  Standard Prior networks models output the parameters of a Dirichlet probability distribution. This Dirichlet probability distribution then defines a distribution over categorical probability distributions over the different classes. This hierarchical approach allows to better capture uncertainty. The presented approach extends this framework to regression tasks. So, instead of returning the parameters of a Dirichlet distribution, it returns the parameters of a Normal-Wishart distribution, which then defines a probability distribution over Normal distributions, and, in turn, each Normal distribution defines a probability distribution over the value of the target variable.  \n\n\nPros:\n* The presented approach is sound and addresses a relevant problem, which is modelling uncertainty for regression problems. \n* A method for distilling an ensemble model into a single model while maintaining accuracy is also proposed. \n* The proposed approach does not incur in computational and memory overheads like standard deep ensembles. \n* This work properly approaches technical difficulties (such as employing numerical stable precision parametrizations of the Normal-Wishart distribution) that arise in this kind of problems.\n\nCons: \n* The presented approach does not introduce any novel idea or insight. It's a relatively simple extension of a previously published method. \n* The empirical results do not show a clear advantage of the presented approach wrt previously published proposals. \n* The advantage of having a small computational and memory overhead is not properly evaluated with other proposals which also have a small  computational and memory overhead [1] (although this proposal has not been defined for regression problems, the adaptation to regression is as simple as the adaptation of the DeepEnsembles models employed in this work). \n\n\nI can not recommend the acceptation of this paper because I find the originality of the work quite limited. Although the extension of prior networks to regression task is mot really straightforward because of technical issues related to the problem of learning the parameters of a Normal-Wishart distribution. The general strategy to do that exactly matches the previous steps employed when introducing prior networks.  In consequence, this work does not provide any new relevant insight into the problem of modelling uncertainty and learning models with well-calibrated predictions. \n\n\nMinor comments:\n- Eq (14): T parameter is not defined. Temperature? \t\n- Typo at the end of Page 5: [-25,20] --> [-25,-20]\n- ENSM is defined after Table 1. \n- Fix the following reference:\nAndrey Malinin and Mark JF Gales. Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness. 2019. \n\nPost-rebuttal:  I thank  the authors' effort for the improvement of the manuscript following the comments of the different reviewers. I think the overall quality of the paper has really improved. But, after many thoughts, I still think there is a limited novelty in this paper. I have increased my score to 5. But I can not recommend this paper for publication. \n\n  adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept.\n\n[1]  Wen, Y., Tran, D., & Ba, J. (2020). BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning. arXiv preprint arXiv:2002.06715.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "_GrpdkEvnoU",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Official Blind Review #1",
                "comment": "Prior Networks (Malinin & Gales, 2018) use Dirichlet prior over categorical predictive distributions to distill ensembles for classification tasks. This paper extends Prior Networks to the regression setting by using a Normal-Wishart prior in order to attempt to match the predictive diversity. The authors define the model and loss terms including analytical derivation and evaluate their proposed approach with synthetic data, UCI datasets and monocular depth estimation. \n\n_Strengths_:\n- The paper is well-written and clearly structured.\n- Most design choices are justified.\n- Simple idea (in a good way!) which seemed to work well, shown by the evaluation.\n\n_Weaknesses_:\n- Most of the work seems to be heavily based on Prior Networks (Malinin & Gales, 2018). Even Section 2.1 seems to be exactly like the Subsection in the paper about Prior Networks. This paper mainly focuses on an extension to the regression task. Therefore, the contribution / novelty of this paper is incremental. However, I still think the authors did a good job to present a general distillation method for regression task. Therefore, I would consider the novelty a minor weakness.\n- I am on the fence about specifying the OOD dataset for learning with the loss in Eq. 8. I believe it is difficult to decide what kind of model to use for generating the OOD dataset, thus, the model choice can lead to large differences in performance. This is not really discussed. Further, the models trained have more data available for training, I believe it is not quite fair to compare against models which only have been trained on in-domain-data.\n- There are no comparisons to other approaches for distillation of regression tasks. I understand, that this paper wants to show a viable general approach for regression distillation, however, this work is not the first one to do so and therefore should consider existing work.\n\n_Overall assessment_: For me, this paper is borderline. The weaknesses, especially the OOD dataset used for training and the lack of comparisons in the evaluation are concerns. However, I like the idea and the execution so therefore, I would recommend a weak accept (6).\n\n_Detailed comments and questions_:\n- OOD data: I have seen that you have an ablation for the degree of regularization on the OOD dataset. However, what about different OOD data? Why choose KITTY and not a different dataset? Were there any large difference in performance?\n- Table 3: I notice that NLL performance of distilled models are better than the actual ensemble, how can this be?\n- OOD detection for monocular depth estimation: Did you also trained the comparing models with the OOD data, e.g. DD?\n- Comparing models: Have you consider comparing your model to other ones, e.g. [1, 2]? This could improve your paper and approach to show that it also consider existing work on regression distillation.\n\n_Post-rebuttal_:\nI really appreciate the authors adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept.\n\n[1] Chen, G., Choi, W., Yu, X., Han, T. and Chandraker, M., 2017. Learning efficient object detection models with knowledge distillation. In Advances in Neural Information Processing Systems (pp. 742-751).\n[2] Saputra, M.R.U., de Gusmao, P.P., Almalioglu, Y., Markham, A. and Trigoni, N., 2019. Distilling knowledge from a deep pose regressor network. In Proceedings of the IEEE International Conference on Computer Vision (pp. 263-272).",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "uJpIqmq2Bp5",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Final revision",
                "comment": "We added a final revision to fix a mistake in eq. 13 and fix a few typos.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yG1VHxH7Mzi",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Updated Manuscript Text",
                "comment": "Dear Reviewers,\n\nAs per the reviewers' and public comments, we have provided additional comparisons to Deep Evidential Regression and Mixture-Density Distillation, as we have described in our previous post. Now we have made changes to the text to reflect the reviewers' other comments. We hope that these changes sufficiently address all of your concerns.\n\nWe describe the changes section by section:\n\nIntroduction (minor changes)\n1. Modified second paragraph to mention evidential approaches\n2. Modified final paragraph to correctly point to roots of idea\n\nRegression Prior Networks (lots of changes)\n1. Sections styles changed for extra space\n2. Added clarifications into discussions of RKL loss function, including role of OOD data, and effect of beta.\n3. Added a discussion of EnD and MD-EnD to ensemble distribution distillation section\n4. Added a final \"Related work\" section, which discusses Deep Evidential Regression and efficient ensemble methods (batch ensemble.\n\nSynthetic Experiment\n1. X-axis range in images widened, so that it is clear from figure C that knowledge uncertainty rises sharply\n\nUCI Experiments (minor tweaks)\n1. Section reworked, experimental protocol clarified.\n2. Added reference to appendix which C3 which discusses PRR\n\nMonocular Depth Estimation (SIGNIFICANTLY reworked for clarity)\n1. Clarified the depth estimation performance metrics and made a forward reference to appendix D1, which they are described.\n2. Added experiments on DER and MD-EnD to both tables 3 and 4\n3. Restructured discussion of OOD experiments. Behaviours explains.\n4. Added forward reference to an additional set of OOD detection experiments in the appendix.\n5. Added forward reference to examples of IN/OOD data so that it is easy to see *exactly* what the models are trying to discriminate between.\n6. Expanded discussion about the difficulty of choosing appropriate OOD data, which highlights that EnD^2 is the superior approach, as it doesn't suffer from this difficulty.\n\nConclusion (Minor clarification at the end)\n\n\n\nWe thank all the reviewers for their effort!\n\nSincerely,\nAuthors\n\n\n\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wWmPr0CWGN-",
                "reply_to": "aLiVwZ4XDHC",
                "title": "Thank you for the stimulating discussion!",
                "comment": "We've updated the paper and are about to make a post describing all the updates. We've added a discussion about BatchEnsembles and the Multi-head distillation method.\n\nWe appreciate the effort you put into this discussion!\n\nMany thanks,\nAuthors\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aLiVwZ4XDHC",
                "reply_to": "V-QYbIwm34",
                "title": "Reply to Response to R4",
                "comment": "Thanks again for your nice replay!\n\nI think my concerns, in terms of comparison, with other related works are already addressed. The comparison with multi output heads is also reasonable and enough for me. Even though, I think it would be worth to also include the discussion of other methods like BatchEnsembles.\n\nRegarding novelty and relevance of the work, I still have the concerns I rose in my original review. But, I promise I will give new thoughts in the light of other reviewers and all your responses. \n\nThanks for the fruitful discussion. ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EmqRkAeB_sh",
                "reply_to": "ZoIDbGzIXZw",
                "title": "Response to reviewer 2",
                "comment": "Thanks for you for your comments!\n\nWe explored the effect of gamma in the depth estimation setting in the appendix, table 11. The effect of beta is primarily to make the the expected (under Normal-Wishart) negative-log-likelihood be a tight bound to the NLL of the expected (vs expected NLL). High beta makes the bound tight and the training to be more accurate. We will add this this to the discussion and make it more clear.\n\nRegarding a discussion of Dirichlet Prior Networks - unfortunately, the space is rather limited to be able to discuss everything in detail. We will try to improve the discussion of RPNs such that it is more self-contained. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yQTOTbRfj0t",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "New results in manuscript",
                "comment": "Dear All,\n\nWe tried to provide new results in the open review system, but it turns out that it is very poor (and inconsistent across write/preview and what it actually posts) at representing tables. Thus, we have updated TABLE 3 and TABLE 4 in the manuscript, where we have added results for:\n\nDeep Evidential Regression (ArXiv 2019 version) (will only be displayed during rebuttal)\n\nDeep evidential Regression (NeurIPS 2020 pre-proceedings version) [2] \n\nMixture Density Distillation (only NYU Depth V2 so far, KITTI still training...) [3,4]\n\nNOTE - we are still in the process of updating the text. This update is purely intended to demonstrate updated results.\n\n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" ArXiv. 2019 (version 1)\n\n[2] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n\n[3] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al). \n\n[4] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "V-QYbIwm34",
                "reply_to": "QPCxH9YJxku",
                "title": "Response to R4",
                "comment": "We are very happy to provide a discussion of various alternative approaches to making ensembles computationally cheaper. Indeed, we\u2019ve found 2 papers on a similar approach to distilling an ensemble into a single model [1,2] by having multiple output heads, where each head is meant to replicate the behaviour of a particular ensemble member. We believe that this is as close a baseline as we can make - it is almost identical in compute to EnD^2, it's also a distillation approach, and it also attempts to preserve ensemble diversity. We have provided these results  in TABLE 3 and TABLE 4 of the updated manuscript. Generally, this works a little better for predictive quality than EnD^2, and worse for OOD detection. Results on Kitti for MDD are not ready yet, but are being calculated.  Do you find these results sufficient? \n\nRegarding BatchEnsembles - we\u2019ve had a closer look, and we currently don\u2019t actually understand how it is cheaper *at run time*. While it is true that a BatchEnsemble model has about as many parameters as a single model *on disk*, at *run time* it trades of increased use of GPU memory (batch is replicated) for efficient use of said GPU. Thus, it may be faster than sequential evaluation of an explicit ensemble, but it certainty is not more memory efficient at run time. We will certainly cite, mention and discuss this range of works. However, implementing it is non-trivial -the libraries you\u2019ve sent us are in Tensorflow, not Pytorch, so we cannot directly carry them over. \n\nIf you insist, we CAN promise to implement BatchEnsembles and add this into the camera ready paper (if this paper is accepted). We would definitely keep this promise, as it would be quite embarrassing to make it publicly and then break it.  \n\n[1] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al).\n[2] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wP-LoPyTYVB",
                "reply_to": "UjYwY1rS0MR",
                "title": "Experimental results",
                "comment": "We initially tried to add table here, but unfortunately it turns out that the system ignores formatting and just dumps numbers. Results are presented in TABLE 3 and TABLE 4 of updated manuscript. \n\nFor all versions of DER (2019 ArXiv and 2020 Neurips) we used a weight of 0.1 on the evidence regulariser. For DER 2020 we checked the implementation of the student NLL against the pytorch version and made sure that everything is correctly parameterised.\n\nThe results show a few things. \n\nFirstly, the old version of DER (ArXiv 2019) doesn't work well, both in terms of predictive performance, and in terms of uncertainty estimation. Which expected, due to the error on the loss.  \n\nSecondly, the new version of DER (NeurIPS 2020 preproceedings) works much better. In terms of predictive performance it is comparable to a single probabilistic DenseDepth model , though still with a minor degradation. In terms of OOD detection performance of models trained on NYU- it  yields rather competitive performance, marginally worse than the ensemble, and outperformed by EnD^2.\n\nThirdly, on KITTI OOD detection the LSUN OOD data is OOD not only because it is indoors, but also because it represents images which are very close to the camera, relative to images seen in Kitti, which features a range of depths. Here, all models, except RPN + RKL, interpret the OOD data as being in-domain using measures of total uncertainty. Using measures of ensemble diversity (knowledge uncertainty), ensembles, RPN-RKL and EnD^2 are able to detect OOD images successfully. Notably, DER does not seem to be able to. The reason for this is that the evidence regulariser biases the DER model tol yield high evidence in regions of low absolute error, and low evidence in regions of high absolute error. As a result, regions which are closer (bottom half of kitty images) always have higher evidence. As LSUN OOD is very close to the camera, the DER model yields high evidence. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UjYwY1rS0MR",
                "reply_to": "v2KrN3U9lHY",
                "title": "Response to A. Amini",
                "comment": "Thanks for your comment! \n\nRegarding Evidential Deep Learning, particularly [2] - we think it is a rather elegant alternative interpretation for uncertainty estimation, rooted in Dempster-Schafer Theory of evidence, which yields a model which is structurally identical to a Dirichlet Prior Network. However, we are sceptical of the principal claim that this is a reliable single-model uncertainty estimation approach which doesn\u2019t require OOD data or indeed any other approach to enforcing a particular behaviour for OOD inputs which has an understandable mechanism of action. However, that is only our opinion - a rigorous large-scale validation is necessary and clearly would be a useful future direction of investigation. \n\nRegarding your paper (congrats on getting into NeurIPS!). We see our work as being very much an extension and verification of the ideas proposed in [3]. We became aware of your work around the same time as we began ours, however, to the best of our knowledge (until your comment) it was a submission at ICLR2020. Furthermore, upon examination at the time, we determined that the loss function (expectation of square error given samples from the Normal-inverse-Gamma) had an error in the derivation (equations 7-9, derivation 7.1.2 eq. 22-23 in the appendix). As a result, we had no grounds on which to believe in the validity of the results. Looking at the ArXiv submission now - it has not been updated within the last year and still contains the error . \n\nWith respect to the experimental setup - while we both use UCI (which is standard) and NYU Depth v2, our evaluations are quite different. We have used a standard architecture, provided detailed performance comparisons to baselines in depth estimation [4,6], and analysed the properties of several uncertainty measures via ROC-AUC against a range of OOD datasets.\n\nHowever, we have now found your new NeurIPS2020 version in the pre-proceedings (which were released after the ICLR2021 submission deadline), and we see that the mathematical error has now been fixed. In fact, an altogether different loss function (NLL of the student distribution) is used in addition to the evidence regularizer. The results are largely the same. \n\nWe\u2019ve implemented DER both as it is on ArXiv and as it is in the NeurIPS2020 pre-proceedings, and present the results in the next post. Unfortunately, a direct number-for-number comparison to your work is not possible, as there are no summary performance results for your model, and figure 4B contains a range for RMSE which is about 20 times smaller than what is reported in the depth estimation literature [4,6] (you\u2019ve probably scaled something differently). Furthermore, you use a different OOD dataset which seems to be very easy to separate out, as all models achieve a ROC-AUC of about 0.99. \n\nWe will add these results to our paper (omitting the old DER), and cite your work (and the original Evidential work), in our paper. We shall upload an updated version shortly. \n\n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n[2] Sensoy, Murat, et al. \"Evidential deep learning to quantify classification uncertainty.\" Advances in Neural Information Processing Systems. 2018.\n[3] Uncertainty Estimation in Deep Learning with Application to Spoken Language Assessment, Malinin, 2019. PhD Thesis\n[4] High Quality Monocular Depth Estimation via Transfer Learning (Alhashim & Wonka, 2018)\n[5] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al).\n[6] https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2 ,\n[7] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QPCxH9YJxku",
                "reply_to": "7Qx49nQWUAt",
                "title": "Reply to Authors comments",
                "comment": "Thank you for reply! \n\nI agree deep ensembles can be considered as an upper bound. My point is that there are alternative methods like BatchEnsembles, Rank-1 BNNs, SNGP, etc. (this repo https://github.com/google/uncertainty-baselines contains the references and high-quality open source implementations of all of them) which could be easily adapted for regression and which have much lower time and memory complexity than ensemble methods. In my opinion,  at least one of them should be considered here as relevant baseline, because  only comparing wrt deep ensembles gives the impression that your method is the only available alternative that provides  a big reduction in time and memory complexity  wrt deep ensembles. I think it is fair to show (or at least discuss) that there are other approaches that can be employed here to strongly reduce the memory and time complexity of deep ensembles. \n\nThe lack of novelty of this paper, as acknowledged by other reviewers, puts much more pressure in the empirical evaluation. As I said before, there are well-established prior works which directly address the high memory and time complexity of deep ensembles that can be easily adapted to regression and which, in my opinion, should be considered by this work. ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "v2KrN3U9lHY",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Some missing related work",
                "comment": "Thanks for submitting this work! In line with many of the reviewer comments regarding novelty, I was also wondering about the relation of the proposed contribution to published evidential deep learning (EDL) approaches [1,2]. Namely, published at NeurIPS this year, Deep Evidential Regression [1] also proposes learning a 1D Normal-Wishart distribution directly to infer representations of uncertainty specifically in the continuous regression domain as well (not classification). The proposed contribution presented here, like [1], also provides experimental results on nearly identical tasks from UCI and on monocular depth estimation. Also, note that deep evidential networks are structurally identical to prior networks (PN), with the only differences being in their respective objective functions (PN additionally require OOD data to train with, EDL does not). Given that a preprint of [1] appeared over a year ago and is now peer-reviewed/published, as well as the foundational work done in the classification domain [2] is over two years old now, I think it would be very helpful for the authors to cite these papers and and discuss their contributions relative to these works. \n\nI also hope this will help orient reviewers to the context for this submission and perhaps to some contributions that may have been missed. \n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n\n[2] Sensoy, Murat, et al. \"Evidential deep learning to quantify classification uncertainty.\" Advances in Neural Information Processing Systems. 2018.",
                "writer": "public",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZoIDbGzIXZw",
                "reply_to": "FRAmiZWGSy",
                "title": "Response to Authors",
                "comment": "Thank you for your reply.\n\nWhat I was missing is a high-level description of prior networks. A background section on already existing prior networks for e.g. classification could be nice to include.\n\nRegarding the parameters, I was referring to the gamma parameter in Eq. (8) and the beta parameter in Eq. (11). ",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "on7otAuRWqC",
                "reply_to": "6l-gyq0bIrO",
                "title": "Reply to Comments of Reviewer 3",
                "comment": "Thank you for your detailed comments! Allow us to address them:\n\nSEC 2.2 - \n  A. Z is indeed a constant that doesn't depend on the parameters of the model. We will make this clear. \n  B. I'm afraid that this isn't the case. The OOD loss doesn't regularise the choice of beta. Rather the OOD loss is supposed to inform the model of regions beyond which it has no understanding of the data. Clearly, this requires one to decide on and choose an OOD dataset, which is non-trivial. \nC. p(y | mu, Lambda) represents a Normal distribution sampled from the Normal-Wishart. \n\nSEC 2.3 - Yes, this is what I mean - the dataset can be seen an empirical distribution to which we minimise KL, or equivalently, maximise likelihood. Phi represents the parameters of the model into which we are distribution-distilling the ensemble. \n\nSEC 3.  ENSM is the Deep Ensemble. Respectfully, the behaviour of the estimates of data uncertainty out of domain is not relevant - data uncertainty is only important in-domain. Indeed, we cannot give any guarantees on the behaviour of data uncertainty in the OOD region. What we actually care about is that estimates of *knowledge uncertainty* increase as we move further out of domain, which is the case (though perhaps not so easy to see from the picture). We will update the picture to make this clearer.\n\nSEC 4   \nA. We will make the experimental protocol clearer in this section. We will add the description of the Prediction Rejection Ratio in the appendix. It shows what part of the best possible error-detection performance our algorithm covers. \n\nB. UCI datasets are very common datasets for evaluation in related works, that\u2019s why we decided to add them despite their simplicity.\n\nC. To obtain train-OOD-data for RKL, we used factor analysis with increased noise and latent variance. This is a simple generative model. We trained it on in-domain data and added noise to the latent variables to generate out-of-domain examples for RKL. This generative model is simple and appropriate for table data, while GANs are not usual for table data. Also, UCI datasets have few examples and small feature spaces, therefore it could be hard to train GANs on them.\n\nD. For the evaluation of OOD-detection performance, we took parts of other UCI datasets as OOD data. We made sure that the OOD-data comes from different domains and feature distributions are different. We felt that this was the best we could, as, to the best of our knowledge, there has been no established research on OOD detection for tabular datasets.\n\nSEC 5\n\nPerformance metrics in table 3 are usual for Monocular Depth Estimation. They describe model performance from different sides and are usually shown in all papers on this topic. A good description of these metrics can be found in the original Monocular Depth Estimation paper \u201cDepth Map Prediction from a Single Image using a Multi-Scale Deep Network\u201d by Eigen et al., in section 4.3. \n\nDelta 1,2,3 shows a percent of predictions such that the maximum of two fractions: (a) between predictions and targets, (b) between targets and predictions is less than corresponding thresholds: 1.25, 1.25^2, and 1.25^3. Rel stands for absolute relative error and log10 for RMSE between logarithms of predictions and targets. These losses show different properties of the model: deltas help to understand confidence intervals of the model, Rel shows the ratio between prediction error and target, and log10 shows error in the log-space. \n\nWe will add the definition of these metrics to the text and  attempt to simplify table 3 as much as possible.\n\nWe fully understand where you are coming from regarding table 4 and figure 3. We will rewrite this section and make it more understandable, it was hard to fit everything into a given space.\n\nRegarding Table 4 and the behaviour of the NWPN (RPN+RKL) model - we hypothesise this is the result of the interaction between the in-domain and OOD training data. It was very hard to get the models to appropriately train. Likely because discrimination between ID/OOD is a very global task (global scene understanding), while depth estimation requires more local data. The tasks are therefore anti-correlated in training. In contrast, EnD$^2$ doesn't suffer from the same problems and only relies on ID training data.\n\nThus, what we aim to show is that: 1) EnD$^2$ can appropriately replicate and surpass the ensemble's OOD performance. 2) NWPN (RPN+RKL) can sometimes do near-perfect OOD detection, but isn't as reliable in this particular task with this choice of OOD data. \n\n\nIn Figure 3 the left image is from KITTI and the right image is from NYU datasets. Using these images we aim to show that error of a prediction correlates with increased uncertainty of the model. Additionally, we wanted to show how the uncertainties of the ensemble and EnD$^2$ model compare, and we can see that the EnD$^2$ model consistently yields higher uncertainties, as it over-estimates the support of the ensemble.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "FRAmiZWGSy",
                "reply_to": "KMA7obppU9F",
                "title": "Reply to Reviewer 2 Comments",
                "comment": "Thank you for your comments! We will now address your comments point-by-point:\n\n1. Regarding alternative Bayesian baselines:\n\nThe approach we use to generate ensembles  - Deep Ensembles [4], are already the current go-to SOTA Bayesian approach to uncertainty estimation [1,2,3]. Approaches like Dropout, while also capable of generating ensemble, are shown to be consistently inferior. Variational Inference is typically even worse and has never been successfully scaled to complex tasks such as Depth Estimation, to our knowledge.\n\nOur favoured proposed approach - Ensemble Distribution Distillation for regression, allows us to take a SOTA DeepEnsemble (which is the baseline relative to which we compare) and distill it into a single model, generally preserving most of the ensemble\u2019s gains. This allows us to replicate both the ensemble\u2019s predictive performance as well as uncertainty measures at the computational and memory cost of a single model. Thus, suffer a minor reduction in predictive quality (and no loss in the quality of uncertainty estimates) for an M-fold (where M is the ensemble size) reduction in computational and memory cost relative to the ensemble baseline. \n\n       [1] Can you trust your model\u2019s uncertainty? Evaluating predictive uncertainty under dataset shift.\n\n       [2] Pitfalls of in-domain uncertainty estimation and ensembling in deep learning.\n\n       [3] Deep ensembles: A loss landscape perspective.\n\n       [4] Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n\n\n2. Regarding additional training parameters - Could you please be more specific, so that we could address your concerns in detail?\n\n3. Regarding the difficulty of understanding the paper - Are there particular changes you would like us to implement which you think would make this paper more accessible? ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7Qx49nQWUAt",
                "reply_to": "awTxN3m2fOi",
                "title": "Reply to Reviewer 4 Comments",
                "comment": "Thank you for your review! Please allow us to address your concerns:\n\n1. Regarding empirical results:\n\n Could you please elaborate what you would see as a clear advantage? In terms of inference-time compute and memory the M-fold (where M is the ensemble size) advantage over Ensembles is clear. In terms of predictive performance - we outperform single models, and get close to the ensemble. Replicating the ensemble\u2019s predictive performance completely is an upper bound. In terms of OOD Ensemble-Distribution Distilled  RPNs outperform the ensemble. If there some specific comparison you would like us to provide which would convince you?\n\n2. Regarding BatchEnsemble:\n\nBatchEnsembles are interesting, however an efficient implementation of BatchEnsembles is non-trivial and there is no available code in pytorch (The original work was done in Edward). A naive implementation would be as expensive during inference as DeepEnsembles, if not more so, as it may require a larger ensemble to reach the same performance. If you insist, we will explore this approach, but this will likely be infeasible within the time-frame of the rebuttal period. \n\nP.S. Thank you for finding the minor errors. We will fix them. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "DYGxsFPHho-",
                "reply_to": "_GrpdkEvnoU",
                "title": "Reply to Review 1 comments.",
                "comment": "Thank you for your review! Allow us to address your concerns on a point-by-point basis. \n\nREGARDING WEAKNESSES\n\n1. We agree with your concerns regarding the choice of OOD dataset. Defining an appropriate one for classification tasks is already non-trivial - doing so is even more challenging for regression. This is why we place greater emphasis on Ensemble Distribution Distillation - it does not require an OOD dataset and yields superior predictive performance relative to RKL-trained Regression Prior Networks.  \n\nWe will use the extra page to present a discussion regarding difficulties of using an OOD dataset, and will shortly upload an updated manuscript. \n\n2. With regards to regression distillation, we would like to point out that previous work has examined the distillation of a *single model  into a single model*. In our work we consider distillation of *an ensemble of probabilistic models into a single probabilistic model*. Limited prior work has examined this scenario, and it is difficult to provide a sensible baseline . We have attempted to do so through Ensemble Distillation (EnD), though it seems this approach also has its limitations. \n\nIt is, in general, not entirely clear whether combining an ensemble of probabilistic models is better done as an arithmetic or geometric mixture. A full analysis of ensembles of probabilistic regression models deserves an investigation of its own. Furthermore, to our knowledge, probabilistic ensemble distillation for regression has been a generally under-explored area. If you could point us to a more appropriate baseline, we would be happy to consider it!\n \t\nWe will add a detailed discussion of this issue into section 2.3 and upload an updated manuscript shortly. \n\nREGARDING DETAILED COMMENTS\n\n1 We were limited in the compute we had available for this project and decided to focus on the ablation study we did, rather than swapping out OOD datasets. In general, for Depth Estimation, we would like to place greater emphasis on RPNs trained through EnD$^2$, rather than RPNs trained via RKL on OOD datasets.\n\nIndeed, one of the conceptual reasons for not further exploring choice of OOD datasets for RPN+RKL is that we believe (and show) that RPNs+EnD$^2$ to be the superior approach. \n\nWe will clarify this point in an updated manuscript we will shortly upload. \n\n2. We believe this is a result of the fact that the EnD$^2$ will overestimate the support of the ensemble (as a natural consequence of ML training). As a result, it will be less over-confident. \n\n3. We didn\u2019t. To be clear - we intended our main comparison for Depth Estimation to be Ensembles vs  EnD$^2$ . Note that RPNs trained via RKL on OOD data in section 5 suffer degraded predictive performance. On the other hand, RPNs trained via  EnD$^2$ show better predictive performance (relative to EnD, Single models and RPN+RKL). \n\n4. : Thank you for pointing out this work. However, as previously stated, these papers consider the distillation of single model into single model, and thus cannot be used as a meaningful baselines. However we will cite them when discussing the nature of regression distillation and highlighting how our work is different.  \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "e8Fzb_fYVd",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Addressing concerns regarding novelty",
                "comment": "Dear Reviewers,\n\nAll of you have expressed concerns regarding the novelty and originality of our work. We would like to address this issue and explain why we think this merits a paper. \n\nIn our interactions with other researchers, and especially with industrial ML practitioners, we noticed that many people thought that the correct extension of Prior Networks to regression tasks would be to take a non-probabilistic regression model and place a Normal distribution over the target variable. As is clear from our work, this is not correct. Thus, one of the main motivations for this paper was to address this common misunderstanding and show that the correct way to extend Prior Networks and Ensemble Distribution Distillation to regression tasks.\n\nIn order to convey our message as clearly as possible we explicitly structured the paper around the parallel between Dirichlet and Normal-Wishart Prior Networks to make it absolutely self-evident what the correct approach is. In this regard we seem to have succeeded a little too well, as all of you note how the extension is straightforward and incremental. We would respectfully ask you to consider that this extension is not as evident to the majority of the ML community as we make it seem in this work. Notably, since the publication of the original paper on Dirichlet Prior Networks (Malinin and Gales, 2018), to our knowledge, Prior Networks have not been extended to regression, despite the popularity of the approach for classification. Thus, the value of our work is in extending a powerful uncertainty estimation approach for classification to regression, resolving a common misconception, and clearly presenting the mathematical basis for this extension. \n\nWe address your remaining concerns on a point-by-point basis and will shortly upload an updated manuscript.\n\nSincerely,\nAuthors\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KMA7obppU9F",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "An nice paper based on incremental work",
                "comment": "Summary of the Paper:\n\n        This paper introduces regression prior networks. These are models that aim at capture predictive uncertainty, both epistemic and aleatoric, in the context of regression problems. Regression prior networks can also be used to compress an ensemble of predictors into a single model while keeping the benefits of the ensemble. That is, better predictive performance and uncertainty estimates. The method is validated on several problems from the UCI repository and compared with ensemble methods.\n\nSpecific details:\n\n        I believe that this is a nice paper that illustrates an appealing method for uncertainty estimation in the context of neural networks. My main concern, however, is that it builds heavily on previous work. In particular, prior networks have already been proposed for classification and they have also been used to distill (compress) an ensemble. There is hence not much novelty here, only the extension to regression problems since, previously, only classification problems have been addressed. The use of prior networks for ensemble distillation is also not new. All this questions the novelty of the proposed approach.\n\n        The extension to regression seems to follow very closely the work already carried out for classification. The only difference is that a Normal Wishart distribution is used instead of a Dirichlet distribution.\n\n        The experiments carried out are extensive and consider different tasks involving prediction accuracy and out of distribution data detection. My main concern, however, is that no comparison is carried out with alternative methods to estimate prediction uncertainty such as those of Bayesian neural networks using variational inference or dropout. The authors should comment on the advantages of their method with respect to these techniques.\n\n        The method proposed is also complicated and has several training parameters. The authors give specific values for them, but it is not clear the motivation for them or the sensitivity to their values.\n\n        The paper is clearly written but heavily relies on previous work, making the reading difficult for someone who is not familiar with it. The paper is not self-contained.\n\n        Summing up I believe that this could be an interesting contribution for the conference, suffering from a reduced amount of novelty.",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "6l-gyq0bIrO",
                "reply_to": "iclr_2021_ygWoT6hOc28",
                "title": "Clarifications needed",
                "comment": "This paper addresses interpretable uncertainty quantification for data driven models. In particular, the authors focus on a sub-class of methods known as Prior Networks and attempt to extend these methods to regression tasks as existing approaches address classification only. The author contribution is thus clearly stated and positioned w.r.t. prior arts and tackle a non-trivial issue.\n\nIn the classification setting, the Dirichlet distribution is pretty much the universal model for the parameters of multinomial distributions. For regression, i.e. continuous r.v., there is no such universal solution and the authors chose to focus on outputs that have a normal distribution. The parameters of this latter are assumed to be normal-Wishart. Although, the proposed method is de facto non-applicable to other types of distributions, it can be argued that this already covers a majority of situations. \n\nThe paper is rather well organized and seems technically sound. This said, a few mathematical details are missing and, most importantly, the experiments are not very convincing. These concerns, also with other minor remarks are detailed below, section by section.\n\nsec 2.2\n\nMaybe give the explicit definition of Z to clarify that is does not depend on network parameters.\nThe presence of the OOD loss term in (8) is a bit artificial as it boils down to regularizing because of the choice of beta. Is this choice systematic ?\nIn (9), how is p(y | mu, Lambda) computed ? Is it a T distribution ?\n\n2.3\n\n(12) lacks clarity : dataset is equal to an empirical distribution... Do you mean p hat is a sum of Dirac ?\nWhat does phi represent ?\n\n3\nThe acronym ENSM is not explained. I believe this corresponds to the deep ensemble. \nThe Prior Networks achieve a form of disambiguation but the quality of it is a bit disappointing compared to ENSM. In particular, data uncertainty raises quickly for out-of-domain inputs. \n\n4\n\nThe presentation of the experimental protocol in 4 lacks clarity thereby impairing the interpretation of the results. The definition of the unconventional performance criteria [(Malinin et al. 2020] must be recalled (at least in an appendix). \nIn addition, as honestly mentioned by the authors, these datasets may not offer sufficiently rich problems to provide interesting comparisons. Besides, the way that OOD data is generated does not seem to necessarily produce inputs that are not covered by the in-domain distribution. Perhaps, the authors could use a \"bad GAN\" to obtain such data points, I mean a GAN where the generator and the discriminator would co-operate instead of being adversaries. If it converges, the generator would produce synthetic inputs that are easy to discriminate, thus far from true inputs. \n\n5\nWhile the dataset used in this section is more challenging, the experiment description is confusing. Again, performance criteria are not sufficiently explained and the general message becomes cryptic. Table 3 is overly complicated, I think RMSE is fairly enough to depict regression performances. Moreover, the definition of some columns are missing.\nIn Table 4, the performances of the methods seem quite unstable. For example, NWPN works fairly well for a given dataset configuration for one knowledge uncertainty criterion but fails miserably using another criterion on the same data.\nOn Fig 3, from what dataset are these image coming from ? Why are these or that object presumably \"unknown\" to the model ?\nI think the whole section deserves some re-writing.\n\nFinal remark : there are a few English mistakes that should be wiped out. \n\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "This paper",
                "Sentiment Expression": "a useful contribution",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "This paper",
                "Sentiment Expression": "has significantly helped with strengthening",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "this work",
                "Sentiment Expression": "continue to improve",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            }
        ]
    },
    "9U4gLR_lRP": {
        "paper_id": "nips_2022_9U4gLR_lRP",
        "paper_title": "Logit Margin Matters: Improving Transferable Targeted Adversarial Attack by Logit Calibration",
        "paper_abstract": "Previous works have extensively studied the transferability of adversarial samples in untargeted black-box scenarios. However, it still remains challenging to craft the targeted adversarial examples with higher transferability than non-targeted ones. Recent studies reveal that the traditional Cross-Entropy (CE) loss function is insufficient to learn transferable targeted perturbations due to the issue of vanishing gradient. In this work, we provide a comprehensive investigation of the CE function and find that the logit margin between the targeted and non-targeted classes will quickly obtain saturated in CE, which largely limits the transferability. Therefore, in this paper, we devote to the goal of enlarging logit margins and propose two simple and effective logit calibration methods, which are achieved by downscaling the logits with a temperature factor and an adaptive margin, respectively. Both of them can effectively encourage the optimization to produce larger logit margins and lead to higher transferability. Besides, we show that minimizing the cosine distance between the adversarial examples and the targeted classifier can further improve the transferability, which is benefited from downscaling logits via L2-normalization. Experiments conducted on the ImageNet dataset validate the effectiveness of the proposed methods, which outperforms the state-of-the-art methods in black-box targeted attacks. The source code for our method is available at https://anonymous.4open.science/r/Target-Attack-72EB/README.md.",
        "paper_acceptance": "Reject",
        "meta_review": "In this paper, the authors propose novel method to improve transferability of targeted adversarial attacks by enlarging the margin between targeted logit and non-target logits.  Experiments on ImageNet with different methods demonstrated the effectiveness of the method. However, as is pointed out by the reviewers that there exist high overlap between the paper and the existing works, which significantly hinders the novelties of the paper. The paper are expected to clarify the novelty and provide more comprehensive evaluations. \n\n\n",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "04OYiCm6jg",
                "writer": "official_reviewer",
                "reply_to": "7eZgZ-rivOg",
                "title": "Thanks for the authors' response.",
                "comment": " I would like to thank the authors for their response and have checked the revised version. I agree with Reviewer WQZv that the change of the current version is falling into a major revision. In particular, I would like to highlight the high overlap between the previous submitted manuscript and the existing work. Therefore, I remain my previous rating and still vote for reject.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "VacpnSA35Nc",
                "writer": "author",
                "reply_to": "jNXvtEGBFMIS",
                "title": "Summary of Revision",
                "comment": " Thanks for your comments and constructive feedback. We have uploaded a revision to address the concerns. The notable changes are below.\n\n1. We revised the introduction section to clarify the distinction between the Logit [30] and moved the logit margin figure (Fig. 1) to the introduction to better illustrate the motivation and contribution of this study (Section 1).\n\n2. We thoroughly rewrote the related work section to address the similarity issue pointed out by the reviewer qX4X (Section 2).\n\n3. We added the experiments on the varied targets and T=10/20 in the combining logit calibrations. Besides, the suggestion for achieving a better-targeted attack is added (Section 4).\n\n4. To better present the tables, we reported the average targeted transfer success rate with three digits at most for Tables 1, 2, & 3 instead of the average number of successfully attacked samples with four digits (Section 4).\n\n5. We carefully polished the manuscript and corrected some typos and grammar mistakes.\n\n6. More experiment results during the rebuttal period are added into the supplementary.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sT14tZIeoddW",
                "writer": "author",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "Summary of Revision",
                "comment": " We thank the reviewers for their positive comments and constructive feedback. We have uploaded a revised manuscript based on the reviewers\u2019 feedback and have highlighted changes from the original submission in blue. We summarize the notable changes below.\n\n1. We revised the introduction section to clarify the distinction between the Logit [30] and moved the logit margin figure (Fig. 1) to the introduction to better illustrate the motivation and contribution of this study (Section 1).\n\n2. We thoroughly rewrote the related work section to address the similarity issue pointed out by the reviewer qX4X (Section 2).\n\n3. We added the experiments on the varied targets and T=10/20 in the combining logit calibrations. Besides, the suggestion for achieving a better-targeted attack is added (Section 4).\n\n4. To better present the tables, we reported the average targeted transfer success rate with three digits at most for Tables 1, 2, & 3 instead of the average number of successfully attacked samples with four digits (Section 4).\n\n5. We carefully polished the manuscript and corrected some typos and grammar mistakes.\n\n6. More experiment results during the rebuttal period are added into the supplementary.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "jNXvtEGBFMIS",
                "writer": "official_reviewer",
                "reply_to": "sIAr738LGW9",
                "title": "  ",
                "comment": " Thanks for providing the response.  I am worried all of these changes are falling more into a major revision and would not be within the limit of the conference. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sIAr738LGW9",
                "writer": "author",
                "reply_to": "OyP0Pw8xB5",
                "title": "Results on another dataset, attacking Google API, and transfer with varied targets",
                "comment": " >**Exp 1:** Experiments on another dataset (e.g., CIFAR-10, MNIST, SVHN)\n>\n>**Response 1:**  During the rebuttal, we conducted the experiments on the CIFAR-10 dataset under the untargeted attack setting based on the code provided by [a]. The ResNet-18 is used as the white-box model for crafting the perturbation by training with the I-FGSM for 20 iterations. The DenseNet, GoogLeNet and SENet18 are black-box models. Table 1 reported the fooling rate of attacking the 10,000 images in the CIFAR-10 testing set. \n>\n>From Table 1, we can find that the fooling rate continually increases along with the T in the white-box attack. In transfer black-box attacks, the best fooling rates are obtained at T=5 or T=10, and the fooling rate will decrease when further increases T. These results also can validate the effectiveness of logit calibration in non-targeted attacks on a small dataset.\n>\n>[a] Enhancing Adversarial Example Transferability with an Intermediate Level Attack, *ICCV 2019*.\n\nTable 1: The transfer untargeted fooling rate of training with ResNet-18 and testing by the DenseNet-121, GoogleNet and SENet-18 on CIFAR-10.\n|       | ResNet-18*| DenseNet-121 | GoogLeNet   | SENet-18 |\n| -     | :-:  | :-:  | :-:  |:-:   |\n|T=0.5  |89.77 |50.23 |37.43 |51.04 |\n|T=1    |91.61 |50.78 |37.30 |51.20 |\n|T=2    |91.39 |51.14 |37.60 |51.65 |\n|T=5    |92.01 |55.56 |41.77 |55.74 |\n|T=10   |94.04 |54.76 |42.41 |55.10 |\n|T=20   |94.20 |53.33 |41.31 |54.11 |\n\n>**Exp 2:** A real-world attack on the Google Cloud Vision API.\n>\n>**Response 6:** We randomly select 100 images and compute the attacking performance of the ensemble of four CNNs using the same evaluation protocol in [30]. The results are as follows. We can find that the results of the Logit and CE (T=5) are very similar. But the Margin-based calibration performs worse than Logit and CE (T=5).\n\nTable 2: Non-targeted and targeted transfer success rates (%) on Google Cloud Vision.\n|       | Logit | CE (T=5)   | Margin |\n| -     | :-:  | :-:  |  :-:   |\n| Targeted    |  16| 15  | 12 | \n| Non-targeted| 51 | 53 | 42 |\n\n>**Exp 3:** The targeted success rates for transfer with varied targets.\n>\n>**Response 3:** The targeted transfer success rate with varied targets is reported in Table 3, and we can have the following findings. **(1)** The three types of logit calibration methods can improve the targeted transfer success rate over the original CE. The angle-based calibration has the best performance. But, we notice that the margin-based calibration doesn't work well in this setting. **(2)** The Temperature-based (T=5, 10) and the Angle-based calibrations can outperform the Logit loss by a large margin, especially the Angle-based calibration.\n\nTable 3: Targeted transfer success rate (%) when varying the target from the high-ranked class to low. (Average of 5 times)\n|      | 2nd  | 10th | 100th | 200th | 500th | 800th | 1000th |\n| :-:  | :-:  | :-:  | :-:   | :-:   | :-:   | :-:   |  :-:   |\n|Logit | 83.7 | 83.2 | 77.3  | 74.5  | 71.5  | 64.9  |  52.4  |\n|CE    | 77.4 | 58.6 | 34.0  | 26.9  | 23.7  | 16.7  |  7.0   |\n|CE/5  | 91.3 | 88.7 | 80.7  | 77.1  | 75.8  | 70.1  |  58.8  |\n|CE/10 | 89.0 | 87.8 | 82.8  | 81.0  | 79.2  | 73.5  |  62.5  |\n|Margin| 87.4 | 81.7 | 67.4  | 61.3  | 51.6  | 43.1  |  23.0  |\n|Angle | 92.4 | 89.1 | 82.2  | 80.3  | 79.2  | 76.1  |  66.3  |\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "OyP0Pw8xB5",
                "writer": "author",
                "reply_to": "JcXe9C2n82K",
                "title": "Response to Reviewer WQZv ",
                "comment": " Thanks for your comments and valuable suggestions for the presentation. We would like to address your concerns in the following aspects.\n\n>**Comment 1:** Clarified the distinction between [30] in the introduction. \n>\n>**Response 1:** Thanks for your suggestion. We will rewrite this part in the introduction to better clarify the contributions of this study.\n\n>**Comment 2:** To have a better presentation of the tables.\n>\n>**Response 2:** We agree that a better presentation of the tables is needed. Currently, we use tables instead of line graphs that capture progress through iterations mainly due to the results of different calibration methods being very similar, and their lines will largely overlap with others. In the revision, we will report the average targeted transfer success rate with 3digits at most for Tables 1 & 3 instead of the average number of successfully attacked samples with 4 digits, and replace Table 2 with line graphs.  \n\n>**Comment 3:** Why not introduce a single best receipt and present everything else as ablations? \n>\n>**Response 3:** The main reasons are: **(1)** The primary goal is solving the saturated issue in the CE loss for learning better transferable targeted adversarial attacks. Therefore, we first evaluate the effectiveness of different logit calibrations. **(2)** Since the logit calibration works, we then test their mutual effects by combining them jointly. However, we find that the optimal combination is different for different models, and there isn't a universal receipt for them. Consequently, we didn't introduce a single best receipt and present else as ablations.\n>\n>On the other aspect, based on the results in the manuscript and new results in the rebuttal, we might suggest using (T=5 + Margin) or (T=5 + Angle) for CNNs with more layers and the single Margin-based calibration for CNNs with fewer layers. \n\n>**Comment 4:** Why $T=5$ +Margin or Angle in Table 3? \n>\n>**Response 4:** We currently use the same $T=5$ mainly based on the result of ResNet-50, instead of the optimal $T$ for each model. During the rebuttal, we test the performance of $T=10,20$ + (Margin or Angle). The results are reported in Table 1. We can find that the $T$ has a marginal influence in the combination of \"$T$ + Margin\", while largely increasing the performance of \"$T$ + Angle\" of VGG16. \n\n**Table 1.** The comparison of combining logit calibration.\n\n(1) Surrogate model: **ResNet-50**\n|       | Dense121  | VGG16     | Inc-v3     |\n| -     | :-:       | :-:        | :-:        |\n|T=5 + Margin  |338.2/698.4/772    |239.6/590/655.4   |33.4/96/111      |\n|T=5 + Angle   |345.2/742.6/823.8  |256.2/664.8/721.6 |35.8/104.6/131.4 |\n|T=10 + Margin |326.8/694.6/772.8  |227.8/593.6/663.2 |129.4/96.8/114.6 |\n|T=10 + Angle  |329.6/697.6/790.6  |244.2/590.2/689.4 |33.6/99.8/128.6  |\n|T=20 + Margin |330/691.6/762.4    |230.8/584.4/658.2 |31.6/95/117.8    |\n|T=20 + Angle  |342.2/686.2/764.6  |247.4/587/666.2   |34.4/97.4/126.8  |\n|Margin+Angle  |344/708.4/781.4    |242.6/601.8/673.8 |35/103.6/125.8   |\n\n(2) Surrogate model: **Denss121**\n|       | ResNet50  | VGG-16     | Inc-v3     |\n| -     | :-:       | :-:        | :-:        |\n|T=5 + Margin |192.6/442.6/477.8  |141.2/377.4/408.4 |25.4/74.8/93.6 |\n|T=5 + Angle  |202.6/526.6/619.2  |158.2/450.2/536.4 |23.4/92/127.2  |\n|T=10 + Margin|183.2/441.4/491.2  |136.6/369.4/416.4 |24.4/82.8/91.8 |\n|T=10 + Angle |193.8/472/561.2    |148.2/400.6/470.2 |25.2/82.8/109.8|\n|T=20 + Margin|191/433.8/485.4    |138.8/366.6/414.2 |23.6/78.2/95.4 |\n|T=20 + Angle |199.6/443.8/508.6  |155.4/383.8/437   |24.6/82.4/95.2 |\n|Margin+Angle |198.8/465.6/527.4  |152.2/392.8/445.4 |27/82/99       |\n\n(3) Surrogate model: **VGG16**\n|       | ResNet50    | Dense121   | Inc-v3     |\n| -     | :-:       | :-:         | :-:        |\n|T=5 + Margin  |34.8/101.8/114    |37.2/123.6/145.6 | 3/10.8/13    |\n|T=5 + Angle   |21.6/25/23.4      |23.6/25.6/23.2   | 1.6/1.4/1.6  |\n|T=10 + Margin |31.6/107.2/117.4  |34.4/129.4/149.6 | 2.2/10.4/14.2|\n|T=10 + Angle  |34/62/50.8        |34.8/75/70.2     | 2.4/6.4/5.8  |\n|T=20 + Margin |34.6/100.8/118.2  |33.6/120.2/148.6 | 2.8/12.4/14.4|\n|T=20 + Angle  |32.4/96.6/101     |38.8/119.4/133.2 | 2.8/10.4/12  |\n|Margin+Angle  |33/98.4/111.2     |35.4/126.4/146   | 2.6/12/14    |\n\n(4) Surrogate model: **Inc-v3**\n|       | Dense121  | VGG-16     | Inc-v3     |\n| -     | :-:       | :-:        | :-:        |\n|T=5 + Margin |4.8/14.4/16    |6.2/21.2/28.6 | 5/17.2/28.4  |\n|T=5 + Angle  |5.2/16/20.4    |5.8/19.6/31.2 | 5.4/16.6/24.6|\n|T=10 + Margin|5.4/14/19.2    |4.6/18.8/30.4 | 3.2/14.8/23  |\n|T=10 + Angle |5.8/13.4/18.6  |6/19.6/32.2   | 4.6/16.4/25.6|\n|T=20 + Margin|6.4/12.2/19.4  |5/19/29       | 4.8/16.4/26.8|\n|T=20 + Angle |6.4/16.2/20.4  |5.6/19.6/35.2 | 4.8/17/28.8  |\n|Margin+Angle |6.4/14/21      |5.6/17/31.2   | 5.4/15.4/26.2|\n\n>**Comment 5:** Limitation of this study.\n>\n>**Response 5:** Thanks for your valuable suggestions. We will add this information in the revision.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IfQv4z1bM6K",
                "writer": "author",
                "reply_to": "bNMs61pCbvC",
                "title": "Response to Reviewer Boef",
                "comment": " Thanks for your comments. We would like to address your concerns in the following aspects.\n\n>**Comment 1:** The influence of different $T$ in CE.\n>\n>**Response 1:** \n>\n>(1) ***A large $T$ for VGG-16 and Inc-V3:*** In the Response to Reviewer SQcN, we guess that the T is related to the model depth, in which a large $T$ is preferred for the CNN models with few layers. Compared with the ResNet-50 and DenseNet-121, VGG-16 and Inc-V3 have few layers, and better performance is obtained using large $T$. \n>\n>(2) ***The results of continually increasing $T$:*** In the supplementary, we analyzed the relation between the Logit loss in [30] and the CE calibrated by a large $T$. The gradient of Logit loss is $\\frac{\\partial L_{Logit}}{\\partial \\phi(\\hat{x})} = - W_t$, and the gradient of CE with a large $T$ is $\\frac{\\partial L_{CE}^T}{\\partial \\phi(\\hat{x})} \\approx - \\frac{W_t}{T}$. Since the I-FGSM only considers the Sign of gradient while neglecting the magnitude, then the optimization of the CE calibrated by a large $T$ is nearly equivalent to the Logit loss. In Table 1 and Figure 1 in the supplementary, we reported the comparison results of $T=50, 100$, and the Logit loss. Their results are very similar to each other, which verifies our analysis between the Logit loss and using a large $T$ in CE.\n>\n>Therefore, the performance of the targeted attack will get saturated when T continually increases since it is nearly equivalent to the Logit loss.\n\n>**Comment 2:** The relation between three calibration methods.\n>\n>**Response 2:** In this study, we investigate temperature-based, margin-based, and angle-based logit calibrations to validate the main hypothesis of our study that \u201cenlarging the logit margins can increase the targeted transferability.\u201d \n>\n>The temperature-based is the simplest one which only calibrates the logits by a constant value of T.  However, the optimal T is different for different models, as shown in Tables 1 & 2. Therefore, we investigate the margin-based and angle-based calibrations to deal with this hyper-parameter issue.  The margin-based method adaptively computed the \u201cT\u201d based on the Top-2 logits of each iteration instead of using a constant value. On the aspect, since $z_i= W_i*x + b$ and the L2 norm of $W_i$ is different for each class $i$, we further perform the calibration by normalizing the classifier weight $W_i$ of each class $i$ and the feature $x$ to the unit length by L2-normalization. This calibration is actually computing the cosine between the $W_i$ and $x$ while without considering their norms. Therefore, we term it angle-based calibration.\n\n>**Comment 3:** The option for the best targeted attack.\n>\n>**Response 3:** Since the best combinations of different models are different, we currently cannot have a universal receipt for each model. Based on the results in the manuscript and new results in the rebuttal, we might suggest using (T=5 + Margin) or (T=5 + Angle) for CNNs with more layers and the single Margin-based calibration for CNNs with fewer layers. \n\n>**Comment 4:** Typos and Grammar mistakes.\n>\n>**Response 4:** We will carefully polish the manuscript.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "gLRl60iGOpe",
                "writer": "author",
                "reply_to": "IDrIEbjfT7Y",
                "title": "Response to Reviewer SQcN ",
                "comment": " Thanks for your comments. First, we will revise all tables to have a better presentation in the revision. Then, we would like to address your concerns about the interpretation of Table 1. \n\n>**Comment:** Potential interpretation of the results in Table 1.\n\n>**Response:** We argue the main reason for better results obtained by Margin-based calibration for the VGG-16 is mainly due to the influence of model depth. For the CNN models with fewer layers, a large normalization factor \u201cT\u201d is preferred to achieve higher targeted transferability. In our Margin-based calibration, the denominator \u201cT\u201d (logit margin between the first and second logits) will keep increasing along with the optimization iterations and thus leads to better performance. \n>\n>To further check the influence of depth, we leverage the ResNet-18 with fewer layers as the surrogate model and reported the results in the following Table 1. We also find that a large T and the margin-based calibration are preferred. \n\n>**Table 1.** The average number (#) of successfully attacked targeted samples with the ResNet-18 as the surrogate model.\n|       | Inc-v3         | ResNet-50       | Dense-121       | VGG-16 |\n| -     | :-:       | :-:        | :-:        |:-: |\n|CE     |21/30.4/29.6    |191.8/239.8/259.8|185.8/239.6/246.2|158.6/192.6/190.4|\n|CE/5   |39.2/108/119    |278.2/606.8/636.2|271.8/574.8/615.6|237.2/530/565.8|\n|CE/10  |36.2/111.6/132.4|259.2/597.4/668.6|258.6/571.8/642.2|222.4/530/596.6|\n|CE/20  |38.8/113.8/129.8|251.8/578/641.8  |248.2/543.2/607  |211.4/497.4/571.2|\n|Margin |41/113/130.8    |273/601.4/653.2  |273.2/572.6/629  |234.2/535.4/586.2|\n|Angle  |36.6/81.8/83.6  |271.4/514.8/542.6|280.6/527.6/557.4|239/449.4/462|\n|Logits |37.2/100.2/122  |247.8/556.2/606.8|243.2/536.4/585. |212.4/494.2/548.6|",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7eZgZ-rivOg",
                "writer": "author",
                "reply_to": "eqDmaQ47r91",
                "title": "Authors' Response ",
                "comment": " Thanks for your feedback. We would like to address your concerns in the following two aspects.\n\n>**Comment 1:** The related work section.\n>\n>**Response 1:** The structure of our current related work is mainly based on the following considerations. (1) The [1] highly inspired this study, and we followed the academic writing skills of [1] to some extent. (2) The I-FGSM, MI-FGSM, TI-FGSM, and DI-FGSM have been used as the baseline in the experiments. Besides, the optimization of only using the Sign of gradient in the I-FGSM is essential for our analysis of the relation between Temperature-calibration with large T and the Logits loss function. (3) The Po-Trip and the Logits are two main comparison methods, and then we also introduce them in detail.\n> \n>We will rewrite the related work section in the revision to avoid this similarity issue.\n\n----\n\n>**Comment 2:** Marginal Improvement and Contribution.\n>\n>**Response 2:** We totally disagree with your comments that the contribution of this work is limited by only achieving marginal experimental gains. \n>\n>* First, we would like to **recap the primary goal of this study**, which mainly aims to analyze why the widely used CE loss function can not generate adversarial samples with higher targeted transferability. However, previous studies only reveal this issue due to the vanished gradient issue without further analysis. In this study, we take a close analysis of the CE loss and find that the logit margin between the targeted and non-targeted classes quickly gets saturated during the optimization process, hindering the CE's transferability.\n>\n>*  Second, **how to solve this issue**? Based on our analysis, we then explored three different logit calibration methods to deal with the saturated issue of logit margin. The experiment results valid our findings for the problem. Besides, in the supplementary, we further analyze that the Logit loss in [1] is nearly equivalent to the Temperature-calibration with large T. \n>\n>*  Third, **what is not the goal**? We do not intend to beat the state-of-the-art by a large margin. Although the logit calibrations slightly outperform the Logit for most cases, they can significantly increase the performance of the original CE. Besides, we also notice the results of combined logit calibrations in Table 3, which can outperform the Logit by more than 10% when using the ResNet50 and Dense121 as surrogate models. The additional experiment on the difficult transfer with varied targets suggested by the Reviewer WQZv can further show the effectiveness of logit calibration in the targeted attack.\n> \n>Based on the above explanation, we believe our investigation in this study can provide valuable insight for future researchers by using the logit calibration from both attack and defense.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eqDmaQ47r91",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "",
                "comment": " This work proposed three different calibration methods, temprature-based, margin-based and angle-based temperature scaling to enlarge the margin between targeted logit and non-target logits to improve transferability of targeted adversarial attacks. This work is highly inspired by the work [1] and perform experiments to show the proposed methods are better than other existing methods.\n\n\n[1] \"On Success and Simplicity: A Second Look at Transferable Targeted Attacks\".\nZhengyu Zhao, Zhuoran Liu, Martha Larson. NeurIPS 2021. First of all, after comparing the related work in [1] and this work, there is a huge amount of overlapping of the equations or rewriting the sentences. This significantly destroys the overall quality of the work.\n\nSecond, the improvement of the proposed method over [1] is marginal compared to the improvement of [1] over cross-entropy loss.\n\nThird, the contribution of this work is limited. Although the authors proposed different temperature-scaling based methods to improve transferability of targeted attacks, which only achieve limited experimental gains, this work did not provide extra useful insight to this research area.\n\n[1] \"On Success and Simplicity: A Second Look at Transferable Targeted Attacks\". Zhengyu Zhao, Zhuoran Liu, Martha Larson. NeurIPS 2021. In general, it leaves me a poor impression when I realize the great similarity between the Related Works in this work and that in [1]. Although the authors try to reframe the sentences, it's still very unprofessional to structure related works with such a strong similarity with another existing work.  See above.",
                "rating": 2,
                "confidence": 5
            },
            {
                "review_id": "IDrIEbjfT7Y",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "",
                "comment": " The authors propose a novel and effective method to improve the transferability of adversarial attacks. They increase the logit margins between targeted and non-targeted classes, which can quickly become saturated in cross-entropy loss. Strengths:\n1. The findings are very interesting and the motivation is well-explained.\n2. Comprehensive experiments are presented and the combining logit calibrations have significantly better performance than previous methods.\n\nWeaknesses:\n1. The proposed method has various settings and hyper-parameters. Compared to the simple Logit method, the proposed method needs more effort for tuning or need combining logit calibrations to achieve better performance. This can make the method less attractive to the community.\n3. There is no theoretical analysis to support the empirical findings.\n2. The presentation of the results needs to be improved. All tables contain tons of numbers, which makes it hard for the reader to get the point in a short time. Could authors provide any interpretation of the results in Table 1? For example, why do Margin and Angle have better performance when the surrogate models are VGG17 and Inc-v3, but have lower performance for ResNet50 and Dense121? N/A",
                "rating": 6,
                "confidence": 5
            },
            {
                "review_id": "bNMs61pCbvC",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "",
                "comment": " This paper designs a new logit calibration method which is inspired by knowledge distillation. The method uses logit calibrations in the CE loss function so that it can improve the targeted adversarial attack with higher transferability than other attack methods with cross-entropy loss. Except for the primary temperature-based method, this paper designs margin-based and angle-based methods to solve different surrogate models and different norms. The strengths of this paper are:\n\nThis paper designs a new cross-entropy (CE) loss function to improve the targeted adversarial attack, which performs better than Logit (NIPS21).\n\nExcept for the temperature-based method, this paper designs margin-based and angle-based methods to solve different surrogate models and norms.\n\nThe weakness of this paper are:\n\nThis paper follows `Zhengyu Zhao, Zhuoran Liu, and Martha Larson. On success and simplicity: A second look at transferable targeted attacks. NeurIPS, 34, 2021.' from academic writing skills and code in specific. However, instead of Zhao et al. designing the Logit loss and using it to generate universal adversarial perturbations, this paper's method does not have any additional functions such as UAP. \n\nMoreover, this paper's method only exceeds the Logit loss by around 10%, which is not a significant improvement. Therefore, this paper lacks novelty.\n\nThe equation 14 seems to have some mistakes. On the left of the equation, would $z_{i}$ be $\\tilde{z}_{i}$?\n\nA few grammar problems in this paper should be improved. For example, in line 275, it should be \"be similar to\"; in line 23, it should be \"Following many approaches\"; in line 27, it should be \"it is vital to explore.\"\n Firstly, for the influence of different $T$ in CE, this paper claims that when the surrogate model is VGG16 and Inc-V3, a larger $T$ obtains better transferability. However, I am curious about is there a limitation of $T$ on VGG16 and IncV3. For example, after the ASR achieves 600, the performance of the targeted attack will decrease when $T$ continually increase. And then, although this method is based on CE, it would be better if the authors designed a new name to describe it. The relationship between temperature-based, margin-based, and angle-based logit calibration is unclear. This paper claims that the margin-based one is designed to face different surrogate models, and the angle-based one is designed to solve the influence of various norms. However, in the experiments, the performance on T=5, T=10, Margin, and Angle does not prove the relation between them. This paper does not evaluate which method is the best for the targeted attack, or in other words, which option should I choose if I need to achieve the highest attack success rate?",
                "rating": 5,
                "confidence": 4
            },
            {
                "review_id": "JcXe9C2n82K",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_9U4gLR_lRP",
                "title": "",
                "comment": " The paper targets improving the transferability of adversarial attack using the logit calibration. Despite the recent success in untargeted black-box attacks, the targeted transferability of adversarial attacks remains challenging. The paper takes a closer look at the vanishing gradient issue in the CE loss function which is commonly used to learn transferable adversarial samples and suggests that the logit margin between the targeted and non-targeted classes quickly gets saturated during the optimization process. So, to improve transferability they aim to enlarging logit margins which consequently reduce saturation and  enable longer optimization and iterations. The paper investigates three different types of logit calibrations including temperature-based, angle-based and margin-based inspired by previous studies and techniques.  Experiment conducted using ImageNet dataset and different methods including ResNet50, DenseNet-121, VGG-16 and Inception-v3. Results are compared to SOTA methods including Po+Trip, Logit, and TTP.  Strength:\n- Quality: The writing quality is very good, very easy to follow, there are areas that can be improved but nothing major.\n- Clarity: Very clear. Easy to understand the motivation and the thought process of different method\u2019s component. \n- Significance and novelty: Novelty is a bit limited and built up mostly on top of previous methods,  but it is also not a weakness because this work attempts to solve an interesting problem and the analysis and results are valuable. The results are somewhat important. Mostly inspired by Logit [30], future researchers might use the suggested logit calibration and increase the number of iterations for targeted attacks.\n\nWeaknesses:\n- The quality of results and presentation of it could be improved significantly. (see below)\n- The distinction between [30] and this paper should be clarified and the introduction in page 2 [line 36-56] can benefit from a re-writing. (see below)\n - There is a large novelty overlap between [30] and the current method. It is proper to get the similarity and distinction discussed upfront. Specifically,  discussion around CE starting line 36 is getting very confusing and blurry going through line 56. Vanishing gradient and the use of Logit loss has been discussed and proposed in previous arts, for example [30]. This gets discussed later at line 145+ and in method, however, I feel the contribution of the current work can get discussed in a more clear way and upfront in introduction. \n\nExperiments are limited and can get improved:\n- First the organization of the results is not optimized or ideal. (1) Following Table 1, 2 and 3 is very hard and replacing this with line graphs that capture progress through iterations would be very beneficial. (2) Overall collective of Table 1, 2, 3 seems to be more exploratory and ablation tables rather than the main results. The main message that I read from these two tables are T=10, 20 and a combination of Margin + Angle or T + Angle can result in the best outcome. So, why not introduce a single best receipt and present everything else as ablations? (3) Also I would suggest sticking with conventional methods such as heat-map to summarized heavy tables, as it is a norm in adversarial attack literature.  [21] have good examples of result presentations.\n\n- One relevant question, is also as Table 2 suggests the best outcome is coming from T=10 or T=20 so why in Table 3 we analyze the effect of combining logits as T=5 +Margin or Angle? If any underlying study suggests this, results should be provided.  \n\nThe results could be strengthened by:\n- Providing experiments on another dataset (e.g., CIFAR-10, MNIST, SVHN) Since the proposed method works well on ImageNet, it could only be a minor concern.\n- Incorporating study of a real-world attack for example the Google Cloud Vision API.\n- Providing targeted success rates for transfer with varied targets.\n I cannot find any specific discussion around the potential negative social impact of this work. Also, the limitation of the method was not addressed in the paper.\n\nTo improve this part, discussion around the benefits of adversarial attack research can get discussed. Potentially this can motivate the AI community to design stronger defenses against transferable attacks, and  in the long run such results can be directly used for social good applications, such as protecting privacy. On the contrary, there are applications that can benefit from transferable attacks in a harmful manner to damage the outcome of any AI system, e.g. imaging a scenario that someone uses such attacks to intrude with the outcome of a medical AI device.  \n\nI also suggest that authors discuss the limitation of the work, failure cases and processing time.\n\nOverall, I enjoy reviewing this paper and looking forward to reading the authors' responses. \n",
                "rating": 5,
                "confidence": 4
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "high overlap between the paper and the existing works",
                "Sentiment Expression": "significantly hinders the novelties",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the novelty and provide more comprehensive evaluations",
                "Sentiment Expression": "expected to clarify",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not at all"
            }
        ]
    },
    "3h1iwXmYVVJ": {
        "paper_id": "nips_2021_3h1iwXmYVVJ",
        "paper_title": "Implicit Regularization in Matrix Sensing via Mirror Descent",
        "paper_abstract": "We study discrete-time mirror descent applied to the unregularized empirical risk in matrix sensing. In both the general case of rectangular matrices and the particular case of positive semidefinite matrices, a simple potential-based analysis in terms of the Bregman divergence allows us to establish convergence of mirror descent---with different choices of the mirror maps---to a matrix that, among all global minimizers of the empirical risk, minimizes a quantity explicitly related to the nuclear norm, the Frobenius norm, and the von Neumann entropy. In both cases, this characterization implies that mirror descent, a first-order algorithm minimizing the unregularized empirical risk, recovers low-rank matrices under the same set of assumptions that are sufficient to guarantee recovery for nuclear-norm minimization. When the sensing matrices are symmetric and commute, we show that gradient descent with full-rank factorized parametrization is a first-order approximation to mirror descent, in which case we obtain an explicit characterization of the implicit bias of gradient flow as a by-product.\n",
        "paper_acceptance": "accept",
        "meta_review": "All reviewers are satisfied by the authors' response and agree that this is well-written paper with solid contributions, though some of the results could be considered incremental given the existing work in [13,35]. The authors are encouraged to provide additional discussion on the points raised by the reviewers.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "tkVV-C3McwC",
                "writer": "official_reviewer",
                "reply_to": "iHUCezSWYTL",
                "title": "Final Score",
                "comment": " Dear authors,\n\nthank you for your responses. I have now raised my score to 7. \n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "vl3AQRH5Ops",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "",
                "comment": "The authors analyze the convergence of  mirror descent  in matrix sensing for specific selection of mirror maps. Specifically, they prove that when the mirror map is the spectral hypentropy or spectral entropy then MD converges to global minimizers of the empirical objective minimize quantities related to the nuclear, the Frobenius or the Von Neumann entropy.   \nStrengths\n- The main contribution of the paper is the derivation of Theorems 1 and 2 which establish convergence of mirror descent (MD) to global minimizers of the empirical loss of the matrix sensing problem that minimize certain quantities related to nuclear norm, Frobenius norm and von Neumann entropy. These results are interesting and provide an insight into the interplay between the geometries induced in mirror descent and the implicit regularization phenomena that are brought up. \n- Theorems 3 and 4 present recovery guarantees for the proposed mirror descent algorithm, the first of this kind in the framework of implicit regularization.\n\nWeaknesses\n- The contribution of these results to already existing work is somewhat limited given Theorem 1 of [19], which proves that MD converges to minimizers of the Bregman divergence. That being said, though the authors provide an alternative proof and focus on specific mirror maps, the added knowledge seems not to be  that significant.  The authors should make more clear the advantage of their proof as compared to the one followed in Theorem 1 of [19]. \n- From a practical point of view, the selection of the mirror maps gives rise to computationally expensive SVD steps per iteration of the algorithm. That being said, it is so clear what are the advantages of an implicitly regularized algorithm compared to an explicitly regularized one.\n\nMinor comments \n\n- The first 3 paragraphs of Section 1.1  present the contribution without explicitly referring to the added knowledge to the existing literature. The authors should rewrite that part and clarify what point is new and what is not.\n- Typos: \n - line 105 : numerical\n- line 109 : large\n- line 146: identity\n-line 164: descent\n-line 345: regularization Yes",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "x-_fYE0xKIy",
                "writer": "official_reviewer",
                "reply_to": "EA-IM1AqCgn",
                "title": "Final Score",
                "comment": " Thank you for your response. After reading the other reviewers' assessments, I would like to keep my score.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "WjkPYw-MVXj",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "",
                "comment": "In this manuscript, the authors study the trajectory of mirror descent to optimize the unregularized empirical risk functional to solve low-rank matrix estimation problems. They characterize the trajectory of mirror descent with different mirror maps and showcase an implicit bias towards certain structures in solutions. In particular, when equipped with either the spectral hypentropy map or spectral entropy map, they show that mirror descent converges to a global minimizer that minimizes a particular quantity depending on the singular values of the resulting matrix. This quantity interpolates between the nuclear norm and Frobenius norm for the spectral hypentropy map and is a linear combination of the nuclear norm and von Neumann entropy for the spectral entropy map. They also show algorithmic guarantees for mirror descent to solve matrix sensing and matrix completion problems that operate with sample complexities on par with traditional nuclear norm minimization approaches, without explicit regularization. Finally, a connection between gradient descent over the parameterization $UU^T - VV^T$ and mirror descent is shown and toy experiments show mirror descent and gradient descent behave similarly.  # Originality\n\nThere has been a flurry of interest in understanding implicit bias in matrix factorization problems, and this work provides an interesting perspective by studying the solutions chosen by mirror descent methods for various mirror maps. Similar results have been reported in [13], but the proof techniques in this work (direct analysis of mirror descent trajectory) and [13] (KKT conditions) differ from one another. The precise formulae presented on which minimizers are chosen appear to be novel as well. The shown equivalence between exponentiated gradient and mirror descent is nice, but not entirely novel as it appears in [35] for the vector-valued case.\n\n# Quality\n\n- Strengths: - The paper is well-written. - To the reviewers knowledge, applying mirror descent to solve matrix factorization problems and analyzing its implicit bias towards low-rank solutions appears novel. Moreover, the characterization of their preferred solutions holds in fairly general settings (e.g., no assumptions on the measurement matrices $A_i$ aside from the existence of a $0$ loss matrix). - The results on recovery guarantees for matrix sensing/completion hold with sample complexity on par with convex nuclear norm minimization techniques. - The connection between exponentiated gradient and mirror descent is interesting and extends previously known results to show the convergence of mirror descent to small nuclear norm solutions in the small initialization scale regime. \n\n- Weaknesses: - Mirror descent would not be a practical algorithm to use due to computational complexity and seems to perform essentially the same as standard gradient descent - No convergence rates are provided in the theoretical analysis of applying mirror descent to matrix sensing/completion. The results only establish convergence to a neighborhood of the underlying low-rank matrix. Moreover, these neighborhoods depend logarithmically on the parameters $\\beta$ (for hypentropy) and $\\alpha$, the initialization size, for the entropy map so that to achieve $\\epsilon$ precision, one would need $\\beta$ or $\\alpha$ to behave like $\\exp(-1/\\epsilon)$. In addition, in the matrix completion setting, a term depending on the sample complexity appears in the estimation bound, which would require $m = \\Omega(nn\u2019\\log^2(n\u2019))$ to make small, but the theorem operates in the $\\Omega(r (n + n\u2019)\\log^2(n\u2019))$ regime. Hence, while the sample complexity in the theorem does scale in the same way as nuclear norm approaches, the resulting error bounds could potentially be quite large even for exponentially small $\\beta$ or $\\alpha$. Could the authors comment on this point and, perhaps, why the error bound scales this way? My main concern is that the bound may be vacuous when operating in the traditional nuclear norm sample complexity regime.\n\n# Clarity\n\nOverall, the paper is well-written and easy to follow. Related work seems to be cited well and the authors do a good job of discussing their results in relation to others on implicit bias in matrix factorization. Here are a couple of typos I found:\n- pg 1 line 33 \u201cwhen then\u201d -> \u201cwhen the\u201d, pg 3 line 105 \u201cnumercal\u201d -> \u201cnumerical\u201d, pg 3 line 109 \u201clargeg\u201d -> \u201clarge\u201d\n\n# Significance\n\nThis work shows that mirror descent in matrix estimation problems exhibits an implicit bias towards certain structured solutions, and precisely characterizes this structure. I think that this does inspire research into further analyzing the role of mirror maps in other estimation problems and in analyzing the potential implicit bias of other first order algorithms. There are no clear societal impacts of this work. The authors discuss some of the downsides and limitations of their approach (namely, the computational cost of applying mirror descent). Another limitation that should also be discussed is the potential looseness of the error bounds.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "sElTzDoMJs",
                "writer": "official_reviewer",
                "reply_to": "r5t6-nXXcP4",
                "title": "Response to authors",
                "comment": " Dear authors,\n\nThank you for the response. After reading the other reviews and author responses, I have decided to raise my score. I think that the paper provides a nice contribution in showcasing further evidence of mirror descent\u2019s implicit bias, and has the potential to inspire future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mQxDVw6EGJE",
                "writer": "author",
                "reply_to": "8diaieCu2Y",
                "title": "Response to Reviewer oUMM",
                "comment": " We thank the reviewer for their feedback. The primary focus of our paper was indeed to understand how the implicit bias of mirror descent can be leveraged to solve matrix recovery problems rather than to argue that mirror descent is a computationally competitive algorithm for this problem. Typically, algorithms based on implicit regularization (i.e. mirror descent or gradient descent with full-rank factorized parametrization) cannot match the computational efficiency of gradient descent with low-rank factorized parametrization (see lines 354-356). Please find our responses to the questions below.\n\n1. We thank the reviewer for pointing this out and we have made the dependence on $\\beta$ more explicit in Theorem 1. We remark that in Theorems 3 and 4, where an upper bounds on $\\beta$ is assumed, the step size $\\eta$ can be chosen independently from $\\beta$.\n\n2. Indeed, a smaller value for $\\beta$ leads to a slower convergence of mirror descent (initially). This can be perhaps most directly seen from Proposition 5, which shows that $\\beta$ corresponds to the initialization size in the exponentiated gradient algorithm defined in (15) (under the assumption that the sensing matrices are symmetric and commute). Hence, a small value for $\\beta$ means that the algorithm defined in (15) does not move much initially, as both $\\mathbf{U}_t$ and $\\mathbf{V}_t$ start from small initial values and are updated multiplicatively. While a rigorous analysis of the convergence speed of mirror descent was outside the scope of our paper, we remark that precise convergence speed guarantees showing a similar behaviour have previously been established for mirror descent (in the vector-case) for the problem of sparse phase retrieval in [35].\n\n3. We believe that there is indeed a tradeoff between computational complexity and statistical error controlled by $\\beta$, similar to the tradeoff established in the context of sparse phase retrieval in [35]. We point out that the logarithmic dependence on $\\beta$ and $\\alpha$ is likely not sharp as we observed in our experiments (we observed similar results for the dependence on $\\beta$, and we omitted those experiments given the similarity to the experiments in Section 6 and Appendix C).\n\n4. The analysis of the evolution of the Bregman divergence is a general tool that can be employed to study the trajectory of mirror descent. Beyond characterizing the implicit bias of mirror descent, it has been previously used to establish the convergence of mirror descent for a general class of non-convex optimization problems [39], and, more specifically, the potential-based analysis of mirror descent has been used to establish minimax-optimal rates of convergence in noisy sparse phase retrieval [36], for instance. We remark that in previous related works that establish convergence speed guarantees, e.g. [19, 35, 36], a sample complexity that scales quadratically in the respective notions of sparsity in matrix sensing and sparse phase retrieval is needed, while our results only require a linear sample complexity, but do not establish any convergence speed guarantees for the estimation error. We will add a discussion on this point in the main paper, and we hope that our work can inspire future research in this direction, possibly bridging this gap and establishing rigorous convergence speed guarantees when only a sample complexity that scales linearly in the respective notion of sparsity is assumed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iHUCezSWYTL",
                "writer": "author",
                "reply_to": "vl3AQRH5Ops",
                "title": "Response to Reviewer dunA",
                "comment": " We thank the reviewer for their feedback. Please find our responses below.\n\n1. We believe that the reviewer is referring to Theorem 1 of [13]. The main advantages of our proof technique over the KKT optimality conditions-based proof of Theorem 1 in [13] are 1) the fact that convergence of mirror descent does not need to be assumed, and 2) that this type of potential-based analysis of mirror descent is also applicable to more general non-convex problems (e.g. [39]) and can, in principle, be used to establish convergence speed guarantees (e.g. [35, 36]), see also lines 88-95. We remark that in previous related works that establish convergence speed guarantees, e.g. [19, 35, 36], a sample complexity that scales quadratically in the respective notions of sparsity in matrix sensing and sparse phase retrieval is required, while our results only require a linear sample complexity, but do not establish any convergence speed guarantees for the estimation error. We will add a discussion about this in the main paper, and we hope that our work can inspire future research in this direction, possibly bridging this gap and establishing rigorous convergence speed guarantees when only a sample complexity that scales linearly in the respective notion of sparsity is assumed. \n\n2. Indeed, mirror descent is computationally more expensive than gradient descent due to an SVD in each iteration of the algorithm. We point out that, as elaborated in lines 344-356 and Section A of the Appendix, the SVD can be avoided in the positive semidefinite case, where we only need to compute matrix exponentials, which, though more expensive than matrix multiplication, are typically cheaper to compute than an SVD.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EA-IM1AqCgn",
                "writer": "author",
                "reply_to": "qDuuo2OrrzW",
                "title": "Response to Reviewer ZKpj",
                "comment": " We thank the reviewer for their feedback. We fully agree with the reviewer and believe that both points are very interesting directions for future research.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "r5t6-nXXcP4",
                "writer": "author",
                "reply_to": "WjkPYw-MVXj",
                "title": "Response to Reviewer sWYY",
                "comment": " We thank the reviewer for their feedback.\n\nRegarding the logarithmic dependence of the error bounds in Theorems 3 and 4 on $\\beta$ and $\\alpha$: even though the error bounds in equations (13) and (14) involve a term of the form $\\frac{nn\u2019\\log^2n\u2019}{m}$, both bounds tend to zero for any $n,n\u2019,m$ in the limit $\\beta,\\alpha\\rightarrow 0$. While this would require an impractically small $\\beta$ or $\\alpha$, our error bounds demonstrate that mirror descent can recover a planted matrix $\\mathbf{X}^\\star$ from $\\Omega(r(n+n\u2019)\\log^2(n\u2019))$ samples (exactly in the limit $\\beta,\\alpha \\rightarrow 0$, which is a setting often considered in the literature, e.g. [14, 20, 34]).\n\nWe obtain the logarithmic dependence on $\\beta$ and $\\alpha$ since our error bounds are established by 1) bounding the nuclear norm of the limiting point of mirror descent using Theorems 1 and 2, and 2) using this bound on the nuclear norm to control the estimation error $||\\mathbf{X}_t-\\mathbf{X}^\\star||_F$. In step 1), the upper bound on the nuclear norm inherits the logarithmic dependence on $\\beta$ and $\\alpha$ from the characterization of the limiting point of mirror descent in Theorems 1 and 2 (which is exact).\n\nAs we point out in lines 246-252, error bounds with a polynomial dependence on the initialization size $\\alpha$ as well as convergence speed guarantees have been established using additional assumptions for gradient descent in [19]. We expect that the analysis of [19] can be adapted to mirror descent, since the operations $\\exp$ and $\\log$ leave the eigenspaces of symmetric matrices invariant, and we expect a similar adaptive decomposition of $\\mathbb{S}^n_+$ into a rank-$r$ subspace, to which $\\mathbf{X}_t$ is approximately confined, and a rank-$(n-r)$ subspace where $\\mathbf{X}_t$ remains small, to be possible. In the present paper, our focus was on understanding how the implicit bias of mirror descent can be leveraged to solve matrix sensing and completion, under assumptions matching those required by nuclear norm minimization (in particular assuming a sample complexity that scales linearly in the rank $r$ of $\\mathbf{X}^\\star$ rather than quadratically as in [19]). We will add a discussion about this in the main paper, and we leave an analysis that establishes error bounds that do not depend polynomially on $\\beta$ and $\\alpha$, and convergence guarantees, desirably without additional assumptions compared to Theorems 3 and 4 (in particular with a linear sample complexity), to future work. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "qDuuo2OrrzW",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "",
                "comment": "The paper provides the convergence analysis of mirror descent for matrix sensing to particular minimum norm solutions. The updates that are studied are induced by the hypentropy and von Neumann divergences. The authors analyze multiple interesting cases including matrix completion and the case where the instances satisfy the RIP. They also provide some experimental evaluations to validate their findings.  The paper studies an interesting problem and greatly generalizes the results in [13] and [14]. There are a few remarks about the analysis:\n\n1) Although hypentropy is an interesting divergence, it is only an approximation of the squared Euclidean and KL divergence in the limit cases. A more natural extension is the tempered KL divergence which was partially analyzed in [1]. I believe most of the analysis may carry over.\n\n2) The construction in Eq. (15) is interesting, but not exactly related to the reparameterization of the Exponentiated Gradient Unnormalized (EGU) algorithm. The exact reparameterization for the vector case was discussed in (Amid and Warmuth, 2020) along with regret bounds for the discrete case. It would be interesting to extend the analysis to the matrix case where matrix U is updated using gradient descent.\n\nReference:\nEhsan Amid and Manfred K. Warmuth. \"Winnowing with Gradient Descent\". COLT, 2020. Yes",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "8diaieCu2Y",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_3h1iwXmYVVJ",
                "title": "",
                "comment": "This paper studies the implicit regularization of a particular algorithm, mirror descent, for matrix recovery problems.   I think the paper presents an interesting phenomenon that is also present in mirror descent.  I believe the paper only attempts to explain some phenomenon in mirror descent, rather than arguing that the mirror descent method is the correct or a competitive method in solving matrix recovery problems.   \n\nI have a few questions that I would like to hear the author's responses. \n\n1. In theorem 1, does the stepsize eta depends on the parameter beta as well? \n\n2. I wonder how beta affects the algorithmic performance, i.e., the convergence speed of the algorithm? Can the author comment on the relationship between the magnitude of beta and the speed of the mirror descent method?\n\n3. I notice that the bias/difference between X_infty and X* in Theorem 3 and 4 depends logarithmically on the parameter beta. So one wants to choose a very smaller beta to reduce the bias. It seems in Theorem 4, one needs beta to be exp(-n^(1/2)) so the RHS of (13) can be smaller than ||X*||_*. Is there a tradeoff between computational complexity, and statistical error, i.e., the difference between  X_infty and X*,  in choosing beta? \n\n4. I think characterizing the implicit regularization phenomenon is interesting by itself. But I wonder whether there are any broader applications/impacts of the results or techniques used in the paper.  A few limitations are discussed in the end. ",
                "rating": 7,
                "confidence": 4
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "this is well-written paper with solid contributions",
                "Sentiment Expression": "All reviewers are satisfied",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "some of the results could be considered incremental given the existing work",
                "Sentiment Expression": "could be considered incremental",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "mOO-LfEVZK": {
        "paper_id": "iclr_2021_mOO-LfEVZK",
        "paper_title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering",
        "paper_abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods (e.g., adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, i.e., the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "Two reviewers expressed clear concerns about the paper but the authors did not provide any response. ",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "oxM2g9iZ9IY",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "No theory, experiments only",
                "comment": "Results: To defend against adversarial attacks, this work experimentally analyzes the feature distribution of traditionally- trained CNNs for gaining more knowledge about adversarial examples. Two properties, i.e., the non-clustering property and confusing-distance property, of the feature distribution are identified by means of t-SNE visualization and clustering analysis (showing the limitations regarding representativeness) in Figure 1. The authors introduce a loss function which separates out cluster centers of CNN output features, setting them as far as possible - so that model accuracy is preserved while strengthening robustness. They test on two datasets: CIFAR10, MNIST, and show improvements in \"robustness\" of the model. \n\nStrong points: The experiments presented are promising in terms of increasing robustness of the learned models. \n\nWeak points: Experiments are only conducted on two datasets, it's unclear how generalization these results are. Further, there is no theoretical development regarding manifolds in the feature space.  \n\nMinor typing errors: \n\"indication of the clean accuracy\" \n\"using PGD optimizationm,\"\n\"an input images x\"\n\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "DiDrtOmSKyF",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "This paper proposes a manifold aware training strategy to learn compact features and improve the robustness of CNNs.",
                "comment": "This paper proposes to leverage the manifold aware training to learn compact representation. The authors proposes to enforce the learned representation along with generated vectors for different clusters, which implicitly enlarge the margin of the prediction.\nHowever the technical contribution as using the three-term loss to improve robustness is limited. In particular, it's is unclear what the equation (10) and (11) try to prove without a concrete theorem or lemma statement. \n\nFrom the empirical performance, it looks promising from table 2 but it's also quite clear that the TRADES loss BIBO dominates the performance, and without adding this loss, the proposed MAT training cannot achieve high robustness. This is as expected and also render the proposed method less effective.  \nIn addition, TRADES is evaluated on ImageNet and it would be good for the work to evaluate on ImageNet to demonstrate the generalization ability and scalability. It would also be good to explain why without the BIBO loss, the robustness against adaptive attack of MAT is almost 0 which again shows the weakness of the main proposed method.\n\nIt would also be necessary to provide analysis for the properties of the learned representation. For instance, if it is compact features, whether its rank is indeed lower, and whether the entropy of the learned features is indeed low in order to claim the consistent and compact feature representation. \n",
                "rating": 5,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "tD4cYRakmsO",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "Recommendation to Accept",
                "comment": "Summary:\nThis paper tackles the problem of training models that are robust to adversarial inputs. The authors starts by observing that previous models generate embeddings that can both (i) place same-class embeddings in different clusters and (ii) different-class embeddings in close proximity. They then introduce new loss functions that penalize these behaviors and design a training procedure (MAT) around these new losses. Finally, they show favorable performance of MAT compared to state-of-the-art techniques for addressing adversarial robustness.\n\nReasons for score:\nOverall, I vote for accepting. Training adversarially robust models is an important problem, and the paper\u2019s experimental validation that the features of prior methods (TRADES) exhibit (i) non-clustering and (ii) confusing distance motivates the approach they take. The loss functions are explicitly designed to combat these issues, and the experimental results clearly show the favorability of the MAT procedure. In addition, the ablation study of the various components of the loss functions also adds some insight into the results. The paper is also very well written.\n\nCons:\nIt would be of interest to have some theoretical justification for the approach. Regarding the loss functions, it seems that BIBO should be a consequence of penalizing FTC loss and SO loss, and should not be explicitly needed (this is also somewhat consistent with the results of Table 2). Finally, some of the experimental results can be explored further. For example, in the ablation study, some of the experiments perform better without one of the loss functions, and it may help to explain such behavior. \n\nClarity / Typos:\nThe paper is very well written. A couple of minor points:\nFeature compactness - Maybe explain this phrase better in the introduction (explained well in Section 3 introduction)\nEqn 1: Maybe write J(f(x), y) and J(f(x), f(x\u2019)) instead\n",
                "rating": 7,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "zzQzZ9CO3wY",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "The defense evaluation is not correct",
                "comment": "This work proposes a defense that combines prior work on learning features that are compact for samples from the same but dispersed for samples from different classes (MMD by Pang et al.) with (a) a method to find better class centers, (b) a gradient-norm regularization and (c) an adversarial training regularization.\n\nUnfortunately, the reported results on the robustness of the defense are clearly wrong. For one, the core part of this defense by Pang et al. was broken by [1] which is not mentioned here. More importantly, the adversarial attacks employed here are not suited for finding minimal adversarial perturbations against the proposed defense. This can be seen most clearly in Figure 2 (or Table 10 in the appendix): If we allow a perturbation with L-infinity norm of 0.5 on MNIST, then we can always find an adversarial perturbation simply by setting the whole image to a flat gray value of 0.5. In turn, any effective adversarial attack should drive network performance down to at least random baseline performance (10%) for epsilon = 0.5. Instead, the paper reports > 99% accuracy for this value under a PGD attack, which means that PGD is totally ineffective against the given defense and a very different adaptive attack would be needed to accurately measure its robustness. Similarly, in Table 3 the attack success of targeted attacks is often higher than for untargeted attacks, again a clear sign for ineffective attacks. The work also uses an adaptive attack which works better for some versions of MAT but performs similar to PGD in other cases. Hence, the adaptive attack employed here are not good enought.\n\nThe reason why the proposed attacks fail against the defense are probably simple: for one, the attacks optimise a different classificatioon loss then what is actually used by the model. Second, both auxiliary losses may give rise to gradient masking, the most common issue for gradient-based attacks to fail against a defense. I highly suggest the authors study [1] to get familiar with how to engineer strong adaptive attacks.\n\n[1] On Adaptive Attacks to Adversarial Example Defenses, Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry, NeurIPS 2020, https://arxiv.org/abs/2002.08347",
                "rating": 1,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "VdxOoQuC6FI",
                "reply_to": "iclr_2021_mOO-LfEVZK",
                "title": "AnonReviewer4 Review",
                "comment": "# Summary\n\nThe authors propose a novel training process called Manifold-Aware\nTraining (MAT) to increase the robustness of the CNN against adversarial\nexamples. The authors compare MAT against the state-of-the-art in\ndefenses against adversarial evasion attacks (i.e., TRADES and MCC) and\nshow their approach outperforms it.\n\n# Strengths\n\n+  Interesting intuition of performing training \"in the\" manifold\n+  Interesting intuition to support SO and BIBO losses\n\n# Weaknesses\n\n-  Lack of comparison with a similar approach\n-  Lack of conclusive remarks / actionable points\n\n# Comments\n\nI praise the authors intuition of exploring the possibility of training\na classifier by exploiting knowledge of the manifold - its immediate\nimplication is that of focusing on lower dimensions of compact features\nthat would be more robust to manipulation (and thus adversarial\nattacks). I also particularly appreciate the threat model and the fact\nthe approach is evaluated in a white-box setting, according to Carlini\net al. (2019). While the authors' intuition is interesting, I wonder how\neasy this is to achieve in practice. In general, we have no knowledge of\nthe underlying manifold and I thus wonder what guarantees this approach\nwould provide. The results seem to show no clear loss-dependent trend\nand I thus wonder whether we can easily draw conclusive remarks. (For\ninstance, should we use SO and BIBO always? From a theoretical\nperspective, it seems so, but experiments seem to show otherwise.)\n\nFigure 1 is interesting as it shows that the representative features of\nsame-class samples are not always similar to one another. Wasn't this\nalready explored in Szegedy et al.? Perhaps not visually, but the fact\nthat objects close in the input space get eventually separated in the\nlatent space across the layers of the CNN is quite known. Also, a\nsimilar approach to the authors' proposal seems to be explored by Crecchi\net al. [1]. It would be interesting to compare and position MAT against\nthis.\n\n## Additional Comments\n\nIn Section 3.2, the authors propose two auxiliary loss functions to\nfurther improve the robustness of MAT. I wonder whether the BIBO loss\nwould just suffice for the purpose, instead of relying on the\nsecond-order loss too. I appreciate the explanation in Section 3.2.3 but\nit would be interesting to understand how one should expect to tune\nalpha and beta accordingly. \n\nResults on CIFAR10 seem less stable than compared to those on MNIST. In\nparticular, there is no trend that shows that relying on SO and BIBO on\na clean dataset provides better results than with a plain FTC loss:\n94%->85%->95%->83%; why the 95%? Is that expected? Similar reasoning\ncan actually be applied to MNIST too when one looks at PGD:\n61%->99%->82->99; why 82%? Is this expected? In contrast, TRADES seem to\nshow an expected trend (even when BIBO loss is considered).\n\nThe authors rely on the library 'foolbox' - my impression was that\ncleverhans [2] represented the state-of-the-art when it comes to\nexperimenting with adversarial ML attacks. What advantages does foolbox\nhave compared to cleverhans?\n\nAlthough off-topic for this work, it would be interesting to understand\nwhether MAT would be beneficial in defending against adversarial attacks\nthat consider realizable attacks (in the problem space). Figure 2 shows\nthe stability of MAT robustness for increasing values of perturbations.\nAdversarial attacks in the problem-space might need to consider\nadditional constraints while being non-necessarily constrained in a\nlp-norm [3].\n\n[1] Crecchi et al. Detecting Adversarial Examples through Nonlinear\nDimensionality Reduction. ESANN 2019\n(https://pralab.diee.unica.it/sites/default/files/crecchi19-esann.pdf)\n\n[2] http://www.cleverhans.io/\n\n[3] https://s2lab.kcl.ac.uk/projects/intriguing/ (IEEE S&P 2020)\n\n### Minor Typos\n\n\"optimizationm\" -> \"optimization\"",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "expressed clear concerns",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not at all"
            }
        ]
    },
    "bmGLlsX_iJl": {
        "paper_id": "iclr_2022_bmGLlsX_iJl",
        "paper_title": "EMFlow: Data Imputation in Latent Space via EM and Deep Flow Models",
        "paper_abstract": "The presence of missing values within high-dimensional data is an ubiquitous problem for many applied sciences. A serious limitation of many available data mining and machine learning methods is their inability to handle partially missing values and so an integrated approach that combines imputation and model estimation is vital for down-stream analysis. A computationally fast algorithm, called EMFlow, is introduced that performs imputation in a latent space via an online version of Expectation-Maximization (EM) algorithm by using a normalizing flow (NF) model which maps the data space to a latent space. The proposed EMFlow algorithm is iterative, involving updating the parameters of online EM and NF alternatively. Extensive experimental results for high-dimensional multivariate and image datasets are presented to illustrate the superior performance of the EMFlow compared to a couple of recently available methods in terms of both predictive accuracy and speed of algorithmic convergence.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper proposes a data imputation method for MCAR and MAR data by combining EM and normalizing flows.  The paper is clearly written.  The idea is interesting and they show better performance compared to MCFlow and competing methods on ten multivariate UCI data, MNIST and CFAR10 image data.\n\nIssues regarding limited novelty compared to MCFlow was raised.\nIssues regarding the validity of Assumption 2 on the dependencies in the latent space and observation space was also raised.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "11SUrc0VF-e",
                "writer": "author",
                "reply_to": "8LLp1OznxG",
                "title": "Further questions/concerns?",
                "comment": " We thank the reviewer again for detailed read and constructive comment, and hope that you find our response satisfying. If you have any further questions or concerns, we are willing to provide more explanations and experiments. Meanwhile, if our previous response addresses your concerns, we sincerely hope that you may rearrange the score.\n\nWe want to emphasize again the novelty of this work. The latent space of EMFlow operates in a completely different way than MCFlow: the online EM _explicitly_ performs the imputation while _learns_ the parameters of the base distribution during the training. Such design leads to significant improvement on 1) imputation accuracy, 2) computational efficiency and 3) robustness to hyperparameters and initialization compared to MCFlow and other most recent competitive methods.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YwuzdRW-2Ds",
                "writer": "official_reviewer",
                "reply_to": "LiYGAQYepqF",
                "title": "Response to author follow up",
                "comment": " Thank you for the thorough discussion and for additional experiments, particularly clarifying the initialization across methods (both using NN across methods, and random initialization optimization). This paper seems broadly applicable and of interest to the community, there are slight limitations in scalability to larger datasets but overall the method seems well-justified. My score remains positive.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wTQapKwa4sE",
                "writer": "official_reviewer",
                "reply_to": "pjoIZ2kG9jS",
                "title": "Thanks for the response",
                "comment": " I would thank the authors for the effort in the response to my concerns. \nI would say sorry for some of my initial unclear comments, luckily we make it clear in the discussion session and it turns out the discussion converged at some point. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "pjoIZ2kG9jS",
                "writer": "author",
                "reply_to": "1l78YK1VJ4T",
                "title": "Further response",
                "comment": " The points made by the reviewer is well taken but given the limited time and space for the paper, we can address all of the concerns in a future work. We have already empirically illustrated the computational efficiency and accuracy of our methods compared to most recent competitive methods in the area, but, clearly, more work is required in this area to elaborate on some of the issues raised by the reviewer.\n\nWe also want to note that EMFlow shares similarities with the identifiable iFlow proposed in [1]. In iFlow, the base distribution $P_Z(\\cdot|\\mathbf{u})$ is a factorized exponential family distribution where the auxiliary variable $\\mathbf{u}$ decides the natural parameters though MLP. It is also noted that the exponential families have universal approximation capabilities. In EMFlow, the base distribution is a Gaussian $P_Z(\\cdot|\\boldsymbol{\\mu}$, $\\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu}(\\mathbf{m})$ and $\\boldsymbol{\\Sigma}(\\mathbf{m})$ is conditioned on the auxiliary missing mask $\\mathbf{m}$ though the EM procedure in Equation (5). Furthermore, iFlow optimizes the marginal likelihood $P_X(\\mathbf{x}|\\mathbf{u})$, which is what we optimize in Equation (11). The difference is that EMFlow conditions $\\boldsymbol{\\mu}(\\mathbf{m})$ and $\\boldsymbol{\\Sigma}(\\mathbf{m})$ on _batches_ of $\\mathbf{m}$, such that the results of iFlow may not be directly applied to EMFlow. But it can be a direction for future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1l78YK1VJ4T",
                "writer": "official_reviewer",
                "reply_to": "kCiL8rm3zBz",
                "title": "A concern about Assumption 2, the dependencies and the correspondence of ($Xi$,$X_j$) and ($S_i$, $S_j$), and applying MAR to the latent space.",
                "comment": " I would thank the authors for the response and for providing the other references. However, here is my concern:\n\n**(I)**\nI knew the difference between ICA and NF. A further elaboration on the identifiability of ICA is to take us on the same page, which means that one should properly utilize the latent space, be careful about the correspondence between the latent space and observation space, and notice what information/conclusions/assumptions can be used/guaranteed not. (_In short, [1] uses the auxiliary variable to have the identifiability which is not the case of the work._ )\nUnfortunately, Assumption 2 is not the case. One should justify it and provide the condition and further discussion for it because it is fundamental and essential for the correctness of the method. For example, what does the method do, and how does it guarantee that Assumption 2 holds.\n\n**(II)** The authors distinguish correspondence from dependencies for the proposed method.\nHowever, I didn't quite get it. For example, what would be the case that $X_i$ and $S_j$ have no correspondence but the dependencies of $X_i$ and $X_j$ implies the dependencies between $S_i$ and $S_j$.\n\n**(III)** The **correspondence** matters which is used in the method. As shown in Eqn. (6), it uses the correspondence of $X_i$ and $S_j$ (in the paper it is $Z_i$), because $z_i = f_{\\psi}^{-1} (x_i)$, I guess if $x_i$ is missing, the method considers $z_i$ as missing as well by using the correspondence. Furthermore, the MAR assumption is applied to $s_m$/$z_m$ and $s_o$/$z_o$ where MAR is an assumption based on $x_o$ and $x_m$. Without the correspondence, how does the MAR hold in the latent space for the corresponding variable $z_o$ and $z_m$?\n\n**(IV)** As for the **dependencies** in the latent space and observation space, which is also assumed in Assumption 2. \nI am not convinced given the figure in [2] such that to believe Assumption 2 holds for the work in general.\nFrom the empirical result, we could say when it holds, it works. However, it is essential to include the justification, discussion, and guarantee/condition of the assumption, e.g., that why it should hold, and when it holds, as mentioned in **(I)**.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kCiL8rm3zBz",
                "writer": "author",
                "reply_to": "7QMhBH83_Bq",
                "title": "Further explanation on identifiability",
                "comment": " We very much appreciate for your efforts to provide additional feedback with more elaboration. We hope you find our further explanation on identifiability satisfying.\n\nFirst of all, although nonlinear ICA and NF share some structural similarities, they have quite different goals: nonlinear ICA is primarily used for source identification and separation, while NF is primarily used to approximate the probability distribution of the data. We also note that recently [1] bridges\u00a0the gap between NF and nonlinear ICA by developing a flow-based model for estimating latent representations with\ntheoretical results on identifiability using equivalence classes.\n\nDue to the crucial distinction between the perceived goals of ICA and NF, identifiability\u00a0of the parameters of the map is not necessarily the focus of this work. The primary goal of EMFlow is to perform imputation using posterior predictive distribution of missing values\nconditioned on the observed values using the NF to model the data distribution, and then use estimated predictive distribution on test data. Thus, identifiability of model parameters and their unique estimation may not be necessary. This is true for almost all NNs and so identifiability of parameters are usually ignored when predictions are the main goals. In fact, EMFlow only assumes a learnable correlation between the inter-feature-dependencies of the data and latent spaces, instead of coordinate correspondence (e.g., the dependencies between $X_i$ and $X_j$ imply the dependencies between $S_i$ and $S_j$ ).\n\nEmpirically, such coordinate correspondence does exist for the current implementation of this work. A nice work to show such correspondence\u00a0is [2] that concludes \"there exists a correspondence between the coordinates in an image and in its learned\nrepresentation\" (see Figure 2 in section 5). However, as explained before, coordinate correspondence is not necessary.\u00a0To address this issue, we carried out additional experiments where a random permutation layer was inserted between the affine\ncoupling layers of Real NVP, such that the coordinate correspondence would be eliminated. The empirical results show\u00a0that the introduction of such random permutations makes no significant impact in terms of accuracy of imputed values. However,\nwe also noticed that the convergence speed becomes slower possibly due to the fact that the learnable correlation becomes more complex.\n\n[1]\u00a0Li, Shen, Bryan Hooi, and Gim Hee Lee. &quot;Identifying through flows for recovering latent\nrepresentations.&quot;\u00a0arXiv preprint arXiv:1909.12555\u00a0(2019).\n\n[2]\u00a0Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. &quot;Why normalizing flows fail to detect\nout-of-distribution data.&quot;\u00a0arXiv preprint arXiv:2006.0854",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7QMhBH83_Bq",
                "writer": "official_reviewer",
                "reply_to": "CLYnDJXAEwm",
                "title": "A further elaboration and discussion ",
                "comment": " I would thank the authors for the response and would like to further discuss my main concerns. Let's see can we converge to the same page.\n\nI would like to further elaborate my point. The formulation of nonlinear ICA is: $x =f(s)$, where $x$ represents for the observed variables and $s$ represents for the noise/source signal. This is what NF is doing by using the change of variable formula: $p_x(x) = p_s(s)| det J_T (s)|^\u22121$. \n\nGiven an observed vector $[x_1, x_2,...,x_m]$, one can get $[s_1, s_2 ,..., s_m]$ with the proposed method. My question is that without certain **identifiability**, how are the dimension information preserved, i.e., should $s_i$ be corresponding to and used for interpreting $x_i$? Furthermore, how the inter-feature dependencies are preserved, i.e., the dependencies between $X_i$ and $X_j$ imply the dependencies between $S_i$ and $S_j$? For example, one non-identifiable case can be that given $p(s)$, there is an equivalent class of $f$; another case can be that given $f$, $p(s)$ is invariant to any rotation of $s$.\n\nMoreover, I didn't get how Rosenblatt transform can deal with my concern regarding the dependencies and identifiability. I would sincerely ask the authors to further explain it for further discussion.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CLYnDJXAEwm",
                "writer": "author",
                "reply_to": "P0QsASnSkq",
                "title": "Response to reviewer Twou",
                "comment": " We thank the reviewer for taking time in reviewing our work, and  appreciate the in-depth comments on the assumptions of this work. However, we believe that there seem to be significant misunderstandings of our work in the review, as outlined below.\n\n1. __\"According to the studies in factor analysis and nonlinear ICA, the latent representation is not identifiable. So it is not straightforward to believe that the dependencies in the latent space are consistent with the dependencies in the observation space\"__\n\n    It is to be noted that the NF framework is distinctly different from that of standard factor analysis, PCA or ICA and their nonlinear generalizations as the NF is built using a sequence of compositions of invertible maps. Thus, the identifiability issues are not confronted by NFs. Moreover, although its not straightforward to explicitly characterize dependence structure in the data space from those in the latent space (even though NF involves complicated invertible maps), the existence of such a map is well known (e.g, via the Rosenblatt transform).\n    \n2. __\"the neighbors in the latent/source variable space are not necessary to be the neighbors in the observation space.\"__\n\n    The NF used in this work (Real NVP) performs element-wise transformation. That is, there is a univariable transformation between $x_i$ and $z_i$ for $i=1,\\ldots,p$. So, the neighbors in the latent space are the neighbors in the data space. \n    \n3. __\"Another fact about NF is that even though the covariance matrix of NF is an identity matrix, ..., Why we could believe that they are corresponding to inter-feature dependencies in the observed data?\"__\n    \n    There is a trade-off between the complexity of the base distribution and the expressiveness of the transformations in NF. For example, to model heavy-tailed distributions, either the base distribution should also be heavy-tailed or the transformation should be more expressive than Lipschitz-continuous affine transformations ([1]). That is, we can have the base distribution to capture some characteristics of the target distribution, rather than letting the transformations to do all the jobs. In this work, we deliberately assume a full covariance matrix in the latent space and which in turn allows it to capture the inter-feature dependency in the latent space. Considering that the transformations are element-wise, we further assume that the inter-feature dependencies in the latent and data spaces can be correlated. Such assumption is verified in the experiments in the sense that the imputed data vectors (transformed from imputed latent vectors) have reasonable accuracy.\n    \n4. __\"more comparison in the experiment section would be required, especially, the works using VAE and the methods without using generative models\"__\n    \n    We added VAE-based MIWAE ([2]) as to our experiments and the results is shown in Appendix G.2. Since the authors of MIWAE didn't release the code on image datasets, we can only tune and test it on MNIST within this limited time frame. The results shows that EMFlow and MIWAE have very close results on UCI datasets, but EMFlow outperforms MIWAE on MNIST. We also note that EMFlow still converges much faster than MIWAE.\n\nReferences\n\n[1] Jaini, Priyank, et al. \"Tails of lipschitz triangular flows.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2] Mattei, Pierre-Alexandre, and Jes Frellsen. \"MIWAE: Deep generative modelling and imputation of incomplete data sets.\" International Conference on Machine Learning. PMLR, 2019.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rUetbqYQl0",
                "writer": "author",
                "reply_to": "WvlKhCyqo-e",
                "title": "Response to reviewer zdAh ",
                "comment": " We\u2019re glad you had a positive impression on our paper. Please find our point-wise responses below to your insightful feedback.\n\n1. __\"time series dataset which also have additional layers of correlation between time points\"__\n    \n    The [Air(Quality) dataset](https://archive.ics.uci.edu/ml/datasets/Air+quality) included in our experiments is itself a time series dataset. But we admit that we didn't test EMFlow systematically on time series datasets. We understand that EMFlow and all other competing methods have an implicit assumption that each observation is independently drawn from an underlying distribution, and thus don't account for the correlations between between time points. To mitigate this issue, we think there might be at least two solutions to explore:\n    * Stack some lagged observations and apply EMFlow (like what is done for fitting AR models).\n    * Develop NFs that estimate time-dependent distributions, i.e., include time as an additional dimension in the data and latent space. There is already some interesting works in this area [1].\n    \n    These are definitely worth pursuing as possible extensions of our proposed EMflow algorithm that accounts for correlation across the vector of observations.\n    \n2. __\"The author did not show the performance in MNAR. Therefore the novelty remains limited\"__\n\n    We admit that EMFlow is not designed for MNAR as we don't explicitly model the missing mechanism. We expect significant change needed for EMFlow to work on MNAR, like including another component to incorporate the domain knowledge of the missing process and learn the missing mechanism. It is well known that handling MNAR data requires strong modeling assumptions that depends on specific nature of data collection process and hence generic version like EMflow (applicable for MAR) or any other imputation methods would not work without domain knowledge. For this reason, we initially focused our application to MAR case for broader applications.  \n    \n3. __\"The author also need to realize the limitation for all EM method comparing to FCS multiple imputation which considering the uncertainty of the imputed value\"__\n    \n    It's true that EMFlow currently only does single imputation and thus the uncertainty of imputed values is missing. But it's possible to obtain the standard errors of the parameters (i.e., the covariance matrix) estimated by EM ([2], [3]), which could be potentially used for multiple imputation in the future work.\n    \n    We also want to note that FCS methods normally need to specify a fully conditional distribution for each feature and sample from them sequentially in each iteration, which prevents FCS's application for high-dimensional data. However, it is also well known that FCs may not always uniquely determine a joint distribution or even lead to improper distributions if some regularity conditions (eg, as required for Hammersley-Clifford-Besag type results) are not met or not checked.\n    \n4. __\"one can not assume similar performance when the method is tested on longitudinal clinical data, unless tested systematically on actual data\"__\n    \n    We added a longitudinal dataset ([Web](https://www.kaggle.com/c/web-traffic-time-series-forecasting)) for in our experiments in Table 1. Although it is not a clinical one and we don't have systematical test on such data, we believe that EMFlow can be applied to longitudinal clinical datasets as long as each subject is assumed to be independent.\n\nReferences\n\n[1] Both, Gert-Jan, and Remy Kusters. \"Temporal Normalizing Flows.\" arXiv preprint arXiv:1912.09092 (2019).\n\n[2] Meng, Xiao-Li, and Donald B. Rubin. \"Using EM to obtain asymptotic variance-covariance matrices: The SEM algorithm.\" Journal of the American Statistical Association 86.416 (1991): 899-909\n\n[3] Jamshidian, Mortaza, and Robert I. Jennrich. \"Standard errors for EM estimation.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 62.2 (2000): 257-270.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "zD1g9lJPSIl",
                "writer": "author",
                "reply_to": "8LLp1OznxG",
                "title": "Response to reviewer L7zK",
                "comment": " We sincerely appreciate your comments and suggestions to strengthen our paper, and hope our responses help to address your concerns.\n\n1. __\"my main concern is that the novelty of the paper seems to be limited\", \"the major difference is that this paper replaces the sampling with the EM algorithm\"__\n\n    We think sampling is part of both EMFlow and MCFlow. That is, a sample is taken from the base distribution in the latent space and then goes though the transformation layers. The major difference is that MCFlow has an \"implicit\" sampler in the latent space, which is a standard MLP. This MLP finds samples of latent vectors by maximizing the log-likelihood of imputed vectors in the data space. \n    \n    On the other hand, EMFlow has an \"explicit\" sampler in the latent space: the online EM explicitly replace the missing parts of latent vectors with the conditional means of the base distribution. Such sampling happens completely in the latent space. To achieve this, we assume a Gaussian base distribution with a general covariance matrix in the latent space that will be learned during the optimization. To the best of our knowledge, Gaussian with unstructured covariance had not been used as the base distribution of NFs for applications other than density estimation (e.g., [1], [2]).\n\n2. __\"I would be very interested to see discussions about how interpretability can be enhanced by this proposed model\"__\n\n    The interpretability of EMFlow comes from the explicit sampler in the latent space (i.e. online EM). In MCFlow, the MLP in the latent space is a black box and the difference between the input ($\\mathbf{z}$) and output ($\\widehat{\\mathbf{z}}$) latent vectors is unclear. In EMFlow, with a probabilistic model in the latent space, we know exactly how $\\mathbf{z}$ is transformed to $\\widehat{\\mathbf{z}}$, and $\\widehat{\\mathbf{z}}$ has higher log-likelihood in the latent space than $\\mathbf{z}$.\n    \n    Such interpretability bring some possibilities of extending EMFlow. For example, there is established work on obtaining the standard errors of the parameters estimated by EM (e.g., [3], [4]). It would be interesting to see how the standard errors based on latent data can be used to estimate the uncertainty of the imputation in the data space by the use of delta-method that can estimate the propagation on uncertainty.\n\n3. __\"Another minor typo\"__\n\n    Thanks for pointing it out and it's fixed.\n\n\nReferences\n\n[1] Laszkiewicz, Mike, Johannes Lederer, and Asja Fischer. \"Copula-Based Normalizing Flows.\" arXiv preprint arXiv:2107.07352 (2021).\n\n[2] Jaini, Priyank, et al. \"Tails of lipschitz triangular flows.\" International Conference on Machine Learning. PMLR, 2020.\n\n[3] Meng, Xiao-Li, and Donald B. Rubin. \"Using EM to obtain asymptotic variance-covariance matrices: The SEM algorithm.\" Journal of the American Statistical Association 86.416 (1991): 899-909\n\n[4] Jamshidian, Mortaza, and Robert I. Jennrich. \"Standard errors for EM estimation.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 62.2 (2000): 257-270.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LiYGAQYepqF",
                "writer": "author",
                "reply_to": "CFYFwWY_OqB",
                "title": "Response to reviewer tdhx (part II) ",
                "comment": " 6. __\"Could EMFlow be adapted to work with low-rank covariance estimators?\"__\n\n    Yes, we think low-rank covariance estimator is a promising direction to make EMFlow scale to larger datasets. Instead of assuming a full-rank covariance matrix in the latent space, a low-rank/structured/sparse covariance estimator is more appealing for image datasets. We could possibly adapt some of the earlier works in this area as illustrated by [3] and [4]. But to make such adaption feasible, we expect notable change in the part of EM imputation as NF may no longer be strictly identifiable. \n\n7. __\"Or perhaps the authors could show further analysis of convergence of the covariance matrix estimation during optimization?\"__\n\n     We show the convergence of covariance matrix estimation in terms of Frobenius norm in Appendix G.3. \n\n8. __\"Can EMFlow be adapted to impute categorical data?\"__\n\n    It is another interesting direction of the future work of this paper. One possibility is to replace the RealNVP used in the paper with NFs that directly work on categorical features (e.g. [5], [6], [7]), and adjust the objectives accordingly. \n\nReferences\n\n[1] Ledoit, Olivier, and Michael Wolf. \"A well-conditioned estimator for large-dimensional covariance matrices.\" Journal of multivariate analysis 88.2 (2004): 365-411.\n\n[2] Won, Joong\u2010Ho, et al. \"Condition\u2010number\u2010regularized covariance estimation.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 75.3 (2013): 427-450.\n\n[3] Zhou, Sheng-Long, et al. \"Sparse and low-rank covariance matrix estimation.\" Journal of the Operations Research Society of China 3.2 (2015): 231-250.\n\n[4] Chen, Xixian, Michael R. Lyu, and Irwin King. \"Toward efficient and accurate covariance matrix estimation on compressed data.\" International Conference on Machine Learning. PMLR, 2017.\n\n[5] Hoogeboom, Emiel, et al. \"Integer discrete flows and lossless compression.\" arXiv preprint arXiv:1905.07376 (2019).\n\n[6] Tran, Dustin, et al. \"Discrete flows: Invertible generative models of discrete data.\" Advances in Neural Information Processing Systems 32 (2019): 14719-14728.\n\n[7] Lippe, Phillip, and Efstratios Gavves. \"Categorical normalizing flows via continuous transformations.\" arXiv preprint arXiv:2006.09790 (2020).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "CFYFwWY_OqB",
                "writer": "author",
                "reply_to": "22dnWja_Rf",
                "title": "Response to reviewer tdhx (part I)",
                "comment": " Thank you for appreciating the strengths of our work and the constructive suggestions for potential improvement. Please find point-wise responses to your concerns below. We also amended our Appendix in the updated manuscript based on your comments.\n\n1. __\"the paper does not compare to GAN-based imputation methods, which might outperform simpler architectures\"__\n\n    We did compare EMFlow with GAN-based methods such as MisGAN and GAIN, and spent a fair amount of time for tuning them. For UCI datasets and smaller image datasets (compared to CelebA), EMFlow outperforms MisGAN and GAIN as shown in Table 1 and Table 2 in the paper. \n\n2. __\"The tasks in the empirical section, while not trivial, are not the most difficult examples\"__\n\n    The main barrier of applying EMFlow to larger tasks like CelebA is the estimation of covariance matrix in the latent space (as you also mentioned). To have a well-conditioned covariance matrix, a batch size larger than the data dimension is desired, which is impractical for some tasks like CelebA even with the help of super batch introduced in Appendix D. \n\n    We tried some methods for high dimensional covariance estimation ([1] and [2]) but didn\u2019t get very promising results. We admit that it is the current limitation of EMFlow and making EMFlow scale to larger tasks should be the priority of future works. \n\n    But we also want to emphasize that, EMFlow performs very well on datasets of moderate sizes in terms of accuracy and the ease of training (fast convergence, robust to the choice of hype-parameters). EMFlow is also robust to initial imputation (please see our response (5) for further details). \n\n3. __\"The dimensionality and general dataset descriptions are missing from the text and should be included\"__\n\n    The description of UCI datasets was already included in Appendix E, and the dimensions of MNIST and CIFAR-10 are provided in the text of Sec 4.2. \n\n4. __\"EMFlow is able to be seeded with NN imputation, whereas MCFlow is not\"__\n\n   All the methods experimented in this paper (e.g., EMFlow and MCFlow) use the NN imputation as the starting point on image datasets. So, the use of initial NN imputation is perhaps not a significant contributing factor to the final performance difference between EMFlow and MCFlow. \n\n5. __\"Finally, to what degree is the fast convergence a result of the warm-start imputation?\"__\n\n    We added appendix G.3 to investigate the impact of warm-start on the convergence speed. Specifically, we compare the NN imputation and zero imputation as the starting point of imputation on MNIST. We find that the convergence of training loss, test set RMSE and the covariance matrix estimation under both initiation schemes remain similar and relatively insensitive, the initial differences being rather negligible. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "22dnWja_Rf",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "",
                "comment": "This paper presents a novel imputation method for high-dimensional datasets that typically serve as benchmarks for machine learning methods. This method (EMFlow) innovates by training a normalizing flow network to map input data samples to a multivariate Gaussian, where imputation is performed via an online version of expectation-maximization (EM), which is commonly used for missing data imputation. EMFlow is applied across regression tasks in datasets in the UCI machine learning dataset repository, and standard image classification datasets, MNIST and CIFAR-10. Empirical results show strong performance for missing data imputation, as well as downstream classification from these imputations, and the model design choices and well-constructed architecture make for easy training and fast optimization convergence. EMFlow builds most closely upon MCFlow, mentioned in the paper. Both models use normalizing flows (NF) to capture the input data distribution, and MCFlow uses a standard MLP to find a latent vector maximizing the log-likelihood of the missing data. EMFlow uses NFs with EM to maximize the probability of the latent vector (under a probabilistic model \u2014 a multivariate normal) corresponding to the missing data. EMFlow has strong empirical results compared to the baseline of MCFlow. While the paper does not compare to GAN-based imputation methods, which might outperform simpler architectures, the lack of comparison seems fair given the ease of training EMFlow relative to a GAN-based method. Particularly looking at convergence traces in Figure 2. Overall the text is clear, well-organized, and easy to follow; there are only minor typos.\n\nMain strengths:\nThe architecture presented here is relatively straightforward, and does not have many hyperparameters, leading to relatively easy training and implementation. The choice of a multivariate gaussian latent space distribution leads easy conditioning and marginalization, and thus lends itself well to the online EM algorithm presented for imputation. The empirical results are strong, and the methodology is appealing for the reasons listed above. While not far from MCFlows, it provides enough of a conceptual and performance improvement to be a significant contribution to that work.\n\nMain weaknesses and areas to address:\nThe tasks in the empirical section, while not trivial, are not the most difficult examples. MisGAN, for instance, highlights results on CelebA, a more complicated learning task, as well as a higher-dimensional dataset than any listed in the paper. It might strengthen the paper to present results on a higher-dimensional dataset like CelebA or others. Towards this point, it might also be worth evaluating the FID score of imputed images on a dataset like CelebA, rather than just reporting classification accuracy or RMSE.\n\nThe dimensionality and general dataset descriptions are missing from the text and should be included. Particularly of importance is to include the dimensionality of the various datasets analyzed to support the claim that EMFlow works well for high-dimensional data.\n\nThe nearest neighbor imputation in MNIST and CIFAR-10 to seed EMFLow is quite reasonable. However, this should be compared to as a baseline as well. It is not clear the difference this warm-start makes in practice. Furthermore, EMFlow is able to be seeded with NN imputation, whereas MCFlow is not. EMFlow produces much better looking and smoother images than MCFlow, which contains far more noise, on CIFAR-10, and these differences should be due to algorithm changes in EMFlow and not NN imputation. The concrete suggestion here is to either report NN imputation as a baseline method on it\u2019s own, or seed EMFlow with a random imputation as a form of ablation experiment. A third type of ablation experiment different from the two mentioned can be left up to the authors. Finally, to what degree is the fast convergence a result of the warm-start imputation?\n\nHigh-dimensional image data are thought to lie on lower-dimensional manifolds. It is unlikely that the covariance matrix ($\\Sigma$) learned in Z-space has full rank. As the data dimensionality continues to scale, it seems this methodology of estimating the empirical covariance will likely not scale as well, even with the robust estimator in (31). Could EMFlow be adapted to work with low-rank covariance estimators? Or perhaps the authors could show further analysis of convergence of the covariance matrix estimation during optimization.\n\nCan EMFLow be adapted to impute categorical data? Imputation is a key and general ML problem. EMFlow is an intuitive framework methodologically and in practice seems easy to train and performs well on real data. It is principled in its approach \u2014 relying on the rigorous EM algorithm as its base. However, EMFlow (albeit fairly due to training complexity) does not compare to what might be state-of-the-art GAN-based imputation methods. It is also unclear how EMFlow would scale to even higher-dimensional datasets given the modeling assumptions.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "8LLp1OznxG",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "",
                "comment": "This paper presents a model named EMFlow, which performs data imputation in the latent space using the online EM algorithm together with the normalizing flow models. The normalizing flow models aim to capture the complete data density $p_X$ and the bidirectional mapping between the data space and the latent space, even when the data is only partially observed. The parameters in the latent space are updated using an online EM algorithm. Thanks to the feature-wise mapping, the dependency between features in the data space is carried over to the latent space and hence the imputation can be done. Evaluation using ten UCI datasets, MNIST, and CIFAR-10 datasets show impressive improvement against baseline models and the convergence is faster than MCFlow. The quality of the paper is generally good. The proposed model combines the online EM and flow models to do imputation, which looks to me a quite reasonable design. The model is evaluated using a number of UCI multi-variable datasets and two image datasets (MNIST and CIFAR-10). The performance is generally better than baselines including MCFlow. Because sampling is avoided by using EM, it converges faster than MCFlow.\n\nThe presentation of the paper is quite clear. It is organized, well-written, and quite easy to follow. However, my main concern is that the novelty of the paper seems to be limited, especially when compared with MCFlow; it seems to me that the major difference is that this paper replaces the sampling with the EM algorithm. It is a very reasonable extension, yet it has already been widely studied in various domains and applications.\n\nThe authors claim in the introduction section that EM can be applied in an interpretable way, which motivates the authors to use EM in this paper, but this point is not further discussed in the paper. I would be very interested to see discussions about how interpretability can be enhanced by this proposed model.\n\nAnother minor typo: in the line below Eq. (2), should it be $\\subseteq$ instead of $\\in$ between $\\mathcal{X}^\\prime_i$ and $\\mathcal{X}$? The presentation is clear, the model is theoretically sound, and experiments show impressive improvement against baseline models for most of the dataset and missing rates. However, the novelty is somewhat limited, especially when compared with MCFlow.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "P0QsASnSkq",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "",
                "comment": "The paper aims at imputing missing data which are MCAR and MAR. \nFor modeling the observed data distribution, it utilizes the framework of normalizing flow of which the latent variable/source variable space is Gaussian.\nBy assuming the consistency of inter-feature dependencies in the latent/source variable space, it applies online EM for the imputation of the latent space variables.\nIn the experiments, the proposed method, EMFlow is compared with GAIN, MisGAN, and MCFlow. Although the imputation of MCAR and MAR scenarios has been studied for decades and in theory is not a problem, its applications in high-dimensional and real-world data with complicated distributions can be interesting from a practical perspective.\n\nThe work adapts the online EM algorithm for the missing data imputation. I quickly go through the online EM extension, which seems sound and no problem.\n\nHowever, I have some concerns regarding the assumptions of NF.\nIt would be necessary to justify the assumptions, especially, why the two assumptions are reasonable to believe.\n\nFirstly, inter-feature dependencies in the observation space are different from the inter-feature dependencies in the latent space. According to the studies in factor analysis and nonlinear ICA, the latent representation is not identifiable. So it is not straightforward to believe that the dependencies in the latent space are consistent with the dependencies in the observation space. Then, why would it be reasonable to do imputation in the latent space?\n\nSecondly, note that according to the property of the change of variable formulation used by NF, the neighbors in the latent/source variable space are not necessary to be the neighbors in the observation space.\nAnother fact about NF is that even though the covariance matrix of NF is an identity matrix, it can still model the data distribution well. Then what are the dependencies modeled by the covariance of the multivariate Gaussian distribution in the latent space? Why we could believe that they are corresponding to inter-feature dependencies in the observed data?\n\nA final minor concern is about the experiments. To have a better view of the comparison and the benefits of the proposed method, more comparison in the experiment section would be required, especially, the works using VAE and the methods without using generative models.\n The work focuses on MCAR and MAR cases and extends NF and online EM for the missing data problem, which can be interesting for applications from a practical perspective. But the assumptions and main idea of the work can be lack justification, i.e., the relationship between the dependencies in the latent space and observation space needs to be elaborated and clarified. Moreover, a more thorough comparison with other related works would be helpful to better evaluate the proposed method.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "WvlKhCyqo-e",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_bmGLlsX_iJl",
                "title": "",
                "comment": "The authors propose a novel architecture EMFlow for missing data imputation. The authors also show the results of various experiments with multivariate and image datasets. Finally, the authors report the accuracy of post-imputation classification on image datasets. The study is well designed. however, there are few limitations that needs to be acknowledged.\n\nThe data structure is using imaging data as example. This kind of dataset having high neighborhood correlation and imputation using EM with latent space will be less challenging comparing to time series dataset which also have additional layers of correlation between time points. Perhaps need to show the performance on those datasets.\n\nThe author did not show the performance in MNAR. Therefore the novelty remains limited.\n\nThe author also need to realize the limitation for all EM method comparing to FCS multiple imputation which considering the uncertainty of the imputed value. The study and the expected results are acceptable; however, this framework was tested on imaging dataset, which has unique characteristics. for example, one can not assume similar performance when the method is tested on longitudinal clinical data, unless tested systematically on actual data. ",
                "rating": 6,
                "confidence": 5
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "is clearly written",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The idea",
                "Sentiment Expression": "is interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "performance compared to MCFlow and competing methods on ten multivariate UCI data, MNIST and CFAR10 image data",
                "Sentiment Expression": "better",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "novelty compared to MCFlow",
                "Sentiment Expression": "limited",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "validity of Assumption 2 on the dependencies in the latent space and observation space",
                "Sentiment Expression": "issues regarding",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "NbaEmFm2mUW": {
        "paper_id": "nips_2021_NbaEmFm2mUW",
        "paper_title": "Hierarchical Skills for Efficient Exploration",
        "paper_abstract": "In reinforcement learning, pre-trained low-level skills have the potential to greatly facilitate exploration. However, prior knowledge of the downstream task is required to strike the right balance between generality (fine-grained control) and specificity (faster learning) in skill design. In previous work on continuous control, the sensitivity of methods to this trade-off has not been addressed explicitly, as locomotion provides a suitable prior for navigation tasks, which have been of foremost interest. In this work, we analyze this trade-off for low-level policy pre-training with a new benchmark suite of  diverse, sparse-reward tasks for bipedal robots. We alleviate the need for prior knowledge by proposing a hierarchical skill learning framework that acquires skills of varying complexity in an unsupervised manner. For utilization on downstream tasks, we present a three-layered hierarchical learning algorithm to automatically trade off between general and specific skills as required by the respective task. In our experiments, we show that our approach performs this trade-off effectively and achieves better results than current state-of-the-art methods for end-to-end hierarchical reinforcement learning and unsupervised skill discovery.\n",
        "paper_acceptance": "accept",
        "meta_review": "The paper proposes a new way of hierarchical goal-based learning. There have been multiple examples of hierarchical strategies where higher levels set goals for the lower levels. However, often such goals are set in a quite low dimensional space, either because the state space is low dimensional to begin with or because a subset of dimensions, e.g. corresponding to the COM coordinates, are pre-specified to be the relevant ones. The paper proposes that which dimensions are relevant for the goal are task dependent, and so it should be up to the higher level policy to choose which dimensions are relevant for goal-setting. In this way, the higher level policy can make the goal for a skill more general or specific, allow a better trade-off between these factors. \n\nThe reviewers had mixed opinions initially, but additional results from the authors convinced some reviewers to update their scores, resulting in a unanimous accept recommendation. Summarizing their opinions:\n- Originality: the proposed approach is interesting & original. The authors also propose an original and interesting new suite of benchmarking tasks. \n- Technical quality: Initially, the opinions were mixed on quality, as some reviewers deemed important baselines to be missing. With the provision of additional HRL baseline, the reviewers were satisfied on quality. \n- Relevance and significance: The problem is relevant for NeurIPS. The results were not super surprising (e.g., the 'full goal space' baseline also worked pretty well across tasks), however, reviewers pointed out the paper might be an important step towards solving sparse reward task. \n- On clarity, the reviews were a bit mixed, from 'difficult to parse' to 'clear and well explained', with specific issues pointed out for possible improvement. \n\nOverall, the paper proposes an original new method and (taken into account the new results), sufficiently evaluates it in the context of relevant baselines. The paper could certainly be improved further, but I think as is it will be an interesting addition to the NeurIPS program. \n\nI had one additional minor comment: In the introduction, it is mentioned that \"In the large body of work ... on HRL ... relies explicitly or implicitly on prior knowledge that low-level skills should control the center of mass\" (lines 22-25). While I agree that this is a common assumption, I don't think it's true for the all current HRL methods as seems implied here (e.g. the option-critic lets the agent control any dimensions, feudal networks do use a subspace but the subspace is learned), and in particular, the reference given for this statement in line 25 are mostly methods where the state consists only of the COM, thus, it's inevitable that a HRL method would control the COM rather than a particular assumption. The HIRO paper, for example, would be a better example. \nHIRO: \"Data efficient reinfocement learning\", Nachum et al.\nOption-critic: \"The option-critic architecture\", Bacon et al. \nFeudal Networks: \"Feudal networks for reinforcement learning\", Vezhnevets et al. \n ",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "lyHk1Dz1QOZ",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "",
                "comment": "This work proposes a benchmark task with bipedal robots instead of locomotion tasks, where the bipedal robots are low dimensional and perform various movements. It also proposes a hierarchical reinforcement learning method with three levels: a policy that specifies a goal space (a set of features to operate on), a policy to specify the goal configuration given that goal space, and a low level policy to reach the desired goal configuration. It learns the low level policies through unsupervised learning, and then optimizes off policy the high level options by optimizing the value function.  Abstract: It is not clear throughout the abstract what the \"inductive bias\" is. It is not clear how the \"the potential to facilitate exploration\" is an inductive bias. since this is the primary distinction on which this work centers, this makes the abstract confusing.\n\nIntroduction:\n24 While it is true that much of the work in exploration with hierarchical reinforcement learning deals in navigation or navigation-like domains where the center of mass of the agent is particularly useful, there also exists a large body of work which deals with exploration related to object manipulation where this assumption is not the case. Thus, the generalization expressed here is misleading, since it highlights that \"the work that benefits from center of mass knowledge has an implicit bias towards center of mass knowledge.\"\n\n30 While this is true of policies, it may not be the case that low level skills are more widely applicable, since individual skills, low level or high level might be limited to a a very small set of behavior. It would probably be more appropriate to refer to this in the context of policies.\n\nRelated work:\n77 While re-usability across different high level actions is a significant benefit, this underscores the idea that one of the large benefits of Hierarchical reinforcement learning is for exploration. Even if a primitive is only used for one task, it might be that the learning procedure is able to exhibit gains in performance or sample efficiency even without generalizability. While it is likely that navigation tasks as a test domain does introduce implicit biases, it is not clear from the description in this section why this is the case.\n\nHierarchical skill learning: \nThe \"introduction\" section for this component lacks a heading, and it is not clear exactly what it is expressing. Is this part a description of the low level policies? A description of the state space or of general terms? At this point it remains unclear what exactly the high level policy comprises, and why there are task specific features and additional objects separated out to be accessed only by the high level policy. While these components gain some clarity later, the ordering makes this confusing.\n\n107 It is not clear up to this point if the argument is being made for more specific, or less specific policies. However, while such a low-level policy as reaching a goal configuration after short time is not navigation related, it is certainly very specific to robotic motion.\n\n3.1 While it is reasonable to train low level skills over different sets of features, perhaps the most important property related to these would be how they are chosen. It is not clear if these features are selected as random samples or specified. However, random sampling seems like a suspect way, as if the number of features exceeds even a small amount the number of possible sets explodes exponentially.\n\nTraining the low level policies only by unsupervised pre-training also seems like it could introduce issues. In particular, while the low level space might afford many bad ways of controlling the agent, there are probably only a limited number of useful ways to control the agent. There should be a tradeoff between using the higher level policies to specify what is learned at the lower levels, and simply exploring with the lower level policies, but at present the former appears to be completely ignored.\n\n3.2 Equation numbers would be much appreciated in this section, especially since there are clear changes being made to the typical bellman equation/value function, not the least of which is in the notation of taking the feature mean. \n166 While matching sign seems like it should have an effect on optimization, by negating the log of the features, this seems to change the meaning of the equation. \n\n170 It is not entirely clear how the equation in line 164 arrives at the one in 170. In particular, it seems that the loss is the negative expected value, but then the log|F| component has disappeared.\n\n173 A more in-depth description of how the \\alpha and \\beta loss terms are defined is necessary. At this point, it is simply provided as a given without clear explanation, especially since the intuitive meaning of H^f, H^g is not made clear.\n\nBenchmark Environments:\nWhile these environments are provided as one of the clear contributions of this work, they are described fleetingly. It would be useful to note why properties like center of mass or other normal navigation \"implicit biases\" do not apply to these cases. \n\nExperimental results:\n211 With only 5 features, these proposed domains do not actually differ too significantly from other mujoco locomotion tasks that are more commonly given. As highlighted before, the number of features also seems necessary for this method to work since the subset sampling would explode exponentially otherwise.\n\n227 It is still unclear what emphasis is being made about \"no single skill\" In particular, it should be expected in any skill learning framework that one skill does not dominate, otherwise there would be no point in using the framework at all.\n\nFigure 4: it is hard to parse the results, in particular those of SD*. Is this meant to outperform the proposed method?\n\n5.2 The proposed baselines, while interesting, do not always capture a fair comparison. In particular they are all single level skill learning methods except for DIAYN, but in this case DIAYN-C does not appear to be ideal for this case.\n\nFigure 6: It is not entirely clear why SAC would completely fail for the given tasks. It would be useful to see a comparison against SAC where it is able to learn at least some useful behavior, or find another baseline that does give valid results. There should exist algorithms which function on humanoid walker.\n\nOverall, this work proposes an interesting way of selecting features to control over and an effective 3 level hierarchy which has encouraging results. While the idea of selecting features has been proposed, it has not been shown in a multi-level hierarchy. However, the writing is sufficiently difficult to parse such that it is difficult to determine how exactly this method is novel from existing work, except that it encodes more specific information for the tasks. It is also difficult to determine from the writing exactly what features of the proposed method contribute to the success. Furthermore, the experiments are limited because they test on a new domain against baselines that do not seem like fair comparison on that domain. \n\nOriginality: marginal\n\nQuality: marginal\n\nClarity: poor\n\nSignificance: below marginal\n This work has a limited discussion of limitations, particularly how closely the method is tied to the task being performed. It has no discussion of societal impact, though robotics has a large variety of effects on modern society so a discussion could have occurred.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "37c7nyaQeAt",
                "writer": "official_reviewer",
                "reply_to": "0uHnse0kLue",
                "title": "Follow up on response",
                "comment": " Hello Authors, your responses provided valuable insight and some assurance on the clarity of the paper. The expanded list of baselines also addresses one of my primary concerns, and I'm willing to raise my score one point.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-vHeKD8czT2",
                "writer": "author",
                "reply_to": "PpHS5gLIwlY",
                "title": "Did our response address your concerns?",
                "comment": " Hello Reviewer gM3y, we would be thankful if you can confirm that we addressed your concerns in our response, and let us know if any issues remain. To summarize, in our response we:\n- pointed to the video in the supplementary material, which contains comparisons of behaviors of HSD-3 and several baselines\n- clarified the number of seeds (which is present in the main paper but was missing for Table 1).\n- will cite Heess et al., 2016 and Hasenclever et al., 2020 in the related work section.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "0uHnse0kLue",
                "writer": "author",
                "reply_to": "cX3Jz9UXZJY",
                "title": "Did our response address your concerns?",
                "comment": " Hello Reviewer ECxq, we were hoping that you can confirm whether we addressed your concerns in our response, and let us know if any issues remain. In summary, our response:\n- clarifies the notion of inductive bias we use in the paper, and the derivation of the value and policy gradients based on SAC\n- explains why we consider the comparison to existing baselines to be fair\n- clarifies our contributions in terms of novelty and our proposed benchmark suite\n\nAs discussed in our [response to Reviewer opgb](https://openreview.net/forum?id=NbaEmFm2mUW&noteId=UmJN7xl4T4R), we will also expand the list of baselines considered with end-to-end HRL algorithm (HIDIO) for the camera-ready version and add an experiment concerning exploration behavior.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dXoCjd_9KIj",
                "writer": "author",
                "reply_to": "jSswNNRiJ_Z",
                "title": "Additional HIDIO results",
                "comment": " We thank the reviewer again for this discussion. Sweeping over the hyper-parameters provided in the HIDIO paper (discrimator input: {state, state-action, action, state-difference}, latent vector dimension {8, 12}, rollout length {25, 50, 100}, replay buffer length per parallel actor {50000, 200000}, 3 different runs (seeds)) on the Hurdles task resulted in a few combinations which achieved small positive returns during evaluations. In the table below, we list the maximum evaluation return of those combinations over the course of training (8 million steps). For comparison, both SD and HSD-3 achieve returns above 12 consistently. \n\n| discr input      |   latent dim |   rollout length |   replay buffer length |   run |   max return |\n|:-----------------|-------------:|-----------------:|-----------------------:|------:|-------------:|\n| state_difference |           12 |              100 |                 200000 |     0 |         0.18 |\n| state_difference |           12 |              100 |                 200000 |     1 |         0.02 |\n| state_difference |           12 |              100 |                 200000 |     2 |         0.02 |\n| state_difference |           12 |               50 |                 200000 |     1 |         0.68 |\n| state_difference |           12 |               50 |                 200000 |     2 |         0.34 |\n| state_difference |            8 |               50 |                  50000 |     1 |         0.38 |",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_WUliaG3gV",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "",
                "comment": "The authors propose a 3-level hierarchical method, one that operates on a feature space, another that operates on a feature-conditioned goal space, and finally a low level policy that outputs actions in the environment conditioned on everything above and the current state. They pretrain the policies in a pretraining environment before applying it to the task at hand. The authors also introduce a new suite of environments to test hierarchical RL. The issues brought up by the authors about current HRL methods are enlightening and the method is a novel contribution. Results are fine, with subtle/no improvements over the baselines on some tasks, and substantive improvements on others. I believe the paper has some issues with baseline comparisons and tasks, which I hope will be addressed in the rebuttal. As such, I am currently learning towards not accepting the paper.  ## Paper Strengths\n**Paper Framing/Originality**\nPointing out the issues with existing HRL algorithms\u2019 focus on learning skills that are relevant mainly for controlling center of mass is an interesting contribution. Furthermore, the algorithm that results is novel, while also incorporating ideas from modern papers such as the DYNE step-conditioned critic. The inclusion (and supposed release) of the benchmark environments are a benefit to the community as some of them seem like solid tasks to test hierarchical algorithms on.\n\n**Results Significance**\nI think the paper does a good job of demonstrating the importance of goal-space separation with results that demonstrate significant improvements over baselines on some of the tasks. Furthermore, it\u2019s rare to see evaluations over 3 seeds, while the authors evaluate on 9 seeds in total. The inductive bias experiment on HIRO in the appendix (sec G) was very enlightening.\nThe experiment shown in Figure 3 is also very comprehensive and informative.\n\n**Clarity**\nOverall, the writing is generally clear and mathematical derivations are pretty well explained. \n\n## Paper Weaknesses\n\n**Sparse-reward tasks**\nI think more analysis on similar environments with different robots should be given. Tmake general claims about this method, there should be examples of many robots, not just two. Furthermore, the demonstrating an experiment like Figure 3 for another robot (perhaps the Humanoid that\u2019s already included) would be helpful.\n\n**Experiments**\nThere should be comparisons with some works which learn skills without explicit assumptions of skill/task types and do not require pre-training skills, e.g. HIDIO (Hierarchical Reinforcement Learning By Discovering Intrinsic Options, Zhang et al. 2021), HiPPO (Sub-policy Adaptation for Hierarchical Reinforcement Learning, Li et al. 2020). These comparisons would further demonstrate the advantages of having the initial pretraining environments (which the comparison to SAC-HIRO already contributes to) and explicit goal space learning when compared with more modern HRL methods, both of which have demonstrated improvements over HIRO in their experiments. In a sense, HIRO is the only true SOTA HRL baseline in this paper so this would need to be addressed.\n\nThere\u2019s little explicit analysis that studies how exploration is explicitly affected by these skills, despite it being stressed in the introduction. This can be partially remedied by adding more analysis in Section 5.1 regarding exploration specific to each environment (I think Sec 5.1 is also just generally lacking more analysis). \n\n\n**Clarity**\nI think that an example of the feature set F should be given earlier in the paper, it\u2019s confusing to learn bits and pieces about feature sets F throughout section 3 before getting an example (perhaps at L108). This also makes the distance-based reward (L135) clearer when introduced.\n\nYou should bold or highlight the best performances for each column in Table 1. That would improve clarity here greatly.\n\n\n**Minor Issues**\nSome grammatical hiccups throughout the 3rd paragraph of the introduction, making it a little harder to read.\n\n\nFigure 4: \u201caveraged over 0.5M environment steps\u201d this should be 5M steps\n\nL271: \u201cfor SG\u201d -> \u201cfor SD\u201d? Or for subgoals?\n\nAppendix L597, equation:  F -> |F|\n\n## Questions for the authors\n\nHow do you anticipate extending this to visual environments? \n\nWhy are you normalizing the entropy of $\\pi^g$ by $|F|$? Isn\u2019t the size of $F$ fixed?\n\nIt seems that many of the fixed skill experiments achieve performance nearly on par with full HSD-3. Why is this the case when it seems in Figure 5 many skills are needed?\n\n\n--UPDATE--: raised score from 5 to 6 after response\n Yes.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "jSswNNRiJ_Z",
                "writer": "official_reviewer",
                "reply_to": "UmJN7xl4T4R",
                "title": "Thanks for the results",
                "comment": " Thanks for your response. Please do update this response whenever you get more results, but I am now satisfied that you have addressed my main complaint and will be raising my score in response, assuming these results will be finalized and added into the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cX3Jz9UXZJY",
                "writer": "author",
                "reply_to": "oXMz_sONu6H",
                "title": "Follow-up to Response to Reviewer ECxq",
                "comment": " We'd like to follow up on our response and verify whether we clarified the questions of Reviewer ECxq. We are happy to participate in further discussion if any question persists.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UmJN7xl4T4R",
                "writer": "author",
                "reply_to": "9vmtQqxXN0F",
                "title": "Results for HIDIO and Exploration Behavior",
                "comment": " We thank the reviewer for following up on the additional experiments.\n\n### HIDIO Baseline\n\nSo far, we have run HIDIO on 4 out of 7 tasks (we left out GoalWall and Stairs for now since these are more difficult, and HurdlesLimbo). The results for different discriminator features are as follows (mean values over 3 seeds, after 5M environment steps, comparable to the numbers in Table 1 of our paper):\n\n```\nDiscriminator feature  | Hurdles | Limbo | Stairs | PoleBalance\n================================================================\nState                  | -0.12   | -0.09 | -0.02  | 81.69  \nAction                 | -0.98   | -0.90 | -0.99  | 128.25\nStateAction            | -0.09   | -0.01 | -0.04  | 112.01\n```\n\nNegative returns are obtained if the Walker falls over in the course of an episode. We've used the hyper-parameters for the Pusher/Reacher experiments in the HIDIO paper, and are currently sweeping the parameter ranges given in D.1.3 on the Hurdles task. We will follow up with the results from the sweep within the next two days.\n\n### Analyzing Exploration\n\nWe ran an experiment with both (non-hierarchical) SAC and HSD-3 on the GoalWall environmet (one of the most challenging tasks of the benchmark suite), and counted the number of unique states using the SimHash method from http://arxiv.org/abs/1611.04717. After 5M steps, HSD-3 visited roughly twice as many unique states compared to SAC (400k vs. 200k, https://imgur.com/a/qqHVW6P). We will generate these curves for the remaining methods and tasks for the updated version of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9vmtQqxXN0F",
                "writer": "official_reviewer",
                "reply_to": "X5cBjZt1HE2",
                "title": "Results table for other HRL",
                "comment": " Thanks for the response.\n\n> We thank the Reviewer for pointing out further opportunities to clarify our contributions. We are currently performing further training runs with HIDIO and will follow up with a results table shortly.\n\n\n> Furthermore, we will attempt to visualize the state space covered over time by counting ``unique'' states via pseudo-counts (using a hash function) for all the algorithms in Table 5 (and HIDIO, if possible).\n\nDo you have a follow-up with these results? I would appreciate if the authors are able to add these results table as we are now in the last week of reviewer discussion and this would be taken into consideration in my evaluation of the paper.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UcT1T4U4Hvx",
                "writer": "author",
                "reply_to": "uXbayW9K0D",
                "title": "Response to further questions of Reviewer tdmg",
                "comment": " These are all good questions, and we are happy to answer them. \n\n1. For the first point raised, we refer to our answer to Reviewer opgb below ([link](https://openreview.net/forum?id=NbaEmFm2mUW&noteId=X5cBjZt1HE2)).\n\n2. We thank you for pointing out the ALLSTEPS reference, which does indeed eschew motion capture data, but in exchange for a dense, carefully designed reward function and task curricula. We will include it in our related work section.\n\n3. A forward reward alone is not sufficient for either GoalWall or PoleBalance, and HSD-3 achieves good results in both of them (with the Walker robot). The focus on sparse-reward environments is motivated by the fact that as tasks grow in complexity, coming up with suitably shaped rewards requires an increasing amount of effort. Naturally, deriving good rewards from e.g. demonstrations is another way to tackle complex tasks, but this direction is orthogonal to our work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "X5cBjZt1HE2",
                "writer": "author",
                "reply_to": "-YGT4kQGiiN",
                "title": "Response to further question of Reviewer opgb",
                "comment": " We thank the Reviewer for pointing out further opportunities to clarify our contributions. We are currently performing further training runs with HIDIO and will follow up with a results table shortly.\n\nRegarding analysis of exploration behavior, we would first like to stress that our sparse-reward tasks require efficient exploration for success in the first place. The results in Figure 3 can be directly related to the learning curves in Table 5 (Appendix). We admit that full learning curves would provide further insight into how different goal spaces affect learning in each environment, and we will add such curves for the results in Figure 3 in the Appendix. Furthermore, we will attempt to visualize the state space covered over time by counting ``unique'' states via pseudo-counts (using a hash function) for all the algorithms in Table 5 (and HIDIO, if possible).",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uXbayW9K0D",
                "writer": "official_reviewer",
                "reply_to": "YyaCJgN6Gwq",
                "title": "further questions",
                "comment": " Thanks for your response. Some questions remain:\n\n1. I think reviewer opgb raises a good point about analysis of how analysis is affected. It will be good if the authors can comment on this point. And I may adjust my score based on the response to that question.\n\n2. In the response to reviewer ECxq, the authors claim \"Our work demonstrates skill learning and usage for a Humanoid robot without supervision from motion capture data, which has not been shown previously outside of navigation tasks.\" That is not true. Please refer to literature in computer animation. For example, ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills, by Xie et al can do more challenging locomotion tasks presented here without motion capture.\n\n3. While I understand the scope of the paper is to solve sparse reward tasks, but my question remains, most of the tasks presented can be solved via very simple forward progress reward (this is not a complicated inductive bias). And the only task that I couldn't figure out a simple dense reward alternative is not solvable by the proposed approach. I think this limitation should be addressed, e.g, in the appendix.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-YGT4kQGiiN",
                "writer": "official_reviewer",
                "reply_to": "PxmK2het2rL",
                "title": "Response to authors (1)",
                "comment": " Thanks for clarifying my questions and addressing some of my concerns. Regarding some of your comments, I have a few things to point out.\n\n> Regarding HiPPO, the experiments in the original paper (https://openreview.net/forum?id=ByeWogStDS) are performed in environments where PPO works well already, while non-HRL methods seem to fail almost completely in our tasks. We hence did not consider it as a baseline.\n\nI don't think that its experiments being performed in environments where PPO already works well implies that it'll fail on your tasks. In HIDIO's experiments, in which I believe they run an unmodified version of HiPPO, HiPPO performs well on some tasks where flat policies fail. In fact, some of the tasks in HiPPO are semantically similar to the ones evaluated in your environment (there's a \"Block Hopper\" and \"Block Half-Cheetah\" in HiPPO). However, I think it's OK to just include one modern hierarchical baseline, which HIDIO satisfies. Specifically regarding HIDIO:\n\n> In quick preliminary experiments on Hurdles, Stairs, and GoalWall, using the actions as input to the discriminator, HIDIO did not make meaningful progress in 5M steps apart from learning to not fall over. We will do additional runs using the parameter ranges and variants listed in the paper and add it as a baseline for the camera ready.\n\nCan you include a preliminary table of results in a future response to this comment? I understand it's hard to setup your environment tasks and perform a full hyperparameter search for any method with an unfamiliar codebase in a short amount of time, so feel free to just put preliminary results, but not having the extra baseline is my primary concern and I would like to see it fully addressed. \n\n\nFurthermore, the authors did not address this concern of mine:\n> There\u2019s little explicit analysis that studies how exploration is explicitly affected by these skills, despite it being stressed in the introduction. This can be partially remedied by adding more analysis in Section 5.1 regarding exploration specific to each environment (I think Sec 5.1 is also just generally lacking more analysis).\n\nCould you address this in a response? I think Section 5.1 should be expanded upon, or given limited space, an additional pointer to the appendix added and extra analysis inserted there. It's very interesting as is.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PxmK2het2rL",
                "writer": "author",
                "reply_to": "_WUliaG3gV",
                "title": "Response to Reviewer opgb",
                "comment": " We thank the reviewer for their valuable feedback and insightful comments. We will address the potential weaknesses that were pointed out individually:\n\n**Sparse-reward tasks:** In general, we view our benchmark task as a first installment and to be potentially extended with further tasks and robots in the future as we agree that methods should be benchmarked in as many scenarios as possible. For this work, we limited our focus on bipedal robots of different complexity. The experiment on individual goal spaces (Figure 3) has also been performed for the Humanoid, although we limited the number of candidate goal spaces to those including translation in X direction. The results are provided in Figure 8 in the Supplementary and paint a similar picture (no single best goal space across tasks).\n\n**Experiments:** The extended results in the Supplementary (Table 5) include an additional end-to-end baseline (Switching Ensemble, from http://arxiv.org/abs/1909.10618). While it doesn't learn a high-level policy, it has been shown to be on par with learning a set of discrete low-level options in http://arxiv.org/abs/1909.10618. Regarding HiPPO, the experiments in the original paper (https://openreview.net/forum?id=ByeWogStDS) are performed in environments where PPO works well already, while non-HRL methods seem to fail almost completely in our tasks. We hence did not consider it as a baseline. HIDIO was published only recently (ICLR 2021). In quick preliminary experiments on Hurdles, Stairs, and GoalWall, using the actions as input to the discriminator, HIDIO did not make meaningful progress in 5M steps apart from learning to not fall over. We will do additional runs using the parameter ranges and variants listed in the paper, and add it as a baseline for the camera ready.\n\n**Clarity:** We provide an example of our skills in the introduction, but will also pick it up again in Section 3 for clarity. We will also address the minor issues that the reviewer helpfully pointed out.\n\n**Questions:**\n- *Visual environments:* Image observations don't directly allow for definitions of goal spaces as in our work. We think that investigating whether these goal spaces can be learned would be worthwhile, as a means to both remove the remaining prior of manual goal space in our method, and to tackle more complex state spaces such as images. Another possibility would be unsupervised keypoint extraction, e.g., as in \"Unsupervised Learning of Object Keypoints for Perception and Control\" (http://arxiv.org/abs/1906.11883), and to construct goal spaces around those keypoints. For the particular environments in our paper, it would also be possible to restrict pre-training tasks to proprioceptive inputs and supply the high-level policy with image observations of downstream tasks, such as in \"Hierarchical Visuomotor Control of Humanoids\" (https://openreview.net/forum?id=BJfYvo09Y7).\n- *Normalization of entropy:* The size of \\mathcal{F} and the total number of goal space features is fixed, but the subsets F \\in \\mathcal{F} differ in size. For example, for F={xpos}, there is only one continuous action for \\pi^g, while for F={xpos,zpos,left_foot} there are 4 actions (left_foot corresponds to two actual features). We normalize the entropy by |F| to not bias the Q-function towards large feature subsets, which would be the case for the standard SAC formulation.\n- *Many skills achieve good results:* All our individual skills are goal-based policies in a continuous goal space and can be used to express a relatively large range of different motions on their own. For example, it's possible to find goal sequences to move forward in X direction by just controlling a single foot (Fig. 3). This is because the skill policy needs to balance the agent to not fall over, and other features, like the torso position, are not constrained to specific values in this case. Finding these goal sequences can however be challenging, which is demonstrated by the mediocre performance of HSD-Bandit. HDS-3 is free to switch between different skills (i.e., goal spaces), and in Figure 5 we demonstrate that semantically meaningful switching sequences can arise. Here, we don't place any constraints on finding a small set of best goal spaces.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "YyaCJgN6Gwq",
                "writer": "author",
                "reply_to": "tIkES810p5r",
                "title": "Response to Reviewer tdmg",
                "comment": " We thank the reviewer for their valuable feedback and comments. We will address the smaller issues pointed out in the write-up and provide detailed answers to the listed cons and questions below:\n\n**Comutational requirement for pre-training:** For the Walker, we use 10M steps for pre-training a shared policy for all goal spaces considered. With uniform sampling over goal spaces, this leads to an expectation of about 322k environment steps per goal space, or skill. Learning a single skill (e.g., moving towards an X-position) in an isolated manner with SAC required significantly more samples in preliminary experiments, and this only increases for more complex skills. Considering the fact that we learn skill policies for 31 goal spaces simultaneously, we think that the resource requirements for pre-training are reasonable.\nFor clarity, we will also attach per-skill learning curves with the average rewards reached (in training) to the Appendix.\n\n**Task design:** We agree that designing these tasks is non-trivial, and we consider the investigation into further tasks (especially for Humanoid robots) as a worthwhile future endeavor. We believe that the current suite represents a good first step with 7 varied environments.\n\n**Performance on Gaps:** This task is indeed very challenging since a positive reward is only observed if the robot reaches the next platform. If it touches the Gap (slightly lower than the floor), it receives the same -1 reward as for falling over.\n\n**Performance gain over non-HRL with forward reward:** So far, we found that in dense-reward settings (Hurdles, Limbo, HurdleLimbo, Gaps, Stairs with a reward corresponding to the translation in X-direction per step), the final performance of our method and other HRL approaches we investigated is lower than for plain SAC. We believe this is mainly a limitation of using fixed low-level skills. In the future, it would be interesting to study how to adapt the low-level skills on downstream tasks, or how to use the HRL policy purely for exploration to get good initial traces; we believe this to be out scope for this submission though, as the focus is on improved exploration in sparse-reward settings.\n\n**SAC on Stairs:** Only one out of 9 seeds managed to make meaningful learning progress on the Stairs task, which is not enough to significantly move the average reported in Figure 4. It can be noticed in Table 1, and we will add a corresponding remark in the discussion of results in 5.2 for clarity.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "PpHS5gLIwlY",
                "writer": "author",
                "reply_to": "tSEJJQ_Rrh",
                "title": "Response to Reviewer gM3y",
                "comment": " We thank the reviewer for their thoughtful feedback and comments. We kindly refer the reviewer to the detailed video presentation in the Supplementary ZIP archive (file hsd3.mp4) and will add a corresponding reference in the main body. We show rollouts of the pre-training stage and downstream tasks for both robots, and compare the behavior of HSD-3 to several baselines.\n\nWalker experiments were run with 9 seeds each (for methods with pre-trained skills we use 3 pre-training and 3 high-level training seeds). A lower bound of the number of episodes can be derived from the time limit: for the Walker, 5M steps correspond to at least 5000 episodes (time limit 1000) for all tasks except GoalWall (at least 20000 episodes, time limit 250). All environments implement early termination if the robot falls over, so in practice the total number of episodes will be higher.\n\nWe will add a motivation for the selection of our goal space features, and will also reference Heess et al., 2016 (skill learning with a navigation prior) and Hasenclever et al., 2020 (highlighting importance of in-domain motion capture data (as a prior), and proposing joint skill learning and downstream task training as a remedy). We chose the s^+ notation to highlight the fact that it represents additional information introduced by downstream tasks.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oXMz_sONu6H",
                "writer": "author",
                "reply_to": "lyHk1Dz1QOZ",
                "title": "Response to Reviewer ECxq",
                "comment": " We thank the reviewer for the extensive commentary and suggestions. We group the issues that were raised and respond to them individually.\n\n**Inductive bias:** What we call inductive bias is the prior that is used for exploration on downstream tasks, and hence represents a priori knowledge about what comprises useful behavior in a given environment. Settling for more generality will make pre-trained skills applicable to a larger class of environments, but exploration will be more challenging. Hence, a trade-off arises. For example, controlling the center of mass is effective in navigation environments but not helpful for kicking a soccer ball. This would intuitively require control of the robot's extremities, which is again not useful for exploration in navigation tasks. We will try to present this in a clearer manner in the introduction.\n\n**Novelty:** We clearly work out the role of priors in pre-trained skills, and propose a novel three-level architecture to effectively tackle the trade-off that arises when introducing these priors. Our work demonstrates skill learning and usage for a Humanoid robot without supervision from motion capture data, which has not been shown previously outside of navigation tasks.\n\n**Baselines:** The failure of SAC is explained by the sparse-reward nature of our tasks that require agents with effective exploration capabilities. Occasional SAC runs do make progress on the Stairs task, and after a longer training time on Hurdles (Table 5 & 6). In Table 5, we also give results for the Switching Ensemble (from http://arxiv.org/abs/1909.10618), which improves exploration for SAC and finds effective solutions occasionally. We believe that this answers the reviewer's point on providing a simple baseline that works at least sporadically. DIAYN-C embeds a fixed number of discrete skills in a continuous space, and can hence interpolate between them (http://arxiv.org/abs/1807.10299); we regard it as a superior formulation of DIAYN. In contrast to the baselines, our method is novel in its usage of multiple skills, with each one implemented as a goal-based policy. We are not aware of any similar multi-skill algorithm that would be applicable to our scenario. SD* is considered a topline because it requires exhaustive evaluation on a downstream task -- it is the best goal space, selected a posteriori.\n\n**Benchmark Tasks:** While locomotion is an integral part of the majority of tasks, it is not sufficient to perform well across all of them. This is demonstrated in Figure 3: controlling the center of mass roughly corresponds to controlling X,Y and Z features which does indeed work well on 4 out of 7 tasks, but works poorly on the other 3. The Gaps task, for example, clearly requires control of at least one foot. Further, we can't completely follow the connection drawn by the reviewer between the number of goal space features and the similarity to existing MuJoCo locomotion tasks. The tasks are defined irrespective of the goal space features, and the features have been selected to enable a variety of behaviors for Walker robots (position of torso and the two lower appendages).\n\n**Unsupervised pre-training; guiding skill learning with high-level policies:** In this work, we focused on the scenario of first acquiring pre-trained skill policies, and then utilizing them in unseen downstream tasks. In preliminary experiments, running HSD-3 with uninitialized skills (i.e., from scratch) did not work well; perhaps this could be mitigated with additional inductive biases on which skills should be learned at what point in time, or with a CoMic-like setup (http://proceedings.mlr.press/v119/hasenclever20a.html).\n\n**Section 3.2:** The equations are taken from the SAC paper (https://arxiv.org/abs/1812.05905v2) and adapted to our setting. In the interest of brevity, we do not include the full derivation of the SAC losses but refer to the original paper instead. We will motivate the presence of two temperature loss terms in the camera-ready version; the main idea is that the two high-level policies are sufficiently different and benefit from independent entropy regularization.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "tIkES810p5r",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "",
                "comment": "This paper presents a hierarchical framework for training locomotion policies for tasks with sparse reward. Low level policies are trained to achieve a selection of subgoals and high level policies are trained by exploring in the goal spaces. Challenging locomotion tasks are introduced to show the effectiveness of the proposed methods.  Originality: This paper presents a three level hierarchical policy with a low level policy generated via training on general tasks. This is new to me.\n\nQuality: The paper is technically sound to me.\n\nClarity: Paper is well written.\n\nSignificance: The result is not very impressive since most tasks can be solved via a simple forward progress reward. But it presents an important step towards solving tasks with sparse reward using a hierarchical policy.\n\nPro:\n\n1. The training of low level policies based on goal spaces specified by position of torso and foot.\n\n2. A three level hierarchical policy and an extension of SAC to accommodate the resulting action distribution.\n\n3. Challenging sparse reward tasks for locomotion.\n\nCon:\n\n1.  The computation required for training even the low level skills are significant. It will be nice to provide more details on the low level training. The low level skills do not seem challenging to me (at least for the walker) so it seems 3 days on 2 GPUs is a lot.\n\n2. Tasks such that designing a simple dense reward (such as the forward progress reward) is none trivial will really demonstrate the strength of hierarchical framework. e.g., the GoalWall task presented. More in Question 2 below.\n\nQuestion:\n\n1. It is not clear to me why Gap will be more challenging than other tasks. Maybe the reward design needs to be modified? e.g., a negative reward for touching the gap can cause the robot want to terminate as soon as possible. Some failure mode of the gap task will help illustrate the issues.\n\n2. Most of the tasks can also be solved via a simple forward progress reward (with the exception of GoalWall, which is difficult to solve in the current framework anyway). It will be interesting to see what is the performance gain the proposed approach has over baseline methods under this setup.\n\n3. In Figure 6, SAC makes no progress on Stairs, but that is not the case in Table 5 in the Appendix. The video also shows SAC makes some progress on Stairs. Yes, limitation is discussed.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "tSEJJQ_Rrh",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_NbaEmFm2mUW",
                "title": "",
                "comment": "This paper argues that, in the context of pre-trained low level skills, there is a trade-off between generality and learning speed. It makes two contributions: firstly, it proposes a benchmarking suite of sparse reward tasks that require different motor skills to study this trade off. Secondly, the paper proposes a hierarchical skill learning algorithm that attempts to trade off generality and learning speed directly. The proposed algorithm outperforms reasonable baselines.  This paper argues that, in the context of pre-trained low level skills, there is a trade-off between generality and learning speed. It makes two contributions: firstly, it proposes a benchmarking suite of sparse reward tasks that require different motor skills to study this trade off. Secondly, the paper proposes a hierarchical skill learning algorithm that attempts to trade off generality and learning speed directly. The proposed algorithm outperforms reasonable baselines.\n\nThe proposed benchmark environments are a number of obstacle courses (e.g. stairs, gaps, hurdles) as well as a pole balancing task. The tasks are set up for the 2d walker as well as a 3d humanoid.\n\nThe proposed algorithm pre-trains skills first defining a set of features. Then the agent is trained to control subsets of the full feature set. The subsets and target features are randomly sampled during pre-training. Selecting the feature set induces and the target features induce an inductive bias. I would have like to see more discussion of how the features and target feature distributions are selected. As an aside, I think for more complex embodiments, I think using demonstrations or mocap data to define the targets would be an interesting approach. To reuse the learned skills, the skills are frozen and a hierarchical policy is learned that first selects the subset of features to control and then specifies a target. \n\nI think the related works section is reasonably comprehensive but should cite\n\n    Heess et al.  \u201cLearning and Transfer of Modulated Locomotor Controllers.\u201d arXiv Preprint arXiv:1610. 05182.\n\nas an early example of low level controllers in deep RL. Another relevant paper is\n\n    Hasenclever et al. \u201cCoMic: Complementary Task Learning & Mimicry for Reusable Skills.\u201d ICML 2020. http://proceedings.mlr.press/v119/hasenclever20a.html. \n\nthat studies (among other things) the same trade-off between generality and learning speed in the context of skills acquired from mocap data.\n\nIn a first experiment, the performance with different fixed goal spaces is compared. The results indicate that no single goal space works best on all tasks: there is indeed a trade-off between speed and generality. However it's worth noting that the full goal space works pretty well across tasks. In the other experiments, the proposed methods performs favourably relative to baselines. \n\nOverall, I think this is a nice paper and I am leaning towards acceptance.\n\nMinor comments:\n- page 4, section 3.1 last paragraph: I'm a little confused by this. If you don't reset why call the experience segments episode? Isn't it just one long episode at that point?\n- Figure 1: I found the $s^+$ notation confusing. Why not $s^t$\n- I would quite like to see some videos of the resulting skills and policies. This is something that other papers in this space provide\n- Table 1: How many seeds and episodes do your experiments correspond to?\n- Figure 6: typo \"are now show\"\n The experimental results with the humanoid are limited in terms of the goal spaces and the full goal space baseline appears to work just as well if not better. This suggests that substantially more work is needed to scale up to more complex embodiments. This is acknowledged by the authors.",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the proposed approach",
                "Sentiment Expression": "interesting & original",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "Technical quality",
                "Sentiment Expression": "mixed",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "additional HRL baseline",
                "Sentiment Expression": "the reviewers were satisfied",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The problem",
                "Sentiment Expression": "relevant for NeurIPS",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "The results",
                "Sentiment Expression": "not super surprising",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "clarity",
                "Sentiment Expression": "a bit mixed",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the paper",
                "Sentiment Expression": "sufficiently evaluates it",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "could certainly be improved further",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "YYHXJOawkPb": {
        "paper_id": "iclr_2022_YYHXJOawkPb",
        "paper_title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning",
        "paper_abstract": "Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit \u201ceffective robustness\u201d and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit  effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models.",
        "paper_acceptance": "Reject",
        "meta_review": "Thank you for your submission to NeurIPS.  The reviewers are quite split on this paper, but some remain substantially negative even after discussion.  I'm a bit more optimistic about the paper: the observed increase then decrease in ER during fine-tuning _does_ strike me as a fundamentally interesting phenomenon, and I believe that papers that present such phenomena can be valuable contributions even without more fundamental \"explanations\" of the observations.  My recommendation, therefore, ultimately rests largely on the fact that I think (as is honestly evidenced by the reviews to a large degree), the presentation and contextualization of these results can be substantially improved in a future revision of the paper.  Specifically, the fact that several reviewers found the results obvious and/or not sufficiently substantiated suggests that the basic premises here are still failing to land.  I would strongly suggest revisions that clarified these points in a resubmission.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "AJfZMLnWW0t",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "",
                "comment": "In this paper, the authors conduct a thorough empirical investigation of effective robustness during fine-tuning and have several observations: 1. models pre-trained on larger datasets in the middle of fine-tuning, as well as zero-shot pre-trained models, exhibit high amounts of effective robustness, but the effective robustness vanishes at convergence; 2. the effective robustness increases with the larger size, more diversity, and higher example difficulty of the dataset; 3. models that have effective robustness make different predictions than standard models and are able to correctly classify examples that no standard models get right. Besides, they discuss several potential solutions to mitigate the problem of vanishing of effective robustness during fine-tuning, but find that none of them are able to maintain high effective robustness at high in-distribution accuracy.  I think this paper has the following strengths: \n\n1. I think identifying models that have effective robustness and understanding their properties is an important and interesting problem. This paper has some empirical observations under this direction. \n\n2. Enough details are included for the experiments. \n\n3. Overall, the paper is well-written and the related work is properly discussed. \n\nHowever, I think this paper has the following weaknesses: \n\n1. My major concern is that the contribution is not very significant. The authors have some empirical observations, but those observations are not very useful and don't help us understand the problem better. For the models in the middle of fine-tuning, although they exhibit a high amount of effective robustness, the accuracy of those models on the in-distribution dataset is not high and thus such kinds of models may not be useful. Also, when the fine-tuning converges, the models have high accuracy on the in-distribution dataset but don't have effective robustness. Thus, the models obtained via fine-tuning don't have clear advantages over previous models. Besides, although the authors discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy, none of those methods work. So such a discussion may not be useful.\n\n2. They only have some empirical observations, but don't have analysis for them. For example, they only show that the effective robustness generally increases throughout fine-tuning, peaks, and then gradually disappears towards the end of fine-tuning empirically, but don't analyze or explain why such a phenomenon exists. It is unclear whether such a phenomenon is general or it just exists on some datasets. It seems Figure 3 in the paper shows that such a phenomenon doesn't exist on ImageNet-R and ObjectNet when using ImageNet as in-distribution. \n\n3. Some claims are not well supported by results. For example, the authors claim that the pre-trained models in the middle of fine-tuning, as well as zero-shot pre-trained models, represent an entire class of models that exhibit high amounts of effective robustness. I think this claim may not be true. There might be other training methods that could lead to better effective robustness and also high accuracy. The authors only explore the methods of fine-tuning and zero-shot evaluation. Thus, it is hard to claim that they represent an entire class of models that exhibit high amounts of effective robustness. The claim that the models with effective robustness make different predictions than standard models and are able to correctly classify examples that no standard models get right, is also not well supported by the results. They only select 4.8% of images that none of the testbed models get correct and show that the model that has effective robustness with the best in-distribution performance gets 10% of these examples correct. I think such results cannot support the claim that the models with effective robustness are able to correctly classify examples that no standard models get right since only 10% of these examples are predicted correctly by the model that has effective robustness with the best in-distribution performance. Also, it seems these results could not demonstrate that the models with effective robustness have prediction diversity. \n\n4. Some observations may already be known to the community. For example, the observation that the effective robustness increases with the larger size and more diversity of the dataset seems obvious. \n\n I think this paper doesn't make enough contributions and the claims are not well supported by results. Also, they don't provide analysis for the observations and the observations may not be helpful in understanding the problem. Thus, I think this paper is not ready for publication. \n\n***[Post Rebuttal]***\n\nAfter reading the rebuttal, I think my major concerns still remain: the contributions are not very significant and the findings may not be useful. I still think that the models studied in this paper are not enough to represent all models that exhibit ER. The authors need to explore other kinds of models that have ER (and also have high accuracy). Thus, I keep my original rating and think the paper is not ready for publication. ",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "kqHAA0FV4bI",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "",
                "comment": "This paper highlights important variables impacting the effective robustness (ER) of a pre-trained, fine-tuned model. \nThe authors identify that increasing model size, dataset size, and example difficulty improves the ER of a pre-trained, fine-tuned model.\nThe experiments suggest that the zero-shot component of CLIP plays a significant role in the high value of ER CLIP achieves.\nThe investigation of ER on dominance probability shows that models with high ER have high dominance probability.\nThe authors also present a negative result showing that several reasonable approaches to maintaining high ER while fine-tuning fail. Strengths:\nThe paper is very clearly written and has a thorough experimental section validating the authors' claims.\nThe authors have a thorough selection of experiments that validate their claims.\n\nWeaknesses:\nOne weakness of this paper is that the authors do not properly define fine-tuning. While its meaning is implicit, fine-tuning is a key concept in this paper, so having a clear definition of the term seems necessary. This is especially true when considering multiple fine-tuning steps such as when fine-tuning a fine-tuned model BiT-M-21k on CIFAR-10.\n\nThe authors use a pre-trained or randomly initialized model on a large dataset, fine-tune on a smaller dataset and measure OOD accuracy on an analogous dataset to the fine-tuned dataset. It would help if the authors give some examples of when such a training procedure would be useful. Usually fine-tuning is carried out on the distribution that the model is going to be evaluated on.\n\nAn analysis of the relation between the fine-tuning dataset and the OOD test set would be useful. Right now the relationship is alluded to based on natural distribution shifts, but it's not clear how this might generalize to other types of distribution shifts. Overall this paper is very thorough. The authors set out to investigate the role fine-tuning has on OOD robustness and they successfully identify several key variables to consider. There are many experiments in the main paper as well as in the appendix that validate their claim. This work will be very valuable to the community as it provides some insight into what variables lead to OOD robustness for pre-trained, fine-tuned models.",
                "rating": 8,
                "confidence": 3
            },
            {
                "review_id": "ifpIjM6pki3",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "",
                "comment": "In the manuscript entitled, \"The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning\", the authors present an empirical investigation of model exhibiting a property known as 'effective robustness'.  In particular, their focus is on how 'effective robustness' changes during fine tuning and on the characteristics of these models. Apologies to the authors for what may sound like a rather glib judgement of this submission (and for which I acknowledge through the confidence scores below that my opinion is not absolute as I do not work directly in the space of image classification), but the results and conclusions of this paper seem remarkably obvious.  Namely, that models that have been pre-trained on a large collection of different datasets tend to lose their strengths at predicting out of distribution as they are progressively fine-tuned towards predicting a specific type of data.  And that when these models are performing in the 'effective robustness' mode the types of in sample problems they find easy (alt. hard) are different to those that models trained on the dataset at hand find easy (alt. hard).  The perils of over-fitting to a particular training set are well known and strategies to avoid this and improve generalisation are a major component of ongoing work in the machine learning (see e.g. Roger Grosse's comp sci lecture notes: https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L09%20Generalization.pdf ).  To change my mind on this point would require additional discussion by the authors to connect this work to general principles of machine learning and establish the novelty of the insights reached from these numerical experiments.\n\nThat said, my many years in research have taught me that sometimes results that seem to me to be 'remarkably obvious' are actually not so for the general audience, and that 'simple' examples demonstrating such principles can actually have a large impact and generate huge citation indices.  I mean this genuinely; not trying to be cynical here.  So for that reason I would respect the decision of other reviewers and the AEs if this paper was in fact recommended for the conference series.  Certainly, having a reference to point to for e.g. the fact that self-driving cars probably shouldn't spend too much time refining their algorithms to over-fit to a commuter's every day journey to work (this being inevitably at the expense of performance when he/she wants to take a drive in the countryside), could actually be very useful. Conclusions seem 'obvious' to this reviewer, but willing to consider other opinions.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "iTwhMw28rWC",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_YYHXJOawkPb",
                "title": "",
                "comment": "\nThe paper conducts an empirical study into an interesting problem of robustness of deep models on out of distribution data. The paper finds that the pre-trained models exhibit better effective robustness during training which will disappear upon convergence of the same models.\n ### Summary:\n\nThe paper conducts an empirical study into an interesting problem of robustness of deep models on out of distribution data. The paper finds that the pre-trained models exhibit better effective robustness during training which will disappear upon convergence of the same models.\n\n### Pros:\n\nThe paper is well-written, easy to understand and follow along.\n\nMost significant of all is that this paper has an extensive study of various initializations with pretrained models for vision problems.\n\nThe breadth of explorations such as pre-trained models ER during training, data set size, example difficulty, \n\n### Cons:\n\nare there proper bounds for the ER values, what would it really mean to have higher value, lower value etc, can you briefly explain?\n\nER ~ 0 for CIFAR-10 (Figure 3a) at the end of training, exactly the point at which any of the models are having the corresponding best accuracies on IN set. The only difference at that point is, the accuracy of various models is different which is already known and well-studied in literature. Similar for Imagenet, it is visible at low accuracy, and the trends are visible as the accuracy gets better similar to CIFAR. The question is, why should anyone care, if the ER is high in the middle of training at low accuracy? This is not well-justified in the current version of the paper. Also, the reasons for the peaks in ER during training are not justified, why are they intriguing? -- is it because the pre-trained models change significantly to the down-stream tasks, or something else? The random initializations don\u2019t fluctuate that much, why not investigate these observations in detail?\n\nIn Figure 4b, why further fine-tune only the BiT-M-1k model, what happens if you further fine-tune all the models? This experiment is not a fair comparison, not all models see the same amount of data.\n\nAgain, in Figure 4c and the corresponding appendix, why would anyone use a low accuracy classifier when one knows it will perform bad on the hard to classify examples, in that case ER is not even a thing to worry in the first place, accuracy becomes the first concern. Fine, at least the ones that the classifier can classify, there is better robustness but not entirely convincing though. \n\nThis paper relies heavily on Taori et al. (2020), which seem to have a number of unresolved concerns, most important of all is that the paper is a bit short on novelty, however, the empirical study in itself is interesting.\n\nShow the same findings hold for at least one more domain, for example, NLP.\n\nOverall, the paper has breadth in the number of experiments and the directions that it explores without enough depth and justifications to a majority of findings.\n\n Overall, the paper has breadth in the number of experiments and the directions that it explores without enough depth and justifications to a majority of findings. Also, the paper lacks novelty or detailed analysis of the proposed concepts. I would give it a score of 4.",
                "rating": 3,
                "confidence": 4
            }
        ],
        "label": "val",
        "gpt4_judgements": [
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "substantially negative",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the observed increase then decrease in ER during fine-tuning",
                "Sentiment Expression": "fundamentally interesting phenomenon",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the presentation and contextualization of these results",
                "Sentiment Expression": "can be substantially improved",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the results",
                "Sentiment Expression": "obvious and/or not sufficiently substantiated",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            }
        ]
    },
    "HkeuD34KPH": {
        "paper_id": "iclr_2020_HkeuD34KPH",
        "paper_title": "SSE-PT: Sequential Recommendation Via Personalized Transformer",
        "paper_abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.",
        "paper_acceptance": "reject",
        "meta_review": "The paper proposes to improve sequential recommendation by extending SASRec (from prior work) by adding user embedding with SSE regularization.  The authors show that the proposed method outperforms several baselines on five datasets.\n\nThe paper received two weak accepts and one reject.  Reviewers expressed concerns about the limited/scattered technical contribution.  Reviewers were also concerned about the quality of the experiment results and need to compare against more baselines.  After examining some related work, the AC agrees with the reviewers that there is also many recent relevant work such as BERT4Rec that should be cited and discussed.  It would make the paper stronger if the authors can demonstrate that adding the user embedding to another method such as BERT4Rec can improve the performance of that model.  Regarding R3's concerns about the comparison against HGN, the authors indicates there are differences in the length of sequences considered and that some method may work better for shorter sequences while their method works better for longer sequences.  These details seems important to include in the paper. \n\nIn the AC's opinion, the paper quality is borderline and the work is of limited interest to the ICLR community.  Such would would be more appreciated in the recommender systems community.  The authors are encouraged to improve the paper with improved discussion of more recent work such as BERT4Rec, add comparisons against these more recent work, incorporate various suggestions from the reviewers, and resubmit to an appropriate venue.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "SJenAyg6tH",
                "reply_to": "iclr_2020_HkeuD34KPH",
                "title": "Official Blind Review #3",
                "comment": "The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. \n\nThe novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. \n\nThe experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. \n\nUpdate: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score. ",
                "rating": 1,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "S1ej5Lf7jB",
                "reply_to": "B1xkMwxJYB",
                "title": "Personalization of NLP models (such as Transformer and BERT) is an important research direction",
                "comment": "Hi Reviewer, thank you very much for your constructive and just-to-the-point feedback. \n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and is proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).\n\nWe will definitely correct the typos in our final version of the paper and will include more baselines such as Fossil, MARank, and/or BERT4Rec into our final version of the paper. We think our work is orthogonal to important works like BERT4Rec, because BERT4Rec is essentially another transformer-based approach, which may also benefit from our proposed personalization scheme. We will try to see if a similar technique to ours for SASRec also works for un-personalized models such as BERT4Rec. Even BERT4Rec authors also stated in future work section: \"Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.\" We think our work is first of this kind exploring this direction.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rylW9BGXoS",
                "reply_to": "Hyg-LKcJcS",
                "title": "Exponential decay idea does work empirically better than Uniform one",
                "comment": "Hi Reviewer, Thank you very much for your insightful feedback and suggestions.\n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and is proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).\n\nYes, we should definitely include CIKM'19 BERT4Rec in the final version of the paper, which we were not aware of. We think our work is orthogonal to important works like BERT4Rec, because BERT4Rec is essentially another transformer-based approach, which may also benefit from our proposed personalization scheme. We will try to see if a similar technique to ours for SASRec also works for un-personalized models such as BERT4Rec. Even BERT4Rec authors also stated in future work section: \"Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.\" We think our work is first of this kind exploring this direction.\n\nAlso, your idea of sampling start index $v$ based on the recency (e.g., with exponential decay) sounds very intuitive and could be very promising. We did a quick experiment, we find using exponential decay gives slightly better results on movielen1m data when we use max length of 50. We find using combination of our idea and your idea (most of weight on the last start index but rest of times we sample start index based on recency with exponential decay) empirically performs better, giving NDCG@10 of 0.59945 versus 0.59509 and Recall@10 of 0.81109 versus 0.80414. I think our works point to a future direction that worth more explorations, both empirically and theoretically.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HJeSNEfXoH",
                "reply_to": "SJenAyg6tH",
                "title": "Clarifying Doubts on Experimental Section",
                "comment": "Hi Reviewer, thank you very much for raising your confusion to us on experiments. We will do a better job in clarifying on how we compared with HGN in experimental section. \n\nTo explain why HGN is not doing as well as SASRec in our reported results: First, it is worth noting that the HGN paper not only used completely different datasets, but also used very distinct evaluation procedures from SASRec. Instead of predicting next engaged item, it has 10% interactions in test set, such that one prediction is correct as long as it falls into the test set, while we (and SASRec) are doing another task of predicting precisely next item, in which there is only 1 correct answer. So the task they consider is an easier task than us. Moreover, If you read the HGN paper carefully, they are mainly focused on accommodating very short sequences. In paper's experiments, they use hyper-parameter $L = 5$, where $L$ is the length of sequence used for training and inference. On contrast, our method SSE-PT and SASRec uses $L = 200$ for Movielens1m and $L=50$ for other datasets. We think that mainly accounts for the difference in original paper's reported performances and our reported performances. It is very possible that for very short sequences, HGN works quite well, better than SASRec as they have shown in their paper. We will add this delicate detail to the final version of the paper to avoid any confusions for future readers. Moreover, we modified original HGN codes to make HGN's evaluation the same as that of SASRec and open sourced at: https://github.com/SSE-PT/SSE-PT/tree/master/HGN_baseline. You have a look at our codes for both our SSE-PT and HGN baseline.\n\nAs to your other comments. \n1. Yes, it is correct that first few rows (A to F and H, I) of results in Table 1 are from SASRec paper, the reason is that we use the exactly same experimental settings on exactly same datasets. So we decided to trust the results reported in SASRec paper for older methods. We include those earlier baselines for completeness but those are not as important as SASRec because SASRec has been shown to outperform those methods. We did re-run SASRec and got slightly better results in Table 1 than the ones originally reported in SASRec paper.\n\n2. Yes, Table 3 we use different metrics than Table 1, because we realize the NDCG@10, Recall@10 does not accurately reflect how bad over-fitting is as NDCG@5 and Recall@5. The percentages of improvement for using a well-suited regularization are much more dramatic once you switch the metrics to top 5 from top 10. This means good regularization are extremely important for top k ranking results, especially when $k$ is small. The results in Table 3 would still hold for top 10 but less dramatic for percentages of gains. On the other hand, because we want to make a fair comparison with SASRec on the same datasets, we chose to use same top-10 metrics in Table 1. This is our reasoning as to why metrics used in Table 1 and 3 are different.  \n\n3. As to the ablation study, the ablation study of personalization is done in Table 10 in Appendix and we have had a dedicated section 4.3 for different ablation studies done for each component of the model.\n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and \nis proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1xkMwxJYB",
                "reply_to": "iclr_2020_HkeuD34KPH",
                "title": "Official Blind Review #2",
                "comment": "In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user $i$, i.e., $u_i$, shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques.\n\nExtensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods.\n\nOverall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. \n\nMy major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance.\n\nSome suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec.\n\nSome minors:\nTypo: in the paragraph below Eq.(3): user $l$ -> user $i$\nTypo: FPMF, PFMC in different places\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Hyg-LKcJcS",
                "reply_to": "iclr_2020_HkeuD34KPH",
                "title": "Official Blind Review #1",
                "comment": "The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE  regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches.\n\nDetailed comments:\n\n1)\tThe technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental.\n\n2)    In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them.\n\n3)\tIn SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability?\n\n4\uff09 Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. \n\n\nTo summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a \u201cweak accept\u201d to this paper given the reasons listed above.\n\n\n[1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "technical contribution",
                "Sentiment Expression": "limited/scattered",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the quality of the experiment results",
                "Sentiment Expression": "concerned",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "the paper quality",
                "Sentiment Expression": "is borderline",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "adding the user embedding to another method such as BERT4Rec",
                "Sentiment Expression": "can improve the performance of that model",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "These details",
                "Sentiment Expression": "seems important",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "XL9DWRG7mJn": {
        "paper_id": "nips_2021_XL9DWRG7mJn",
        "paper_title": "Rethinking gradient sparsification as total error minimization",
        "paper_abstract": "Atal Sahu, Aritra Dutta, Ahmed M. Abdelmoniem, Trambak Banerjee, Marco Canini, Panos Kalnis",
        "paper_acceptance": "accept",
        "meta_review": "This paper reformulates an existing problem (how to sparsify gradients in distributed training) and proposes to minimize a new objective (the total compression error subject to communication constraint as opposed to per-iteration compression error). This change of viewpoint leads to a new algorithm (hard threshold algorithm with variable sparsity). The authors show the effectiveness of the proposed algorithm through theoretical bounds and experiments. All reviewers agree that this is a valuable contribution. Comments from previous submission to ICML are adequately addressed.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "O7Pa-FIoBJ8",
                "writer": "author",
                "reply_to": "sb4i9bO5c6F",
                "title": "Thank you for your positive reassessment",
                "comment": " We thank the reviewer for the positive feedback, and for pointing out a meaningful direction for future research. We will clearly elaborate our use of the word optimal, as mentioned in the response to reviewer o9PJ.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sb4i9bO5c6F",
                "writer": "official_reviewer",
                "reply_to": "8BKgSsPRwr8",
                "title": "Comments after rebuttal",
                "comment": " Many thanks to the authors' detailed replies and other reviewers' insights into this paper. I do appreciate the great value of this work, but I am still worried about the claim of optimality, even though I understand that this optimality is restricted to the model the authors proposed. My understanding is that from the upper bounds in Theorem 1, no matter what optimization trajectories, the upper bound only depends on the total compression errors, and the hard-threshod is optimal for any fixed optimization trajectories as long as the right threshold is chosen, and thus the optimality (please let me know if I misunderstood this). However, Theorem 1 only provides an upper bound, if we are minimizing an upper bound, claiming optimal seems to me kind of strong. I understand this upper bound motivates the hard-threshold compression methods, and the paper presents very solid theoretical and experimental results for this method. However, in terms of communication efficiency, i.e. bits over guaranteed optimization errors, there is no clear theoretical improvements shown, so maybe this can be some future works. I agree with other reviewers' votes and raise my score to 7.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Inez-R5X9DB",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "",
                "comment": "This paper analyzes the hard-threshold sparsifier in distributed SGD with convergence analysis and extensive experiments. Main contributions include: 1. Provides upper bounds of the optimization errors for the hard-threshold sparsified distributed SGD, which improves from top-k compressor with linear speedup and compressor operator parameter dependence. 2. Conducts extensive experiments to demonstrate the benefits of hard-threshold sparsifier, achieving much better performance given the same average compression density (# of used coordinates / # of coordinates of DNN weights).   1. I am trying to understand why the proposed communication-complexity model is new, it seems to me it is an adaptive compression operator allowing different # of coordinates to be sent for each iteration. And the section 4.4 and Lemma 3 seems incorrect to me, since the optimization over B is sequential, the choice of first block will change the second block of A, so I didn\u2019t quite follow why the proposed sparisfier is optimal for this communication-complexity model. \n2. On the confusion of total error minimization. In the paragraph starting from line 44, it seems to the authors are trying to present a new perspective that is not minimizing per iteration compression error but total compression errors, but the last sentence seems to me conveying the message of considering fixed total communication budget, which is exactly what I think other papers have discussed, how to tradeoff optimization error with total communication budgets. So there may be some confusion in this paragraph.\n3. The authors provide upper bounds of optimization errors for the proposed spasifier, in strongly convex, convex, and nonconvex settings respectively. Those upper bounds improve from the results using top-k sparsifier in terms of linear speedup and compressor parameter dependence. \n4. The provided upper bounds, however, seem to me not clearly they are more communication-efficient than the top-k sparsifier, since there is no characterization of the total number of coordinates being used in theory. Ideally, if we set the threshold be very small, most coordinates will be used. In experiments, the authors provide solid results showing that the hard-threshold sparsifier does use less coordinates in total than the top-k. \n5. The total error minimization perspective seems to me more like an observation of the consequence using a hard-threshold sparsifier, it may need further arguments to show it is the reason for better performance. \n6. The paper is in general well written and very clear, the extensive experiments are helpful for the understanding of the practical benefits of the hard-threshold sparsifier.  Yes.",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "hO-JUkgT_HK",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "",
                "comment": "The paper studies the role of compression operators on the convergence of Distributed SGD with Error Feedback. In particular, via simple observations, the authors conclude that a hard-threshold sparsifier with a carefully tuned threshold parameter minimizes the total error appearing in the analysis because of the presence of compression. Moreover, they show empirically the connection between poor behavior of EF-SGD with Top-k compression and the severe error accumulation.\n\nMotivated by these observations, the authors derive new convergence guarantees for EF-SGD with absolute compressors. This class of compressors covers hard-threshold sparsifier. The derived bounds show that the compression does not affect the slowest terms in the bound. Moreover, the authors derived the first complexity result in the non-convex case for EF-SGD with $\\delta$-cnotraction operators without bounded gradient assumption and $n > 1$ (though, under bounded data dissimilarity). Finally, the paper contains a good empirical study of the performance of EF-SGD with the hard-threshold sparsifier. The authors also provide an insight on how to tune the threshold parameter in order to outperform EF-SGD with the Top-k operator.  ## Strengths\n\n1. **Simple but important observations about total error minimization.** The paper provides a closer look at the convergence of EF-SGD and identifies what quantity should be minimized in order to get better results. That is, via the sequence of simple observations, authors show that a hard-thresholding sparsifier (with fine-tuned threshold parameter) is the optimal choice in terms of the total error minimization. Moreover, the authors properly explain all the details and support their theoretical observations with empirical findings (e.g., see Figures 1 and 3).\n\n2. **Clarity and proofs.** The paper is clearly written. The proofs are easy to follow and contain only a couple of typos.\n\n3. **New results for EF-SGD.** The authors derived new results for the convergence of EF-SGD with absolute compression for strongly convex, convex, and non-convex objectives. The slowest terms in the derived bounds have a linear speedup and are not affected by compression-dependent parameters. This is a good property since the derived bounds match the ones for SGD without compression if the target accuracy is small enough / the number of communication rounds is large enough. Moreover, the authors derived the first convergence result for EF-SGD for $\\delta$-contraction operators without assuming boundedness of the gradients, but under Assumption 4, that bounds dissimilarity between local loss functions. Although the proofs substantially rely on the known techniques, the obtained results are quite good.\n\n4. **Numerical experiments** show a connection between the behavior of EF-SGD with Top-k and hard-thresholding sparsifier and \"error buildup\". Therefore, these numerical results justify the insights provided in Section 4.\n\n## Weaknesses\n\n1. **No analysis for the arbitrary heterogeneous case for non-convex objectives (minor).** The derived bounds in the non-convex case substantially rely on Assumption 4 that bounds the dissimilarity between local loss functions. Although this is a significant limitation, previous works on EF-SGD rely on even stronger assumptions. Therefore, this is a minor drawback.\n\n## Questions and Comments\n\n1. **Rates for EF-SGD with $\\delta$-contraction operator.** The rates shown in Remarks 5 and 7 can be significantly improved via the results from [19]. Although, in [19], Assumption 3 is not considered it can be easily cast in the general framework from [19] via the following derivation: $\\frac{M}{n}\\sum_{i=1}^n\\|\\nabla f_i(x^k)\\|^2 + \\sigma^2 \\leq 2LM(f(x^k) - f(x^*)) + \\frac{M}{n}\\sum_{i=1}^n \\|\\nabla f_i(x^*)\\|^2 + \\sigma^2$. Using the results from [19] one can actually show the linear speedup even for $\\delta$-contraction operators in the $\\mathcal{O}(1/T)$ and $\\mathcal{O}(1/\\sqrt{T})$ decaying terms for $\\mu > 0$ and $\\mu = 0$ respectively. Therefore, the conclusion from lines 270-272 is not correct.\n\n2. **equation after line 654:** the enumerator in the second term should be $\\mu L^2(1+M/n)^2R_0 + L\\kappa^2 \\ln(T)$.\n\n3. **line 247, $\\nu = \\gamma_t \\kappa$:** It is better not to use $\\kappa$ in the definition of $\\nu$ because $\\kappa$ usually denotes the condition number of the problem in the optimization literature.\n\n4. **lines 283-284:** This is done for $\\delta$-contraction operators in [19] and in Qian, X., Dong, H., Richt\u00e1rik, P., & Zhang, T. Error Compensated Loopless SVRG for Distributed Optimization, Qian, X., Richt\u00e1rik, P., & Zhang, T. (2020). Error compensated distributed SGD can be accelerated. arXiv preprint arXiv:2010.00091.\n\n5. **Lemmas 6 and 7.** First of all, one should add that $\\gamma = \\min\\left(\\frac{1}{d}, \\sqrt{\\frac{r_0}{cT}}\\right)$. Next, Lemma 7 can be tightened when $c$ is small, see Lemma D.3 from [19].\n\n6. **Lemma 9.** When $c = 0$ this result is incorrect since the logarithmic factor becomes infinity. See Lemma D.2 from [19] for the correct version.\n\n7. **Lemma 11.** It is better to cite the assumptions of the lemma in the statement (or at the beginning of the subsection).\n\n8. **lines 623-627:** The discussion in these lines should be rewritten after applying the corrections suggested in comment 1. Moreover, one should also say that $\\lambda$ can be large.\n\n## Comment after rebuttal\nI thank the authors for their response. I have read other reviews as well. Overall, my evaluation of the work remains the same. Therefore, I recommend the paper for acceptance and hope that the authors will apply all necessary corrections mentioned in the reviews. The authors adequately addressed the limitations and potential negative societal impact of their work.",
                "rating": 7,
                "confidence": 5
            },
            {
                "review_id": "izpo_kBPLuT",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "",
                "comment": "The paper considers gradient sparsification for learning in distributed setup, and advocates using a hard-threshold sparsifier combined with error-feedback mechanism. The paper shows that such algorithm is _optimal_ in a certain sense, and give several convergence guarantees for the error-feedback algorithms using absolute compressors and relative compressors. The empirical performance of HT and top-k compressors are also compared.  Post-rebuttal: Thank you for the reply. My concerns have been addressed well. Raising the score to 7.\n\n---\n\n__TL;DR.__ I think this paper presents several meaningful theoretical results as a contribution. However, I think some of the paper's claims are being quite oversold.\n\n__Strengths.__ Some of the theoretical results are definitely very cool to have. I believe that the convergence results in Section 5 (Theorems 2--5) is a nice contribution, and would be of interest to the distributed learning society, especially to those who study error-feedback mechanisms. Also, the proof technique going through the perturbed iteration analysis (via Lemma 10) is quite neat. Finally, the manuscript seems to discuss the related work relatively well.\n\n__Claims on optimality.__ I am very worried about the paper's claims about the optimality. The word \"optimal\" is a very bold, and should be used with a great care (in my opinion), as they could be quite misleading without delivering the assumptions and conditions it relies on. For instance, the abstract states that top-k is \"communication-optimal given a per-iteration $k$-element budget.\" But, what exactly does the word \"communication-optimal\" here mean? It is very easy to understand the statement as saying that such algorithm gives the hypothesis with a smallest loss---either in expected or high-probability sense. If I understood correctly, I think the paper is pointing to lemma 2, where authors state that Top-k gives the sparsified version of the error-feedback (or actually any signal) that has the smallest squared distortion from the original gradient signal. This also relies on the assumption that the choice of sparsification does not affect the subsequent gradient signals. This discrepancy gets more significant for the claims on the optimality of hard-threshold methods, where this \"independency assumption\" is critical for the proof. I believe that these ill-specified claims on the (possibly vacuous notions of) optimality should either be toned down to a certain degree to help readers better understand what the paper is contributing. Also, I think presenting the optimality claims in a form of lemmata without proofs is unnatural, no matter how straightforward the proofs are. I recommend either stating the claims in plain words without a formalization, or provide the result-specific assumptions clearly in the lemma statement---so that it is self-contained---and give at least a formal proof.\n\n__Stating the limitations.___ If I understood correctly, such hard-threshold sparsifiers may need a very high communication throughput at some epochs (mostly earlier). However, there are many setups such high peak communication rate is undesirable, due to the limited capacity of the communication channel (but when the delay is crucial). In such cases, having a constant communication rate could be beneficial. I think authors should discuss such scenarios to appropriately deliver the cases where the considered hard-threshold methods are desirable, and the cases they are not (sorry if I missed these parts).\n\n__Clarity: Why error-feedback?__ I am not entirely sure what is the big motivation behind considering the error-feedback mechanisms for this paper. Line 51 says: \"Consequently, we consider sparsification using the error-feedback mechanism, ...\" but I couldn't really locate the part that necessitates considering error-feedback. Could you please further clarify?\n\n__Clarity: $\\gamma_t$.__ The quantity $\\gamma_t$ appears at line 133, but I do not think this quantity is defined or introduced properly in the text.\n\n__Clarity: Assumption 2.__ I do not think Assumption 2 is explicitly assumed in any theorem appearing in the main text. But if I am correct, it is implicitly used for every theorems that use $R_0 := \\lVert x_T - x^\\star \\rVert$ in its bounds.\n\n__Question: $v$-dependency__ It was unexpected to me that the convergence guarantees for the absolute compressors $C_v ( \\cdot ) $ does not depend explicitly on the compression factor $v$, especially considering the fact that the guarantees for the $\\delta$-contraction operators contain a term that is inversely proportional to $\\delta$ (see Theorem 2 & Remark 5, for instance). Any further discussion explaining why such discrepancy happens (especially for the case $\\delta \\to 0, v \\to \\infty$) would be very nice to have; does this suggest the existence of a tighter bound for $\\delta$-contraction operators under additional assumptions the size of $p_{i,t}$?\n\n__Suggestion: Discussing $P_T$.__ While the quantity $P_T$ appears in many optimization literature, the meaning of the quantity may not be very straightforward for the readers who are relatively new to the field (like myself), especially because it gives the performance bound for $\\bar{x}_T$ generated by some weights $w_t$. Giving more ideas about the quantity may help the readers a lot, including whether one can choose $w_t$ arbitrarily or not, whether it implies that we should use an averaging scheme... I think authors should additionally mention when can such variable-communication-load methods may not useful or usable.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "8BKgSsPRwr8",
                "writer": "author",
                "reply_to": "Inez-R5X9DB",
                "title": "Author Response to Reviewer TZ2h",
                "comment": " We sincerely thank the reviewer for the positive assessment of our paper and for constructive feedback. Below we address the points mentioned by the reviewer:\n\n\n**1.Communication-complexity model and total error minimization.** We thank the reviewer for pointing this out. We accept that there is a slight misunderstanding about the simplification in our communication-complexity model. As mentioned in line 192, we assume a simplified model---Instead of the error-corrected update, $\\gamma g_t +e_t$, we consider a sequence of *fixed* vectors $(a_t)_{t \\in T}$  and formalized the optimization problems (5) and (6) in Section 4.4. Hence as the reviewer asserted, the optimization in Section 4.4 is not sequential. We will clarify this. \n\nWe note that the existing communication-optimal strategies [18, 38, 13, 4] minimize the compression factor (see Footnote 2) under a budget for each vector; please see lines 221-227. In contrast, the novelty of our communication-complexity model is that it better captures the total error. We owe this insight to Theorem 1 that accounts for the effect of sparsification in the entire training process. Please refer to the statement of Theorem 1, which presents the convergence of distributed error-feedback SGD with compressed communication (here, sparsification). The third term on the right hand side of the inequality captures the effect of compression between the error-corrected update, $\\gamma g_t +e_t$, and its compressed form, $C(\\gamma g_t +e_t)$ over all iterations, $t=0,1,\\cdots, T-1$. This is a well-accepted theoretical result and appears abundantly in the literature; please see [29,46]. Motivated by this result, we consider the total error perspective as a communication complexity model----where simplified total error is minimized under a total communication budget. Hard-threshold sparsifier comes out as the communication-optimal compressor under this communication complexity model. Therefore, the total-error minimization is not the consequence of using the hard-threshold sparsifier; it is the reason behind the hard-threshold as a communication-optimal sparsifier. Moreover, we substantiate this with insights from our experiments in Figures 1, 3, 5, and 6. \n\nNext, we respectfully note that we use the overall communication budget to denote the total communication throughout the training.\n\n\n**2. Convergence of hard-threshold.** We thank the reviewer for this question. The reviewer is correct---Our convergence results do not show that the hard-threshold is more communication-efficient than Top-$k$. Because we do not characterize the average data transmission for a threshold. However, we have demonstrated that hard-threshold is communication-optimal in our communication-complexity model. Our communication-complexity model is motivated by the EF-SGD non-convex convergence result, and it provides us insight into why hard-threshold has better convergence than Top-$k$ in practice.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wpfn0XbVep",
                "writer": "author",
                "reply_to": "izpo_kBPLuT",
                "title": "Author Response to Reviewer o9PJ",
                "comment": " We are grateful to the reviewer for the constructive feedback and for providing a positive assessment of our paper. The reviewer has raised some valid questions and provided many mindful suggestions. The following are our responses to the reviewer\u2019s comments: \n\n**Claims on optimality:** We thank the reviewer for this comment. Our use of the word *optimal* is motivated by recent literature in compressed distributed optimization [ 39, 18, 13, 4] that focus on the compression-error (see Footnote 2). Any compressed optimization convergence analysis captures the effect of compression via the compression-error, and this effect is always inverse---the lower the compression error, the better the optimization upper bound is. Please refer to one-step descent Lemmas 12 and 15. To derive a convergence rate from these lemmas, we need to use the worst-case compression error/factor . For instance, to derive a convergence rate for $\\delta$-contraction operators in Remark 5 and Remark 7, one uses the worst-case compression factor in equation (7) and lines 211-212. Due to this, [39, 18, 13, 4] directly optimize for this worst-case compression factor and call their compressors as *optimal*. \n\nHowever, for gradient sparsification with a fixed $k$ element communication per iteration, this worst-case bound is not insightful. For example, to sparsify a $d$-dimensional vector, both Random-$k$, and Top-$k$ have the same compression factor, $k/d$, although Top-$k$ performs significantly better in practice than Random-$k$; please see further discussion in [8]. But we know that for a given signal, Top-$k$ attains the lowest compression-error among all sparsifiers with $k$ element communication budget. And therefore, we state Top-$k$ as the communication-optimal sparsifier under a fixed $k$-element communication budget. Precisely, this is the message behind Lemma 2 as the reviewer has correctly identified. We will elaborate on this in detail in the final version.\n\nNext, we will mitigate the confusion regarding Lemma 3. We agree that we should provide proof for Lemma 3, and will do so in the Appendix of the revised paper. Please note that we have been careful to stress throughout the text that hard-threshold is communication optimal *in our communication complexity model*, and have stated that the total-error cannot be directly minimized (Lines 175-177). We coined the term *total-error* because this term captures the compression error in the entire training process. We formalize our communication complexity model in (5) after simplifying the total error; please see Lines 191-193. \n\n**Stating the limitations.** We thank the reviewer for pointing this out and we agree with the comment. Indeed there are scenarios where a predetermined compression ratio for an iteration is desirable. Examples include dynamic network environments such as a public cloud or a shared cluster with colocated jobs [Abdelmoniem et al., 2021]. In such a setting, one may want to adjust the compression knobs according to the current network bandwidth so that training finishes within a time budget, and therefore hard-threshold is not a good candidate for this setting. \n\nIn a standard distributed cluster setting with a dedicated network, if communication is a bottleneck for Top-$k$, i.e., there is not a complete overlap between communication and computation, so a hard-threshold with the same total communication volume will have non-overlapped communication in the iterations with high data transmission, but may also have completely overlapped communication in iterations with low data transmission. Thus, hard-threshold can have less non-overlapped communication time than Top-$k$ in this case. This happens in large-scale CRT models such as DeepLight which has 90% of non-overlapping communication. Considering the opposite, if there is complete overlap between computation and communication for Top-$k$, then a hard-threshold with the same total communication volume may have non-overlapped communication time in some iterations with high data transmission. Here, we ignored two important aspects of hard-threshold: (i) Hard-threshold has better statistical efficiency than Top-$k$, thus one may require smaller iterations to a target accuracy. (ii) Hard-threshold has negligible compression overhead in comparison to Top-$k$ (lines 73-79). \n\n[Abdelmoniem et al., 2021] DC2: Delay-aware compression control for distributed machine learning. IEEE INFOCOMM 2021.\n\n**Why error-feedback?** This is a critical question and we thank the reviewer for asking this. Please allow us to discuss the development of the error-feedback theory in this context. Error-feedback was first empirically introduced by Seide et al. [40] in 2014, to alleviate the convergence of 1-bit low-precision SGD in training language models. But, for the next 5 years, the community was unaware about why error-feedback is an important technique. In 2018, Stich et al. [45] were the first to theoretically establish the convergence of SGD using $\\delta$-contraction operators with error-feedback, which was extended to the distributed setting by Zheng et al. in [55]. More interestingly, in subsequent work, Karimireddy et al. [29] theoretically showed that error-feedback can remedy the convergence issues of aggressive quantizers, such as 1-bit/Sign SGD, as well as biased $\\delta$-sparsifiers, such as Top-$k$, Random-$k$, etc. We also refer to [46] for a detailed discussion on the error-feedback framework. In a nutshell, without error-feedback, most sparsifiers (which also belong to the class of $\\delta$-contraction operators) diverge [29,46]. Moreover, the best compression ratios for gradient sparsification are achieved when we use error-feedback. Please refer to Table 1 in the comprehensive survey [54], where all implemented sparsifiers use error-feedback. This makes error-feedback an \"indispensable and essential\" technique for gradient sparsification. And this was the \"big motivation\" of using error-feedback in our work as it is focussed on gradient sparsification. However, we also thank the reviewer for pointing out \"Line 51 says: \"Consequently, we consider sparsification using the error-feedback mechanism, ....\".\" We will rewrite this line to justify why we use the error-feedback. \n\n**Clarity:$\\gamma_t$**. Thank you for this important observation. We will clearly mention that $\\gamma_t>0$ is the stepsize sequence. \n*Assumption 2.* We thank the reviewer for catching this typo and rectifying us. We will mention Assumption 2 in the theorems where it is supposed to appear. \n\n**$\\upsilon$-dependency.** We apologize for the confusion. Absolute compressors\u2019 convergence is in terms of $\\kappa$; please see lines 244-250. We will make this clearer by directly compressing the error-compensated gradients in Algorithm 1 instead of the error-compensated updates as done presently. \n\n**Discussing $P_T$.** The quantity $P_T$ in our paper denotes the expected suboptimality gap, $E[f(\\bar{x}_T)]-f^\\star$ at averaged iterate, $\\bar{x}_T$, where $f^*$ is the global minimum, defined in Assumption 2. We understand and appreciate the reviewer's concern in clarifying the meaning behind this quantity. Here $w_t$ is a carefully chosen set of weights such that we achieve the convergence result. We will highlight its meaning with references at the beginning of Section 5.1 where it appears for the first time. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "oiEJMsDqEUb",
                "writer": "author",
                "reply_to": "-lB5hoBJkV",
                "title": "Author Response to Reviewer eex4",
                "comment": " We thank the reviewer for the effort in reviewing our paper. We also thank the reviewer for providing a positive assessment of our work and for appreciating our communication-complexity model which is indeed a key contribution of the paper. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "aChnufbJY_s",
                "writer": "author",
                "reply_to": "hO-JUkgT_HK",
                "title": "Author Response to Reviewer VFA3",
                "comment": " We thank the reviewer for the positive review of our paper. We are glad that the reviewer considers total error minimization as an important contribution to our paper. Indeed, this is the heart of our paper. Below we discuss the questions and the comments of the reviewer.\n\n**No analysis for the arbitrary heterogeneous case for non-convex objectives (minor).** We sincerely thank the reviewer for pointing out this interesting aspect. We will address the arbitrary heterogeneous case in our future work.\n\n## Questions/Comments\n\n**1. Rates for EF-SGD with $\\delta$-contraction operator.** We sincerely thank the reviewer for bringing the tighter rates in [19] to our notice. We will rewrite this discussion accordingly.\n\n**2. Typo after line 654.** We thank the reviewer for indicating this typo. We will correct this. \n\n**3. Notation $\\kappa$.** We thank the reviewer for this comment; and yes, you are right. We will use a better notation in the final version of the paper.\n\n**4. Related works.** We thank the reviewer for pointing out these works. We will indeed mention them and include more discussions in a proper context in the final version of the paper. \n\n**5. Lemmas 6 and 7.** We thank the reviewer for giving us this insight. We will use the tighter result from [19].\n\n**6. Lemma 9.** We thank the reviewer for spotting this. We will correct it.\n\n**7. Assumption in Lemma 11.** We thank the reviewer for this insightful comment. Indeed we will state the assumptions in the main statement of Lemma 11. \n\n**8. Lines 623-627.** As mentioned in comment 5, we will now compare absolute compressors and $\\delta$-contraction operators using [19]. We will also state that $\\lambda$ can be arbitrarily large.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-lB5hoBJkV",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_XL9DWRG7mJn",
                "title": "",
                "comment": "In this paper, the authors demonstrate that, in the context of distributed optimization problems with $n$ workers, the hard-threshold sparsifier is the optimal sparsifier for a proposed communication complexity model which where the goal is to minimize the total error for a sequence of responses. This allows the authors to compare the sum of compression errors for various algorithms and to demonstrate that while for the per-iteration $k$-element budget the Top-$k$ sparsifier is optimal, when it comes to total error the hard threshold sparsifier is better. \n\nThe authors also compare the convergence rates of the top-$k$ sparsifier vs the hard threshold sparsifer for image classification, language modeling, and recommendation tasks.   I find this paper interesting and vote to accept it. I think it is an interesting result but unfortunately I am not an expert in the field and so am not sure about it's significance in the context of the field. I do like the communication complexity model.  This is a theoretical paper and has limited societal impact. ",
                "rating": 7,
                "confidence": 3
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "this",
                "Sentiment Expression": "valuable contribution",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "r1exVhActQ": {
        "paper_id": "iclr_2019_r1exVhActQ",
        "paper_title": "DEEP-TRIM: REVISITING L1 REGULARIZATION FOR CONNECTION PRUNING OF DEEP NETWORK",
        "paper_abstract": "State-of-the-art deep neural networks (DNNs) typically have tens of millions of parameters, which might not fit into the upper levels of the memory hierarchy, thus increasing the inference time and energy consumption significantly, and prohibiting their use on edge devices such as mobile phones. The compression of DNN models has therefore become an active area of research recently, with \\emph{connection pruning} emerging as one of the most successful strategies. A very natural approach is to prune connections of DNNs via \u21131 regularization, but recent empirical investigations have suggested that this does not work as well in the context of DNN compression. In this work, we revisit this simple strategy and analyze it rigorously, to show that: (a) any \\emph{stationary point} of an \u21131-regularized layerwise-pruning objective has its number of non-zero elements bounded by the number of penalized prediction logits, regardless of the strength of the regularization; (b) successful pruning highly relies on an accurate optimization solver, and there is a trade-off between compression speed and distortion of prediction accuracy, controlled by the strength of regularization. Our theoretical results thus suggest that \u21131 pruning could be successful provided we use an accurate optimization solver. We corroborate this in our experiments, where we show that simple \u21131 regularization with an Adamax-L1(cumulative) solver gives pruning ratio competitive to the state-of-the-art.",
        "paper_acceptance": "rejected-papers",
        "meta_review": "This paper studies the properties of L1 regularization for deep neural network. It contains some interesting results, e.g. the stationary point of an l1 regularized layer has bounded number of non-zero elements. On the other hand, the majority of reviewers has concerns on that experimental supports are weak and suggests rejection. Therefore, a final rejection is proposed.",
        "meta_review_title": "A study on sparse properties of L1-regularization in deep neural networks, yet experimental supports seem week.",
        "reviews": [
            {
                "review_id": "BJlZt9ptRX",
                "reply_to": "BylbaEivn7",
                "title": "Reply to AnonReviewer2",
                "comment": "We thank the reviewer for the feedback and comments.\n\n(1) \"whether the theory for (5)  is rigorously justified by the experiments\":\n\nWhile our theorem is designed for the layerwise objective (5), in practice for simplicity we find that directly optimize (8) yields promising results is more simple. We will show experimental results for both (5) and (8) in future revisions. Note that by optimizing (8), we achieve satisfactory results satisfying our bounds from analyzing (5) in all experiments in this work.\n\n(2) Regarding the bound tightness:\n\nWe perform experiments on Cifar 10 with Vgglike-networks with different \\lambda values by compressing the last 2 FC layer. \nWe would like to point out that the bound for NNZ per-layer in this setting is 50000 * K_s, which depends on the number of supports in the stationary point.\n\nIf a max-margin loss is used, K_s can be close to 1, which would give us an NNZ bound around 50000, which is not far from the empirical compressed NNZ (~ 10000).\n\nepsilon     |   1e-4     |    1e-5   |   1e-6    |   1e-7    |  1e-8   |   1e-9  | 1e-10  |      0     |\nnnz_fc1    |   9052    |    9947   |   10046 |   10053 | 10054  | 10054 | 10054 | 262144|\nnnz_fc2    |   4549    |    4567   |   4570   |   4570   |  4570   |  4570  |  4570  |   5120  |\ntrain_acc  |  0.9970  |  0.9974  |  0.9979 |  0.9970 | 0.9969 | 0.9972| 0.9969| 0.9970 |\n\n(3) regarding minor points:\n\nWe will fix the mistakes and typos in future revisions.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HkxH-cpFCQ",
                "reply_to": "Byevdiz3nm",
                "title": "Reply to AnonReviewer1",
                "comment": "We thank the reviewer for the nice feedback and concerns.\n\n(1) the assumption of \u201cgeneral position\u201d:\n\nThe columns of V do not need to be independent to be in general position. It is sufficient if V is drawn from any continuous probability distribution. In other words, the assumption holds as long as we add a very small continuously-distributed perturbation to V. Note general position is a much weaker condition than the RIP condition used widely in sparse recovery.\n\n\n(2) Theorem 1 claims the sparse inequality holds for any \\lambda:\n\nTo validate that the sparse inequality holds for any \\lambda, we perform experiments on Cifar 10 with Vgglike-networks with different \\lambda values by compressing the last 2 FC layer. \nThe result is shown below:\n\nepsilon     |   1e-4     |    1e-5   |   1e-6    |   1e-7    |  1e-8   |   1e-9  | 1e-10  |      0     |\nnnz_fc1    |   9052    |    9947   |   10046 |   10053 | 10054  | 10054 | 10054 | 262144|\nnnz_fc2    |   4549    |    4567   |   4570   |   4570   |  4570   |  4570  |  4570  |   5120  |\ntrain_acc  |  0.9970  |  0.9974  |  0.9979 |  0.9970 | 0.9969 | 0.9972| 0.9969| 0.9970 |\ntest_acc   |  0.9271  |  0.9270  |  0.9266 |  0.9267 | 0.9264 | 0.9262| 0.9265| 0.9268 |\n\nWe note that we perform SGD with L1 regularizer to train the network as a pretraining step. Empirically, we find that after the L1 norm is penalized, even a very small epsilon can lead to very sparse solutions. (However, when epsilon is too small, the converging time may grow a lot.) For epsilon >= 1e-9, the nnz_fc1 becomes <= 10054 for the first training epoch. However, for epsilon = 1e-10, nnz_fc1 drops to 10054 after the second epoch.\n\n(2) regarding minor points:\n\nWe will fix the mistakes and typos in future revisions.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "B1lBnKatR7",
                "reply_to": "SkeePQCJTQ",
                "title": "Reply to AnonReviewer4",
                "comment": "We thank the reviewer for the feedback.\n\n1) About \"Ignoring the latest improvement in (C. Louizos et al., 2017) and (J. Achterhold et al.)\":\n\nWhile we thank the reviewer for providing us more related works,  it worths noticing that pruning ratios in (C. Louizos et al., 2017), (J. Achterhold et al.) are not as strong as our compared baseline \"Variational Dropout\". For example, for LeNet on Mnist, the former have ~0.65%, while the latter (and our result) are less than 0.4%, and for VGG on CIFAR-10, the former have ~5.5%, while the latter (and our result) are less than 2%. That is, both our method and VD has better results compared to the two related works.\n\nNote many results provided in (C. Louizos et al., 2017), (J. Achterhold et al.) are for simultaneous pruning and quantization, while our submission focuses more on investigating the pruning effect of the simple L1 regularizer. In this work, we focus on the weight pruning ratio without quantization.\n\n(2) About  comment \"Repeating the old story from other papers\":\n\nOur story focuses more on the analysis of \"problem\" instead of the \"algorithm\". In other words, we argue that different problems have different compression rate, depending on their number of supporting labels, when a simple L1-regularized pruning objective is used.  The algorithm we proposed is just a tool for helping our iterates getting closer to the stationary points.\n\n(3) About comment \"quite limited novelty\":\n\nFirstly, our novelty lies more on the analysis of the pruning objective than on the algorithm. Second, it is a wrong impression that we are proposing ADAM over SGD.  Our proposition for the algorithm is the \"L1 cumulative\" technique as a general extension module to modify any stochastic-gradient-based algorithms, such as SGD and ADAM, into a sparsity-inducing solver.\n\n(4) About comment \"lacking solid experiments\":\n\nThe sentence is an editorial mistake. We will strengthen our experiments in future revisions.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SkeePQCJTQ",
                "reply_to": "iclr_2019_r1exVhActQ",
                "title": "Repeating the old story from other papers, quit limited novelty, lacking solid experiments",
                "comment": "The main concerns come from the following parts:\n\n\n(1) Repeating the old story from other papers:\nA large part of math is from previous works, which seems not enough for the ICLR conference.\nIt is very surprising that the authors totally ignore the latest improvements in neural network compression. Their approach is extremely far away from the state of the art in terms of both methodological excellence and experimental results. The authors should read through at least some of the papers I list below, differentiate their approach from these pioneer works, and properly justify their position within the literature. They also need to show a clear improvement on all these existing pieces of work. \n\n(2) quite limited novelty:\nIn my opinion, the core contribution is replacing SGD with Adam.\nFor network compression, it is common to add L1 Penalty to loss function. The main difference of this paper is change SGD to Adam, which seems not enough. \n\n(3) lacking solid experiments:\nIn section Experiment, the authors claim \"Finally, we show the trade-off for pruning Resnet-50 on the ILSVRC dataset.\", but I cannot find the results. \n\nIs the ResNet-32 too complex for cifar-10? Of course, it can be easily pruned if the model is too much capacity for a simple dataset.  Why not try the Resnet-20 first?\n\n[1] C. Louizos et al., Bayesian Compression for Deep Learning, NIPS, 2017\n[2] J. Achterhold et al., Variational Network Quantization, ICLR, 2018",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "Byevdiz3nm",
                "reply_to": "iclr_2019_r1exVhActQ",
                "title": "an interesting perspective on the L1 regularization of neural network",
                "comment": "This paper discusses the effect of L1 penalization for deep neural network. In particular it shows the stationary point of an l1 regularized layer has bounded non-zero elements. \n\nThe perspective of the proof is interesting: By chain rule, the stationary point satisfies nnz(W^j) linear equations, but the subgradients of the loss function w.r.t. the logits have at most N\\times ks variables. If the coefficients of the linear equation are distributed in general positions, then the number of variables should not be larger than the number of equations. \n\nWhile I mostly like the paper, I would like to point out some possible issues:\n\nmain concerns: \n\n1. the columns of V may not be independent during the optimization(training) process. In this situation, I am not quite sure if the assumption of \u201cgeneral position\u201d still holds. I understand that in literatures of Lasso and sparse coding it is common to assume \u201cgeneral position\u201d. But in those problems the coefficient matrix is not Jacobian from a learning procedure. \n\n2. the claim is a little bit counter intuitive: Theorem 1 claims the sparse inequality holds for any \\lambda. It is against the empirical observation that when lambda is extremely small, effect of the regularizer tends to be almost zero. Can authors also show this effects empirically, i.e., when the regularization coefficients decrease, the nnz does not vary much? (Maybe there is some optimization details or approximations I missed?)\n\nSome minor notation issues:\n1. in theorem 1: dim(W^{(j)})=d should be dim(vec(W^{(j)}))=d\n2. in theorem 1: Even though I understand what you are trying to say, I would suggest we describe the jacobian matrix V in details. Especially it is confusing to stack vec(X^J) (vec(W^j)) in the description.\n3. the notations of subgradient and gradient are used without claim\n",
                "rating": 6,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "BylbaEivn7",
                "reply_to": "iclr_2019_r1exVhActQ",
                "title": "Nice Theoretical Insights, but Not Sure How Experiments  Substantiate the Theory",
                "comment": "The paper theoretically analyzes the sparsity property of the stationary point of layerwise l1-regularized network trimming. Experiments are conducted to show that reaching a stationary point of the optimization can help to deliver good performance. Specific comments follow.\n\n1. While the paper analyzes the properties of the stationary point of the layerwise objective (5), the experiments seem to be conducted based on the different joint objective (8). Experimental results of optimizing (5) seem missing. While the reviewer understands that (5) and (8)  are closely related, and the theoretical insights for (5) can potentially translate to the scenario in (8), the reviewer is not sure whether the theory for (5)  is rigorously justified by the experiments.\n\n2. It is also unclear how tight the bound provided by Theorem 1 is.  Is the bound vacuous? Relevant statistics in the experiments might need to be reported to elucidate this point.\n\n3. It is also unclear how the trade-off in point (b) of the abstract is justified in the experiments.\n\nMinor Points:\npage 2, the definition of $X^{(j)}$, the index of $l$ and $j$ seem to be typos.\npage 2, definition 1, the definition of the bracket need to be specified. \npage 4, the concept of stationary point and general position can be introduced before presenting Theorem 1 to improve readability.\npage 4, Corollary 1, should it be $nnz(\\hat{W})\\le JN k_{\\mathcal{S}}$?\npage 7, Table 2, FLOPS should be FLOP? \npage 8, is FLOP related to the time/speed needed for compression? If so, it should be specified. If not, compression runtime should also be reported.\n\n\n\n\n",
                "rating": 4,
                "confidence": 3,
                "writer": "official_reviewer"
            }
        ],
        "label": "val",
        "gpt4_judgements": [
            {
                "Content Expression": "experimental supports",
                "Sentiment Expression": "are weak",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "a0yodLze7gs": {
        "paper_id": "iclr_2021_a0yodLze7gs",
        "paper_title": "Disentangling Action Sequences: Discovering Correlated Samples",
        "paper_abstract": "Disentanglement is a highly desirable property of representation due to its similarity with human\u2019s understanding and reasoning. This improves interpretability, enables the performance of down-stream tasks, and enables controllable generative models.However, this domain is challenged by the abstract notion and incomplete theories to support unsupervised disentanglement learning. We demonstrate the data itself, such as the orientation of images, plays a crucial role in disentanglement and instead of the factors, and the disentangled representations align the latent variables with the action sequences. We further introduce the concept of disentangling action sequences which facilitates the description of the behaviours of the existing disentangling approaches. An analogy for this process is to discover the commonality between the things and categorizing them. \n      \n      Furthermore, we analyze the inductive biases on the data and find that the latent information thresholds are correlated with the significance of the actions. For the supervised and unsupervised settings, we respectively introduce two methods to measure the thresholds. We further propose a novel framework, fractional variational autoencoder (FVAE), to disentangle the action sequences with different significance step-by-step. Experimental results on dSprites and 3D Chairs show that FVAE improves the stability of disentanglement.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The initial round of reviews showed a consensus among the reviewers that the presentation of the paper was poor, the novelty was unclear, claims were not properly justified, and the experimental evaluation and discussion were quite insufficient. The authors provided a rebuttal and an updated version of the paper. Although the updated paper demonstrated that the proposed approach indeed provides some benefits, it appears that the authors were not successful to address the numerous but constructive reviewers' comments.\n\nThe paper is not ready for publication in ICLR 2021 and can benefit from major revisions and careful proofreading. ",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "0Lnoczyks-X",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Interesting model idea w.r.t. annealing capacity of latent representation, but relation to competing SOTA approaches requires more clarity",
                "comment": "### Summary:\nIn this submission, a common modelling assumption for unsupervised disentanglement is challenged: that the disentangled representation follows the independence structure of the underlying (data generating) factors. Instead, the paper proposes to consider *action sequences* which describe how datapoints are interrelated. The paper provides evidence that the capacity of the latent representation (controlled by Lagrange parameter beta in beta-VAE related models) is related to the significance of particular action sequence for disentanglement. To leverage this insight, the fractional VAE (FVAE) is proposed, consisting of several sub-encoders and different training stages. The disentangling properties of the FVAE is demonstrated on the dSprites and 3D chairs datasets, with the FVAE performing favourably to the beta-VAE w.r.t. the Mutual Information Gap (MIG) disentanglement metric on dSprites.\n\n### Strengths:\n- Novelty / relevance: The submission addresses the important topics of inductive biases and disentangling factors in learning disentangled representations and suggest the interesting and novel concept of action sequences which seems to be related to the general idea of uncovering symmetries with deep latent variable models. In particular, the annealing approach with respect to the KL-divergence Lagrange parameter beta in the FVAE setting to separate \u201csignificant modes\u201d might pose a relevant insight useful in other related approaches and to a more broader audience. \n\n### Weaknesses:\n- Technical quality / significance: The submission mentions the similarities to approaches like AnnealedVAE by Burgess et al. and qualitatively discusses differences and relates some results to this competing approach, but an empirical evaluation of the proposed approach to the competing method is missing. This is quite important, as the technical details of annealing the capacity of the latent representation seem very much alike. Also comparing the disentangling scores to more state-of-the-art approaches like FactorVAE (Kim and Minh) would be important. The evaluation is solely done with respect to the beta-VAE which might not be the most relevant competitor here. For instance, figure 3 in Burgess et al. reports a similar finding as provided in figure 7a in the submission, i.e. controlling the information capacity disentangles first positional / translational factors, then scale and then orientation / shape. Therefore, it is difficult to assess the validity of the claims of the proposed approach and whether a significantly different contribution than in Burgess et al. is made.\n- Figure 5a suggests for dSprites that position/translation, scale and shape are the relevant actions in that order. However, the result in figure 7a suggest, that first translation, then scale and lastly rotation are gradually disentangled which seems to contradict the first result in figure 5a. Shouldn\u2019t these be the same?\n- Figure 6a and 6b are not explained or discussed and their interpretation is not clear. A reader might be familiar with similar plots e.g. in the paper by Higgins et al., but still the key insight should be stated somewhat more clearly in the paper.\n- Clarity: At times it is difficult to follow the presentation of the content in the paper and in some cases I find it hard to follow the statements and conclusions. For instance:\n- Toy example in section 3.1, especially last paragraph: I believe the interpretation of the results in figure 1 requires a little bit more explanation. As I understand it, the ground-truth factors here are the positions X, Y of the rectangles. The dataset provides the variables (i) orientation of the rectangle, (ii) coordinates in either Cartesian or polar coordinate system. I do not quite follow the statement that *\u201c[\u2026] learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).\u201d* In case (A1, A3) I would say the latent representation is the same up to permutation of coordinate axes / rotations, which is inherent to VAE / PCA approaches. I.e. the meaning of the axes would be still the same (up to these transformations). Therefore, I am also not quite sure about the statement: *\u201cAs we have shown in Sec. 3.1, the orientation of the rectangle can affect the direction of the disentangled representation\u201d* (p. 5). The \u201cdirection\u201d of the representation is less relevant, as the interpretation of the axes is still the same. However, I might miss the point which is tried to be made here. Could the authors comment on that?\n- Definition of action sequence, section 3.2: The paper tries to motivate \u201caction sequences\u201d but in my opinion the notion remains somewhat unclear in an abstract setting. A \u201cmeaningful action sequence\u201d is defined as *\u201ca sequence / ordered permutation of elements from a subset of the dataset, which reveals the relationship among the elements\u201d*, with elements being images here. In a simple example as scaling or translating objects, this notion and the distinction to \u201cground-truth factors\u201d might be clearer. However, in more complex / less structured examples, say images of faces, the difference between \u201caction sequence\u201d and \u201cground-truth factors\u201d is not very clear to me. The paper suggests for a more formal definition to consider Higgins et al. but, in order to be self-contained and clear, a more explicit and formal definition of this notion is required in the paper, in my opinion. Could the authors maybe provide a more formal definition? \n\n### Additional Feedback:\n- Page 6 and figure 3: *\u201cPlease note that the maximum for both are reached when theta=90 and L is at its maximum.\u201d* Figure 3 suggests that the maximum (yellow region) is reached for large L and theta close to 0 or about 180. It seems that there is a discrepancy between the description and the figure.\n- Figure 5, page 7: In 7b the legend specifies integers, but it is not clear, what these integers encode. And is it maybe *\u201cKL divergence vs beta\u201d* (-> *\u201dy against x\u201d*) in the caption?\n\n- Abstract (p. 1): I would suggest rephrasing the following sentence:  *\u201cWe demonstrate the data itself, such as the orientation of images, plays a crucial role in disentanglement and instead of the factors, and the disentangled representations align the latent variables with the action sequences.\u201d*\nMaybe get rid of the first *\u201cand\u201d* as well as making clear what *\u201cfactors\u201d* (maybe rather *\u201cground-truth / separating factors\u201d*?) are meant. On the first read, this sentence was quite confusing to me.\n- Introduction (p. 1): Second sentence, *\u201cthinking\u201d* -> *\u201cthink\u201d*.\n- Introduction (p. 1): Third sentence, *\u201c[\u2026] single glance this is because [\u2026]\u201d* -> *\u201c[\u2026] single glance. This is [\u2026]\u201d*.\n- Introduction (p. 1): Notion paragraph first word, *\u201cthe\u201d* -> *\u201cThe\u201d*.\n- Introduction (p. 1): Notion paragraph, *\u201c[\u2026] a question arise here is [\u2026]\u201d* -> *\u201c[\u2026] a question which arises here is: [\u2026]\u201d*.\n- Figure 1, caption: *\u201cleaned\u201d* -> *\u201clearned\u201d*.\n- Section 3.3, incomplete sentence after equation 5 or unnecessary *\u201c,\u201d*.\n- Section 4, page 6: *\u201c[\u2026] leading to the disentangling process decays [\u2026]\u201d* -> *\u201d[\u2026] decaying [\u2026]\u201d*\n- Section 4, page 6: *\u201c[\u2026] targeted action into the leaned codes.\u201c* -> *\u201c[\u2026] learned [\u2026]\u201d*\n- Figure 7, page 8: Full stop *\u201c.\u201d* missing in the last sentence of the caption.\n\n### Recommendation:\nIn general, the paper deals with relevant issues in learning disentangled representations and provides interesting tools to address some of these aspects. In particular, the annealing procedure in the FVAE is potentially a relevant contribution. However, the relation to similar approaches is not evaluate adequately, in my opinion, which makes it difficult to assess the justification of some claims. Also, a careful revision of the submission seems advisable which might clarify some of the aspects raised above. In the current form, I believe that the paper is not ready for publication and I would rather see this submission rejected. Nevertheless, I am willing to reconsider my rating if the authors are able to address some of the concerns and questions raised above.\n\n### Post-Rebuttal:\nI want to thank the authors for their responses and clarifications. I think the revision already improved the quality of the submission quite a bit. However, I still believe that there are some aspects which need a better presentation and clearer discussion. \n\nFor example, a more direct discussion and (empirical) comparison to other approaches like AnnealedVAE is necessary, as also other reviewers pointed out, to justify the points made (qualitatively) in the paper. The added results in figure 6c already provide results in that direction.  \n\nI appreciate the clarifications in the notions of action and action sequence. Although I agree that the notions are comprehensible in the toy example and dSprites setting, I still think that the point I raised in my initial review applies. In order to provide a well-defined notion a more formal definition is required. To me it is still unclear what an action sequence in the case of e.g. images of faces should be.\n\nI genuinely believe that the proposed approach might pose a relevant contribution but the paper lacks an adequate presentation at the moment, in my opinion. Therefore, I stand with my initial recommendation that this submission is not ready for publication and I endorse rejecting the paper. However, I would like to encourage the authors to do a major revision taking the issues raised by the reviewers into consideration and to submit again.\n\n\n### References: \n- Higgins et al., \u201cbeta-VAE: Learning basic visual concepts with a constrained\nvariational framework\u201d, ICLR 2017.\n- Kim and Mnih, \u201cDisentangling by factorising\u201d, ICML 2018.\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "02b3uKKfUTh",
                "reply_to": "mwf_pCynhc9",
                "title": "Response to Reviewer1",
                "comment": "Thanks for your detailed feedback and the insightful reviews!\n\n> What's the KL divergence here? \n\nThat's right. The KL divergence is the regularization term of the VAE objective.\n\n> KL divergence is consistent with that of entropy.\n\nBasically, the relationship between the KL term and entropy has not yet been verified. We admit that mainly is a hypothesis, though we conduct two experiments in Fig. 3 and 11. However, the current works, such as AnnealedVAE and PCA Directions indicate similar results that different actions have different thresholds. Indeed, such a conclusion can hardly induce; hence, abundant experiments are required. However, the available datasets are insufficient to verify it.\n\n> What's definition of the label here?\n\nThe dSprites is an artificial dataset and contains the factors information. The labels are ground-truth factors. Although our method is an unsupervised approach, the calculation of action's entropy needs label information.\n\n> The authors arranged three stages for dSprites. \n\nThe direct answer is prior to this dataset. We already know it consists of five factors, and two are similar (posX, posY); the shape is not a good action. Besides, we also provide an annealing test in Sec 4.6 to no prior information case. Increasing the number of stages has little influence on disentanglement, but it does waste the computational resource.\n\n> training objective function\n\nThe objective function is beta-VAE. The only difference is the training process.\n\n> How are the curves in Fig. 5 derived?\n\nEach line denotes the dimensional KL diverge over \u03b2 increasing. For the supervised case, we can name the dimension by its most informative factor. For the unsupervised case, we show their index of dimension.\n\n> Thus, the following three-stage training process is questionable.\n\nFor now, FVAE needs the participation of humans. It may not be a bad idea because unsupervised disentanglement learning without inductive biases is impossible. The main purpose of this work is not to propose a competent disentangling method. We focus on the interpretation of why VAEs can disentangle. In this work, we try to provide an insight into combining the data and the representation, and the thresholds could help disentanglement.\n\nReference:\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.\n- Michal Rolinek; Dominik Zietlow; Georg Martius: Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "xZGTs_fBrux",
                "reply_to": "uCwzsSi3Am5",
                "title": "Response to Reviewer4",
                "comment": "Thanks for your detailed feedback and the insightful reviews! \n\n**Ambiguous definitions**\n\n1. Action : *the continuous set of images over a certain direction.*\n2. Action sequence: the discrete action or a sequence of sampled images from the action.\n3. Generating action sequence: the action sequences traversing the ground-truth factors.\n4. Learned action sequence: the action sequences traversing the latent variables.\n5. Entropy of action:\n\n$$H(S') = - \\frac{1}{N}\\sum_{x_i \\in S'} \\mathrm{log} \n        (\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp^{-\\frac{(x_i-\\bar{X})^2}{2\\sigma^2} })$$\nwhere $S'$ is the set of an action, $x$ is the sampled images form this action, $\\bar{X}$ is the mean of the action.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mxmoeU86x3q",
                "reply_to": "t4Js5vRgHqo",
                "title": "Response to reviewer 3.",
                "comment": "Thanks for your detailed feedback and the insightful reviews!\n\n**The difference between Annealed VAE**\nThe interpretation of disentanglement: AnnealedVAE argues that the information bottleneck enforces the model to encode \"the most significant improvement in data log-likelihood,\" which leads to disentanglement.  In contrast, we claim that a high regularization penalty on KL divergence prevents the insignificant action sequences from being encoded. In other words, the key to disentanglement is learning information solely.  \n\n> Would this help disentangle position at first then orientation of rectangles?\n\nWe are sorry for the confusing description that A1-3 have the same two-dimension factors denoting images' position. The orientation of images is a fixed property of the dataset. \"The most significant improvement\" should have the largest variation, which can also understand from the PCA theory. In fact, the current theories (information bottleneck, PCA Directions) support the results of A1-3. The action sequence moving along the rectangle's short side has the largest variation and improves the log-likelihood the most significantly. Hence, we say disentangling action sequences is a proper description of disentanglement.\n\n> a formal definition of the inductive bias is still unavailable.\n\nThough Burgess and Rol\u0131nek indicate the inductive bias, they don't propose a calculation for that. \n\n> existing models disentangle the ground-truth factors by accident.\n\nRol\u0131nek used a similar expression, \"Variational Autoencoders Pursue PCA Directions (by Accident).\" That means the success of the current approaches (beta-VAE, TC-VAE, AnnealedVAE, DIP-VAE, FactorVAE) mainly contributes to the well-designed dataset. For instance, they fail to disentangle in the cases of A1-3. If the disentangled representation depends on the data, these approaches don't guarantee the disentanglement when facing an unknown dataset.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LvWAinpHjnj",
                "reply_to": "0Lnoczyks-X",
                "title": "Response to Reviewer 2",
                "comment": "Thanks for your detailed feedback and the insightful reviews! We feel sorry about failing to show the comprehensive results of our work. We hope our responses correctly answer your concerns.\n\n**The different contributions** \n1. The interpretation of disentanglement: AnnealedVAE argues that the information bottleneck enforces the model to encode \"the most significant improvement in data log-likelihood,\" which leads to disentanglement.  In contrast, we claim that a high regularization penalty on KL divergence prevents the insignificant action sequences from being encoded. In other words, the key to disentanglement is learning information solely. \n2. As far as we know, we are the first to define the inductive biases on the data and associate it with disentanglement. Though Burgess and Rol\u0131nek indicate something similar, we give a former and explicit definition. \n3. We improve disentanglement by solving re-entanglement.\n\n**Explanation of toy examples** We want to show some evidence of inductive biases on the data in this part. The learned action sequences should match the generating action sequence precisely for the popular view of disentanglement learning. However, the experimental results of A2 and A3 reveal that the current disentangling approach learns a significant action sequence or the principal component on the data.  We will update this figure for easy understanding. A1 and A3 have the same generating action, but the model learns two different action sequences. In contrast, A1 and A2 have different generating actions, but the model learns similar action sequences.\n\n**Contradiction** The orders should be the same if they are all actions.  However, there are only three types of shape, eclipse, square, and heart.  We don't feel surprised by this result because *shape* is not an action like others having internal frames. An action should consist of a series of continuous images. \n\n**Notion** Action: The continuous set of images over a certain direction. \n\nThis notion denotes the real action in reality, i.e., a ball falls. However, the action is infeasible for the machine, and we have to sample the discrete action as *an action sequence*. Therefore, a subset of the dataset varying one factor is an action sequence. We call the action sequences generated by the ground-truth factors *generating actions* or just *actions*, and the reconstructed sequences by the decoder *learned action sequences* or just *action sequences*.\n\n\nReference:\n- Higgins et al., \u201cbeta-VAE: Learning basic visual concepts with a constrained variational framework\u201d, ICLR 2017.\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.\n- Michal Rolinek; Dominik Zietlow; Georg Martius: Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019.\n- Francesco et al., Challenging common assumptions in the unsupervised learning of disentangled representations. In 36th International Conference on Machine Learning, ICML 2019.\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "knDDkRot8Q8",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Response to All Reviewers",
                "comment": "I apologize for my poor writing skills and all the defects of our paper. It seems necessary to reclaim the motivation of this work. This paper's main purpose is to emphasize the importance of the data itself on disentanglement learning. We argue that a proper definition of disentanglement is disentangling explanatory action sequences because an isolating sample and its representation are insufficient for disentanglement or interpretability.  The internal relationships between the samples are the key to understanding and disentanglement.\n\nThe current works mainly try to interpret disentanglement from the model perspective.\n1. AnnealedVAE claims that an information bottleneck enforces the model to find a local minimum for the objective, and \"which are aligned with factors of variation.\" It suggests that increasing information on latent leads disentanglement.\n2. Rol\u0131nek shows the similarity between PCA and VAE about \"the local behavior of promoting both reconstruction and orthogonality.\"\nHowever, we believe that the data plays a primary role, and the model is secondary. Therefore, we examine the effects of the data by four cases in Sec. 3. The special cases show little correlation between the learned representation and the ground-truth. Though PCA-like behaviors can interpret the A1-A3, it needs more explanations to interpret A4. The other difficulty is measuring the principal component quantitatively. Dividing the dataset into action sequences also helps us calculate the entropy of actions or the variation of components.\n\nOur method is similar to AnnealedVAE both in results and the method; nevertheless, the interpretation is the main difference. We argue that the step values of beta instead of gradually modifying are vital to disentanglement. The other reason is the phenomenon of re-entanglement. The disentanglement metric reaches the highest in the middle phase, and it falls on the last phases. Here is the MIG score of one trail (beta-vae):\n\n| Step | discrete_mig        |\n|------|---------------------|\n| 230  | 0.3756360504736129  |\n| 461  | 0.37883395407382375 |\n| 693  | 0.399240366607318*  |\n| 924  | 0.3772557544010944  |\n| 1156 | 0.3333029456260484  |\n| 1387 | 0.3523813802097221  |\n| 1618 | 0.3801707492631984  |\n| 1850 | 0.349180837978194   |\n| 2081 | 0.3663198905598549  |\n| 2313 | 0.34369199263536326 |\n| 2544 | 0.3663410570974127  |\n| 2775 | 0.353156190268057   |\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mwf_pCynhc9",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Algorithm is not clearly explained and more experiments are needed.",
                "comment": "Summary:\n\nThe authors proposed fractional variational autoencoder (FVAE) for the learning of disentangled representation where the action sequences can be extracted step-by-step. Experiments are shown to illustrate how the algorithm works.\n\n#################\n\n\n. The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. Is it the same as the objective of \\beta-VAE?\n\n. Fig.3: 1) What's the KL divergence here? Is it between the posterior and the prior? 2) It's claimed that the trend of KL divergence is consistent with that of entropy. But it is hard to see from Fig. 3. 3) It is claimed that the significance of action is related to the capacity of learned latent information. Based on Fig. 3, this conclusion is not convincing. Also, Fig. 3 is obtained based on a toy dataset. To claim it as a main contribution, the conclusion needs to be verified on other datasets as well.  \n\n. Section 4.1: What's definition of the label here? It's not clear. Is it like the types of shapes on dSprites?\n\n. Section 4.1: The training on dSprites includes two phases: find thresholds and then train different stages. 1) The authors arranged  three stages for dSprites. This seems arbitrary. Why not four or five stages? 2) What's the training objective function of each stage? 3) How are the curves in Fig. 5 derived? More explanation is required.\n\n. Section 4.2: It is claimed that ``One can recognize three points where the latent information suddenly increases: 60, 20, 4.'' This is hard to see from Fig. 5b) as all curves look smooth. Thus, the following three-stage training process is questionable. The training for unlabeled task needs more study.\n\n. The experiments are limited. There are a lot of papers regarding disentangled representation, and the authors only compared with \\beta-VAE. \n\n\n",
                "rating": 2,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "t4Js5vRgHqo",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Disentangling action sequences is interesting but more details and experimental results are needed. ",
                "comment": "Summary: \nThis paper addresses the problem of disentangling representations using Variational Autoencoders. In particular, the authors introduce the concept of disentangling action sequences and propose the fractional variational autoencoder framework to disentangle them step-by-step. To this end, they analyze the inductive biases on the data and define latent information thresholds which are correlated with the significance of the actions.\n\n##################################################################\n\nStrengths:\n- The paper tackles the important problem of disentangling representations.\n- Overall, the paper is well structured. In particular, the introduction section clearly motivates the problem and summarises existing approaches.\n- The idea of disentangling action sequences is interesting as it allows to analyse the inductive bias on the data.\n\n##################################################################\n\nWeaknesses:\n- Fractional variational autoencoder (FVAE) proposed in this paper is closely related to the work of Burgess et al. (2018). Although authors include a brief discussion comparing both methods, the main novelty of FVAE (i.e. explicitly avoid mixing the factors and defining thresholds to prevent re-entanglement for extremely high capacity) is still not sufficiently emphasised throughout the paper. Moreover, it would be good to include experimental comparisons to Annealed VAE (for instance in Figure 6) to give more insights on the relevance of the proposed approach.\n- Description of the toy dataset family is not easily understood and it would be good to clarify annotations in Figure 1(a). Since Figure 9 of Appendix is clear enough, it might be nice to include it in the main paper to help the reader follow the analysis of the corresponding experiment. In the latter, it is shown that the disentangled representations are not invariant to orientation of rectangles (A1, A3). Here, one can assume that positions x and y and orientation of rectangles contribute differently to reconstruction. Hence, it would be interesting to see the effect of progressively increasing the bottleneck capacity on the obtained representations, as proposed in Burgess et al. (2018). Would this help disentangle position at first then orientation of rectangles?\n- While a quantitative analysis has been provided in Figure 6 using the MIG metric to compare FVAE and Beta-VAE, it is still insufficient to make clear conclusions on the performance of the proposed method. Several metrics (e.g. Mutual Information Gap, Modularity, etc.) and evaluation benchmarks have been integrated in DisLib (Locatello et al. (2019)) allowing easy evaluations of disentangling approaches. I recommend using it for further quantitative analysis. \n- In Section 2, authors present the concept of disentangling representations and describe the work of Locatello et al. (2019) which shows the necessity of inductive bias to unsupervisedly disentangle the underlying factors. After mentioning that \u201ca formal definition of the inductive bias is still unavailable\u201d, this point would be more clear with some examples, for instance the assumption in Burgess et al. (2018) that Beta-VAE aligns latent dimensions with components that make different contributions to reconstruction.\n- In Section 3.1, authors mention that existing models disentangle the ground-truth factors by accident. This would be a little misleading to previous claims on the role of inductive bias on the data (or the model) which allows to achieve disentanglement  Locatello et al. (2019). I suggest more clarification to this point.\n",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "uCwzsSi3Am5",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "This paper gives a new kind of comprehension to disentangled representation learning. In this paper, existing unsupervised disentangled methods are trying to obtain disentangled action sequences instead of independent factors. Through the proposed FVAE model, action sequences with different levels of significance can be obtained step by step, and experiment results show that the weight \\beta has positive correlation with the significance.",
                "comment": "Pros:\n1. This paper gives a new comprehension of existing unsupervised disentangled representation learning method, regarding it as finding commonality of input data and disentangling action sequence information in factors.\n2.  This paper gives a new idea that \\beta has positive relationship with action significance and conduct an experiment to validate it.\n3.  This paper proposes a new variant of VAE called FVAE which learns the disentangled action sequence step by step.\nCons:\n1.  This paper mainly gives descriptions of insights while lacking some formulations to explain the settings and methods better.\n2.  Experimental results are not well organized, some axis lack corresponding labels, like Figure 5. Figures are not clear, like numbers in Figure 1 (a). \n3.  Some definitions are ambiguous, like x in equation 5. \n4.  Some descriptions in the paper are confusing, e.g. \u201cWe argue that the factors are not the key to disentanglement since the learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).\u201d This experiment, from my perspective, shows that the learned factors are disentangled in a particular form which is not consistent with the preset ground truth. And different action sequences are also different factors. Section 3.1 might be described in a more considerate way to show what the experiment results really indicate. \n5. There exist some typos in this paper, like \u201cleaned\u201d for \u201clearned\u201d.\n\nOverall review:\nThis paper gives a new comprehension of existing disentangled representation learning by regarding it as finding disentangled action sequences, which is interesting and has some good insights. However, some ideas should be supported by clearer formulations and some conclusions of experiments are not valid. Moreover, the logic of this paper is a little unclear, and experimental figures are incomplete. With some modifications, this paper could be an excellent paper..\n",
                "rating": 6,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "e2zns8ME8EK",
                "reply_to": "iclr_2021_a0yodLze7gs",
                "title": "Review 5",
                "comment": "Manuscript Summary\n====\n\nThis paper constructs a disentanglement problem from a temporally causal view, where data are observed in sequences, and where actions cause those observations to change as the sequence progresses in time. Their stated objective is to recover the specific actions and their parameters (e.g. rotation and translation, and their magnitudes/signs).\n\nThe authors thus construct a \"Fractional VAE\" (FVAE), and then construct sequences from Dsprites and Chairs based on their statement of the problem.\n\nInitial Decision (from this reviewer), Review, and Reasoning\n====\n\nI think this paper should be rejected; were this a journal, I would suggest at least major revision.\n\nOverall the concept of bringing temporal causality (which for some cases *is* valid as the causal diagram/frame) into the disentanglement problem statement is a good idea. However, after that point in the manuscript, I cannot understand what has done. For example, section 2 is a restatement of previous work, and section 3 begins with an explanation of the dataset construction. Section 3.3, section 4, and Figure 4A I think describe the paired asymmetric autoencoder method, but a sparse few paragraphs are given at this point. What they _do_ describe is a set of \"sub-encoders\" with varying compression rates $\\beta$. However, beyond varying the rates, it's not clear how particular \"ground truth\" factors (e.g. $\\theta, L$) can be selected for and locked in to specific latent factors in an unsupervised manner, or even if this should happen.\n\nBy varying $\\beta$ we receive different amounts of information in the representation, but how can we ensure across \"learning phases\" disentanglement? Further, if these KL divergences are set to different $\\beta$, this means we don't have a divergence for the joint representation? (the concatenation of the sub-encoders) So how can we ensure that these are disentangled themselves? While successively learned encodings would optimally not include previously encoded data, why would these encoders learn separate concepts instead of coarse grained representation with all concepts to successively finer representations (or refinements to those coarse grained representations) with each successive sub-encoder.  Or are these separate phases repeated?\n\nPerhaps these questions have answers in the positive, but they should be answered by the manuscript.\n\nI further cannot make a connection between the actions sequences and the training methods/arch. I think I have understood both (...save for the above highlighted problems), but I cannot understand where the sequences come in practically speaking, even modulo the aforementioned issues. How does the FVAE or its training scheme use this information? Does it use this information?\n\nI think the positive experimental results in Figure 6c mean that there is something here. However, I cannot tell given section 4 what is actually being done.\n\nSuggestions\n====\n\nI suggest a clear procedure section with numbered steps. It is of vital importance that the reader understand what has been done. If it is already there, it should be made much more obvious/clear.\n\nI think the connection between sequences of images under actions and the proposed method needs to be made, or, if I missed this connection, should be made clear.\n\nThe initial portion of section 3 concerning dataset construction might also be moved to much later.\n\nThere are philosophically challenging sections which I did not comment on in the review portion. I think these are approximately orthogonal to the method due to the scope of the problem: rotation/translation may be disentangled. Can dog breeds *be* disentangled, even in theory (example from Section 3)? Disentanglement of simple mechanisms/\"actions\" are perfectly acceptable at least in my opinion for the state of the field at this moment. Using more complex examples may not be helpful. Similarly, the discussion in section 1 raises questions that are unrelated to the later method. Since a literature review is undertaken in Section 2, the paper could have started at \"In this paper, we first demonstrate that instead of the ground-truth factors the disentangling approaches [should] learn [disentangled] actions.\", with an update for phrasing.\n\nI would also give the paper another read through for grammar.",
                "rating": 3,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "test",
        "gpt4_judgements": [
            {
                "Content Expression": "the presentation of the paper, the novelty, claims, the experimental evaluation and discussion",
                "Sentiment Expression": "was poor, was unclear, were not properly justified, were quite insufficient",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Strong negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the updated paper, the authors",
                "Sentiment Expression": "demonstrated that the proposed approach indeed provides some benefits, were not successful to address the numerous but constructive reviewers' comments",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "is not ready for publication in ICLR 2021 and can benefit from major revisions and careful proofreading",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "DGIXvEAJVd": {
        "paper_id": "iclr_2021_DGIXvEAJVd",
        "paper_title": "Learning Chess Blindfolded",
        "paper_abstract": "Transformer language models have made tremendous strides in natural language understanding. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess.  Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that chess notation itself allows for directly probing the world state, without requiring any additional probing-related machinery.  Additionally, we have access to a vast number of chess games coupled with the exact state at every move, allowing us to measure the impact of various ways of including grounding during language model training. Overall, we find that with enough training data, transformer language models can learn to track pieces and predict legal moves when trained solely from move sequences. However, in adverse circumstances (small training sets or prediction following long move histories), providing access to board state information during training can yield consistent improvements.",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "I thank the authors for their submission and very active participation in the author response period. World state tracking is an important problem that encompasses existing problems like coreference resolution. I agree with R2 and R3 that proposing a novel environment in which we can investigate to what extend Transformers can tackle world state tracking should be interesting to the community. The majority of the reviewers agree that this paper presents an interesting benchmark [R2,R3,R4] with good thorough experimental work [R1,R2,R4]. However, R1 is confused about the positioning of the work and R4 finds the work narrow. R2, despite positive review, agrees with this assessment. I agree with this assessment as well and, after discussion with the program chairs, came to the decision that this paper is not ready for publication in its current state. I strongly encourage the authors to incorporate R1's and R4's feedback, in particular with respect to positioning this environment in comparison to TextWorld, and resubmit to the next venue.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "BMMGNmlo1ge",
                "reply_to": "QmDeyu3SOIP",
                "title": "Comparison with other frameworks and contributions of the work ",
                "comment": "We thank the reviewer for their appreciation of our effort and the increase in score. \nOur claims about the frameworks were based on the intended use case. While it may be possible to use frameworks like TextWorld to just predict observations, this is not what the original work or the follow-up work has done. Moreover, it is not clear to us that TextWorld-like environments allow for probing the model in the same way.\n\nOur work demonstrates that in chess we have a tailor-made domain where models can be trained with the language modeling loss, and evaluated on world state tracking via simple prompts. We are not aware of any other framework that readily allows for this. Moreover, our contributions are not just limited to proposing this new framework. The results of using the proposed framework provide insights into how effective transformers are at tracking the world state, their robustness to input perturbations, and, perhaps most importantly, the remaining challenges facing transformer LMs even in this relatively simple domain.\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LQmJrTEqC5a",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Review 1",
                "comment": "Summary: This paper explores the abilities of transformer-based models to do grounded state tracking via chess. They train a GPT-2 on chess games, showing that it can learn the rules of the game by being able to state-track and predict valid next actions.\n\nPros:\n1. A very thorough set of experiments are given to explore how transformers can be used to track states in a game such as chess and results show that with enough data, transformers can predict the locations of pieces across a decent amount of history as well as predict legal actions.\n2. The paper is well-written in terms of general writing clarity and I was able to follow *what* was happening throughout.\n\nCons:\n1. I am a bit lost as to the motivation and positioning behind the paper, i.e. I was unsure as to *why* things were happening as they were. The authors say that they are using transformers to see how transformers can learn grounded language when world states are available but I do not see this work positioned with respect to other work on grounded language nor work on state tracking generally found in model-based RL.\n(i) An example of the former for grounded language learning (given that they cite Bender and Koeller) would be instances such as vision and language navigation (Anderson et al. https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf), the Nethack Learning Environment for game grounding (Kuttler et al. https://arxiv.org/abs/2006.13760), or text games (Cote et al. https://arxiv.org/abs/1806.11532 and Hausknecht et al. https://arxiv.org/abs/1909.05398). I am not sure how state tracking in chess implies that transformers can do grounded language learning.\n(i) In terms of just the state tracking parts, there has already been much work in agents that learn the rules of the world as they play through them. This can happen with World Models (Ha and Schmidthuber https://arxiv.org/abs/1803.10122) or in cases like Alpha-Zero (Silver et al. open access version https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd) which learn the rules of Chess from scratch via self play. How does using a transformer compare to these works?\n2. Given the above and the fact that all of the pieces of the methodology of this work are taken from others (the architecture, training, state representation, etc.) - the main contribution is the experimental design and the results themselves. In this case, I would have liked to see more analysis regarding exactly what properties of the transformer they think is responsible for helping the model to learn and also a potential qualitative analysis of what the failure cases are.\n\nOverall, the paper has some interesting ideas, experiments, and results but is not connected to the motivation/is not positioned well with respect to closely related work.\n\nPost author response:\nSee comment below for further score justification.",
                "rating": 5,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "QmDeyu3SOIP",
                "reply_to": "dKBUxLgokNR",
                "title": "Response to Rebuttal",
                "comment": "I appreciate the authors efforts to clarify my questions and revise their manuscript.\n\nI am satisfied with the answers given differentiating this work from Alpha Zero as well as the additional experiments performed. I would contend though that the differences with the other environments I have provided in point 1 of my initial review are not sufficient. The three dimensions given with respect to differences with TextWorld (and hold for the other envs too) are not entirely accurate. There is nothing in the framework itself that focuses on reward maximization instead of next state probability - its the same as having a chess simulator where you can either focus on predicting the next state or just have an external reward the indicates whether or not you've won the game. It is possible to generate oracle traces, etc. equivalents to this chess dataset in most of these frameworks.  Overall that is to say, chess can also be framed in exactly those three terms and given that these are frameworks and not agents, you cannot say that these three dimensions hold. \n\nThis being said, in appreciation of the author's efforts for the other clarifications - I will increase my score to a 5.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "H6RiRjk3IyA",
                "reply_to": "1Gvj7GQWLUA",
                "title": "Follow up",
                "comment": "We appreciate your positive comments. \n\nTo reiterate the wider implications of this work, our results shed light on the following interesting properties of transformers: (a) they are robust to RAP-like changes in input distribution, and (b) they require access to long context (Appendix C.1), and large training sets. Future work can use the first finding to introduce the world state, or more specifically the output of linguistic analyzers such as coreference, via RAP-like tokens during pre-training and fine-tuning of transformers. RAP-like tokens can also be used for debugging/diagnosing the model\u2019s understanding, similar to the starting square prediction tasks. The second finding can be another motivation to search for new architectures that are adept at understanding long text with access to limited history (Rae et al., 2020), and that require small training sets. The framework we have introduced allows for probing and understanding new architectures that address these challenges. \n\nAdditionally, we confirm in Appendix F that transformers are more robust than LSTMs to changes in input distribution due to RAP. This is because unlike LSTMs/RNNs, transformers have only a weak dependence on positions via position embeddings.  This makes us optimistic about the proposed future work direction. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "a_wNUuuNIz5",
                "reply_to": "1F2i8m95k5K",
                "title": "Additional comments on RAP",
                "comment": "We wanted  to add a couple of more findings on RAP:\n* In RAP, piece types are not added during inference. Though in earlier experiments, which we didn't report, the RAP models used piece types during inference and it certainly helped the model (as suggested by the reviewer). The reasons for not using piece types during inference have already been explained in the previous reply. Note that the oracle model has access to all the piece types during training and inference. \n*  Additionally, we confirm in Appendix F that transformers are more robust than LSTMs to changes in input distribution due to RAP. This is because unlike LSTMs/RNNs, transformers have only a weak dependence on positions via position embedding. \n\nHope this helps. Let us know if any further clarifications are needed. \n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "cLH-0kgzCF",
                "reply_to": "_AD5WkeYFtW",
                "title": "Follow up",
                "comment": "We thank the reviewer for their increase in score. \n\nRegarding the wider implications of this work, we want to say that our results shed light on the following interesting properties of transformers: (a) they are robust to RAP-like changes in input distribution, and (b) they require access to long context (Appendix C.1), and large training sets. Future work can use the first finding to introduce the world state, or more specifically the output of linguistic analyzers such as coreference, via RAP-like tokens during pre-training and fine-tuning of transformers. RAP-like tokens can also be used for debugging/diagnosing the model\u2019s understanding, similar to the starting square prediction tasks. The second finding can be another motivation to search for new architectures that are adept at understanding long text with access to limited history (Rae et al., 2020), and that require small training sets. The framework we have introduced allows for probing and understanding new architectures that address these challenges. \n\nAdditionally, we confirm in Appendix F that transformers are more robust than LSTMs to changes in input distribution due to RAP. This is because unlike LSTMs/RNNs, transformers have only a weak dependence on positions via position embeddings.  ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "o_tBTgw7vE3",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Paper Revision Overview",
                "comment": "We thank all the reviewers for their valuable feedback which we have tried to incorporate in the current revision. This comment is intended to give a consolidated view of all the changes made:\n* We have substantially revised the related section work based on Reviewer 1 and 4's feedback.  We have tried to specify more clearly the contributions of our work in the revised introduction section. \n* \"Legal moves\" are now denoted as LgM to avoid overloading of LM (Reviewer 2\u2019s suggestion), and \"Exact moves\" use the ExM acronym rather than EM.  \n* A detailed analysis of illegal moves in Appendix D. \n* Added a random legal move baseline for exact move evaluation as suggested by Reviewer 2.\n* The entire experimental setup is available at [this URL](https://anonymous.4open.science/r/f0c718e1-16af-4f1a-a129-d4ace9ac6820/)  \n* Variations over the base transformer architecture (GPT2-small) are explored in Appendix C. Specifically, we report results with limited attention window + bigger models. We find that the success of the transformer model in our setting relies on access to the whole history, and the model suffers performance drop with limited attention window. This suggests that the model is not able to learn a compressed state representation even though chess is markovian and the board state can be described in less than 1K bits.   \n* We have made a minor change to the \"oracle\" baseline which has improved its performance. In the earlier version, we used a limited attention window since the language model doesn\u2019t have to track the board state as it\u2019s already provided. There were empirical reasons as well, the model with limited attention window converged faster and hence did better than the model with access to the full history in the earlier stages of training. But we later found that the model with limited attention converged to an inferior perplexity than the one with access to the full attention history. We have updated the oracle model\u2019s description and the corresponding numbers. (More details in response to Reviewer 3)\n* We plan to publicly release the trained models via the [Hugging Face modelhub](https://huggingface.co/models)\n\nFinally, we want to reiterate the contribution of our work (quoted from revised Introduction):\n* Propose chess as a testbed for evaluating world state tracking capabilities of language models.\n* Show that by selecting (and tweaking) the appropriate chess notation, we can probe the language model for aspects of the world state using simple prompts (Section 3).\n* Propose a suite of probing tasks to evaluate language models for chess on world state tracking (Section 5.3). These probing tasks go beyond simple exact match, and use a more fine-grained evaluation, and allow for automated error analysis (Appendix D).\n* Show that given enough training data, transformer language models can learn to track piece locations and predict legal moves at high levels of accuracy.\n* Evaluate the effect of grounding by training and evaluating a spectrum of transformer language models with varying degrees of access to the world state. We find that grounding helps in challenging settings of our proposed probing tasks.\n* Provide insights on transformer language models such as their robustness to incorporating the world state in various ways, their dependence on access to long context, etc.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "_AD5WkeYFtW",
                "reply_to": "-BVhGmQwBEa",
                "title": "Updated score to 5 in light of code release + experimental clarifications, decreased confidence to 3 since score is mostly a function of the narrowness of the contribution, which is more subjective.",
                "comment": "In light of the author response, I have decided to increase the score to 5. I have also decreased my confidence to 3.\nThe main reasons for this score increase are the release of code and data as well as thoughtful clarifications on the experimental setup. This is good experimental work. I also think Appendix C.1 is a good first step towards drawing wider scientific conclusions from this work.\n The main reason not to increase the score further is that I believe the contribution still is quite narrow.  \n\nI chose to decrease confidence in my evaluation since it is now based more on the narrowness of the contribution, which is harder to assess, than on the experimental validity of this work.\n",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-EaJH0Qn9dR",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Interesting new benchmark that would benefit from more connections to the literature. ",
                "comment": "\t\n### Topic\nThis paper explores learning chess from raw notation as a benchmark for the ability of language models to track world state. Chess is an interesting benchmark, as a set of moves can be unambiguously linked to a world state, there are large amounts of data available and the model can easily be probed for its board tracking abilities. The contributions of this paper are twofold: (i) introducing blindfolded chess as a benchmark for grounded language learning and world state tracking, as well as a suite of probing tasks to evaluate models; (ii) empirical evidence that transformer language models can learn both the rules of the game and to track board state.\n\n### Pros:\n-\tBlindfolded chess is an interesting benchmark for grounded language learning and as a testbed for models to track world state. It is unambiguous, data-rich, has a limited vocabulary and models trained on it can easily be probed. This adds to prior papers on learning chess with transformers that have mainly focused on the performance of such models.\n-\tThe use of SAN + RAP trick is an interesting one to be able to probe the current location of pieces.\n-\tThe oracle model is interesting, as it demonstrates the gap between a model that must track world state and one that has access to it. The multi-view model is also interesting as an example of how additional supervision may help the model.\n-\tThe analysis across different data sizes and game length is interesting.\n\n## Cons:\n-\tThe paper feels rushed at times: for instance, there are no references in text to Appendix D, despite it being an interesting demonstration of the analysis that can be done in this environment. \n-\tThe comparison to related work on world state tracking and grounded language learning could be substantially improved. The authors only very briefly mention previous work: the papers on the TextWorld environment (Cote et al, 2018), seem relevant, although the framework is that of interactive environments and RL. Other papers on grounded language learning also seem relevant, such as Alexander G. Ororbia II et al, 2019. More so than the lack of references, an issue with this paper is that few connections are made to wider research issues in grounded language learning / implicit world state tracking. Instead, the text tends to simply state experimental results. This makes the contribution of this paper very narrow.\n-\tIt is not clear why the multi-view trained models underperform on the low data settings compared to the models without the extra supervision. If there is more explicit supervision of the model, should it not be better at tracking world state?\n-\tIt would be good to make clear if this dataset and probing tasks will be released, and the code made available. This would be a welcome step in standardizing this new domain, as prior approaches have all used different settings/datasets/notations/evaluations.\n-\tDespite this being a GPT-small architecture and the authors using a small subset of the available training data, the accuracies are high for legal move predictions on Train L. Those are the ones that are directly related to tracking the world state and knowing which piece movements are allowed. This limits the use of this task as a future benchmark for world state tracking in the data-rich setting. It might be possible to make the task more challenging by asking the language model to predict the entire board state (deconvolution) or by focusing on hard subsets of the task (e.g: pseudo-legal infractions on long histories).\n-\tMany challenges of natural language are not present in this environment, such as coreference, limiting the applicability of results on this task to e.g bAbl. The use of a composite vocabulary (e.g: \u201ce4\u201d instead of \u201ce\u201d, \u201c4\u201d) also limits the compositional challenges of the dataset. However, it is true that using a vocabulary where \u201ce4\u201d is two tokens would make probing more difficult.\n\nMinor comments:\n-\tIncomplete board state: The board state as represented is actually not complete, as it ignores whether a pawn can be taken en passant ( https://en.wikipedia.org/wiki/En_passant ). En passant is rare enough that this might not matter, but it does mean the Oracle/Multi view models do not perfectly capture the state. Note that this seems to also be an issue in Oshri and Khandwala (2015) from whom this representation is derived.\n-\tThe ending square task does not allow us to probe whether the model captures the full range of possible moves for a piece, or whether it selects a subset of those. E.g: the model might reach 100% accuracy on this task without ever moving a rock/bishop/queen by more than one square (less likely that there are pieces in between).\n-\tIt is not clear what you do with games of length 100-150. These seem included in the training set but excluded from the probes. Are those included in the perplexity results of Table 6.\n-\tTypos:\no\tTable 4 breakdown in appendix B\n\n### Recommendation:\n\nI lean reject for this paper.\nThe core idea is interesting, but the paper fails to make connections to wider issues in world state tracking and grounded language learning, making it overly narrow. Both the missing references and the missing links to wider concepts in this litterature are a symptom of that. With a few caveats, the experiments are sound, but the analysis could be improved to go further than simply stating the results. The overall wording and presentation of the paper must also be improved.\n\nQuestions:\n- Are you planning to release the code and data to facilitate work on this topic?\n\n\n\n## Author response update\n\nIn light of the author response, I have decided to increase the score to 5. I have also decreased my confidence to 3.\nThe main reasons for this score increase are the release of code and data as well as thoughtful clarifications on the experimental setup. This is good experimental work. I also think Appendix C.1 is a good first step towards drawing wider scientific conclusions from this work.\n The main reason not to increase the score further is that I believe the contribution still is quite narrow.  \n\nI chose to decrease confidence in my evaluation since it is now based more on the narrowness of the contribution, which is harder to assess, than on the experimental validity of this work.\n",
                "rating": 5,
                "confidence": 3,
                "writer": "official_reviewer"
            },
            {
                "review_id": "1Gvj7GQWLUA",
                "reply_to": "p6aoZ7TolTe",
                "title": "Thanks for the replies!",
                "comment": "Thank you for the detailed replies and adding more baselines. I will stick to my accept score of 7 since I really enjoyed reading the work and thought it had thorough experiments! I won't raise my score further since I sort of agree with other reviewers (partly voiced in my weakness #4) that the work is a bit narrow and does not have too many insights which can be applied to real-world applications. But I appreciate the fact that you have proposed future work in this direction.",
                "writer": "official_reviewer",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "p6aoZ7TolTe",
                "reply_to": "Q1nqxtbXL8",
                "title": "More Analysis Added",
                "comment": "We thank the reviewer for their positive and helpful comments.\n\n> Does model performance / chess quality improve with larger language models like GPT2-md or GPT2-l?\n\nWe have added results with larger models (upto GPT2-medium) in Appendix C.2. For bigger training sets there are minor gains with larger models.   \n\n> What are the kinds of errors these language models make (in terms of legal moves)? Do these errors disappear when you check top-k tokens?\n\nA detailed analysis of illegal moves for the end square prediction is done in Appendix D. We find two prominent error categories: (a) piece(s) obstructing the predicted move, and (b) the current player\u2019s king is in check or remains in check after the predicted move is executed.  \n\n> What are the kinds of moves the model is good at (when considering argmax predictions)? Is it learning any strategy at all? You can measure this quite well automatically using the chess engine scores which indicate who is winning, and comparing the change in scores when the actual move is played vs the language model's predicted move\n\nChess strategy evaluation would be an interesting analysis. However, right now the language model is trained to predict both the winning and losing moves. We would\u2019ve trained the language model in a different way if the focus was trying to train a better chess player. Specifically, adding a prefix bit to indicate the winning player i.e. black or white (and conditioning on that during gameplay) or suppressing the language model loss corresponding to the losing player. This is outside the scope of our current work but an interesting future direction.  \n\n> Finally can insights from (2) and (3) be transferred to other real-world applications? Is there a correlation between the probing literature on natural language processing tasks and the results you find?\n\nWe plan to apply findings from the chess testbed to natural language applications. The RAP experiments have demonstrated that incorporating parts of the world state as tokens in the original sequence is a very effective strategy (similar in spirit to pseudo self attention). In future work, we want to incorporate coreference chains in text via special [ENTITY_i] tokens, where i is the cluster ID, and finetune pre-trained encoders. Similar to RAP, these tokens provide a slice of the world state and can be used as prompts to test the model\u2019s understanding.\n\n >  It will be cool to check other kinds of visual state fusion strategies like pseudo-self attention.  \n\nPseudo-self attention indeed offers an interesting way of incorporating visual modality.  Due to limited time we won\u2019t be able to test it out in the rebuttal phase but thanks for the suggestion.  \n\n> on the bottom of page 6, did you mean Table 2?\n\nYes, thanks for pointing that out. \n\n> A couple of baselines will be useful in Table 2 and 3. The first could be upperbound EM baselines using engines like Stockfish or AlphaGo (since nearly everyone is worse at Chess than them, I expect the EM score to be lower than 100%). The second could be a random lowerbound to EM, where a random move from the set of LM is chosen. Finally it will be good to see EM performance of GPT-2 considering only the set of LM.\n\nWe have added the random legal move baseline to the table. Selecting the top legal move improved the performance on exact end move prediction by UCI in the Train-L setting by 1% absolute in short histories, and 2.7% absolute in long histories. We will add that and the chess engine baseline in the final version.  \n\n> LM is an overloaded acronym which can cause confusion to the reader (language model vs legal move).\n\nThat's a great suggestion. We now refer to legal move as LgM, and exact move as ExM.\n\nWe have added the suggested references. \nLet us know if you have any additional questions or concerns.\n\n\n\n\n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-BVhGmQwBEa",
                "reply_to": "-zsQm3NJ2tD",
                "title": "(Response continued) Trivial Emulation vs Actual Understanding",
                "comment": "> The ending square task does not allow us to probe whether the model captures the full range of possible moves for a piece, or whether it selects a subset of those. E.g: the model might reach 100% accuracy on this task without ever moving a rock/bishop/queen by more than one square (less likely that there are pieces in between).\n\nThis is a very valid concern. However, the models do achieve reasonably high accuracy on the exact move task which makes us think that it's unlikely that the model is just making trivial predictions. \nWe also conducted a preliminary analysis to verify this. The analysis focused on the distance between the starting and ending square in \"king-moves\" ([Python-chess implementation](https://python-chess.readthedocs.io/en/latest/core.html#chess.square_distance)). We only included moves made by rook, bishop, and queen in this analysis because both king and knight make moves constant in king-move distance (pawns were already excluded due to their simple dynamics).   We first present statistics for the ground truth:\n* Actual Ending Square (Short); Filtered prompts (rook, bishop, queen) = 658; Average king-move distance between starting square and ending square = 2.0; Percentage of prompts with ending square at king-move 1 distance =  48.9\n* Actual Ending Square (Long); Filtered prompts (rook, bishop, queen) = 697; Average king-move distance between starting square and ending square = 2.2; Percentage of prompts with ending square at king-move 1 distance =  42.9\n\nNext, we present statistics for the UCI model trained on Train-L:\n* Actual Ending Square (Short); Filtered prompts (rook, bishop, queen) = 658; Average king-move distance between starting square and predicted ending square = 2.0; Percentage of prompts with ending square at king-move 1 distance =  50.8\n* Actual Ending Square (Long); Filtered prompts (rook, bishop, queen) = 697; Average king-move distance between starting square and predicted ending square = 2.1; Percentage of prompts with ending square at king-move 1 distance =  43.6\n\nFiltering the predicted moves by whether they were correct or not didn't change the stats by much. \n\nTo summarize:\n* The average distance covered in ground truth moves made by rook/bishop/queen for short game histories is about 2 (upper-bound of 7)\n* In long histories the average distance covered is slightly higher. This is quite possible because as the game proceeds, there are lesser pieces on the board and thus lesser obstructions. \n* For both histories, more than 40% of moves are within 1 king-move\n* The predicted moves are slightly closer on average in comparison to ground-truth moves. \n* More often than not, the predicted ending squares are more than 1 king-move distance away. \n\nThe broader implications of LM merely emulating chess without understanding chess still needs more investigation. \nIn particular, we plan to investigate the quality of top K predictions of the model where K could either be the number of legal moves possible in the position or all the moves above a chosen probability threshold. \n\n> It is not clear what you do with games of length 100-150. These seem included in the training set but excluded from the probes. Are those included in the perplexity results of Table 6.\n\nThe games of length 100-150 are indeed used for perplexity calculation. These games can also be used in the future to develop harder evaluations. \n\nWe hope this clarifies the concerns raised above. Let us know if you have any additional questions or concerns.",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "-zsQm3NJ2tD",
                "reply_to": "-EaJH0Qn9dR",
                "title": "Added connections to literature",
                "comment": "We thank the reviewer for their detailed and constructive feedback. Below we have tried to address the reviewer\u2019s concerns.\n\n> No references in text to Appendix D, despite it being an interesting demonstration of the analysis that can be done in this environment.\n\nWe have added more details to the illegal move analysis section and also added references to the same in the main text (Appendix D).\n\n> The comparison to related work on world state tracking and grounded language learning could be substantially improved.\n\nWe appreciate the pointers to related work and agree that we could be more expansive in describing connections with and differences from this prior work.  We have accordingly significantly expanded the related work section. With regards to the specific references suggested by the reviewer, we quote the following text from the related work section.\n\nTextWorld-style environments resemble ours in that the true world state is, by construction, available, models trained for a TextWorld environment differ in: \n1. their objective (reward maximization vs. maximizing the probability of the next observation), \n2. in how they are ultimately evaluated (final reward vs. world state tracking), and \n3. in whether we can directly probe the model\u2019s knowledge of the entire state. \n\nThe world models of Ha & Schmidhuber (2018) also maximize the probability of the next observation, but differ along the other two dimensions. Similarly the work by Hermann et al. (2017) and Hill et al. (2017) on developing and using 3D world simulations for learning grounded language has only partial overlap with the objective function, and differ along the other two aspects.\n\nOur work is related to work on grounding in that we are interested in comparing model performance when it does not have access to grounding information to when it does (Bruni et al., 2014; Kiros et al., 2014; Ororbia et al., 2019). However, unlike the work of Ororbia et al. (2019), for instance, the goal is not to improve the performance of language models using access to more of the world state, but to assess how much of this state has been recovered by the model from just learning the language modeling task.\n\n> It is not clear why the multi-view trained models underperform on the low data settings compared to the models without the extra supervision. \n\nRegarding multi-view trained models, our guess is that the board state model may require more data than it is being given in Train-S to train well. Without enough data, a poorly learned representation for board state can adversely interact with the language model. Pretraining the board state model might be a useful strategy. However, in an earlier experiment using a fully-connected network for board state, pretraining the network didn\u2019t help.\n\n> It would be good to make clear if this dataset and probing tasks will be released, and the code made available. \n\nThe entire setup is available at [this URL](https://anonymous.4open.science/r/f0c718e1-16af-4f1a-a129-d4ace9ac6820/). We chose a publicly available chess database to keep the setup reproducible. We also plan to release pretrained models via the huggingface model hub. \n\n> Despite this being a GPT-small architecture .... make the task more challenging ...\n\nThe tasks can be made more challenging in various ways, including the suggestion of the reviewer to predict the whole board state. Some other possible ways are: (a) limiting the attention window of the transformer as chess is almost Markovian (results in Appendix C.1) (b) evaluating on even longer game histories, and (c) focusing on Train-S which we refer to as small but still has more than a million moves.\n\n> Many challenges of natural language are not present in this environment.\n\nWe agree that chess notation is incredibly simple compared to natural language. The goal of our work was to start with a simple setting to determine how well transformer language models are tracking the world state, and how this state tracking capability can be improved with access to the world state during training. We found RAP to be an effective and easy way of adding slices of the world state to the text sequence, which improves performance and can later be used for diagnosing/probing the learned models. In our future work, we plan to explore adding entity-related information, such as coreference chains, via RAP-like tokens during pre-training of natural language models. \n\n> Incomplete board state: The board state as represented is actually not complete, as it ignores whether a pawn can be taken en passant\n\nThe board state representation is indeed incomplete. Information such as the possibility of en passant, counter for threefold repetition, counter for fifty move rule, etc., are missing from the board state. Since this information is needed only in rare cases, we didn\u2019t use it as part of the board state. We have mentioned this in a footnote in the revision.\n\n**Response continued in later comment (limit on characters)**",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "dKBUxLgokNR",
                "reply_to": "LQmJrTEqC5a",
                "title": "Clarifications on positioning of the work",
                "comment": "We appreciate the pointers to related work and agree that we could be more expansive in describing connections with and differences from this prior work.  We have accordingly added this discussion to the related work section.  \n\nIn relating our work to this other work we emphasize that a major goal of this paper is to determine how well models trained just on symbolic inputs (with a language modeling objective) are able to track the world state underlying these symbolic inputs. Certain probing papers also have a similar goal, but Chess/UCI is unique in that we can precisely automate probing tasks and can evaluate them at a fine-grained level, and this probing requires no extra machinery that also may give less direct answers. \n\nWhile TextWorld-style environments resemble ours in that the true world state is, by construction, available, models trained for a TextWorld environment differ in: \n1. their objective (reward maximization vs. maximizing the probability of the next observation), \n2. how they are ultimately evaluated (final reward vs. world state tracking), and \n3. whether we can directly probe the model\u2019s knowledge of the entire state. \n\nThe world models of Ha & Schmidhuber (2018) also maximize the probability of the next observation, but differ along the other two dimensions. Similarly the work by Hermann et al. (2017) and Hill et al. (2017) on developing and using 3D world simulations for learning grounded language has only partial overlap with the objective function, and differ along the other two aspects.\n\nOur work is related to work on grounding in that we are interested in comparing model performance when it does not have access to grounding information to when it does (Bruni et al., 2014; Kiros et al., 2014; Ororbia et al., 2019). However, unlike the work of Ororbia et al. (2019), for instance, the goal is not to improve the performance of language models using access to more of the world state, but to assess how much of this state has been recovered by the model from just learning the language modeling task.\n\nWith regards to AlphaZero, our setup, goals, and training objectives are very different. \n* AlphaZero has access to the board state and rules of chess. AlphaZero starts with random play governed by rules of chess, and learns from just self-play (Without the knowledge of rules of chess and access to actual games it\u2019s impossible to learn anything meaningful). On the other hand, our models don\u2019t have access to the board state during inference (except for the theoretical oracle baseline) and don\u2019t know the rules of chess.\n* A major goal of this paper is to determine how well models trained just on symbolic inputs (with a language modeling objective) are able to track the world state underlying these symbolic inputs. This is very different from AlphaZero\u2019s goal to demonstrate the potential of self-play in closed domains.      \n* AlphaZero\u2019s training objective is to predict the next move that maximizes the reward, where reward is winning the game, while our training objective is to maximize the probability of the next observation, regardless of the quality of the play.\n\nWe hope this clarifies the positioning of our work w.r.t. prior literature. \n\n> I would have liked to see more analysis regarding exactly what properties of the transformer they think is responsible for helping the model to learn and also a potential qualitative analysis of what the failure cases are.\n\n* We present additional results with variations in the basic transformer architecture (GPT2-small) in Appendix C. In Appendix C.1, we compare the effect of limiting access to previous tokens. Even though chess is Markovian, we find that there\u2019s a significant performance drop with limited history. This demonstrates the dependence of the transformer LM's performance on access to unrestricted history, and its limited ability to learn a compressed state representation for the relatively simple domain of chess.   \n* Appendix D has a detailed illegal move analysis, and we have added references to the same in the main text. \n",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "1F2i8m95k5K",
                "reply_to": "n_C9O0bQdV",
                "title": "Clarification on RAP and Oracle Baseline",
                "comment": "We thank the reviewer for their helpful and positive feedback. Below we answer the clarification questions asked by the reviewer.\n\n> I'm a little confused as to why performance peaks with p=0.05/0.15 in the UCI+RAP models. If my understanding is correct, then RAP with p=1.0 would include piece annotations with every move during training time and at inference. So shouldn't this make it easier to track pieces (as the authors themselves note in 4.1)? The reason for this is given as greater \"mismatch between training and inference\" -- I'm not sure what this means.\n\nIn the RAP setting, the **piece types are only added during training** and not during inference, except in the prompt for the starting square prediction task. To clarify this we have added Table 1 with tokenized sequences. The motivation for including RAP in training is twofold:\n* It allows us to probe at test time, where the model thinks each piece is, by simply appending the piece type of interest to any game history's prefix.  \n* We can use the available world state during training but the model doesn\u2019t require it during inference. \n\n> On a related note, I'm also confused by the performance of the Oracle Baseline in Table 3. In some instances, this is outperformed by trained models with less information. But my understanding was that this oracle would serve as an upper bound on model performance. So how are we outperforming the upper bound?\n\nRegarding the performance of the oracle baseline:\n* We have made a minor change to the \"oracle\" baseline which has improved its performance, though it is still not the top performer in two evaluations with Train-L. In the earlier version, we used a limited attention window since the language model doesn\u2019t have to track the board state as it\u2019s already provided. There were empirical reasons as well:  The model with the limited attention window converged faster and hence did better than the model with access to the full history in the earlier stages of training. But we later found that the model with limited attention converged to an inferior perplexity than the one with access to the full attention history. This might be because access to all the previous tokens can better reveal insights about the players involved, and we do see improvements in the Exact move accuracies. We have updated the oracle model\u2019s description and updated the numbers in the latest version. \n* The \"oracle\" baseline is intended to serve as an **approximate upper bound** where the model has access to more of the world state. It\u2019s still a language model and can still make mistakes. The term \"oracle\" is probably a misnomer in our context given its typical use in the ML literature. We plan to change this in the final version but are keeping it for now to avoid too many changes. A more strict upper bound might use the constraints of chess to restrict prediction to legal moves and focus on just predicting the exact moves (suggested by Reviewer 2). That being said, the \"oracle\" model with access to board state doing slightly worse than some of the baselines is still intriguing. Our hypothesis is that the models are currently being trained and tested on both the long and short histories, almost like a multi-task setup. Different models can weigh these \u201cdifferent\u201d tasks differently. Given the evaluation results, the oracle model may be prioritizing the long history tasks over the short history tasks.   \n\n> Couple of minor issues:\nAt the bottom of Page 6, the text references results in Table 3, which should be Table 2.\nSection C in the Appendix is currently empty.\n\nThanks for pointing these out. We have fixed them in the revised version. ",
                "writer": "author",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Q1nqxtbXL8",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "Interesting testbed and surprising results!",
                "comment": "Summary: This paper is an interesting exploratory study analyzing the ability of language models to track the state of a chessboard. The authors adopt a clever chess notation which allows them to probe the language model's state tracking ability by looking at its next word prediction (akin to probes in [1]). Quite remarkably, language models finetuned on chess data store a very accurate state representation, and predict legal moves over 90% of the times even without a visual representation of the board.\n\n-----------------------------------\n\nStrengths of the Paper:\n\n1. A clever probing method to analyze language model's state tracking abilities. \n\n2. Well-designed experiments and very interesting results in a mostly unstudied area.\n\n3. Wel written paper with several baselines and ablations.\n\n-----------------------------------\n\nWeaknesses of the Paper / Possible additional analysis:\n\nWhile the work in itself is very interesting and clean, I would have loved to see more analysis studying the model. This a very rich testbed where a lot of interesting experiments can be done! For instance,\n\n(1) Does model performance / chess quality improve with larger language models like GPT2-md or GPT2-l?  \n(2) What are the kinds of errors these language models make (in terms of legal moves)? Do these errors disappear when you check top-k tokens?  \n(3) What are the kinds of moves the model is good at (when considering argmax predictions)? Is it learning any strategy at all? You can measure this quite well automatically using the chess engine scores which indicate who is winning, and comparing the change in scores when the actual move is played vs the language model's predicted move  \n(4) Finally can insights from (2) and (3) be transferred to other real-world applications? Is there a correlation between the probing literature on natural language processing tasks and the results you find?  \n(5) It will be cool to check other kinds of visual state fusion strategies like pseudo-self attention [2]\n\n-----------------------------------\n\nOther Feedback:\n\n1. on the bottom of page 6, did you mean Table 2?\n2. A couple of baselines will be useful in Table 2 and 3. The first could be upperbound EM baselines using engines like Stockfish or AlphaGo (since nearly everyone is worse at Chess than them, I expect the EM score to be lower than 100%). The second could be a random lowerbound to EM, where a random move from the set of LM is chosen. Finally it will be good to see EM performance of GPT-2 considering only the set of LM.\n3. LM is an overloaded acronym which can cause confusion to the reader (language model vs legal move).\n\n-----------------------------------\n\nOverall Recommendation:\n\nThis is an exciting and rich testbed with a lot of interesting questions to answer. The authors have conducted well-thought experiments and reported interesting results. I'm leaning accept, but I encourage the authors to keep working on this setup (perhaps using some of the suggestions discussed above) and try to check if any of the insights here can be transferred to better understanding of language models on natural language.\n\n-----------------------------------\n\nReferences:\n\n[1] - https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00115  \n[2] - https://arxiv.org/pdf/1908.06938.pdf\n",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "n_C9O0bQdV",
                "reply_to": "iclr_2021_DGIXvEAJVd",
                "title": "An interesting application of transformers to Chess playing",
                "comment": "This paper considers an intriguing problem: can language models, when trained on\npurely textual representations of Chess games, learn the underlying dynamics of\nthe game? The authors argue that this could be a preliminary step towards\ntackling the symbol grounding critique of methods like transformers. When\ntransformers are utilized in natural language settings, it's challenging to\ndetermine whether the models are operating at a pure syntactic level, or whether\nthere is some rudimentary level of \"understanding\", given that the models are\nonly exposed to text. In contrast, in Chess, one can train in a purely textual\nfashion (or with some limited symbol grounding information), and probe how well\nthe result models the state of the underlying Chess position.\n\nThe authors use the GPT2-small based transformer architecture and train from scratch on a dataset of\nhigh-quality Chess games between humans, represented in UCI notation -- a\ntextual listing of the moves made by each player. In some experiments, this\npurely textual input is supplemented with explicit board state information as\nwell, to test the impact of this additional signal. The authors evaluate the\nsystem on two types of inference tasks: a trained model's ability to\nsuccessfully locate a piece on the board, and the model's ability to determine\nwhere to move a chosen piece to next. In each case, there are two\nevaluation metrics: an \"exactness\" metric (i.e., whether the model picked the\nsame piece/move as the human did in the corresponding game) and a \"legality\"\nmetric (i.e., whether the model picked a permissible move). The former metric is\nmore stringent, as it's also measuring the strategic awareness of the model. The\nauthors demonstrate through their experiments that transformers are successful\nat both inference tasks with very high accuracy, particularly when evaluated\nusing the legality metric, using shorter sequences of moves, and larger\ndatasets.\n\nStrengths of the paper:\n  + This is a creative application of transformers to a non-traditional textual\n    inference task. It may inspire others to devise other interesting applications.\n    The results are intriguing and expand our understanding of what may be\n    possible with transformer architectures.\n  + The authors' approach is a fairly straightforward application of off-the-\n    shelf techniques -- and I mean that in a good way. There are no unnecessary\n    complications or ad hoc additions to the system design.\n  + The paper is very clearly written, well-organized, and easy to follow.\n\nAreas for improvement/questions for the authors:\n  - I'm a little confused as to why performance peaks with p=0.05/0.15 in the\n    UCI+RAP models. If my understanding is correct, then RAP with\n    p=1.0 would include piece annotations with *every* move during training\n    time and at inference. So shouldn't this make it easier to track pieces (as\n    the authors themselves note in 4.1)? The reason for this is given as greater\n    \"mismatch between training and inference\" -- I'm not sure what this means.\n  - On a related note, I'm also confused by the performance of the Oracle Baseline\n    in Table 3. In some instances, this is outperformed by trained models with\n    *less* information. But my understanding was that this oracle would serve\n    as an upper bound on model performance. So how are we outperforming the\n    upper bound?\n\nOn balance, I think the strengths of the paper outweigh my concerns, and I\nrecommend ACCEPTANCE.\n\nCouple of minor issues:\n  - At the bottom of Page 6, the text references results in Table 3, which\n    should be Table 2.\n  - Section C in the Appendix is currently empty.\n",
                "rating": 7,
                "confidence": 4,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "presents an interesting benchmark with good thorough experimental work",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the positioning of the work",
                "Sentiment Expression": "R1 is confused",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "the work",
                "Sentiment Expression": "R4 finds the work narrow",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "R2",
                "Sentiment Expression": "positive review",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "not ready for publication in its current state",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "B4OTsjq63T5": {
        "paper_id": "nips_2022_B4OTsjq63T5",
        "paper_title": "Bayesian inference via sparse Hamiltonian flows",
        "paper_abstract": "A Bayesian coreset is a small, weighted subset of data that replaces the full dataset during Bayesian inference, with the goal of reducing computational cost.  Although past work has shown empirically that there often exists a coreset with low inferential error, efficiently constructing such a coreset remains a challenge.  Current methods tend to be slow, require a secondary inference step after coreset construction, and do not provide bounds on the data marginal evidence.  In this work, we introduce a new method---sparse Hamiltonian flows---that addresses all three of these challenges.  The method involves first subsampling the data uniformly, and then optimizing a Hamiltonian flow parametrized by coreset weights and including periodic momentum quasi-refreshment steps.  Theoretical results show that the method enables an exponential compression of the dataset in a representative model, and that the quasi-refreshment steps reduce the KL divergence to the target.  Real and synthetic experiments demonstrate that sparse Hamiltonian flows provide accurate posterior approximations with significantly reduced runtime compared with competing dynamical-system-based inference methods.",
        "paper_acceptance": "Accept",
        "meta_review": "All reviewers agree that the paper proposes an interesting approach to Bayesian inference incorporating coresets with Hamiltonian flows. Although some reviewers have some technical concerns at their first reviews, basically those have been resolved by the authors' responses. Thus, although there are some points that should be modified from the current form, I think we can expect the authors modify the paper in the camera-ready by reflecting the discussion. Based on these, I recommend acceptance for this paper.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "32kXBIzj_s",
                "writer": "author",
                "reply_to": "7LD0fAbOz8M",
                "title": "Continued Response to pCaB",
                "comment": " Thanks for the follow-up! \n\nYour understanding of one of the main strengths of our work is correct: the use of a coreset enables our method to be computationally efficient, and Prop 3.1 shows how big the coreset must be to enable an accurate reproduction of the full posterior. \n\nGeneric normalizing flows (Sylvester, planar, etc.) actually cannot use coresets. Generic flows are constructed of black-box parametrized transformations---the flow structure itself does not use information from the target. In contrast, our sparse Hamiltonian flow directly incorporates the coreset target information (see line 156, eq. 5). This allows us to train the coreset weights.\n\nGeneric normalizing flows also do not generally provide error guarantees (in the sense of \"longer flow provides a lower KL\"). There is some past work on analyzing how expressive these families are, but these results are usually abstract universal approximation results (see, e.g., Theorem 3.1, \"The Expressive Power of a Class of Normalizing Flow Models\", 2020). \n\nIt is certainly possible for a generic normalizing flow to outperform our method on a given problem; but without guarantees, it's hard to say much in advance! (We have also found in our experience that it is hard to train generic flows reliably in practice; they often get stuck in bad local optima.)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7LD0fAbOz8M",
                "writer": "official_reviewer",
                "reply_to": "UUMrDeKALZau",
                "title": "Thanks for the clarifications",
                "comment": " Thanks for the detailed response to my questions. It has improved my understanding of the paper. I look forward to reading the camera-ready version, containing the edits you describe. I will be changing my score to a 7.\n\nIf you can find the time, I have one remaining clarifying question (I realize that we are close to the deadline):\n\nIn your explanation of the advantages of using a Hamiltonian Flow over other methods, you highlight 1) the ability to converge to a target posterior distribution, and 2) that it enables the use of a core set. I'm probably missing something here, but aren't these two properties valid for any choice of normalizing flow? Do I understand correctly that the main advantage is the fact that you 1) can provide guarantees in the form of prop 3.1, and 2) that it is computationally efficient? So, in principle, it might be the case that an different choice of (expressive) flow might provide better results empirically, possibly at a higher computational cost?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "THs8nNQi6I2",
                "writer": "author",
                "reply_to": "6YB6xfMQR2C",
                "title": "Response to Reviewer ofer",
                "comment": " Thank you again for your suggestion! We will include the comparisons in both energy distance and MMD in the supplement for the camera-ready version.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6YB6xfMQR2C",
                "writer": "official_reviewer",
                "reply_to": "rHi0FbR8udI",
                "title": "Thank you for the authors' response",
                "comment": " Many thanks for your response and clarification on the points raised! I just have one more question:\n\n> Indeed the relative covariance error seems slow to converge, but this is not the complete picture. We need to look at the relative covariance error plot together with the relative mean error plot (Fig. 2c) and the KL plot (Fig. 2b). In particular, the relative mean and covariance error plots depict two different aspects of the quality of our target approximation; the KL plot takes both of these into consideration. For this particular problem, our method finds the center of the target before fine tuning the covariance. The monotonic downward trend of the KL divergence shows that our method keeps on improving the target approximation throughout optimization.\n\n> To understand why the relative mean error and KL divergence go up for UHA, it is important to note that UHA operates on the augmented space based on a sequence of distributions that bridge some simple initial distribution and the target distribution. Therefore, it is not guaranteed that all steps of optimization improve the quality of approximation on the marginal space of the latent variables of interest. This explains the increase in the error metrics on the \\theta-marginal space shown in Figs. 2b and 2c. However, we note that from Fig 2a, UHA\u2019s augmented ELBO as the optimization objective that we maximize over shows a monotonic increasing trend.\n\nRegarding this, in addition to looking at the relative mean and cov separately, perhaps it would be clearer if you plotted the energy distance or MMD?",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "V0M5bGlREVMW",
                "writer": "author",
                "reply_to": "Kj7xJtGm1s",
                "title": "Response to Reviewer hoPP",
                "comment": " Thank you for reviewing our manuscript, and for the positive feedback! We provide a point-to-point response to each of the comments in the review below. Don\u2019t hesitate to follow up with any questions; we are happy to answer them.\n\n> To my understanding, SHF makes some assumptions that imply limitations. For example, SHF chooses a random subset of data points. In cases where a random subset of data points is not representative of the entire dataset, SHF might be fast but not useful. Is this correct? I expect that more complicated data types will run into problems with random subsets more easily. Thus SHF might not be an appropriate solution for more complex models and data.\n\nYou are totally correct; there is usually some probability that the subsample we draw will be totally unrepresentative of the full dataset, at which point the coreset construction is flawed from the start. However, our result in Proposition 3.1 provides guidance on how large one should choose the coreset to be to avoid this problem from occurring. In particular, as long as the coreset size M is roughly d \\log(N), where N is the dataset size and d is the \u201cdimension of the log-likelihood function space,\u201d the probability of randomly obtaining a bad subsample is quite small, as it decays at roughly a N^(-d/2) rate.\n\nNow, as you say, the data might be quite complex\u2014in the notation of our paper, this is when that dimension \u201cd\u201d is quite large for the model under consideration. For example, if the data are very high-dimensional (and do not lie on a low-dimensional manifold), the value of \u201cd\u201d may be quite large. In these cases, we may need to use a rather large coreset, and the approach may not be so useful. \n\nWe will add a discussion of this limitation (and others) in the final camera-ready version.\n\n> Is the subset of M values chosen uniformly at random once at the beginning of the process or does the subset change over time?\n\nThe subset of M uniformly subsampled data points is selected once at the beginning and fixed. However, we do update the weights associated with these data points as we run the variational optimization. \n\n> from steps 0-9 the ELBO decreases rather than increases. I don\u2019t understand why that is the case. My naive assumption would be that the blue lines should also go up just not as drastically as the red lines.\n\nThanks for pointing this out \u2013 great observation! Actually, in theory, the ELBO should stay constant during the simulation of Hamiltonian dynamics if the simulation is perfect (see equation after line 114). However, since SHF uses the gradient of the coreset posterior rather than the full posterior to simulate the dynamics, some error will be introduced. Another source of error comes from the fact that we can only approximately simulate Hamiltonian dynamics using leapfrog steps. Both sources of error can cause the ELBO to change between quasi-refreshment steps. This error could either result in an increase (steps 30-39) or decrease (steps 0 to 9) in the ELBO.\n\n> The limitations of the method were discussed insufficiently and should be addressed.\n\nWe will add some text discussing the limitations of our method for the camera-ready version. In fact, one of the limitations of our method has already been touched upon in the response to your comments above. Specifically, our method relies on the assumption that the data are compressible.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "UUMrDeKALZau",
                "writer": "author",
                "reply_to": "XVx60WSMtuy",
                "title": "Continued Response to pCaB",
                "comment": " > \"Figs. 2c and 2d demonstrate...\" To which extent is this explained by the fact that the posterior is Gaussian in this case?\n\nThe quote is just a small remark regarding the plots for this particular example; we did not intend to suggest drawing conclusions outside of this Gaussian location model setting. But we recognize that this may have been implied, and will carefully re-word the text for the camera-ready. Finally, note that we see from Fig 5 that our method outperforms other competing methods under models with non-Gaussian posteriors.\n\n> Why was there a difference in the baseline methods included in Fig 2/3 and Fig 4?\n\nThis work builds upon two lines of literature: coreset construction methods, and Hamiltonian-based VI methods. Each of these plots shows a comparison with methods from each line of past work. Figs. 2-3 compare the sample quality, density evaluation time and sample generation time across the posterior approximations produced by various Hamiltonian-based VI methods. Fig. 4 then compares the quality of the posteriors approximated using the coreset obtained from SHF against other coreset construction methods.\n\nFor Figure 2-3 specifically, it is worth noting that all previous Hamiltonian VI methods do not work with a Bayesian coreset; hence they are often quite slow in the regime of large-scale data. In contrast, SHF incorporates coreset Hamiltonian dynamics, leading to fast training, density evaluation, and i.i.d. sampling.\n\nFor Figure 4 specifically, it is worth noting that all previous coreset construction methods do not enable i.i.d. sampling: one needs to use a subsequent inference method on the coreset posterior after construction. In contrast, SHF does enable i.i.d. sampling and density evaluation with no additional secondary stage.\n\n> I would have liked to see simpler Bayesian Inference baselines such as Laplace and simple mean-field VI included as baselines as well.\n\nThank you for the suggestion. We have added a comparison between our method and Laplace approximation using the same Bayesian linear regression model (Section 4.2) in the supplement in Appendix F. Fig. 8 shows that our method provides a higher quality posterior approximation than the Laplace approximation. Specifically, the approximated KL is around 100 for our method, and around 500 for the Laplace approximation. In general, since Laplace and mean-field VI both use Gaussian distributions to approximate the posterior, we anticipate that these simpler baselines will suffer when the posterior is non-Gaussian.\n\n\n> The paper could be further improved with some reflection on the limitations of the approach.\n\nWe agree that there was not enough attention devoted to this in the current version, and will include a discussion in the camera-ready version. One main limitation of this methodology is that we assume that the data are \"compressible\" in the sense that log-likelihood functions of a subset can be used to represent the full log-likelihood (see our response to comments by Reviewer hoPP). If the data are truly very high-dimensional (i.e. not on some low-dimensional manifold), this may not be the case. Another limitation is that while our quasi-refreshment is simple and works well in practice, more work is required to develop quasi-refreshment methods with general guarantees.\n\n> \"...interleaving MCMC and gradient descent steps... \" This sentence was not entirely clear to me.\n\nWe agree that this could be stated more clearly. The idea is: at each iteration of the optimization, MCMC samples for the current coreset posterior are used to estimate the coreset weight gradient. Since the coreset size (M) is much smaller than that of the full dataset (N), MCMC is not expensive. However, the quality of the MCMC samples may be poor without tuning. And it is not realistic to tune the MCMC sampler at each iteration. We will reword this sentence to make it clearer.\n\n> \"Let C be the universal constant\" ...appears a bit unmotivated here \n\nThe constant C here provides an upper bound on the number of spherical balls needed to cover a d-dimensional unit sphere. It is a technical detail that is not of significant importance. We will revise the paper to provide some more context on this constant.\n\n> \"since w only has the first M entries nonzero\" Was the convention of placing the nonzero elements first mentioned anywhere? \n\nWe mentioned this in line 141; this assumption is merely for notational convenience. We will find a way to make it more prominent in the text!\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "XVx60WSMtuy",
                "writer": "author",
                "reply_to": "_ZYCQCa8iYX",
                "title": "Response to Reviewer pCaB",
                "comment": " Thank you for your efforts in reviewing our manuscript, and for the positive feedback! We provide a point-to-point response to each of the comments in the review below. Don\u2019t hesitate to follow up with any questions; we are happy to answer them.\n\n> ...the authors should provide a dedicated Related Work section to elaborate on the connections to earlier work.\n\nYou are correct that the combination of Hamiltonian-based flow with quasi-refreshment and Bayesian coresets is unique to our work. We can certainly add a dedicated related work section that draws connections to earlier work for the camera-ready version. \n\n> ...explain what the advantages of using a Hamiltonian flow are compared to other choices of normalizing flows.\n\nThis is a great suggestion! The intuition here is that Hamiltonian flow methods involve a sequence of transformations that resemble the steps of Hamiltonian Monte Carlo (HMC). In HMC, we run Hamiltonian dynamics for a while, and then resample the momentum, and repeat; in Hamiltonian flows, we run Hamiltonian dynamics for a while, and then quasi-refresh the momentum, and then repeat. Since we know the Markov chain generated by HMC converges (in distribution) to the target posterior distribution, we expect the Hamiltonian flow to do something similar (although not perfectly, as the quasi-refreshment is not a perfect substitute for exact momentum resampling!).\n\nOne other very important advantage of Hamiltonian flows is that the ODE naturally enables the use of a coreset, which enables us to build a computationally inexpensive flow. By the previous argument, our coreset flow should approximate the coreset posterior reasonably well. Moreover, the optimal coreset should provide a good approximation of the full posterior (for the Gaussian location model this is given by our Proposition 3.1). Therefore, we have constructed a variational family that is both inexpensive to work with and flexible enough to well-approximate the posterior. Standard normalizing flows typically come with no guarantees, and may or may not be expensive to work with depending on design (though more flexible families tend to be more expensive).\n\nWe will add some text outlining these advantages for the camera-ready version. \n\n> ...why this is a representative (or even relevant) example model. Can anything be said about the compression for the posterior of other models? Can anything be said for finite N?\n\nGreat question! By \u201crepresentative,\u201d we mean in terms of how well our result regarding the optimal coreset quality extends. In fact, our proof technique has already been extended to general exponential families (the only difference lies in the choice of sufficient statistic). Unfortunately, this result is in a forthcoming unpublished manuscript which we can\u2019t provide a pointer to quite yet! But if you are willing to take that leap of faith, we note that exponential families can be used to well-approximate a very wide range of models. So we suspect that our result is indicative of a more general setting (at least the fact that the coreset size should scale with some notion of a \u201cdimension\u201d of the model). \n\nRegarding finite N, our proof actually provides a finite-N analysis. See the proof in Appendix C (line 573). However, the finite-N result is somewhat complicated and messy. We chose to present the asymptotic version of the result in the main text due to the simpler final expression.\n\n\n> \"In this work we assume that \\rho_t ~ N(mu, Lambda^-1)\" Does this mean that we are ultimately assuming that the posterior can be approximated by a diagonal Gaussian?  \n\nNo, we do not assume the posterior is a diagonal Gaussian. Let us clarify a bit with two points.\n\n1. The posterior \\pi is set by the likelihood and prior, and is usually not Gaussian. We then augment the posterior with a Gaussian distribution for the momentum. Note that this is not limiting; we are free to pick any \"nice\" distribution for the momentum component. The Gaussian distribution is what people typically use in practice for HMC, and we follow that standard here.\n\n2. The variable \\rho_t is the momentum variable at time t of the Hamiltonian flow. So when we say we assume \\rho_t ~ N(\\mu, \\Lambda^-1), we are making an assumption on the momentum component at time t of our flow. \n\nThe assumption is true for all t if the posterior \\pi is indeed Gaussian. But we actually don\u2019t need this assumption in practical application. The method still works without it, and applies to a wide range of non-Gaussian posteriors. The reason we included this text is because if the assumption holds, the \u201cstandardization\u201d quasi-refreshment in equation (7) is guaranteed to reduce the KL by Proposition 3.3.  This assumption usually will not hold exactly in practice, but we have seen that the standardization quasi-refreshment still provides a significant reduction in KL (Figure 1).\n\nWe will revise the wording in this section for the camera-ready version.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "rHi0FbR8udI",
                "writer": "author",
                "reply_to": "Ev9ZYoRkA3m",
                "title": "Response to Reviewer ofer",
                "comment": " Thank you for reviewing our manuscript, and for your encouraging feedback! We provide a point-to-point response to each of the comments in the review below. Don\u2019t hesitate to follow up with any questions; we are happy to answer them.\n\n> Perhaps some other choices of \\rho_t and R_\\lambda(x) could be explored so that a potential user could understand the sensitivity of the algorithm to these choices\n\nThank you for your suggestion. Indeed, there are many possible choices of the momentum distribution for Hamiltonian dynamics. We chose the Gaussian momentum in the paper because it is the most commonly used in practice, and it enables a simple quasi-refreshment scheme of standardizing the momentum variables. We would like to point out that a discussion on some other possible quasi-refreshment schemes for Gaussian momentum was included in the supplement in Appendix A. In our experiments, we did not observe major differences in performance across the various schemes discussed, and so opted for the simplest one. \n\nAs our work provides a general framework for incorporating momentum refreshments in Hamiltonian-based flow methods, however, it would likely not be too onerous to try out previously-studied alternative Hamiltonian momentum distributions, e.g. the Laplace distribution [1-2]. Developing quasi-refreshment schemes for other momentum distributions is certainly an interesting direction to explore; we leave this to future work. \n\n> I am under the impression that the quasi-refreshment step could also be incorporated into some of the other baselines considered in the experiments, and not just for SHF. If this is the case, how would the baselines then perform?\n\nYou are absolutely correct that we can incorporate the quasi-refreshment step into HIS. In fact, one way to think of our new method is that we (1) replace the tempering step in HIS with quasi-refreshment, and (2) introduce the use of a coreset. Indeed, the quasi-refreshment step is precisely motivated by Proposition 3.2, which states that tempering alone is insufficient for HIS to obtain adequate target approximations, even when they are Gaussian. For UHA specifically, there is no need to incorporate the quasi-refreshment step; the momentum variables are already perfectly refreshed, as they are resampled from a Gaussian (at the cost of introducing many auxiliary variables).\n\n> In Figure 2, relative cov error seems to take a long time to converge, compared to UHA-Full. Why did the purple UHA-Full suddenly go up after ~2000 iterations? It seems like at that point, the approximation is just as good as SHF.\n\nIndeed the relative covariance error seems slow to converge, but this is not the complete picture. We need to look at the relative covariance error plot together with the relative mean error plot (Fig. 2c) and the KL plot (Fig. 2b). In particular, the relative mean and covariance error plots depict two different aspects of the quality of our target approximation; the KL plot takes both of these into consideration. For this particular problem, our method finds the center of the target before fine tuning the covariance. The monotonic downward trend of the KL divergence shows that our method keeps on improving the target approximation throughout optimization. \n\nTo understand why the relative mean error and KL divergence go up for UHA, it is important to note that UHA operates on the augmented space based on a sequence of distributions that bridge some simple initial distribution and the target distribution. Therefore, it is not guaranteed that all steps of optimization improve the quality of approximation on the marginal space of the latent variables of interest. This explains the increase in the error metrics on the \\theta-marginal space shown in Figs. 2b and 2c. However, we note that from Fig 2a, UHA\u2019s augmented ELBO as the optimization objective that we maximize over shows a monotonic increasing trend.\n\nWe realize that we have not been very clear about these interpretations in the paper, and will add some clarification on this in the experiment section for the camera-ready version. Thank you for pointing this out!\n\n[1] Zhang, Y. et al (2016). Laplacian Hamiltonian Monte Carlo. In: Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2016. Lecture Notes in Computer Science(), vol 9851. \n\n[2] Nishimura, A. et al. Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods, Biometrika 107(2), 2020, Pages 365\u2013380.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Ev9ZYoRkA3m",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_B4OTsjq63T5",
                "title": "",
                "comment": " This paper proposes a Bayesian inference methodology incorporating coresets with Hamiltonian flows. The paper demonstrates theoretically the challenges that both coresets and variational inference via Hamiltonian dynamics face, and proposes a fix for both in their algorithm \"Sparse Hamiltonian Flows\". Their method first selects a coreset and then follows a sparsified Hamiltonian flow with quasi-refreshments, which allows the flow to update the momentum. Important or argumentative claims are backed up with theoretical proofs. Experiments on a variety of regression problems demonstrate the superiority of their algorithm over current state-of-the-art coreset compression and variational-flow-based methods. Strengths:\n- Important claims and new insights on coresets and Hamiltonian flows are backed up with theoretical proofs. Where may be difficult to prove in the general case, such as Proposition 3.1, a representative example is given and the claim is proven on it. \n- The proposed idea is very novel and addresses important drawbacks that current coreset methods suffer from.\n- A thorough and clear review of related and past methods is provided\n- Experiments are well-conducted and a variety of datasets, both synthetic and real, are explored\n\nWeaknesses:\n- Perhaps some other choices of $\\rho_t$ and $R_\\lambda(x)$ could be explored so that a potential user could understand the sensitivity of the algorithm to these choices - I am under the impression that the quasi-refreshment step could also be incorporated into some of the other baselines considered in the experiments, and not just for SHF. If this is the case, how would the baselines then perform?\n- In Figure 2, relative cov error seems to take a long time to converge, compared to UHA-Full. Why did the purple UHA-Full suddenly go up after ~2000 iterations? It seems like at that point, the approximation is just as good as SHF. Yes",
                "rating": 9,
                "confidence": 3
            },
            {
                "review_id": "_ZYCQCa8iYX",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_B4OTsjq63T5",
                "title": "",
                "comment": " The paper introduces a new method for constructing Bayesian coresets. The authors demonstrate that a single uniform subsampling of data points is in principle sufficient to obtain an exact coreset, and introduce the sparse Hamiltonian flow to efficiently construct and sample from the corresponding coreset posterior approximation. Notable improvements over other coreset methods are reported in several experiments.\n ### Originality:\nTo my knowledge, the presented method is original. The authors provide a footnote citing concurrent work based on similar ideas, but the combination of Hamiltonian flow approximations to the coreset posterior seems unique to his work. One could argue that the authors should provide a dedicated Related Work section to elaborate on the connections to earlier work.\n\n### Quality:\nThe paper is technically sound, and the claims are carefully developed and well supported. The paper could be further improved with some reflection on the limitations of the approach.\n\n### Clarity:\nThe manuscript is well structured and very clearly written, with helpful introductions to the methodological ingredients that it builds upon.\n\n### Significance:\nThe paper constitutes a significant contribution within research on Bayesian coresets, both in terms of methodology and measured in terms of the performance improvements over other methods. I am not certain how large a contribution it will have to the field of Bayesian inference in general. This would have been easier to assess if the authors had broadened the scope of their baselines to other Bayesian inference procedures.\n ### Detailed comments and questions:\n\nLine 110. After having introduced Hamiltonian Dynamics, the author state that here that it is possible to use it as the basis of a normalizing flow. It would be informative if they could also explain what the advantages of using a Hamiltonian flow are compared to other choices of normalizing flows. I assume this might be explained in [21,22], but it would be helpful if it was reiterated here.\n\nLine 142. The model covered in Proposition 3.1 is referred to as a \"representative example model\". The authors should elaborate on why this is a representative (or even relevant) example model. Can anything be said about the compression for the posterior of other models? Can anything be said for finite N? In particular, it would be interesting to know whether anything could be said about the choice of M in the general setting.\n\nLine 197. \"In this work we assume that p_t ~ N(mu, Lambda^-1)\"\nDoes this mean that we are ultimately assuming that the posterior can be approximated by a diagonal Gaussian? In that case, how is this different from the assumption made in a simple parametric VI setting? In the introduction, the authors explicitly mention the \u201csimple parametric families\u201d as a contrast to the current work. It would be helpful if the contrast between the two was explicitly explained for the case when this parametric assumption of p_t is made.\n\nLine 266. \"Figs. 2c and 2d demonstrate that this reduction in KL divergence is primarily due to a lower relative error in the approximate posterior mean provided by SHF.\"\nTo which extent is this explained by the fact that the posterior is Gaussian in this case? It seems like an idealized setting for the proposed method, given the assumption of Gaussianity of p_t.\n\nIn the Experiments section, the choice of baselines used in the different figures was not clear to me. Why there was a difference in the baseline methods included in Fig 2/3 and Fig 4?\n\nI would have liked to see simpler Bayesian Inference baselines such as Laplace and simple mean-field VI included as baselines as well. This would make it easier for someone not intimately familiar with coreset methods to judge how big an impact these methods have over conventional approaches.\n\nIn the Conclusion section, it would have been helpful with a discussion of the limitations of the proposed approach.\n\n\n### Minor details\n\nLine 81. \"While theoretically not expensive, interleaving MCMC and gradient descent steps is hard to implement and tune, and is too slow to be practical. \"\nThis sentence was not entirely clear to me. First, you state that it is theoretically not expensive, and that it is hard to implement and tune, but then you conclude that it is too slow to be practical, which seems to contradict the first part of the sentence. Please clarify whether the limitation here is fundamental (e.g. efficiency wise), or whether it is practical (difficult to implement and tune).\n\nLine 149. \"Let C be the universal constant from [37], Corollary 1.2.\nThis constant appears a bit unmotivated here right before Propoposition 3.1. It might increase readibility if it was either explained in greater detail, or otherwise moved to the end of the sentence in 152.\n\nLine 163. \"since w only has the first M entries nonzero\"\nWas the convention of placing the nonzero elements first mentioned anywhere? Perhaps I missed it.\n The authors do not discuss the limitations of their method.",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "Kj7xJtGm1s",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_B4OTsjq63T5",
                "title": "",
                "comment": " \u201cBayesian Inference via Sparse Hamiltonian Flows\u201d combines three techniques to make Bayesian Inference faster and more accurate. It combines a) subsampling of the data (core sets), b) sparse flows and c) quasi-refreshments.\n\nThe paper provides theoretical evidence for why these subcomponents reduce the runtime or increase performance (see section 3) and empirical evidence in three different settings. The Sparse Hamiltonian Flows (SHF) clearly and strongly outperform the alternatives in most experiments. \n\n*UPDATE*: thanks for addressing all of my concerns. I update my score from 7 to 8. Great work. Keep it up!\n In short, I think the paper is good and should be published with minor revisions. \n\n*Strengths:*\n- The paper is very well-written and clear\n- The suggested combination of methods clearly and strongly improves performance compared to the alternatives\n- The paper provides a theoretical analysis of why and in which manner the performance improves due to SHF. \n\n*Weaknesses:*\n- The experiments are all in fairly simple settings. The results already convince me that the method is very strong and warrants publication but a more complex experiment would increase this conviction (see questions). \n- The paper says little about its limitations (see questions).\n - Limitations: To my understanding, SHF makes some assumptions that imply limitations. For example, SHF chooses a random subset of data points. In cases where a random subset of data points is not representative of the entire dataset, SHF might be fast but not useful. Is this correct? Are there other limitations of SHF that are not explicitly mentioned?\n- Scale (related to limitations): I expect that more complicated data types will run into problems with random subsets more easily. Thus SHF might not be an appropriate solution for more complex models and data. I\u2019d like to see a more explicit interaction with these points, either by actually running an additional experiment or by stating the implicit assumptions and following consequences in more detail. \n- Subsets: I\u2019m not sure I fully understand the way in which the random subset is chosen. Is the subset of M values chosen uniformly at random once at the beginning of the process or does the subset change over time? \n- Figure 1: I see that during the leapfrog steps (e.g. every 10th iteration), the ELBO jumps up towards a better state. This is in line with the theory. However, from steps 0-9 the ELBO decreases rather than increases. I don\u2019t understand why that is the case. My naive assumption would be that the blue lines should also go up just not as drastically as the red lines. \n I think, the limitations of the method were discussed insufficiently and should be addressed as described in questions 1 and 2. \n\nI\u2019ll use the rest of the section for high-level comments.\n- In its current form, the paper convinces me that SHF decreases runtime and increases performance for datasets with low complexity. The authors show this with their theoretical analysis and empirical experiments. Furthermore, the paper is well-written and the presentation is good. All of this combined already warrants publication in my opinion. \n- The assumptions that SHF makes and the implied limitations are underexplored. I expect that SHF will have a hard time with more complex models and data because it assumes that a random selection of data points is representative of the entire dataset. I think a good response or an additional experiment in this direction would convince me to raise my score further. Note, that I think the paper would be improved, even if the method is more limited than expected. Stating limitations helps readers and practitioners because it defines the scope of possible use cases more clearly. \n- I want to help where I can. In case something is unclear, feel free to ask follow-up questions. \n",
                "rating": 8,
                "confidence": 3
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "the paper proposes an interesting approach to Bayesian inference incorporating coresets with Hamiltonian flows",
                "Sentiment Expression": "interesting",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "1vusesyN7E": {
        "paper_id": "nips_2022_1vusesyN7E",
        "paper_title": "Autoregressive Perturbations for Data Poisoning",
        "paper_abstract": "The prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. Data poisoning attacks have been proposed as a bulwark against scraping, as they make data ``unlearnable'' by adding small, imperceptible perturbations. Unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. In this work, we introduce autoregressive (AR) poisoning, a method that can generate poisoned data without access to the broader dataset. The proposed AR perturbations are generic, can be applied across different datasets, and can poison different architectures. Compared to existing unlearnable methods, our AR poisons are more resistant against common defenses such as adversarial training and strong data augmentations. Our analysis further provides insight into what makes an effective data poison. ",
        "paper_acceptance": "Accept",
        "meta_review": "The paper proposed a novel auto-regressive perturbation method to make the data unlearning. The method is independent to models and data, making it more easy to be used. Reviewers found the idea is novel and intuitively reasonable. The authors responded to reviewers' detailed questions about the method and experiments. The rebuttal succeeded to remove the confusions and convince us about the empirical significance. We suggest the authors improve the paper according to the review comments in the next version. ",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "Id4392fnArt",
                "writer": "official_reviewer",
                "reply_to": "7Et5iM7eKiP",
                "title": "Thank you for the response",
                "comment": " Thank you for the response. I already increased the score to 5 (borderline accept). \n\nWhat the paper proposes is a defense to make the data unlearnable, so developing novel defenses means developing adaptive attacks? In some related literature, designing adaptive attacks is considered necessary to verify the effectiveness of defenses, and lacking adaptive attacks is a weakness or limitation mentioned in many reviews. So the limitation remains. But I do not think it is a big limitation. So I raise the score to 5 since the contributions overweigh the weakness and limitation.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "7Et5iM7eKiP",
                "writer": "author",
                "reply_to": "8bm3gkjHUrR",
                "title": "Response to remaining concern",
                "comment": " > although a simple signal is easily interpolated by a network, it may also be easily eliminated by well-designed denoising techniques\n\nThank you for your additional feedback.  Designing denoisers for autoregressive perturbations, which are generated using AR processes unknown to the victim, requires that the denoiser be agnostic to the exact AR process. Even if AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (because we sample our starting signal from a Gaussian for a 32x32x3 image and an AR process that uses a window size 3x3) (Figure 3, Left). Recovering or removing perturbations is a challenging direction for future work, but developing novel defense techniques is beyond the scope of this paper.  \n\nFurthermore, while adding AR perturbations to training data makes the data easy to fit, estimating the noise from poisoned data may be very challenging, perhaps no less challenging than estimating perturbations added under other indiscriminate poisoning attacks, such as error-max perturbations [1].  We emphasize that how easy the noised training data is to fit is **not** related to the difficulty of recovering the clean data after the noise is applied. Error-max and Error-min perturbations are \u201calmost linearly separable\u201d [2] and yet denoisers which remove perturbations under indiscriminate poisoning attacks to recover model performance remain elusive. The possibility of denoisers as a defense can just as easily be raised against other indiscriminate poisoning methods. We do think this is an interesting direction, and defenses are worth pursuing. If we have addressed your feedback, we hope you will consider increasing your score.\n\n[1] Adversarial Examples Make Strong Poisons, NeurIPS 2021\n\n[2] Availability Attacks Create Shortcuts, KDD 2022",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8bm3gkjHUrR",
                "writer": "official_reviewer",
                "reply_to": "06lSfSHO3eU",
                "title": "Thank you for your comprehensive response",
                "comment": " The response addressed most of my concerns. So I raised my rating. \nOne remaining concern is that, although a simple signal is easily interpolated by a network, it may also be easily eliminated by well-designed denoising techniques, e.g., a denoising method that is specifically designed for the AR noise. Although difficult patterns are hard to learn, they may be more robust against denoising methods.\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "06lSfSHO3eU",
                "writer": "author",
                "reply_to": "Rkvj0Te2L0l",
                "title": "Following Up with Reviewer wQLZ",
                "comment": " Thank you again for your thoughtful review. Does our response help address your feedback? We would appreciate the opportunity to engage further if needed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Pv2nO2V8knN",
                "writer": "official_reviewer",
                "reply_to": "LH7WFi7pZK1",
                "title": "Thanks for addressing my concerns.",
                "comment": " Thanks for the detailed clarification. All concerns have been addressed.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yTu-DyBbts",
                "writer": "author",
                "reply_to": "yc0AsyS3sjy",
                "title": "Response to Reviewer wQLZ [Part 2]",
                "comment": " > Section 3.3 is not easy to follow\u2026The theoretical analysis is not sufficient. The relation between Lemma 3.1 and the effectiveness of the proposed method in poisoning attacks is not obvious.\n\nWe thank the reviewer for pointing this out. We have rewritten Section 3.3 for clarity, focusing on our logic and its relationship to Lemma 3.1. Here is the relevant portion of the new text: A signal that is easily interpolated by a network will be quickly identified and used as a \u201cshortcut,\u201d whereas complex and unpredictable patterns may not be learned until after a network has already extracted useful content-based features [8]. We propose a simple hypothesis: if there exists a simple CNN that can classify autoregressive signals perfectly, then these signals will be easy to learn. By showing that AR filters exist, Lemma 3.1 helps us define the simple CNN that classifies AR signals perfectly. Our experiments demonstrate that our method, motivated by our simple hypothesis, is effective.\n\n### Responses to questions\n> Is the proposed method only applicable to computer vision tasks?\n\nWe only develop perturbations for images, but an AR perturbation can be crafted for any continuous signal. We speculate that our method could work for audio classification as well. We show that AR patterns are easily learned by CNNs, and they are applicable to any setting where you would use a CNN.\n\nThank you again for your feedback. We think that your suggestions have improved our paper. We made a significant effort to address your questions, and would appreciate it if you would consider raising your score in light of our response. Please let us know if you have any additional questions we can address.\n\n[8] The pitfalls of simplicity bias in neural networks, NeurIPS 2020",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wV2VDIlYnkc",
                "writer": "author",
                "reply_to": "4s6zQaGSY6T",
                "title": "Response to Reviewer FXVx [Part 2]",
                "comment": " > It is better that if actual code can be provided for reproduction of the results\n\nWe have uploaded our code repository as supplementary material to our submission. It contains documentation and example Jupyter notebooks. \n\n### Responses to questions\n> It is clear how to generate AR noise at the beginning inside the sliding window. How about the subsequent steps?\n\nTaking Figure 2.2 as an example, if the sliding window slides one step to the right, there is actually only one value (the next white grid cell) to be computed, $x_t$. Equation 5 is applied independently within every window. Put differently, for every window, the value $x_{t-8}$ is always at the top left corner of the window, the value $x_{t-6}$ is always the top right corner, etc. and $x_{t}$ is always the bottom right corner. \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5OtTI6NCsLA",
                "writer": "author",
                "reply_to": "uTTNgZIVubW",
                "title": "Response to Reviewer Jf4X [Part 2]",
                "comment": " ### Responses to questions\n> Are tables 4 and 5 computed over a number of models? \n\nWe have run the results of Table 4 and 5 over a number of models as suggested by the reviewer. We report these below.\n\nFor Table 5 (Adversarial Training), we observe that our proposed method is statistically more effective for perturbation radius $\\rho_a = 0.125$ and $\\rho_a = 0.25$. For every cell in the table, we report mean CIFAR-10 test accuracy over 2 additional models (for a total of 3 models). When considering larger perturbation radii for adversarial training, it is important to recall that adversarial training monotonically (and steeply) decreases test accuracy as the perturbation radius, $\\rho_a$ becomes large [1,2]. Additionally, recent work has shown that adversarial training is an effective defense for several data poisons [3]. Still, our method can better defend against adversarial training at small radii, and is competitive in the case when the radius is large. The results are shown below:\n\n| Attack \\\\ $\\rho_a$  | 0.125           | 0.25            | 0.5             | 0.75            |\n|:----------:|:---------------:|:---------------:|:---------------:|:---------------:|\n| AR (Ours)  | 33.22 &pm; 0.77 | 57.08 &pm; 0.75 | 81.27 &pm; 2.61 | 79.07 &pm; 3.47 |\n| Random     | 86.31 &pm; 0.42 | 84.17 &pm; 0.20 | 80.11 &pm; 0.06 | 76.26 &pm; 0.07 |\n| Regions-4  | 75.05 &pm; 0.35 | 81.23 &pm; 0.11 | 79.71 &pm; 0.05 | 76.47 &pm; 0.34 |\n| Regions-16 | 47.99 &pm; 0.25 | 71.43 &pm; 0.17 | 80.47 &pm; 0.10 | 76.65 &pm; 0.07 |\n| Error-Max  | 33.30 &pm; 0.14 | 72.27 &pm; 2.18 | 81.15 &pm; 3.58 | 78.73 &pm; 4.20 |\n| Error-Min  | 70.66 &pm; 0.41 | 84.80 &pm; 2.38 | 83.04 &pm; 3.24 | 79.11 &pm; 3.46 |\n\n \nFor Table 4 (Mixing Poisons with Clean Data), we run 3 additional models for a total of 4 models. When mixing in clean data we observe our method always leads to a decrease in test accuracy when poisoned data is added. Put another way, in all but one case, it is better to exclude AR poisoned data than to use it for training. AR poisoning also performs better than all the other poisons when a small amount of clean data (5% or 10% clean data) is mixed in. \n\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Random            | 86.40 &pm; 1.24 | 86.99 &pm; 0.19 | 84.98 &pm; 1.85 | 78.08 &pm; 0.94 | 70.69 &pm; 0.87 |\n| Regions-4         | 88.94 &pm; 0.85 | 86.75 &pm; 0.86 | 83.52 &pm; 0.20 | 78.23 &pm; 0.97 | 70.19 &pm; 3.16 |\n| Regions-16        | 88.03 &pm; 0.57 | 86.23 &pm; 0.68 | 83.01 &pm; 0.48 | 76.52 &pm; 0.91 | 67.24 &pm; 1.72 |\n| Error-Max         | 87.83 &pm; 0.74 | 86.83 &pm; 0.48 | 84.70 &pm; 0.61 | 81.63 &pm; 0.63 | 76.48 &pm; 1.72 |\n| Error-Min         | 88.32 &pm; 1.57 | 87.23 &pm; 0.84 | 84.56 &pm; 0.88 | 78.76 &pm; 1.83 | 67.82 &pm; 1.92 |\n\n> What is the fundamental difference between the noises in the related literature and the one produced in the paper?\n\nA set of AR perturbations comprise a provably separable set of image vectors, and we use the manual-specification of CNN parameters to specify the function which separates them. Unlike other methods, AR perturbations are not optimized with a surrogate network and are resistant to strong data augmentations.\n\n[1] Theoretically Principled Trade-off between Robustness and Accuracy, ICML 2019 \\\n[2] Robustness May Be at Odds with Accuracy, ICLR 2019 \\\n[3] Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training, NeurIPS 2021 \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "yc0AsyS3sjy",
                "writer": "author",
                "reply_to": "Rkvj0Te2L0l",
                "title": "Response to Reviewer wQLZ [Part 1]",
                "comment": " We would like to thank the reviewer for their thoughtful feedback, and for considering our method \u201cinteresting.\u201d \n### Responses to concerns\n> Is the proposed method only applicable to $\\ell_2$ norm? \n\nWe initially measured the $\\ell_2$-norm because our poisons are not optimized for a specific $\\ell_p$ constraint. AR perturbations may have single entries which are high and violate a strict $\\ell_{\\infty}$ constraint. Still, our proposed perturbations can be projected onto any $\\ell_p$-norm ball, including $\\ell_{\\infty}$. To demonstrate that AR poisoning *can* work in the $\\ell_{\\infty}$-norm constrained setting, we provide CIFAR-10 test accuracy for a RN-18, where perturbations are of size $\\frac{8}{255}$ in $\\ell_{\\infty}$-norm. \n\n|                       | Standard Aug | +Cutout | +CutMix | +Mixup |\n|-----------------------|--------------|---------|---------|--------|\n| AR (Ours)              |     20.49    |  26.93  |  17.08  |  15.22 |\n\nImportantly, how one fits an AR perturbation $\\delta$ within the constraint that $\\lVert \\delta \\rVert_{\\infty} \\leq \\epsilon = \\frac{8}{255}$ affects performance. Here, we simply compute $\\epsilon \\frac{\\delta}{\\lVert \\delta \\rVert_{\\infty}}$, which may be suboptimal, but we haven\u2019t had the chance to explore different options. Clipping values or taking the scaled sign of $\\delta$ would make the perturbation no longer autoregressive. We agree that the addition of this experiment makes our work more complete, so we have included this new table in the Appendix Table 6. \n\n> The proposed attack requires a high poison rate to be effective? \n\nThe goal of our work is to prevent others from using our poisoned data to increase the performance of their models, or to train their models in the first place, as in [4, 5, 6, 7] which also poison a very high fraction or all of the data.  We have updated Table 4 (with standard deviation over 4 independent runs), and we see that, in all but one case, adding our poisoned data to clean data reduces the test accuracy of models, so we can indeed effectively prevent others from leveraging our data.  This goal, namely preventing the addition of poisoned data from boosting accuracy, is standard in the literature [4, 5, 6, 7].\n\n\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Difference         | -2.66               | -2.95               | -1.89                | +1.48              | -7.51             |\n\n> In Table 4, the lowest poison rate is 0.6\n\nTaking the reviewer\u2019s suggestion, we have also performed new experiments for when the poison proportion is under 60% (i.e. when clean proportion exceeds 40%). We find that it is better to train a network without AR poisoned data when the clean proportion is 50%, 60%, and even 70%, as desired. The goal is to render the data useless for generalization, while maintaining the content of the image.\n\n|  Poison/Clean Proportion          | 90%            | 80%            | 70%            | 60%            | 50%            |\n|------------|----------------|----------------|----------------|----------------|----------------|\n| Clean        | 91.89 &pm; 0.51 | 91.77 &pm; 0.15 | 91.18 &pm; 0.16 | 91.10 &pm; 0.32 | 90.86 &pm; 0.28 |\n| AR (Ours)  | 92.37 &pm; 0.16 | 91.79 &pm; 0.14 | 91.05 &pm; 0.32 | 90.46 &pm; 0.32 | 89.28 &pm; 0.52 |\n| Random     | 92.68 &pm; 0.39 | 92.08 &pm; 0.42 | 91.94 &pm; 0.22 | 90.58 &pm; 1.22 | 89.78 &pm; 0.57 |\n| Regions-4  | 92.43 &pm; 0.26 | 92.15 &pm; 0.17 | 91.47 &pm; 0.15 | 91.32 &pm; 0.77 | 90.16 &pm; 1.14 |\n| Regions-16 | 92.04 &pm; 0.27 | 91.76 &pm; 0.39 | 91.46 &pm; 0.22 | 90.08 &pm; 1.03 | 89.75 &pm; 0.74 |\n| Error-Max  | 91.26 &pm; 0.23 | 91.18 &pm; 0.47 | 90.68 &pm; 0.83 | 90.12 &pm; 0.50 | 88.76 &pm; 0.70 |\n| Error-Min  | 91.99 &pm; 0.16 | 91.71 &pm; 0.72 | 91.98 &pm; 0.17 | 91.28 &pm; 0.89 | 89.83 &pm; 0.48 |\n\n[4] Unlearnable Examples: Making Personal Data Unexploitable, ICLR 2021 \\\n[5] Adversarial Examples Make Strong Poisons, NeurIPS 2021 \\\n[6] Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder, NeurIPS 2019 \\\n[7] TensorClog: An Imperceptible Poisoning Attack on Deep Neural Network Applications, IEEE Access 2019 \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "LH7WFi7pZK1",
                "writer": "author",
                "reply_to": "CnrzvU3QDMJ",
                "title": "Response to Reviewer GeFk",
                "comment": " We thank the reviewer for their time and for indicating that our method is \u201cefficient,\u201d \u201ctechnically sound,\u201d and \u201cvery practical considering real-world applications.\u201d Below, we respond to concerns and then answer posed questions.\n\n### Responses to weaknesses\n\n> Consider that if the parameters for AR are leaked, can it be used to recover the original image?\n\nNo, the leaked parameters are not sufficient to recover the original image. Consider that we start from a random Gaussian starting signal (Figure 2.1) for every image, which is independent from the AR process and not shared, before applying an AR process. This means that even if AR coefficients were leaked, there would still be 372 floating point values unknown to the victim (for a 32x32x3 image with an AR process that uses a window size 3x3). While AR perturbations from the same AR process may look similar, they are unique (Figure 3, Left). \n\n> but adaptive method (if there are any) can be applied to these samples. Or the model trainer could wait for future advancement for the recovery method\n\nWhile there could be potential detection techniques, as the reviewer suggested, developed for different data poisons, removing these AR poisons is not trivial. We agree this would be a topic for future work.\n\n### Responses to questions\n> In experiments section line210: \"We say that poisoning effectiveness drops from setup A to setup B if the network from poison-trained on setup B has higher test set accuracy than the network poison-trained on setup A. \" I find this is confusing.\n\nThis sentence was mainly used to describe \u201cpoisoning effectiveness.\u201d We agree this sentence was confusing, so we have removed it. Other sentences already define what is \u201ceffective\u201d and how to read numbers in the experimental section.\n\n> For experiments in Table 4, for clean only, is it the same subset of data as in mixing poisons/clean?\n\nThe selected clean subset in Table 4 is different, and i.i.d sampled for clean-only training and for each poison. However, we do not believe this impacts the trends we observe. Below, we provide an updated table, where we report results over 4 independent runs, and observe the same trends.\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Random            | 86.40 &pm; 1.24 | 86.99 &pm; 0.19 | 84.98 &pm; 1.85 | 78.08 &pm; 0.94 | 70.69 &pm; 0.87 |\n| Regions-4         | 88.94 &pm; 0.85 | 86.75 &pm; 0.86 | 83.52 &pm; 0.20 | 78.23 &pm; 0.97 | 70.19 &pm; 3.16 |\n| Regions-16        | 88.03 &pm; 0.57 | 86.23 &pm; 0.68 | 83.01 &pm; 0.48 | 76.52 &pm; 0.91 | 67.24 &pm; 1.72 |\n| Error-Max         | 87.83 &pm; 0.74 | 86.83 &pm; 0.48 | 84.70 &pm; 0.61 | 81.63 &pm; 0.63 | 76.48 &pm; 1.72 |\n| Error-Min         | 88.32 &pm; 1.57 | 87.23 &pm; 0.84 | 84.56 &pm; 0.88 | 78.76 &pm; 1.83 | 67.82 &pm; 1.92 |\n\n\nWhen mixing in clean data we observe our method always leads to a decrease in test accuracy when poisoned data is added. Put another way, in all but one case, it is better to exclude AR poisoned data than to use it for training. AR poisoning also performs better than all the other poisons when a small amount of clean data (5% or 10% clean data) is mixed in.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "4s6zQaGSY6T",
                "writer": "author",
                "reply_to": "67TlknM7kNa",
                "title": "Response to Reviewer FXVx [Part 1]",
                "comment": " We thank the reviewer for their thorough evaluation and for mentioning that our paper is \u201cnovel\u2026well-written and easy to follow.\u201d \n### Responses to weaknesses\n> When tested against adversarial training, the performance is not satisfactory\n\nWe have updated Table 5 (Adversarial Training) by performing additional runs, and we observe that our proposed method is statistically more effective for perturbation radius $\\rho_a = 0.125$ and $\\rho_a=0.25$. For every cell in the table, we report mean CIFAR-10 test accuracy over 2 additional models (for a total of 3 models). \n\nWhen considering larger perturbation radii for adversarial training, it is important to recall that adversarial training monotonically decreases test accuracy as the perturbation radius, $\\rho_a$ becomes large [1, 2]. Additionally, recent work has shown that adversarial training is an effective defense for several data poisons [3]. Still, our method can better defend against adversarial training at small radii, and is competitive in the case when the radius is large. Our updated Table 5 is below:\n\n| Attack \\\\ $\\rho_a$  | 0.125           | 0.25            | 0.5             | 0.75            |\n|:----------:|:---------------:|:---------------:|:---------------:|:---------------:|\n| AR (Ours)  | 33.22 &pm; 0.77 | 57.08 &pm; 0.75 | 81.27 &pm; 2.61 | 79.07 &pm; 3.47 |\n| Random     | 86.31 &pm; 0.42 | 84.17 &pm; 0.20 | 80.11 &pm; 0.06 | 76.26 &pm; 0.07 |\n| Regions-4  | 75.05 &pm; 0.35 | 81.23 &pm; 0.11 | 79.71 &pm; 0.05 | 76.47 &pm; 0.34 |\n| Regions-16 | 47.99 &pm; 0.25 | 71.43 &pm; 0.17 | 80.47 &pm; 0.10 | 76.65 &pm; 0.07 |\n| Error-Max  | 33.30 &pm; 0.14 | 72.27 &pm; 2.18 | 81.15 &pm; 3.58 | 78.73 &pm; 4.20 |\n| Error-Min  | 70.66 &pm; 0.41 | 84.80 &pm; 2.38 | 83.04 &pm; 3.24 | 79.11 &pm; 3.46 |\n\n> Assuming that all the data can be poisoned is not realistic\n\nThe goal of our work is to prevent others from using our poisoned data to increase the performance of their models, or to train their models in the first place, as in [4, 5, 6, 7] which also poison a very high fraction or all of the data.  In Table 4 we see that, in all but one case, adding our poisoned data to clean data reduces the test accuracy of models, so that we can indeed effectively prevent others from leveraging our data.  This goal, namely preventing the addition of poisoned data from boosting accuracy, is standard in the literature [4, 5, 6, 7].\n\n| Poison/Clean Proportion | 40%            | 30%            | 20%            | 10%            | 5%           |\n|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Clean             | 90.29 &pm; 0.38 | 88.57 &pm; 0.34 | 85.17 &pm; 1.10 | 74.65 &pm; 4.41 | 70.20 &pm; 5.22 |\n| AR (Ours)         | 87.63 &pm; 0.68 | 85.62 &pm; 0.62 | 83.28 &pm; 0.90 | 76.13 &pm; 2.34 | 62.69 &pm; 5.58 |\n| Difference         | -2.66               | -2.95               | -1.89                | +1.48              | -7.51             |\n\nTaking the reviewer\u2019s suggestion, we have also performed new experiments when the poison proportion is under 60% (i.e. when clean proportion exceeds 40%). We find that it is better to train a network without AR poisoned data when the clean proportion is 50%, 60%, and even 70%, as desired. The goal is to render the data useless for generalization, while maintaining the content of the image.\n\n|  Poison/Clean Proportion          | 90%            | 80%            | 70%            | 60%            | 50%            |\n|------------|----------------|----------------|----------------|----------------|----------------|\n| Clean        | 91.89 &pm; 0.51 | 91.77 &pm; 0.15 | 91.18 &pm; 0.16 | 91.10 &pm; 0.32 | 90.86 &pm; 0.28 |\n| AR (Ours)  | 92.37 &pm; 0.16 | 91.79 &pm; 0.14 | 91.05 &pm; 0.32 | 90.46 &pm; 0.32 | 89.28 &pm; 0.52 |\n| Random     | 92.68 &pm; 0.39 | 92.08 &pm; 0.42 | 91.94 &pm; 0.22 | 90.58 &pm; 1.22 | 89.78 &pm; 0.57 |\n| Regions-4  | 92.43 &pm; 0.26 | 92.15 &pm; 0.17 | 91.47 &pm; 0.15 | 91.32 &pm; 0.77 | 90.16 &pm; 1.14 |\n| Regions-16 | 92.04 &pm; 0.27 | 91.76 &pm; 0.39 | 91.46 &pm; 0.22 | 90.08 &pm; 1.03 | 89.75 &pm; 0.74 |\n| Error-Max  | 91.26 &pm; 0.23 | 91.18 &pm; 0.47 | 90.68 &pm; 0.83 | 90.12 &pm; 0.50 | 88.76 &pm; 0.70 |\n| Error-Min  | 91.99 &pm; 0.16 | 91.71 &pm; 0.72 | 91.98 &pm; 0.17 | 91.28 &pm; 0.89 | 89.83 &pm; 0.48 |\n\n[1] Theoretically Principled Trade-off between Robustness and Accuracy, ICML 2019 \\\n[2] Robustness May Be at Odds with Accuracy, ICLR 2019 \\\n[3] Better Safe Than Sorry: Preventing Delusive Adversaries with Adversarial Training, NeurIPS 2021 \\\n[4] Unlearnable Examples: Making Personal Data Unexploitable, ICLR 2021 \\\n[5] Adversarial Examples Make Strong Poisons, NeurIPS 2021 \\\n[6] Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder, NeurIPS 2019 \\\n[7] TensorClog: An Imperceptible Poisoning Attack on Deep Neural Network Applications, IEEE Access 2019 \\\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "uTTNgZIVubW",
                "writer": "author",
                "reply_to": "9y3pjgOA3cN",
                "title": "Response to Reviewer Jf4X [Part 1]",
                "comment": " We thank the reviewer for their feedback, for asking questions which will improve our work, and for referring to our method as \u201cfascinating.\u201d Below, we respond to concerns and then answer posed questions.\n\n### Responses to weaknesses\n> Unclear how much of the evaluation is an artifact of chosen optimisation hyperparameters\n\nTo test the effect of optimizer and learning rate, we have run additional experiments to confirm the effectiveness of our method. Specifically, we consider 3 optimizers: SGD, SGD+Momentum ($\\beta=0.9$ and Adam ($\\beta_1=0.9$, $\\beta_2=0.999$). We also consider 3 different learning-rate schedules: cosine learning-rate, single-step decay (epoch 50) and multi-step decay (epoch 50, epoch 75) with decay factor of 0.1. Thus, there are a total of 9 optimizer and learning rate combinations. In the following table, for each poison, we report mean CIFAR-10 test accuracy over the 9 combinations of optimizers and learning rates. Our proposed method remains nearly unaffected by choice of hyperparameters, and performs better than all other poisons.\n\n| AR (Ours)      | Random          | Regions-4     | Regions-16     | Error-Max    | Error-Min       |\n|----------------|-----------------|---------------|----------------|--------------|-----------------|\n| 12.23 &pm; 1.22 | 33.39 &pm; 31.24 | 22.89 &pm; 7.6 | 18.73 &pm; 5.52 | 16.6 &pm; 2.8 | 22.96 &pm;  7.16 |\n\n> Unclear how one compares performance of different correlated noises\n\nAll the poisons we consider use the ground truth label as a dependent variable to generate the perturbations. Thus, poisons we consider contain perturbations that may be correlated with the ground truth. While there is an intractable number of possible noise patterns that could be used to perturb data and induce a correlation, we prove that AR perturbations have a particular structure which allows a manually-specified CNN filter to detect it perfectly (Lemma 3.1). Thus, AR perturbations are correlated noise designed for convolutional layers. By making sure that poisons we consider are bounded by the same $\\ell_2$ perceptibility constraint, we are able to compare performance between different kinds of correlated noise.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9y3pjgOA3cN",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "",
                "comment": " Paper considers a setting of `unlearnable examples' where a given dataset is perturbed in a way to make it hard to learn the true task. In essence, data here gets perturbed with correlated noise such that when learning is attempted, models learn to focus on the noise rather than on the true features useful for generalisation. While prior work focused on generating class-wise poisons, in this work the noise is generated per sample using a Markov process, producing linearly separatable noise. Paper thoroughly evaluates the setting and demonstrates that the approach effectively stops generalisation when the whole dataset is poisoned and struggles in a similar way in presence of adversrail training or dilution with clean data.  \nStrengths:\n+ Interesting setting\n+ Idea of hardness of learning is rather fascinating\n\nWeaknesses:\n+ Unclear how much of the evaluation is an artifact of chosen optimisation hyperparameters\n+ Unclear how one compares performance of different correlated noises Thank you very much for the paper, it is a very interesting read! I only have a handful of questions:\n\n1. Are tables 4 and 5 computed over a number of models? Given how close the numbers are, it would be great to know if the differences are observed on distributional level, not just per model\n2. Given that we can produce arbitrary correlated noise of different flavors, how should one think about it? What is the fundamental difference between the noises in the related literature and the one produced in the paper? This naturally leads to my final question.\n3. Given the argument of easier learnability of different noises, it turns the question to how much observed behaviour is an artifact of the optimisation procedure itself? Did you try running the experiments with different lr/optimiser options? \n\nMinor:\n* Punctuation missing around eqs in some places N/a",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "67TlknM7kNa",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "",
                "comment": " This paper proposes a new data poisoning attack to prevent data scraping. The proposed method adds class conditional autoregressive (AR) noise to training data to prevent people from using the data for training, and the method is data and model independent, which means that the same noise can be used to poison different datasets and models of different architectures.\n\nThe intuition behind the idea is that easy to learn noise is more effective at data poisoning, and AR noise generated in the proposed way is easy for neural network to learn. The authors show that a manually specified 3-layer CNN with AR filter can easily learn class information from the AR noise. Experiments on four benchmark datasets (CIFAR10, STL10, SVHN, CIFAR100) show that the proposed method performs better than other four baselines (Error-min, Error-max, Regions, Random noise). Strengths:\n\n- The proposed method is novel as autoregressive process hasn't been used before to do data poisoning. The method is easy to implement and the same AR coefficients can be used for different datasets and architectures as long as the numbers of classes are the same. Though code is not available, pseudo code (algorithms) and implementation details are provided. It is better that if actual code can be provided for reproduction of the results.\n\n- The paper is well-written and easy to follow. Empirical results on four different datasets show that the method performs better than other baselines, both under normal setting and defense settings.\n\nWeakness:\n\n- Though the proposed method performs better than other baselines compared in the paper, when tested against adversarial training, the performance is not satisfactory. It performs similarly to other baselines under this setting and the poisoning effect is not good, especially when the radii is large.\n\n- As pointed out in the paper, assuming that all the data can be poisoned is not realistic. In section 4.3.3, the poisoning methods are evaluated using a mix of poisoned and clean data. Under this setting, the performance of the proposed method is not good and similar to those of other baselines.\n\n About the process of AR noise generation:\n\n- It is clear how to generate AR noise at the beginning inside the sliding window. How about the subsequent steps? Take the example in Figure 2 as an example, if the sliding window slides one step to the right, there are three values to be generated. Are $x_{t-7}$ up to $x_t$ used to generate the next one ($x_{t+1}$)? Then $x_{t-6}$ up to $x_{t+1}$ are used to generate $x_{t+2}$, and so on.  The author points out that the method does not perform well against adversarial training and experiments show that when evaluated using a mix of poisoned and clean data, the performance is also not good.",
                "rating": 6,
                "confidence": 4
            },
            {
                "review_id": "CnrzvU3QDMJ",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "",
                "comment": " This paper proposed autoregressive poisoning techniques to protect data from being exploited by unauthorized machine learning models. The proposed method does not rely on optimizations while generic towards different model architectures and different datasets. This paper also provides insight into why the proposed method is effective. \n Strengths:\n- The proposed method is efficient and technically sound. Existing works rely on optimizations which is the bottleneck. The proposed method does not rely on optimizations, and the parameters for AR are easy to find.  \n- The existing works are also shown that do not transfer well between model architectures or datasets. Experiment results show that one set of AR is generic across different architectures or datasets. \n- The efficient and generic can be very practical considering real-world applications.\n- Experiment results also demonstrated AR generated unlearnable examples are more robust towards augmentations.\n\n---\nLimitations:\n- Once the data is released, the defender may not modifies the data anymore, and the model trainer can retroactively apply new models/methods [1]. The adaptive case should be carefully examined. Consider that if the parameters for AR are leaked, can it be used to recover the original image? Or if a portion of the clean images are leaked, using pair of clean and unlearnable versions, is it possible to reverse the AR process? \n- In section 3.3, the assertion that the noises are easy to learn is more effective for poisoning, this could also mean they are easy to detect. Such as calculating sample-specific loss at the end of each training epoch. Although only detecting such samples does not make them \"learnable,\" but adaptive method (if there are any) can be applied to these samples. Or the model trainer could wait for future advancement for the recovery method as mentioned in [1]. \n\n[1] Data Poisoning Won\u2019t Save You From Facial Recognition, ICML 2021 Workshop AML\n\n---\nAfter the author's response, I increased my rating score to 7. My main concerns over possible reverse operation if parameters are leaked have been well addressed. \n - In experiments section line210:  \"We say that poisoning effectiveness drops from setup A to setup B if the network from poison-trained on setup B has higher test set accuracy than the network poison-trained on setup A. \" I find this is confusing.\n- For experiments in Table 4, for clean only, is it the same subset of data as in mixing poisons/clean?  Please address the potential limitations in the Strengths And Weaknesses section. ",
                "rating": 7,
                "confidence": 5
            },
            {
                "review_id": "Rkvj0Te2L0l",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_1vusesyN7E",
                "title": "",
                "comment": " This paper proposes to use autoregressive processes to generate perturbations for data poisoning. The generated perturbations, despite looking complex, are actually very simple. One advantage of the proposed method is that its generated perturbations are dataset and architecture independent. The paper evaluates the proposed method on multiple datasets and networks, showing the effectiveness of the perturbations when the poison rate is high. The strengths of this paper include\n1. The proposed attack method is interesting.\n2. The proposed method has good transferability.\n\nBut I still have the following concerns:\n\n1. Is the proposed method only applicable to $\\ell_2$ norm? The paper uses the sentence \"We measure AR perturbations\nin $\\ell_2$ because measuring in $\\ell_\\infty$ would underestimate the extent to which these perturbations are less perceptible than purely  $\\ell_\\infty$ random noise\" to explain why it uses $\\ell_2$ norm. But this sentence is hard to follow, and this short explanation is not convincing. The paper should provide more clear and convincing explanation about why it *only* uses $\\ell_2$ norm.\n\n2. The proposed attack requires high poison rate to be effective? In most experiments, the paper uses poison rate 1. In Table, the lowest poison rate is 0.6. The assumption of high poison rate is very strong. In practice, if the data is collected from multiple sources, then the attack is not effective? In the case that the data is collected from one source (the adversary), the entity who trains the model would be more cautious about the quality of data due to the high risk when the data only comes from one source.\n\n3. Section 3.3 is not easy to follow, and the logic is not very clear. I think Section 3.3 is one of the most important parts in the paper since it explains why the proposed method works. After reading Section 3.3, I am still very confused. The relation between Lemma 3.1 and the effectiveness of the proposed method in *poisoning attacks* is not obvious. \n 1. Is the proposed method only applicable to $\\ell_2$ norm?\n2. The proposed attack requires high poison rate to be effective?\n3. Is the proposed method only applicable to computer vision tasks? 1. The paper only studies $\\ell_2$ norm.\n2. The poison rate is high. The lowest poison rate studied in the paper is 0.6.\n3. The theoretical analysis is not sufficient. The relation between Lemma 3.1 and the effectiveness of the proposed method in poisoning attacks is not obvious.\n",
                "rating": 5,
                "confidence": 4
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "auto-regressive perturbation method",
                "Sentiment Expression": "novel",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The method",
                "Sentiment Expression": "more easy to be used",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the idea",
                "Sentiment Expression": "novel and intuitively reasonable",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The authors",
                "Sentiment Expression": "responded",
                "Criteria Facet": "Compliance",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The rebuttal",
                "Sentiment Expression": "succeeded to remove the confusions and convince us about the empirical significance",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "6UtOXn1LwNE": {
        "paper_id": "nips_2022_6UtOXn1LwNE",
        "paper_title": "Models of human preference for learning reward functions",
        "paper_abstract": "The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments. These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling preferences instead as arising from a different statistic: each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences. We also prove that the previous partial return model lacks this identifiability property without preference noise that reveals rewards' relative proportions, and we empirically show that our proposed regret preference model outperforms it with finite training data in otherwise the same setting. Additionally, our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research.",
        "paper_acceptance": "Reject",
        "meta_review": "The submitted paper was reviewed by 4 knowledgable reviewers and the reviewers and authors enganged in intense discussions. The authors clarified many details in these discussion but could not convince the reviewers in all regards (there are still open concerns regardings the proofs and the update proofs came in rather late so that there was insufficient time for the reviewers to further interact; there concerns regarding experiments although I discounted most of those regarding to scalability as I agree with the authors in that regard to some extent; etc.). Moreover, looking at the discussions and the authors' responses, the paper would benefit from making several points more clear/improving their presentation, likely by including parts which came up in the discussions in the paper. Considering all this, I think this paper should go through another round of reviews before it should be accepted and I am recommending rejection of the paper. Please note that it was not easy to come to this decision - there are some important insights and experiments in the paper which should be made available to the community asap. Thus I would honestly encourage the authors to improve their paper considering the reviewers' comments and take-aways from the discussion and submit a revised version of the paper at one of the upcoming conferences. I am already looking forward to seeing an improved version of the paper being published.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "Q3R-cxcQoke",
                "writer": "author",
                "reply_to": "BJ2Tzrn6jJc",
                "title": "Considering our contributions",
                "comment": " Regarding that we only use simple grid worlds (though up to 100 random versions of them), we can only make subjective arguments about the necessity of scalability in addition to our various contributions, which we understand may not persuade you, especially if your mind is already made. \n\nIn our responses here, we have argued that the existence of deep IRL work\u2014noting the inner-loop bottleneck\u2014provides optimism that similar approaches can be applied to learning reward functions with regret-based preference models; without such optimism, the importance of demonstrating scalability in this paper would increase. \n\nAlso, consider that **there are 100s of papers on IRL and there will only be _this one paper_ on reward learning from regret-based preferences, if you and our other reviewers decide it is worthy of publication. The first paper on IRL, in 2000 by Ng and Russell, also only focused on simple problems: two grid worlds and two versions of mountain car. We nonetheless share your strong interest in seeing regret-based reward learning scale to complex, real-world problems**. \n\nBut we worry that it would be a disservice to progress in this popular form of learning reward functions to make showing scalability a hard constraint for publishing this _first_ paper, which we believe **provides urgently needed insight about the partial return preference model that continues to pervade new publications on learning reward functions from pairwise segment preferences, likely _including most that are currently in progress_ by other researchers.** If one instead considers whether the contributions we did make\u2014a human study with empirical results, new preference model, theoretical results, synthetic preferences results, a new learning algorithm that approximates regret, and a clear existence proof that the previously overlooked preference model matters\u2014are together sufficient for publication in NeurIPS, our contributions become a strength of this submission.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "iIZ-k4os-kp",
                "writer": "author",
                "reply_to": "BJ2Tzrn6jJc",
                "title": "Correcting our error",
                "comment": " **We ask you to reconsider your score, given that you wrote that our original response _resolved several of your concerns_ and also depending on how well our responses below and above (in \"Considering our contributions\") satisfy your concerns about our lack of focus on scalability and our theoretical results.**\n\n**For @xps1 and @SeBN:** Thanks for further clarification about your concerns regarding the comparability of Thms 3.1 and 3.2. In our first response to your reviews, we focused in part on how the choice of optimization (max likelihood or otherwise) doesn't matter in the Thm 3.2 proofs, since noiselessly providing preferences based on partial return creates a many-to-one mapping. \n\n**Your point above about the proof of Thm 3.1 not applying to noiseless preferences is well taken, and we agree with you. Thank you for identifying this error.** We originally removed the temperature parameter from the softmax formula because it is redundant with the scaling of the reward and it simplified our notation to not include it. However, as you suggest, for the proof of Thm 3.1 to apply to the noiseless preferences setting of Thm 3.2, a revision is needed. \n\nFortunately, **the required change is minor**. **We can add back the temperature as a optimizable parameter into Eqs 2 and 5 and allow a special case for temperature = 0, where the result is a hard max, i.e., if temperature = 0, the preference is given deterministically to the segment with the higher partial return in Eq 2 or regret in Eq 5.**\n\nWith this tweak, when the temperature is 0, the proof of Thm 3.1 covers both the stochastic and noiseless settings, and the preference generator is realizable in the noiseless setting for Thm 3.1 and in the two proofs of Thm 3.2 (though these two proofs don't rely on the learning algorithm to provide their negative results). In other words, **Thm 3.1 does include the noiseless settings of Thm 3.2 as a special case, but both theorems' proofs required this \"tweak\" to have a learning algorithm that can realize the reward function that created the noiseless preferences.**\n\nThe need for this added special case in the theoretical setting is now clear. However, in practical settings, the originally submitted algorithm (with no temperature) will simply scale reward until the improvement in loss after each epoch of learning is extremely small, which can be used as a stopping criterion, at which point the reward function results in nearly deterministic preferences. Such an approach is effective with noiseless preference labels in our experiments, as seen in Section 6.2 (with further detail in App F.2.1) and in the randomly generated _stochastic_ MDPs in App F.2.5.\n\nThe revisions and clarifications above will be included in the final copy. In particular, we will revise to emphasize that _in noiseless preference label settings_, both theorems provide insight about whether the preferences contain the information required to recover the set of optimal policies (via directly recovering a reward function). **In other words, the theorems are really about each preference model _as a preference generator_, and the learning algorithm used in Thm 3.1 is merely meant to show _what information exists_ in each type of preferences.** For the negative result of Thm 3.2, its two proofs clearly show that some MDPs exist in which preferences based on partial return have the many-to-one mapping we referred to and therefore is not identifiable under _any_ learning algorithm. For the positive result of Thm 3.1, we merely need to show that there is one learning algorithm that permits identifiability in any MDP, which that proof does with the above tweak to allow a special hard max case.\n\n(Note that what we wrote above was summarized and rephrased in our response to @xps1 with the same subject, \"Correcting our error\".)",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "02hkRIywWB",
                "writer": "author",
                "reply_to": "3KddY4BdXaE",
                "title": "Correcting our error",
                "comment": " > I\u2019m not sure I understand this. My concern with this section was not related to the theoretical significance of noiseless settings (by the way, I do think it\u2019s important). I don\u2019t think SeBN suggested this either, so can you clarify what you mean by this?\n\n**We misinterpreted your statement** \"... not convinced that Theorem 3.2 is all that significant\" to be about the noiseless setting, but, assuming we now understand you better, your issue was more specifically with the comparability of 3.1 and 3.2 and that a reward function providing noiseless partial-return-based preferences is not realizable under the algorithm we use in Sec 2 and Definition 1 (and is the common algorithmic component of [9-16]). **We appreciate your added clarification and that your issues do not regard the importance of the setting of Thm 3.2 but rather some technical aspects, since that means we agree that the theoretical problem is impactful and can focus on the solution.**\n\n> In any case, I don\u2019t think either of our points have been addressed here. I'm skeptical of the claimed difference between the regret model and the return model that is presented in Section 3 because it does not appear to be a fair comparison. As SeBN put it quite well: 3.1 and 3.2 do not satisfy the same premise. The noiseless setting falls outside the problem setting established in Eq 2 and 5 where a logistic model is used - it\u2019s misspecified for this model class for finite rewards. Meanwhile, Theorem 3.1 is using this realizability to derive the positive result. This is used in the first line of the proof. I think the comparison would be fair if either the negative result of Theorem 3.2 were strengthened to the case where preference function is realizable but it still fails to be identifiable, or the positive result of Theorem 3.1 were shown to extend to a non-trivial class of problems where there is misspecification.\n\n**We agree with your points above and thank you and @SeBN for pushing back to reveal this error.** In our response to you and @SeBN that is in @SeBN's thread, we share **a small revision to solve the realizability issue** for noiseless preferences. The issue you two identified actually affects both Thm 3.1 (in the noiseless subset of its settings) and Thm 3.2, so the fix for realizability should be applied to both. We also want to emphasize that in our final copy, we will clarify that **the theorems focus on what information such infinite datasets hold when either preference model acts as the preference generator, where any usage of a learning algorithm in the proof Thm 3.1 only serves to show that the information exists in the dataset to recover an equivalent reward function**.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "sZTn5_QoKRC",
                "writer": "author",
                "reply_to": "SxGr_BLFMl1",
                "title": "Resolving remaining clarification questions",
                "comment": " > Thank you. Figure 1 seems to be a safe RL example. That is why I\u2019m a little confused about the motivation.\n\nThank you for explaining your confusion. Although Figure 1 may indeed remind readers of research on safe RL (and safety in autonomous driving, which employs a different definition of the word \"safe\"), here Figure 1 is used as a simple example of how human preferences intuitively do not seem to given based on the partial return of a trajectory (or else the left trajectory would be preferred). We hope that this exchange has helped clarify that this work is unrelated to safe RL; it solely focuses on more accurate inference of the reward functions that drive human preferences. Any RL algorithm can then be used to learn a task from such a human-aligned reward function. If an RL practitioner decided to add safety constraints to such a learned reward function (which however is certainly not the focus of this paper), they could then use various safe RL algorithms to learn from the reward function\u2014created through regret preference models\u2014such as CPO.\n\n> Sorry for the unclear part. In the experiments, there are baselines named as Partial return (noiseless) and Partial return (stochastic). Are the two baselines from [9-16] or created by the paper? There are a lot of partial return methods. Why not compare with the methods from those papers?\n\nUnderstood. Thanks for communicating back and forth with us to get to this point of clarification! **The partial return (stochastic) preference model is taken exactly from [9-16] and is common to all of them. And the \"partial return (stochastic)\" reward learning algorithm\u2014created from joining both the preference model (Eq 2) and maximizing likelihood (Eq 2)\u2014is the common part of each learning algorithm from [9-16].** L96-97 of the currently downloadable version (lines unchanged from the original submission) were meant to state this relationship. We will be more explicit in the final version to prevent confusion like yours. Comparing preference models, which was not the focus of _their_ papers, is our focus. Instead, they focused on largely orthogonal algorithmic elements, like what segment pairs to present for elicitation [13] or how to maximize likelihood (Eq 1) with a deep neural network representation of the reward function [9]. So this baseline appears to be precisely what you are asking for. \n\nThe partial return (noiseless) preference model is _only_ used to generate preferences, not for the reward learning algorithm, as we explain at the start of Section 6.2 (L327\u2013331 in the currently downloadable version). This noiseless preference generator is not typically used by [9-16]. You can see in our comments to all reviewers an explanation of the importance of these noiseless versions of each preference model, despite that they are not typically employed in related work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "BJ2Tzrn6jJc",
                "writer": "official_reviewer",
                "reply_to": "2N6eDn4tzA",
                "title": "Thank you for the response",
                "comment": " I would like to thank the author for the response. It indeed resolves several concerns of mine.\n\nUnfortunately, my concern regarding the main theoretical contribution (Theorem 3.1 and 3.2) of the work remains.\nI am not convinced by the argument of the author that \"3.1 shows that regret is identifiable under the conditions of 3.2 as the setting of 3.2 is a special case of 3.1\". For example, as reviewer xps1 points out in his response, why does the first line of the proof of 3.1 hold if we change to the setting of 3.2 (can minimizing the cross-entropy even work in the setting of 3.2)? Maybe totally re-written proofs of the 2 theorems with a consistent setting are more persuasive. Furthermore, if the noiseless setting is not practical, the common setting should be the one from the logistic observation model in Equation 2 (the setting in 3.1).\n\nBesides, in my view, saying that this work does not focus on scalability to justify the experiment involves only a toy grid world environment is not convincing. While different in the inputs, the solution in this work and IRL have a similar computation bottleneck of an inner loop of policy optimization. However, works on IRL (e.g., the popular Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization by Chelsea Finn 2016) have impressive experimental results. This IRL work has been also cited in many works in the last 5 years.\n\nTherefore, I decide the keep my score.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "SxGr_BLFMl1",
                "writer": "official_reviewer",
                "reply_to": "EczQRsbt7Tg",
                "title": "Feedback again",
                "comment": " >Regarding safe RL, this paper focuses on \n\nThank you. Figure 1 seems to be a safe RL example. That is why I\u2019m a little confused about the motivation. \n\n> about partial return and [9-16]\n\nSorry for the unclear part. In the experiments, there are baselines named as Partial return (noiseless) and Partial return (stochastic). Are the two baselines from [9-16] or created by the paper? There are a lot of partial return methods. Why not compare with the methods from those papers? \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3KddY4BdXaE",
                "writer": "official_reviewer",
                "reply_to": "eXpYT0v_GV",
                "title": "Thanks for the response",
                "comment": " Thanks for your response. I appreciate the details and clarifications.\n\n> Noiseless preferences are not theoretically interesting or should be connected to past work. (@xps1, @SeBN)\n\nI\u2019m not sure I understand this. My concern with this section was not related to the theoretical significance of noiseless settings (by the way, I do think it\u2019s important). I don\u2019t think SeBN suggested this either, so can you clarify what you mean by this?\n\n\nIn any case, I don\u2019t think either of our points have been addressed here. I'm skeptical of the claimed difference between the regret model and the return model that is presented in Section 3 because it does not appear to be a fair comparison. As SeBN put it quite well: 3.1 and 3.2 do not satisfy the same premise. The noiseless setting falls outside the problem setting established in Eq 2 and 5 where a logistic model is used - it\u2019s misspecified for this model class for finite rewards. Meanwhile, Theorem 3.1 is using this realizability to derive the positive result. This is used in the first line of the proof. I think the comparison would be fair if either the negative result of Theorem 3.2 were strengthened to the case where preference function is realizable but it still fails to be identifiable, or the positive result of Theorem 3.1 were shown to extend to a non-trivial class of problems where there is misspecification.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "EczQRsbt7Tg",
                "writer": "author",
                "reply_to": "HdiHOg3ZLK",
                "title": "To improve our clarity further, some explanations and questions",
                "comment": " We appreciate you following up on these questions and topics. We're happy to discuss further and hope to fully understand your confusion and preempt similar confusion for future readers through revisions for clarity.\n\nRegarding **safe RL**, this paper focuses on the problem of how to _learn a reward function_ that is aligned with the interests of human stakeholders, specifically focused on the well-established technique of learning from preferences over pairs of trajectory segments. To our knowledge, safe RL such as CPO, on the other hand, generally assumes that a reward function and a set of constraints _are already given_ and focuses on RL algorithms that do policy improvement while making various guarantees to avoid violating the safety constraints. In short, reward learning and safe RL are different problems that do not seem particularly related. We do see how a different usage of the word \"safety\" could encompass the alignment of reward functions\u2014since poorly aligned objective functions generally are a significant societal threat\u2014despite the safe RL literature not focusing on such alignment.\n\nWe do not understand your question about partial return and [9-16]. Could you elaborate on what you mean and why more than one implementation would be desirable?\n\nRegarding **Figure 1**, we will add some more explanation below to see if any of it addresses your confusion. We implicitly assume a reward function that\u2014in these two segments\u2014only deviates from 0 to reflect damage to the vehicle or harm to the passengers. If it would help to make that information more explicit, please let us know and we will add it. The left segment involves no damage to the vehicle and therefore has 0 reward, even though it leads the car to an end state in which the car will almost certainly have a horrible collision within seconds. The right segment involves minor damage but otherwise does roughly the reverse of the left segment: it takes the car from a start state in which collision will likely happen soon to a safe end state. (Additionally, are you confused about how the figure is communicating the start states and end states, which are simply the beginning and end of the arrows / where the two images of the car are? If so, we can also write that information more clearly.) So by looking at partial return alone (i.e., the sum of reward along the segment but _not_ the start or end state values), the left segment is better despite being suboptimal, but human intuition prefers the right (optimal) segment, and the right segment has a lower deterministic regret (Eq. 3) because deterministic regret does include the start and end state values.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "HdiHOg3ZLK",
                "writer": "official_reviewer",
                "reply_to": "KS33hv1S1j",
                "title": "Thanks and some clarification",
                "comment": " I thank the authors for their responses. However, I notice that some of my comments were not completely addressed. \n\nDo you mean partial turn indicates [9-16] in the experiments? Why just one implementation? \nDo you need to compare with some safety rl methods, like CPO?\n\nFigure 1 does not seem to help improve understanding of the idea. It would be better to provide more explanation about Figure 1 or change an example. Why not add some related work about safety RL? \n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "2N6eDn4tzA",
                "writer": "author",
                "reply_to": "5Eg4LFVc94B",
                "title": "Response to reviewer, part 2",
                "comment": " **Please compare to inverse reinforcement learning (IRL) methods.**\n\nThe inputs to IRL and learning reward functions from pairwise preferences (which appears to be the second most popular method of reward learning after IRL) are different. IRL requires demonstrations, not preferences over segment pairs. However, because a  a regret-based preference model always prefers optimal segments over suboptimal segments, at least one connection can be made. If one assumes that a demonstrated trajectory segment is noiselessly optimal (as in the foundational 2004 Abbeel and Ng IRL / apprenticeship learning paper), then such a demonstration is equivalent to expressing preference or indifference for the demonstrated segment over all other segments (or, equivalently, that no other segment is preferred over the demonstrated segment). However, IRL has its own identifiability issues in noiseless settings (see [http://proceedings.mlr.press/v139/kim21c.html](http://proceedings.mlr.press/v139/kim21c.html)) that, viewed from the lens of preferences, come in part from the \"indifference\" part of the above statement: since there can be multiple optimal actions from a single state, it's not generally correct to assume that a demonstration of one such action shows a preference over all others, and therefore it remains unclear in IRL what _other _actions are optimal. Note that since partial-return-based preferences can prefer suboptimal segments over optimal segments, the common assumption in IRL that demonstrations are optimal does not map as cleanly to partial-return-based preferences.\n\nAs mentioned in our response to all reviewers under \"Practically using regret is challenging\", our regret preference model also relates to IRL in that many IRL algorithms require solving an MDP in the inner loop, like would be required for a perfect measure of regret while learning a reward function.\n\nAn empirical comparison of IRL and learning reward functions from pairwise preferences would be valuable but is out of our scope\u2014we are focused on an established technique that is not IRL, and its close relationship to IRL does not mandate that any research project test IRL algorithms too\u2014and would be its own full research project. Further, how to do an apples-to-apples comparison is non-obvious, since the inputs are different.\n\nWe have added a discussion of the relationship to IRL in App B.5.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "5Eg4LFVc94B",
                "writer": "author",
                "reply_to": "DtNlCuPRJy",
                "title": "Response to reviewer, part 1",
                "comment": " Thank you for your thoughtful review. We appreciate the opportunity to clarify certain technical details, the relationship of this work to inverse reinforcement learning, why noiseless preferences are informative, and why scalability is not our focus.\n\n**Would defining reward as r(s,a) or r(s) affect the results in the paper?**\n\nIt would not. Our definition of reward is a generalization of reward defined over state or state-action pairs, which can be trivially mapped to reward over tuples of state, action, and next state. E.g., r(s,a) is mapped to an r(s,a,s') function simply ignoring the next state, s'.\n\n**\"the proposed regret model is limited to the case of deterministic policies\"**\n\nThis is a misunderstanding. The regret model applies to stochastic policies as well. Our definition of a policy in L68-69 is in stochastic terms. Further, note that the algorithm relies on optimal advantage values; any state-action pair that has support (i.e., non-zero probability) in an optimal policy has an advantage of 0. That's true even if there is more than one action that is optimal from a state. And a policy that stochastically mixes multiple optimal actions from each state will also always create state-action pairs with an advantage of 0 (otherwise it would be sub-optimal). Additionally, please see our response to Reviewer ieTk under \"What if there is a set of optimal policies\u2026\", which is relevant because any MDP with two optimal deterministic policies necessarily has infinite optimal stochastic policies (via mixing them as mentioned above).\n\n**\"if I am not missing anything, Theorem 3.1 and 3.2 are not comparable... in Theorem 3.1, if $regret_{\\tilde{r}}(\\sigma_{1}) < regret_{\\tilde{r}}(\\sigma_{2})$, $P_{regret}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})>0.5$, in Theorem 3.2, if $\\Sigma_{\\tilde{r}}(\\sigma_{1}) > \\Sigma_{\\tilde{r}}({\\sigma_{2}})$, $P_{\\Sigma_r}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})=1$.\"**\n\nPlease see our response to all reviewers under \"Noiseless preferences are not theoretically interesting or should be connected to past work\". Building on that general response, note that they are very comparable in the sense that the setting that matches that of 3.2 is a special case of 3.1. In other words,  \"if $regret_{\\tilde{r}}(\\sigma_{1}) < regret_{\\tilde{r}}(\\sigma_{2})$, $P_{regret}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})>0.5$\" includes all cases where \"if $regret_{\\tilde{r}}(\\sigma_{1}) < regret_{\\tilde{r}}(\\sigma_{2})$, $P_{regret}(\\sigma_1 \\succ \\sigma_2 | {\\tilde{r}})=1$\". Therefore, 3.1 shows that regret is identifiable under the conditions of 3.2, if you swap partial return for regret and flip the > sign, as well as under other conditions.\n\n**\"it raises the question of whether P\u03a3r is indeed non-identifiable if Theorem 3.2 has the same premise as Theorem 3.1.\"**\n\nWe agree and were careful to only claim that partial return is only non-identifiable in this noiseless setting. See L11 (\"without preference noise...\"), L47 (\"without preference noise...\"), L167 (\"without the distribution over preferences providing information\"). We suspect any scale-invariant regression model is identifiable with Boltzmann noise, so both models would be identifiable in that respect.\n\n**\"the work needs to illustrate whether the proposed method is scalable to more practical problems, e.g., by including experiments with neural networks and larger state/action spaces\"**\n\nWe understand that some researchers value showing deep RL applicability of a new approach more than others. However, we don't claim scalability either as a contribution or a characteristic of regret-based preference models and openly discuss how it might be addressed in the latter portion of Appendix F.1. We consider scalability to be one of many important dimensions of engineering and experimentation, but it did not make the cut in this paper for reasons we detail in our general message to all reviewers, under \"This paper does not demonstrate scalability to more complex environments.\"\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "eXpYT0v_GV",
                "writer": "author",
                "reply_to": "p-z5ilzAs5",
                "title": "Response to reviewer, part 2",
                "comment": " **\"The description of the algorithm is very confusing\"** // **\"What are the input rewards and policies? Why can they be empty sometimes? Why would line 4 already estimate those? What procedure is being done to estimate these? How many feature functions are necessary?\"**\n\nWe trust that the above explanations help somewhat with your confusion regarding Algorithm 1. Further, without familiarity with doing general policy iteration (GPI) with successor features (SFs) (e.g., via reading Barreto et al. [24]), the description of Algorithm 1 will necessarily seem very complex, but it's a straightforward application of this concept to approximate Q and state values for arbitrary reward functions. Successor features are a potent tool with applications across RL.\n\nRegarding your specific questions, SF functions are learned _per policy_. So an input set of policies gives us a set of SF functions. We can also use a set of reward functions to create or augment this set of policies, by adding an approximately optimal policy for each reward function. To end up with a set of policies to create SF functions with, we can therefore use either or both of an input set of policies and reward functions. Crucially, these reward functions are _not_ the learned reward function that is the algorithm's goal. They merely are a step in a process that allows us to approximate regret of a segment for an arbitrary reward function. What reward functions and policies should be inserted is an important open question for SF-based methods in general, but our sense is that the performance of GPI under SF-based methods is improved with a greater diversity of SF functions (via a diverse set of policies) and by having some policies that perform decently (but not necessarily perfectly) on reward functions for which state and Q values are being estimated via GPI. \n\nTo determine what number of feature components should be used is essentially asking what features are needed to linearly model a reward function. If the reward function is linear and its components/features are known, then only those features should be used. At the other extreme, if the state and action space are discrete, one could know nothing about the reward function and yet linearly model all possible reward functions by creating a feature for each (s,a,s') that is 1 for (s,a,s') and 0 otherwise. If either are continuous and the reward function's linear features are unknown or the reward is nonlinear, then clear guidance requires the follow-up work on scalability that we discuss elsewhere in these comments as out of scope and also near the end of App F.1.\n\n**\"Theorem 3.1 (and Definition 3.1) is dubious. Implicitly there is an assumption that the dataset covers every \u03c31 and \u03c32 pair and infinite data for each one.\" // \"what assumption \u2026 over the covariates\"**\n\nYou are right that we made this assumption, but it was explicit. We explicitly stated this assumption in L137-138: \"Further assume that in this dataset, all possible n-length segment pairs appear infinitely many times.\" This type of assumption is standard in identifiability proofs, where one wants to know if the parameters of a data-generating model can be recovered under the highly favorable conditions of infinite, exhaustive data. Additionally, no assumption is being made over the covariates; as long as all segment pairs appear infinitely in the data, the proof holds.\n\n**\"P(\u22c5|r) is actually not in the class of models from Section 2.2\u2026 The problem is therefore misspecified, and it\u2019s not surprising that there is an identifiability issue\u2026 an unfair comparison.\"**\n\nAs you correctly point out, Thm 3.1 shows that the class of models from Sec 2.2 achieves identifiability with regret-based preferences. However, Thm 3.2, as a negative result, does need to rely specifically on the class of models from Sec. 2.2. Rather, Thm 3.2 shows that there are MDP\\r tasks in which _there is no class of models that _can recover an equivalent reward function from partial-return-based preferences if the preference generator noiselessly prefers according to partial return. Specifically, we show that the mapping from two reward functions with different sets of optimal policies to partial-return based preferences is a many-to-one-mapping, and therefore the information simply does not exist to invert that mapping and recover a reward function with the same set of optimal policies. We state this more clearly in the uploaded diff.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "KS33hv1S1j",
                "writer": "author",
                "reply_to": "2uZBzxzAyHj",
                "title": "Response to reviewer",
                "comment": " Thank you for your suggestions and in particular for your encouragement to open our code and data.\n\n**The code should be open sourced.**\n\nWhen we submitted, we wrote in the NeurIPS submission checklist (L478\u2013481) that we were seeking approval for open sourcing the code and data, which includes the learning code, the Mechanical Turk UI for gathering human preference data, and our human preferences dataset. Good news: it was approved and all will be shared! We could not find any previous paper that shared a human dataset (and many never test their algorithm for \"human\" preferences with real humans), so we are excited to be providing the first such dataset for others to reproduce our work and do novel research with the dataset.\n\n**\"compare with other baselines, such as methods using [partial return]\"**\n\nThe commonality of [9-16] is that they use partial return and (usually) likelihood maximization, which is _precisely_ the baseline that we use throughout our paper. Future work could compare using different preference models with each of these papers' novelties, such as choosing segment pairs for max info gain, but such work would be extending the baseline we already focus on to new settings, which would be informative but outside our scope. We do want to draw your attention though to the 4 additional baselines we introduce in Appendix B, each receiving more limited evaluation than our partial-return baseline. Since submission, we did however add in App B.3 and F.3 a second baseline, an \"expected return\" preference model, which is halfway between the partial return and deterministic regret models, considering each segment's partial return and end state value (but not start state value).\n\n**\"What is the segment\u2019s desirability?\"**\n\nDesirability is not a technical word in this paper. We only use it in a colloquial sense to mean \"characteristics that would make something (e.g., segment or a reward) more preferable than something else\". We will clarify in the final version.\n\n**\"environments are also limited\" / \"more results\"**\n\nPlease read why we focus on simple environments for scientific reasons, under \"This paper does not demonstrate scalability to more complex environments\" in our general comment to our reviewers. We also note that though our experiments were limited to grid worlds, our synthetic preferences experiments include learning and testing reward functions 100 different randomly generated grid world MDPs. Regarding the amount of results, we have theoretical results; 6 sets of results with synthetic data: the main one in Section 6.2 and 5 more in Appendix F.2; and a large human subjects experiment tested\u2014which required months of UI design and iterative tuning\u2014with correlational and likelihood results in Section 5 and reward-learning results with 3 preference models (Sec 6, but see App F.3 for all 3 models). As described under \"Updates\" in our comment to all reviewers, a new baseline has been added since submission, in the uploaded diff's App B.3 and F.3.\n\n**\"more details about experiments\" / \"How to define a start state of a segment\u2026 [and] end state of a segment?\"**\n\nFor space considerations we put most experimental detail in our appendix. We have added more detail about how segment pairs were chosen for human labeling (App D.3), which implicitly includes how start and end states are chosen. We will also open source our code, which will provide a secondary form of experimental detail.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "p-z5ilzAs5",
                "writer": "author",
                "reply_to": "BpAFeMpOkZI",
                "title": "Response to reviewer, part 1",
                "comment": " Thank you for your suggestions on improving presentation, including for theoretical clarification. We heartily welcome suggestions for improvement and have addressed many of yours in the uploaded diff in the supplementary material.\n\n**\"In Section 4, it mentions that data was collected via two different methodologies. In the end, was data from only the second [collection methodology] used to present the results in the end or was it a mix?\"**\n\nBoth stages' data was used unless otherwise stated. We have added this missing information in Sec 4.3.\n\n**\"... more details about how the actual segments that were presented to the labelers were generated?\"**\n\nYes! We agree such detail is needed and have added it (see App D.3 in the uploaded diff).\n\n**The section organization is confusing. I did not see an algorithm until Sec 6.**\n\nWith your feedback in mind, we have revised the paper to be much clearer regarding a single point that should help with your confusion. Specifically, Section 2 defines complete algorithms through Equations 1 and 2 for partial return and Equations 1 and 5 for regret. This algorithm assumes that regret can be exactly measured, and the Sec 3 theory makes this same assumption. Sec 4\u20136 comprise our experimental analyses (not counting what is in the appendix), with Sec 4 about data gathering, Sec 5 about results _without_ reward function learning, and Sec 6 about results from reward function learning. Alg 1 is presented in Sec 6 because it focuses on creating approximations of regret for its practical application in our experiments, and otherwise it is the same algorithm as in Section 2, which is packed into line 10 of the Alg 1. The uploaded diff shows our revisions to be clearer: Sec 2.3 has a new paragraph entitled \"Algorithms in this paper\" that explicitly states that two algorithms have been defined (with clarification on what they are), points forward to the algorithms that will be defined, and connects the 3 main algorithms with which results they are used to obtain; and the first sentence of Sec 6.1 now clearly states that it is only introducing an approximation of regret to be used in the algorithm from Sec 2.\n\n**For the partial return algorithm, was the same framework used but the model just swapped out? How does this compare with past algorithms that use partial return?**\n\nAlg 1 is not used for learning reward functions with the partial return preference model, since no approximation of regret is needed for learning via partial return. All reward learning with a partial return preference model uses the algorithm from Sec 2. Our algorithm using partial return matches that of numerous past works, including the most cited one on this topic, by Christiano et al.\n\n**\"... discussion of related work is quite sparse\" / \"difficult to place this work in the literature as a result\"**\n\nInstead of an explicit related works section, we discuss related work throughout the paper, where each work is related to the immediate content of the paper. Putting all of these discussions with citations together adds to a substantial discussion. As far as placing the work in the literature, this work is a direct response to the unexamined and taken-on-faith partial-return assumptions of existing literature ([9\u201316]. We welcome any suggestions on how we might make that clearer than we do in L25-33, and any suggestions for additional work we should discuss.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "E8AG-IJiMFb",
                "writer": "author",
                "reply_to": "XFxlZNrtnhn",
                "title": "Response to reviewer",
                "comment": " We thank you for your kind words and for communicating your clear understanding of the paper.\n\n**\"What if there is a set of optimal policies\u2026\"?**\n\nNeither our core algorithm (Section 2) nor our approximation algorithm (Alg 1) assume a single policy. Note that regret is defined over optimal advantages, and all by definition optimal policies have the same Q values and state values (e.g., see Sutton and Barto)\u2014and therefore the same advantages\u2014from any state-action pair, so our definitions are not affected by having more than one \u03c0*.\n\n**\"biggest weakness \u2026 calculating the regret requires one to know the optimal policy under the true reward r (or at least the value and q-value function of that optimal policy under any reward \\tilde{r})\"** \n\nThis is an easy misunderstanding to make. The algorithm requires knowing the optimal state and Q values\u2014or some approximation of them\u2014for the current candidate reward function, which is _not _dependent on the optimal policy for the true reward function. This solve-an-MDP-in-the-inner-loop problem is common to many IRL algorithms, including in the foundational 2000 IRL paper by Ng and Russell, where numerous approximations have been developed to handle the problem tractably. Our Alg 1 addresses this issue in certain settings, and the latter portion of App F.1 discusses how to do so in deep RL settings, as Brown et al. have done already for learning reward from preferences.\n\n**L63 typo**\n\nGood catch and thank you! Fixed.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "mBAuO0Nxf0",
                "writer": "author",
                "reply_to": "q69d1pOaQP",
                "title": "Response to all reviewers, part 2",
                "comment": " **-Common concerns among our reviewers-**\n\n**Practically using regret is challenging.** (@ieTk, @xps1, @SeBN)\n\nWe agree yet do not find this assertion problematic. We have provided evidence\u2014theoretically and with experimentation\u2014that regret is more effective when precisely measured or effectively approximated. The challenge of efficiently creating such approximations presents one clear path for future research and is not a good argument for staying within the local maximum of partial-return-based preference models.\n\nIn fact, like our regret model, inverse reinforcement learning (IRL) was founded on an algorithm that requires solving an MDP in an inner loop of learning a reward function. (For example, see the foundational 2000 Ng and Russell paper on IRL and also Algorithm 1 in the recent IRL survey at [https://arxiv.org/pdf/1806.06877.pdf](https://arxiv.org/pdf/1806.06877.pdf).) This challenge hasn't stopped IRL from being an impactful problem, and handling this inner-loop computational demand is the focus of much IRL research.\n\nAppendix A.3 has been added to address this topic.\n\n**This paper does not demonstrate scalability to more complex environments.** (@ieTk, @SeBN)\n\nWe agree. The focus of this paper is not deep RL or complex tasks, so such scalability\u2014while highly important\u2014was not in scope. As we mention above, this paper seeks a _scientific_ analysis of human preferences and preference models, which conflicts with scaling up to tasks commonly used for deep RL. Specifically, testing in more complex tasks would require coarser approximations of regret, which, as stated earlier, can make results harder to interpret. For example, if a regret-based algorithm fails, we wouldn't know whether regret is to blame or whether the approximation of regret is to blame.\n\nFurther, we believe that algorithms for human-created data should be tested with _human_-created data, only using synthetic data to better understand the algorithms. Gathering human datasets is highly expensive in terms of design and engineering (see [https://bit.ly/humanprefs](https://bit.ly/humanprefs) for our UI that trains subjects and elicits their preferences). Testing these models in other tasks with real human data would have required multiple costly human subjects studies. When the cost of a human subjects study is properly appreciated in contrast to that of a purely computational experiment, we trust that our reviewers will agree that our combined 3 proofs, 3 evaluations of our human preferences dataset (Secs 4 and 6.3, extended in App F.3), and 6 synthetic-data evaluations (Sec 6.2 and App F.2) are more than sufficient for a single paper. Our contribution of a released human dataset will make testing in multiple environments easier for future research efforts in this area.\n\nIn future work on the _application_ of regret preference models, we or other researchers can face the _engineering_ task of scalability. Given that IRL has made tremendous progress in this direction and the cited Brown et al. [30] paper has scaled an algorithm with similar needs to those of Alg 1, we are highly optimistic that the methods to scale can be developed and probably already exist (e.g., in [30] and in the later part of our App F.1, under \"Instantiating Algorithm 1 for reward functions that may be non-linear\").\n\n**Noiseless preferences are not theoretically interesting or should be connected to past work.** (@xps1, @SeBN)\n\nWe explain below (and at the end of Appendix C in the uploaded diff in the supplementary material) why noiseless preferences are important.\n\n\n\n1. An intuitive argument: Noise is often motivated as modeling human error. Having an algorithm rely on noise\u2014structured in a very specific, Boltzmann-rational way\u2014is an undesirable crutch. Noiseless preferences are also feasible, if rare.\n2. In many related settings, noiseless data is assumed for theory or derivations. For instance, the foundational IRL paper by Abbeel and Ng on apprenticeship learning treats demonstrations as noiselessly optimal. Also, \"Reward identification in inverse reinforcement learning\", an ICML 2021 paper, focuses on identifiability with noiseless, perfect demonstrations.\n3. Having structured noise provides information that can help both preference models, but these proofs show that there are cases where the signal behind the noise\u2014either regret or partial return\u2014isn't enough in the partial return case to identify an equivalent reward function. So, in a rough sense, regret is better at using both the signal and the noise, which might explain its superior sample efficiency in our experiments, across both human labels and synthetic labels. Relatedly, the noiseless setting can help us understand each preference model's sample efficiency in a low-noise setting.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "q69d1pOaQP",
                "writer": "author",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "Response to all reviewers, part 1",
                "comment": " We thank our reviewers for their time and thoughtful consideration. We are encouraged that our reviewers wrote that our submission focuses on an interesting problem (@dwHc), provides a \"key insight\" that human preferences are based on more than just instantaneous reward of segments (@ieTk), contains a \"sound and reasonable\" derivation of the segment's regret as the advantage function (@SeBN), and shows that our novel regret-based preference model significantly outperforms the pre-existing partial-return-based model in experiments (@xps1). In particular, we are grateful that your reviews have led to what we consider to be vast improvement in our submission, viewable via the new version we have uploaded and in **the diff we included in supplementary materials**.\n\n**[Recap] What is our goal?** A _scientific_ study of the effect of using different preference models when learning reward functions from human preferences, which had previously been unexamined.\n\n**[Recap] Why this goal?** To correct a perceived mistake in recent research\u2014assuming human preferences are unaffected by state values of trajectory segments\u2014and to create a strong foundation for further research based on insights from theory and simple domains with minimal approximation error.\n\n**[Recap] How do we achieve our goal?** We reveal flaws in the commonly assumed partial-return preference model; propose an improved preference model (based on regret); compare them theoretically and with real human data and synthetic data; and show that the choice of preference model can be impactful.\n\n**[Recap] What is not the goal?** To demonstrate scalability to complex problems, which is important but beyond scope. Such a focus would force coarser approximations of regret, which would obscure the scientific analysis this paper focuses upon. We elaborate in the scalability part of \"Common concerns\" below.\n\n**-Updates-**\n\n**Open source and data.** (@dwHc) As foreshadowed in our NeurIPS checklist, we are thrilled to share that we have approval to **open the learning and experimentation code, the software for the UI and backend to gather human data on Mechanical Turk, and our human preferences dataset.** We could not find any previous paper that had shared a dataset of preferences from real humans and are particularly excited that we can make the first such dataset available.\n\nWe have also added **another baseline preference model**, based on expected return of a segment, and tested it on the human preference dataset (in App B.3 and App F.3 of the uploaded diff in the supplementary material). The regret preference model matches or exceeds the performance of this model as well.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "XFxlZNrtnhn",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "",
                "comment": " The paper deals with the problem of modeling human preferences in a setting where they are presented with two segments and choose to compare the two. Importantly, the authors propose that people compare the segments in terms of their values (measured by a notion of regret) instead of the total reward achieved by each segment. A regret notion---the sum of the negative advantage function value of the segment---is used to summarize the value of a segment. The advantage function is defined under an optimal policy pi^* (optimal wrt the true reward r) and the reward of interest \\tilde{r}. Strength: The paper provides a key insight that preferences are not just a function of the (instantaneous) reward of the segment of interest. Given that humans may care about the goal of a task, their preference towards different segments will necessarily be influenced by their long-term value, i.e., how these segments perform in terms of achieving the goal. The identification results presented in Section 3 nicely summarized the proposed insight. \n\n\nWeakness: The biggest weakness to me is how one can utilize this insight in practice. In particular, calculating the regret requires one to know the optimal policy under the true reward r (or at least the value and q-value function of that optimal policy under any reward \\tilde{r}). In general, even if one provides an optimal policy \\pi^*, the planning problem (and possibly learning problem when the transition matrix is unknown) will make the estimation of the regret very hard. As a related note, in the general framework, the regret definition under a particular optimal policy and L150 (\"And regret(\\sigma|r) > 0 if and only if \\sigma is suboptimal\") in the proof seem to only hold if one assumes there is one single optimal policy \\pi^*. What if there is a set of optimal policies under the true reward r? How should one adjust the regret notion for that?\n Could the authors elaborate more on the setting when there are multiple optimal policies, how one should define regret in such cases, and how the results in Section 3 look like when one chooses different optimal policies to define ther regret?\n\n\n-----\n\nL63: typo in the mathematical expression Yes, the authors have discussed the limitations and societal impact of their work in Appendix A. (When writing the above reviews, I have not read Appendix A of their paper yet.)",
                "rating": 7,
                "confidence": 4
            },
            {
                "review_id": "2uZBzxzAyHj",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "",
                "comment": " The paper studies the issues of reward functions. It considers alignment on the pairs of trajectory segments from human-generated preferences. The paper finds that it is flawed that human preferences are assumed to be informed by partial return. Previous works consider the sum of rewards over a segment. The paper provides an alternative preference model based on the regret of each segment.\n The paper addresses an interesting problem about human preferences. It shows that the partial return preference model can prefer suboptimal actions with lucky outcomes.\nThe paper also provides a method based on the regret of each segment, which is equivalent to the negated sum of an optimal policy\u2019s advantage of each transition in the segment.\nThe paper also provides theoretical comparisons.\n\nThe main weakness is that the experimental results seem not to be enough. The proposed method does not provide comparison with previous methods from [9-16].\nThe environments are also limited. \nIt would be better to provide more results.\nAnother weakness is that the code is not opened. It would be difficult to reproduce the results by others.\n What is the segment\u2019s desirability? \nFigure 1 is a little confusing.  In Figure 1, the right segment should have a higher sum of reward according to humans\u2019 preference.\nHow to define a start state of a segment? And how to define an end state of a segment?\n\nRegret is computed based on a segment. The segment is also partial. \nWhat is the advantage of using regret? \nThe motivation of using desirability is not clear.\n It would be better to provide more results and more details about experiments.\nThe paper needs to compare with other baselines, such as methods using (2).",
                "rating": 4,
                "confidence": 2
            },
            {
                "review_id": "BpAFeMpOkZI",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "",
                "comment": " This paper studies the problem of learning from human preferences. A new preference model is proposed which compares the advantage on the optimal policy between two trajectory segments. Some theoretical results are given that show the method is consistent under some assumptions and that a popular alternative is potentially not consistent. An algorithm is proposed to make use of this model. A new dataset is collected for a simulated delivery problem where workers provided preference labels for segments given to them. The proposed algorithm and model were compared with a partial return model on both a synthetic dataset and the collected dataset. Strengths\n- The proposed preference model seems new to my knowledge and makes sense algorithmically.\n- The theory could be interesting although I have some reservations about correctness and the significance of one of the results.\n- The algorithm seems to perform significantly better than the partial return baseline in empirical evaluations.\n\nWeaknesses\n- The presentation could be improved. Generally speaking, there are many seemingly out of place paragraphs and sections that do not seem to advance the content of the paper. For example, Section 3 is about theoretical comparisons of the learning objectives but then Section 4 abruptly is about an experimental model, but we do not even know what the algorithm looks like or how we will use the models from Section 3 at this point. This is only explained in Section 6.\n- The description of the algorithm is very confusing and way too informal to the point where I am still unclear on what it is actually being done.  \n    - What are the input rewards and policies? Why can they be empty sometimes?\n    - Why would line 4 already estimate those? What procedure is being done to estimate these?\n    - How many feature functions are necessary?\n    - For the partial return algorithm, was the same framework used but the model just swapped out? How does this compare with past algorithms that use partial return? This seems important for the experimental comparison.\n- The discussion of related work is quite sparse and it is difficult to place this work in the literature as a result.\n- Theorem 3.1 (and Definition 3.1) is dubious. Implicitly there is an assumption that the dataset covers every $\\sigma_1$ and $\\sigma_2$ pair and infinite data for each one. Not just that it contains infinite data for some distribution over certain segments, which is what Definition 3.1 is ambiguous about. This only becomes apparent in the proof where this unstated fact is used. \n- I am also not convinced that Theorem 3.2 is all that significant. The setting is different from the preliminaries of the paper: to assume noiseless labels is to say that $P(\\cdot | r)$ is actually not in the class of models from Section 2.2 since the softmax cannot realize this model for finite rewards. The problem is therefore misspecified, and it\u2019s not surprising that there is an identifiability issue, considering that the KL divergence cannot even be driven to zero if the true reward function is plugged in! Note that Theorem 3.1 crucially made use of the fact that the model is realizable to prove the positive result, so I feel that this is an unfair comparison.\n- The computational burden of solving potentially many MDPs to optimize the proposed preference model seems difficult to overcome.\n\n - Can the authors clarify what assumption is being made over the covariates in the dataset $D_{\\geq}$ in Definition 3.1?\n- In Section 4, it mentions that data was collected via two different methodologies. In the end, was data from only the second used to present the results in the end or was it a mix?\n- Can the authors provide more details about how the actual segments that were presented to the labelers were generated? In the appendix it just says that certain trajectories were favored, but it\u2019s unclear to me what that means. Were these generated by demonstrations or an algorithm? Were they generated specifically to have good coverage over the state space?\n See above discussion.",
                "rating": 3,
                "confidence": 3
            },
            {
                "review_id": "DtNlCuPRJy",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_6UtOXn1LwNE",
                "title": "",
                "comment": " The paper aims to learn the reward function from preferences between pairs of trajectory segments by introducing a new notion of the segment's regret that is based on the advantage function. Notable, they argue that the existing approach using the partial returns in the literature is flawed due to the lack of an identifiability property: the ability to recover the reward function underlying the preference dataset. As a result, the importance of using the segment's regret in learning the reward function from preference datasets is highlighted.\n The main contribution of the paper is to introduce a new preference model based on the advantage function. The derivation of the segment's regret as the advantage function is sound and reasonable.\n\nHowever, there are several weaknesses in the paper.\n\n1, if I am not mistaken, the proposed regret model is limited to the case of deterministic policies. How is it extended to stochastic policies?\n\n2, compared with existing approaches that use partial returns, computing the advantage function in the segment's regret is computationally expensive (as mentioned in Appendix A1). While the paper shows an approximation to reduce the computation in Section 6.1, it only works for the linear reward function. Hence, a more detailed discussion on the time complexity of the proposed regret vs. that of the partial returns is necessary, and the work needs to illustrate whether the proposed method is scalable to more practical problems, e.g., by including experiments with neural networks and larger state/action spaces.\n\n3, if I am not missing anything, Theorem 3.1 and 3.2 are not comparable. While both partial returns and the segment's regret use the logistic function (Equation 2 and Equation 5), there is a difference in the two theorems: in Theorem 3.1, if $regret(\\sigma1|\\tilde{r}) < regret(\\sigma_2|\\tilde{r})$, $P_{regret}(\\sigma_1 > \\sigma_2|\\tilde{r}) > 0.5$, in Theorem 3.2, if $\\Sigma_{\\sigma_1} \\tilde{r} > \\Sigma_{\\sigma_2} \\tilde{r}$, $P_{\\Sigma_r}(\\sigma_1 > \\sigma_2|\\tilde{r}) = 1$.\nWhich existing works assume/imply the latter assumption?\nFurthermore, it raises the question of whether $P_{\\Sigma_r}$ is indeed nonidentifiable if Theorem 3.2 has the same premise as Theorem 3.1.\n\n4, there is no comparison with IRL methods (which often rely on the value function or the Q function instead of partial returns): intuitively how they are different, and empirically how they are different (e.g., by experimental results).\n\n5, existing works (e.g., [9,10]), the reward function is define as a function of the state and the action or the state only (i.e., r(s,a), r(s)). Do these reward formulations affect the result in the paper?\n\n6, there is only a toy grid world experiment (a very small state and action space, and linear reward functions) which is quite limited.\n\n\n Please address the weakenesses mentioned above. The scalability of the proposed method to a more realistic environment is not demonstrated.",
                "rating": 4,
                "confidence": 4
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "many details",
                "Sentiment Expression": "could not convince the reviewers",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the proofs and the update proofs",
                "Sentiment Expression": "still open concerns",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Others",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "experiments",
                "Sentiment Expression": "concerns",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "several points",
                "Sentiment Expression": "would benefit from making more clear/improving their presentation",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "this paper",
                "Sentiment Expression": "should go through another round of reviews before it should be accepted",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "important insights and experiments in the paper",
                "Sentiment Expression": "should be made available to the community asap",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            }
        ]
    },
    "7BlQMwp_44p": {
        "paper_id": "nips_2021_7BlQMwp_44p",
        "paper_title": "ReLU Regression with Massart Noise",
        "paper_abstract": "Ilias Diakonikolas, Jong Ho Park, Christos Tzamos",
        "paper_acceptance": "accept",
        "meta_review": "This paper considers the problem of ReLU regression under the Massart noise model that has recently been studied extensively for classification problems. The main result of the paper is an algorithm that does exact parameter learning under certain distributional assumptions. \n\nAll the reviewers appreciated the results of the paper. While it builds on prior techniques in the area, the technical novelty in the work is high enough. Certain technical questions raised by the reviewers were subsequently resolved by the authors' response. Overall this is a solid theory paper and I recommend acceptance.",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "PMXJ7iV3EES",
                "writer": "official_reviewer",
                "reply_to": "ZoZ60fe1L4O",
                "title": "Post-rebuttal",
                "comment": " Thanks for the feedback. I keep my initial score and recommend this paper for acceptance. ",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "K2JFn5SmY_x",
                "writer": "official_reviewer",
                "reply_to": "nIVVQxj8G1P",
                "title": "Final comments to authors",
                "comment": " Thanks for your response,\n\nMost of my concerns are obviated. \nHowever, I keep my score since (at least IMO) the writing of the paper can still be greatly improved and the contents could become more accessible by the general audience. Moreover, some of the transitions between topics are not smooth enough.\n\nOther than that, this work seems to be a valuable technical contribution to the NeurIPS community.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "h9s61mRAQzS",
                "writer": "official_reviewer",
                "reply_to": "ktGH2uNh1O8",
                "title": "reviewer comments",
                "comment": " Thank you for the response. It addressed all my concerns.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ktGH2uNh1O8",
                "writer": "author",
                "reply_to": "1UewisEe8v7",
                "title": "Author response for Reviewer 8NsY",
                "comment": " We thank the reviewer for the helpful suggestions and comments. Below we respond to the main concerns/questions from the reviewer. \n\n**Sample Complexity:** As the reviewer has mentioned, the sample complexity of our algorithm for linear regression with Massart noise is quantitatively higher, compared to the noiseless case (corresponding to $\\eta=0$). While it is an interesting question to improve on the sample complexity of our algorithm, we emphasize that our work gives the *first polynomial sample and time* algorithm with strong recovery guarantees for ReLUs under minimal distributional assumptions. \n\n**Bit complexity Dependence in Runtime:** As the reviewer has noted, the runtime of our algorithm depends on the bit complexity $b$ of the points. This is a common theme in *any* algorithm that uses linear programming as a subroutine. For example, even for the basic problem of learning a linear separator *without noise*, all known algorithms require such a bit complexity dependence. In fact, such a dependence cannot be removed without a strongly polynomial time algorithm for arbitrary linear programs -- a major open problem in computer science. In our specific setting, we solve a linear program (or use the ellipsoid method) to perform exact recovery, which incurs a runtime polynomial in $\\log(1/\\epsilon)$, not $1/\\epsilon$. Given our learning task, we cannot remove the dependence on $b$ without a strongly polynomial algorithm for LPs. However, it is important to note that the sample complexity of our algorithms does not depend on the bit complexity, as a consequence of using radial isotropy. In fact, spectral outlier removal procedures incur a b dependence in the sample complexity, as demonstrated in our PAC learning results (Appendix D). Therefore, we highlight that the bit complexity dependence on the runtime is not unusual and rather that the lack of $b$ in the sample complexity is an achievement.\n\n**Distributional Assumptions and Examples:** The distributional assumption we require, i.e., that $\\Pr[w \\cdot x \\geq 0] \\geq \\lambda$, means that there exists at least some non-trivial probability mass on any homogeneous halfspace. For example, the parameter $\\lambda$ would be $1/2$ for any distribution symmetric around the origin. This is an extremely mild condition that is satisfied by a wide range of distributions. For example, it is satisfied by any mean-zero distribution with non-degenerate covariance matrix. In contrast, virtually all prior results on linear or ReLU regression require at least some non-trivial concentration (tail bounds), anti-concentration, and anti-anti-concentration properties.\n\n**Technical Novelty of the use of Radial-isotropic Transformations:** The central question of our paper is whether there exists realistic label noise models in which efficient learning is possible *without strong distributional assumptions*. We know that stronger adversarial noise models, such as the strong contamination model, have computational hardness results as described in the introduction (Lines 27-34). Moreover, for the Massart noise model, there has been recent work (see references 9, 12 in our paper) that learn halfspaces in a distribution-independent fashion. Analogously, we provide ReLU regression results with only mild assumptions that do not require any strong tail bounds or concentration bounds.\n\nIndeed, as the reviewer points out, if the data is uniformly distributed on the unit sphere, then we do not need to perform radial-isotropic transformations since the data is already well-behaved. However, this is not the case for most distributions, even after normalization. Thus, the use of radial-isotropic transformations allows us to generalize our regression method to apply beyond such well-behaved distributions.\n\n**Computational Cost of Radial-Isotropic Transformation:** Computing a radial-isotropic transformation $A$ can be done in polynomial time as specified in Lemma 2.1. Because we do not need an exact transformation but only an approximate one where $\\gamma = 1/2$, we can use previously established algorithmic results in computing this approximate transform. We describe how to compute $A$ using the algorithmic results of Artstein-Avidan et al. and prove this lemma in Appendix A.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "8eFXmROg3nT",
                "writer": "author",
                "reply_to": "VohWkYfl8yd",
                "title": "Author response for Reviewer Dpt6",
                "comment": " We thank the reviewer for the appreciation of our paper and the helpful suggestions.\n\nFor Definition 1.1, the adversary may change the labels of the (potentially) corrupted samples to arbitrary values. We will also update the paper with clear definitions and discussions with regard to the bit complexity of the parameters and samples.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "nIVVQxj8G1P",
                "writer": "author",
                "reply_to": "6KEmTiPc2J",
                "title": "Author response for Reviewer vpXo",
                "comment": " We thank the reviewer for the helpful suggestions and comments. Below we address the specific questions from the reviewer. \n\n**Robustness under Model Misspecification:** It is indeed an interesting open question and a promising future direction to extend our work to cases where the true function is not actually a ReLU and allow for some form of model misspecification or additive noise as the reviewer suggests. Our theoretical results focus on the realizable model for learning ReLUs (and linear functions) with Massart noise. This is an important and well-studied setting even for the noiseless case (corresponding to $\\eta = 0$), and has been the focus of a number of prior works, including recent papers appearing in top venues (see references [23, 31, 45, 49] and Lines 66-73).\n\n**Description and Definition of Adversary:** We define the Massart adversary in Definition 1.1 and use this definition throughout the paper. In fact, this definition is equivalent to the informal description given in Lines 117-119. This is because the adversary of Lines 117-119 does not have to change all the labels of the randomly selected $\\eta$-fraction. By keeping some of the labels clean, this adversary is equivalent to that of Definition 1.1.\n\n**Intuition Behind the Massart Model:** As the reviewer has mentioned, the Massart noise model is stronger than purely random noise and weaker than the strong contamination model, which has computational hardness results even for well-behaved distributions, as described in Lines 32-34. It aims to capture cases where the samples that are (potentially) corrupted are not correlated, i.e., a uniformly random subset of examples is corrupted. The adversary may corrupt the values of these samples arbitrarily in correlated ways, but may not choose which points to corrupt in advance. This model allows us to escape the computational hardness results that apply when the adversary is all-too-powerful, and obtain efficient and robust regression algorithms under minimal assumptions about how the values are corrupted.\n\n**PAC Learning ReLUs in Line 107:** PAC learning ReLUs is the task of learning a hypothesis $h$ such that $\\Pr_{x \\sim \\mathcal{D}_x}[h(x) \\neq \\mathrm{ReLU}(w^* \\cdot x)] \\leq \\epsilon$ with probability at least $1-\\delta$ *without* any distributional assumptions on $\\mathcal{D}_x$. So it suffices to learn a hypothesis that outputs similar labels as $w^*$ without the hypothesis having to be exactly $w^*$. For instance, we can PAC learn linear functions even in the case that it is information-theoretically impossible to recover $w^*$ exactly, as shown in Appendix D. In the case of Theorem 1.3, we make a mild assumption that $\\Pr(w\\cdot x \\geq 0) \\geq \\lambda$ to *exactly* recover $w^*$, while PAC learning would not make any assumptions and output a close hypothesis $h$.\n\n**Comment regarding Line 144:** Yes, we did not include the transpose since $A$ is symmetric, but we agree that this may be confusing. We will fix the typo and keep our notation consistent. \n\n**Definition of $\\mathcal{S}^{d-1}$:** Yes, the reviewer is correct. We will add this definition for clarity.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ZoZ60fe1L4O",
                "writer": "author",
                "reply_to": "0odb8IOz-v",
                "title": "Author response for Reviewer 463t",
                "comment": " We thank the reviewer for the appreciation of our paper and the helpful suggestions. \n\nRegarding the reviewer\u2019s first comment, we note that our results in the main body of the submission are self-contained and do not depend on Hopkins et al.; only our extended results in the appendix require the facts proved in Hopkins et al. However, we agree that their work provides relevant insight and techniques. We will discuss their work when discussing radial isotropy in the Technical Overview section. \nRegarding the second comment, we note that ReLU regression is significantly more challenging than linear regression. A lot of recent literature aims to address the intricacies of ReLU regression stemming from the non-convexity of $\\ell_1$ and $\\ell_2$ regression. We have already highlighted these difficulties in Section 1.2 (Lines 156-174).\n\nAt a high-level, if we knew a priori which points lie on the positive side of the ReLU, ReLU regression would indeed be a straightforward application of linear regression --  since we can learn the parameter vector $w$ with these points. In the presence of noise, however, it is unclear which region corresponds to the positive part and we must learn it simultaneously. This is one of the main challenges that we overcome. Our algorithm builds on the ideas developed in our linear regression warmup, to iteratively improve the guesses about the positive ReLU region yielding a separation oracle. We will incorporate this additional intuition in the revised version of the paper.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RxciILAjUVl",
                "writer": "author",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "Joint Response",
                "comment": " We thank the reviewers for their time and effort in providing feedback. We are encouraged by the positive feedback, and that the reviewers appreciated the paper for the following: (i) significant and/or interesting results (463t, vpXo, Dpt6), (ii) technical contribution (463t, Dpt6), and (iii) organization/clarity (vpXo, Dpt6, 8NsY). We address the individual questions and comments by the reviewers separately.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6KEmTiPc2J",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "",
                "comment": "The aim of this work is to propose a computationally efficient algorithm for linear and ReLU regression in the presence of a bounded adversarial Massart noise model. It has been tried to make the assumptions on the underlying data distribution as mild as possible, not to mention that (as claimed by the author(s)) some assumptions are necessary from an information-theoretic point of view.\n\nThe core idea is to consider the unknown corrupted samples as outliers which do not follow the simple linear or ReLU rules between their features and corresponding labels. This should be added to the fact that (based on the paper's assumption) the fraction of corrupted samples is less than 1/2, so the clean data points have the majority. This way, author(s) have proposed a linear transformation on the samples in order the make them \"Radially Isotropic\", so (with high probability) the ratio of outliers to clean samples remain as low as possible in *every direction* of the space. Then, some existing robust estimators such as sum of $\\ell_0$ or $\\ell_1$ models have been shown to remove the effect of outliers and acquire the true parameters. I haven't completely checked the proofs, however, the overall idea makes sense to me and is, in fact, quite interesting. In any case, I am not completely familiar with this line of research so I wait to see other reviews in order to assess the novelty of techniques that have been utilized in this work.\n\nWith respect to weaknesses, paper lacks proper discussion at some points which I have explained in the \"Main review\" section. Also, author(s) have assumed a completely clean and noise-free model except for the adversarial part which still includes more than half of the samples. This assumption is a little worrisome in practice, where some minimum levels of, for example, additive noise are *always* present. How robust or sensitive is the proposed method and its theoretical guarantees when the presumed ideal noise-free environment is minimally perturbed?\n\nOverall a well-motivated and fairly well-written paper with some interesting theoretical achievements (as far as I am aware). My vote at this stage is weak-accept.\n  \nMy main concerns are as follows:\n\nAuthor(s) seem to have a good grasp of existing literature in this area, but it hasn't been completely reflected in the Introduction section. Some discussions and comparisons are still vague and need clarification. I believe the classification of prior advancements in terms of: achieved guarantees, distributional assumptions and etc. can be improved by some reorganization, which also helps the reader to position the current work w.r.t. others, more effectively.\n\nThe adversary described in Definition 1.1 does not match with the one that has been explained in Lines (117:119). In Def 1.1, adversary is allowed to alter (at will) each point $\\left(x_i,f\\left(x_i\\right)\\right)$, with probability $\\eta\\left(x_i\\right)\\leq\\eta$. No information regarding $\\eta\\left(\\cdot\\right)$ has been given except that it is bounded by the constant $\\eta$. That means the function $\\eta\\left(\\cdot\\right)$ can also be chosen by an adversary. That is different from the explanation given in Lines (117:119): \"we consider a more restricted adversary that is presented with a uniformly random $\\eta$ fraction of the points, which can be corrupted arbitrarily at will\". So a natural question would be which of them is finally considered in this work?\n\nAlso, more discussion w.r.t. the adversarial perturbation model considered in this work would be helpful. It is not as strong as the \"contamination model\", but obviously hurt more than a purely random noise. What is the intuition behind this particular noise model?\n\nIn Section 1.2, the transition from ReLU and linear regression in the presence of Massart noise to $\\ell_0$ or $\\ell_1$-regression is not smooth. More explanations are needed here, otherwise the core idea behind the results become somehow ambiguous and hard to understand.\n\n\n------------------------------------------------\nMinor comments:\n\n-(Line 107): What author(s) mean by \"It remains an interesting open problem whether similar PAC learning guarantees can be obtained for the case of ReLU regression.\"? Aren't the results from Theorem 1.3 associated with the most general case? Or author(s) are referring to a more general adversarial noise model rather than Massart noise here? \n\n-(Line 144): I guess it should be $w=A^Tw'$, right? It might not be important since $A$ is later assumed to be symmetric. However, it may confuse the readers.\n\n-(Definition 1.4): What is $\\mathcal{S}^{d-1}$? Is it the surface of $d$-dimensional unit sphere? No problem here.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "0odb8IOz-v",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "",
                "comment": "The authors study exact parameter recovery in ReLU regression under a generalization of the semi-random Massart noise model. Assuming certain distributional anti-concentration, the authors propose and analyze an algorithm that runs in polynomial time and recovers the exact parameters with high probability, given a sample of size polynomial in the input dimension and the (inverse of) the Massart parameter. Along the way, they also provide similar guarantees for the simpler case of linear regression.  The paper is a joy to read. The study is important and relevant to the NeurIPS community. To the best of my knowledge, the results are novel and significant. I checked the proofs only at a high level, but the claims seem sound to me.\n\nA couple of comments/questions:\n1. The algorithm design as well as the proofs -- both in linear/ReLU regression -- leverage insights and techniques from the work of Hopkins et al. 2020. While this paper is cited in the appendix, there is no reference to it in the main text. I urge the authors to discuss this reference in the main text.\n2. Given suitable anti-concentration assumptions, what are the main challenges in extending the results from linear regression to ReLU regression? The guarantees in Theorems 1.2 and 1.3 are very similar, and under the anti-concentration assumption for ReLU regression, proof of 1.3 seems like a straightforward extension of 1.2.\n Yes",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "VohWkYfl8yd",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "",
                "comment": "The paper considers the problem of linear and ReLu regression in the Massart noise model, where an adversary is allowed to change the label to an arbitrarily value with some probability at most $\\eta <1/2$. The work develops an efficient algorithm that achieves exact parameter recovery in this model under mild assumptions on the underlying distribution.   **Quality and Clarity**: \n\n-  One of the strong points if the paper is that it is very well-written and well organized. The ideas are presented clearly with intuitive explanations, and the proofs are rigorous. \n\n- The authors are careful in evaluating the strengths/weaknesses of their work. The work seems to be built upon an established literature and the review of related works are informative. \n\n- Minor comments: \n  - Definition 1.1: \"and after inspecting which samples can be corrupted, it may change the label to an arbitrary value\" --> I interpreted this as \"change each label to an arbitrary value\" or \"change the labels to arbitrary values\". Please clarify.\n  - bit complexity of the parameters and samples: this is mentioned a few time but are not defined\n  - the sentence starting at Line 140 is very difficult to read\n\n**Originality**: \n\n-  The paper consider a slightly weakened model (in comparison to the the contamination model), but leads to more generality in the underlying data distribution.\n\n- The result of the work is significantly different (stronger) from the closest comparisons ([32] and [10]), and the methods are designed specifically to address the limitation of those works.\n\n- The generalization of the approach from linear to ReLU regressions are non-trivial in both technical analysis and algorithmic designs. \n\n\n**Significance**:\n\n- As stated in the Originality section, the result of the work is significantly different (stronger) from the closest comparisons ([32] and [10]) and seems to extend existing works in a demonstrable way.\n\n- On the other hand, I'm unfamiliar with some pieces of related work and don't have a strong conviction in how it will impact the field, and would leave the judgement up to other reviewers. \n n/a",
                "rating": 7,
                "confidence": 3
            },
            {
                "review_id": "1UewisEe8v7",
                "writer": "official_reviewer",
                "reply_to": "nips_2021_7BlQMwp_44p",
                "title": "",
                "comment": "This paper studies regression problems in presence of Massart (bounded) noise, where for each sample x_i, the adversary is allowed to change its label/response f(x_i) to an arbitrary value with probability less or equal to eta. Specifically, the authors focus on two problems: linear regression and ReLU regression. For both cases, they make a ``mild yet necessary assumption on the data, that the distribution is not concentrated entirely on any linear subspace; and provide algorithms which exactly recover the true function in polynomial time with constant probability.   I feel the paper is well-written but I have a few concerns on the theoretical guarantee.\n\n- In Theorem 1.2, it turns out that the sample complexity for robust linear regression is suboptimal. In particular, even when the noise rate eta = 0, it is given by O(d^3) which is worse than that of noiseless regression. It also seems unpleasant to have bit complexity dependence in running time.\n\n- In Theorem 1.3, I cannot follow the assumption that $P( w \\cdot x \\geq 0 ) \\geq \\lambda$ for all w.\n\n- What is the computational cost to obtain a radial-isotropic transformation matrix A? This turns out to be a crucial algorithmic component but I did not find a concrete discussion. Note that since you require unit data norm and identity covariance simultaneously, finding such transformation is nontrivial.\n\n- The distributional assumption is informally stated throughout. Can you provide a fews examples that satisify the condition, say Gaussian distributions or uniform distributions?\n\n- I may miss some important messages, but if the data distribution were uniform, then would it be necessary to perform radial-isotropic transformation? My understanding is not being necessary, in which case the technical novelty appears vacuous.  yes",
                "rating": 6,
                "confidence": 4
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "technical novelty in the work",
                "Sentiment Expression": "is high enough",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "theory paper",
                "Sentiment Expression": "is a solid",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "algorithm that does exact parameter learning under certain distributional assumptions",
                "Sentiment Expression": "The main result of the paper is",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "the results of the paper",
                "Sentiment Expression": "reviewers appreciated",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            },
            {
                "Content Expression": "technical questions",
                "Sentiment Expression": "were subsequently resolved",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            }
        ]
    },
    "aKZeBGUJXlH": {
        "paper_id": "iclr_2022_aKZeBGUJXlH",
        "paper_title": "Gradient Broadcast Adaptation: Defending against the backdoor attack in pre-trained models",
        "paper_abstract": "Pre-trained language models (e.g, BERT, GPT-3) have revolutionized the NLP research and fine-tuning becomes the indispensable step of downstream adaptation. However, the covert attack is the emerging threat to the pre-train-then-fine tuning learning paradigm. The backdoor attack is a typical challenge, which the victim model fails on the trigger-activated samples while behaves normally on others. These backdoors could survive the cascading fine-tuning stage, which continually posing the application of pre-trained models. In this paper, we proposed a Gradient Broadcast Adaptation (GBA) method, prevent the model from controlled producing outputs in a trigger-anchor-free manner. We design the prompt-based tuning, flexibly accessing the rare tokens while providing a fair measure of distance in word embedding space. The gradient broadcast alleviates lazy updating of potential triggers and purges the underlying abnormal weights. The GBA defense method is evaluated over five text-classification tasks against three state-of-the-art backdoor attacks. We find our method can cover nearly 100% embedded backdoor with negligible performance loss on clean data.",
        "paper_acceptance": "Reject",
        "meta_review": "This paper introduces a defense method (gradient broadcast adaptation) against backdoor attacks on pretrained language models. It proposes to utilize prompt tuning to guide the perturbed weights back to a normal state and thus helps avoid the degradation of model's generalization ability. \n\nStrengths:\n- Experiments are conducted across multiple datasets with different types of backdoor attacks, demonstrating the effectiveness of the proposed approach\n- The proposed idea is well motivated and intuitive\n\nWeakness:\n- Improvement on experiment results seems marginal\n- Some technical details of the attack setup are unclear\n- Writing of the paper needs improvement",
        "meta_review_title": "Paper Decision",
        "reviews": [
            {
                "review_id": "pqNHkii84y3",
                "writer": "author",
                "reply_to": "wgoWffC-eSD",
                "title": "Response to further feedback",
                "comment": " Thanks very much for your feedback! Please feel free to contact us if you have any other questions.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "Jq7weXsSZIc",
                "writer": "author",
                "reply_to": "ue-VzpP-HmM",
                "title": "Follow Up",
                "comment": " Dear reviewer,\n\nDo you still have any concerns about our manuscript? We are sincerely looking forward to your further feedback!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "RSmjAdNUX0P",
                "writer": "author",
                "reply_to": "3KgBygU9fk",
                "title": "Follow Up",
                "comment": " Dear reviewer,\n\nDo you still have any concerns about our manuscript? We are sincerely looking forward to your further feedback!",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "wgoWffC-eSD",
                "writer": "official_reviewer",
                "reply_to": "1grZVzxVBMD",
                "title": "thanks to the rebuttal",
                "comment": " i like this work and most of my comments has been addressed and thus i keep my score",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "088ANqVY9Ka",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_aKZeBGUJXlH",
                "title": "",
                "comment": "This paper proposes a method to defend against NLP backdoor attacks. The authors propose to calculate the global direction of gradients of loss with respect to input word embeddings and update word embeddings using the global direction. By doing so, rare words can be updated to a \"normal state\" and are expected to be no trigger of attacks anymore. The authors also empirically show the effectiveness of the proposed method. ==========After Rebuttal==========\n\nAfter reading all the comments, I tend to retain my score.\n\n================================\n\n\nStrengths\n1.\tThe proposed method is well-motivated and novel to me. \n2. The proposed method is easy to plug in fine-tuning or prompt pipeline.\n3.\tThe authors conduct experiments and show that the approach can help defend against backdoor attacks, with only a negligible generalization drop.\n\nWeaknesses\n1.\tThe empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa.\n2.\tActually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple.\n\n\\\nPlus, some technical details are not clear to me. See my questions below.\n\nQuestions:\n1. Should the probability ratio in Eq 4  be inside the $\\sum$? Or shall we use $w'$ inside the $\\sum$?\n2. For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch? \n3. The proposed method can help defend against backdoor attacks with only 1% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast.\n4. What is the proposed soft template optimization for prompt? \n\n Given the points listed, I give the current rating here. It would be helpful if the authors can address my concerns.",
                "rating": 5,
                "confidence": 3
            },
            {
                "review_id": "1grZVzxVBMD",
                "writer": "author",
                "reply_to": "Iaku8O8MGI4",
                "title": "Response to Reviewer pjTM",
                "comment": " Thanks for your comments. Please find our response below.\n\n----\n\n$\\textbf{Q1: [Non-parameterized way]}$ There may be other non-parameterized ways, such as information-entropy smoothed or neighbor cluster, in the repair of backdoored tokens.\n\n$\\textbf{A1:}$ Yes, we regard the information-entropy method or the neighbor cluster method as \"static\" methods, which may be helpful in the end-to-end paradigm. Especially when a trained model is prepared for deployment. Such a method can detect the backdoor tokens and then eliminate them to prevent further damage to the whole system. However, the \"static\" method may not be a good default for transfer learning, where the generalization ability matters most. Any static modification to the pre-trained model could reduce the generalization ability for certain downstream tasks. Also, we assume the pre-training objective is not a good helper for targeting the backdoor trigger for the training objective gap between fine-tuning and pre-training. \n\n----\n\n$\\textbf{Q2: [Backdoor-erasing methods in other domain]}$ Are all the current backdoor-erasing methods or backdoor-outline methods (mainly proposed in CV) not useful in the case of pre-trained language models?\n\n$\\textbf{A2:}$ We have surveyed recent popular backdoor-outline methods or backdoor-erasing methods in Section 2. Their limitations are obvious, mainly designed for end-to-end models, never taking the $\\textbf{inheritance of backdoor}$ into consideration. Also, the mainstream defense methods are designed for continuity input, not for the discrete input in text domain.\n\n----\n\n$\\textbf{Q3: [Variant of distilling?]}$ Can we view the GBA method as a variant of neural distillation or pruning?\n\n$\\textbf{A3:}$ For neural distillation, the core idea is to learn knowledge from a teacher model. The only teacher model in GBA is the model itself. GBA distills the \"normal\" gradient from the class tokens to the rare tokens which may be trigger candidates. To some extent, we may view GBA as a variant of self distillation. For pruning, the core idea is to cut the redundant or harmful part of the model. Instead of disabling the trigger tokens, GBA tends to repair the backdoored tokens while preserving the generalization ability of the original pre-trained model.\n\n----\n\n$\\textbf{Q4: [Performance gap]}$  Figure 2 shows a clean acc drop when increasing the data size of clean data from 0\\% to 1\\%. Does GBA hurt the model performance under few-shot settings more than the former distill-based method?\n\n$\\textbf{A4:}$ We assume the clean acc drop may reveal the influence of generalization ability. In Figure 2, we also observe the performance drop of the former distill-based method NAD. Compared with the negligible performance drop of our proposed GBA, NAD leads to more reduction of generalization ability under all data settings.\n\n----\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "ue-VzpP-HmM",
                "writer": "author",
                "reply_to": "088ANqVY9Ka",
                "title": "Response to Reviewer gVmg",
                "comment": " Thanks for your comments. Please find our response below.\n\n----\n\n$\\textbf{Q1: [Experiments on more challenging models]}$ The empirical results may be only marginally significant. For example, in Table 2, the proposed method cannot surpass SOTA under several settings. Plus the current version only conducts experiments on bert-base-uncased. It would be helpful to validate the proposed method using at least one more pre-trained language model like RoBERTa.\n\n$\\textbf{A1:}$ Thanks for the constructive suggestion. We use the RoBERTa as the benchmark and explore more experiments in Appendix D.1. We find RoBERTa is more robust to backdoor attacks but still suffers from strong baselines like BadNets. And more importantly, the results also reveal our proposed GBA defense still works well on RoBERTa, reducing the ASR score to nearly zero.\n\n----\n\n$\\textbf{Q2: [The moderated improvement]}$ Actually I like simple but effective methods. But given that the empirical results are only marginally significant, I am worried that the proposed method might be too simple.\n\n$\\textbf{A2:}$ Firstly, we would apologize that we have misplaced the results in the previous submission. In the re-submitted paper, we have replaced the original NAD results with the NAD and NAD-C. The first is acquired based on NAD's original assumption, and the second is designed to fully show the NAD's potential performance on settings that are impossible to achieve in reality. Besides, the empirical results show that our proposed methods outperformed the original NAD method.\n\n----\n\n$\\textbf{Q3: [The correction for Eq.(4)]}$ Should the probability ratio in Eq 4 be inside the $\\sum$ ? Or shall we use $w'$ inside the $\\sum$?\n\n$\\textbf{A3:}$ Thanks for sharing your thoughts on this minor mistake. We should use $w'$ inside the $\\sum$. In Eq 4, $Q_{Ew}$ is computed for each token $w$ in the vocab with the average gradient of input example, which noted as $\\sum_{w' \\in x_{input}} \\frac{\\nabla_{Ew'}}{N}$.\n\n----\n\n$\\textbf{Q4: [Updating in the minibatch]}$  For each minibatch, does the proposed method update all the embeddings of words in vocab or just update words present in the current batch?\n\n$\\textbf{A4:}$ Yes, the proposed method will update all the embeddings of words in vocab. The frequency of rare tokens in the batch is nearly zero, we will never update them enough if we just update words present in the current batch. This misunderstanding may come from the wrong presentation of Eq 4, and we have corrected it in this version.\n\n----\n\n$\\textbf{Q5: [The few-shot property]}$ The proposed method can help defend against backdoor attacks with only 1\\% of clean training data, while the SOTA method NAD needs more. I am wondering whether this is only because of the few-shot property of prompt, or it is credited to the proposed gradient broadcast.\n\n$\\textbf{A5:}$ We include more ablation studies in Appendix D.2. Without the proposed gradient broadcast, the prompt tuning reaches good performance on clean data, which could be credited to the few-shot property of the prompt. However, it loses the defense capability of backdoor attacks. Please refer to that discussion.\n\n----\n\n$\\textbf{Q6: [Soft template optimization]}$  What is the proposed soft template optimization for prompt?\n\n$\\textbf{A6:}$ In traditional hard-coded template optimization, the template of the downstream task is hand-crafted, like \"<S1> It was [MASK] .\" for sentiment classification. <S1> is the input sentence and [MASK] is the target word like \"positive\" or \"negative\". This paradigm needs extensive computation to find the best hand-crafted template. Instead, we use a soft template to replace the hard-coded input sentence with several learnable tokens \"[a]*\". The Adam optimizer is hard to find the proper discrete tokens \"[a]*\". So we fill the template in the embedding-level instead of token-level (discussed in Section 3.3). We concatenate the embeddings of tokens \u201c[a]*\" with the input sentence and optimize the embeddings of the template token to find the best-match soft template that can maximize the downstream task's performance.  \n\n----\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "3KgBygU9fk",
                "writer": "author",
                "reply_to": "6zHpX3Tk_lY",
                "title": "Response to Reviewer  KqXG",
                "comment": " Thanks for your comments. Please find our response below.\n\n----\n$\\textbf{Q1:[ Assumption on more complicated tasks] }$ The approach seems to be based on the idea that the semantics of the tokens in the same sentence are similar. But this might not be true for more complicated tasks. Unfortunately, the experiments only consider tasks with only few classes, mostly two classes, and just one dataset with 4 classes. In this case, the task becomes learning if a word is related to the target class. More complicated tasks would show how this defense would perform in the real world.\n \n$\\textbf{A1:}$ We were deeply sorry for causing inappropriate understanding from the minor issue in Eq.(4). We have reformulated this equation in the re-submission. We design the gradient broadcast under the assumption that the gradient could be used to measure the semantic distance between rare tokens and the inputs, which motivates us to update the target embeddings of the rare tokens following the common tokens' gradients.\n\nIn this way, we assign an extra pulling force $Q_{Ew}$ to update rare tokens in vocab based on the semantic distance. We believe the proposed gradient broadcast method could be extended to more complicated tasks. As a concrete example, we include more results on the GLUE benchmark in Appendix D.2, and our method performs well on both NLI and regression tasks.\n\n----\n\n$\\textbf{Q2: [Questions on BadNets]}$  The description of the attacks used is not sufficient. While the core algorithms can be learned from the cited papers, the experiment setting doesn't explain how the triggers are chosen, what the portion of backdoored samples in the test dataset, or how they are constructed. Thus, it cannot be inferred if the evaluation was done fairly and properly. For example, BadNets (Gu et al., 2017) doesn't discuss poisoning of text models at all, and this necessary information cannot be found.\n\n$\\textbf{A2:}$ Yes, the BadNets are originally proposed and applied in the CV domain. We mimic the original implementation and find a way to apply it in the NLP domain. In this re-submission, we included more detailed descriptions of attacking and defense implementation in Appendix B.\n\n---- \n\n$\\textbf{Q3: [The paper writing]}$ The paper needs major improvement in writing. There are many errors (e.g., we usually takes, state-of-the-art, We are the first ... method, safely adaptation method, a method which do not ...), unnecessarily repeated sentences or words (e.g., all whole vocab, Sec 3.1), and missing explanations/definitions (e.g., V, $\\theta^*$). Missing information can be understood by a domain expert, but the paper should be self-contained as much as possible if not citing a prior work. Also, many statements are over-generalized.\n\n$\\textbf{A3:}$ Thanks for the constructive suggestion. We have revised the draft and rebuilt the paper in this submission. We will try to list all the improvements and revisions in the final review abstract if it is available.\n\n----\n\n$\\textbf{Q4: [How the gradient updating work]}$ If the trigger does not appear in the training data, the embedding still wouldn't be updated as Q is computed and applied per sentence according to Eq 4. So, it is unclear how this update is significantly different from just updating the token with its own gradient only. Comparing the gradient and Q can be interesting. It almost looks like the effect is using a larger learning rate, but there was not sufficient analysis on this.\n\n$\\textbf{A4:}$ Yes, this problem is related to the minor issue in Eq.(4). In the corrected formulation (of the re-submission version), we compute Q for each token in the vocab (including common tokens and rare tokens). Even if the rare tokens never appear in the input batch, the rare tokens' embeddings will also be updated with the pulling force Q. In Section 5.2 (of the re-submission version), we provide a more detailed analysis between Q and the standard gradient update and testify its effectiveness in erasing the triggers.\n\n----\n\n\n\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "6zHpX3Tk_lY",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_aKZeBGUJXlH",
                "title": "",
                "comment": "This paper proposes a defense against backdoor attack on pre-trained large language models. The proposed defense computes the average of the gradients per input sentence to contribute to updating all tokens in the sentence. The approach is empirically shown to outperform two baselines. Strengths.\n- S1: The simple proposed approach is shown to defend backdoor attack targeting rare tokens.\n- S2: The approach is evaluated on 5 datasets against 4 attacks.\n\nWeaknesses.\n- W1: The approach seems to be based on the idea that the semantics of the tokens in the same sentence are similar. But this might not be true for more complicated tasks. Unfortunately, the experiments only consider tasks with only few classes, mostly two classes, and just one dataset with 4 classes. In this case, the task becomes learning if a word is related to the target class. More complicated tasks would show how this defense would perform in the real world.\n- W2: The description of the attacks used is not sufficient. While the core algorithms can be learned from the cited papers, the experiment setting doesn't explain how the triggers are chosen, what the portion of backdoored samples in the test dataset, or how they are constructed. Thus, it cannot be inferred if the evaluation was done fairly and properly. For example, BadNets (Gu et al., 2017) doesn't discuss poisoning of text models at all, and this necessary information cannot be found.\n- W3: The paper needs major improvement in writing. There are many errors (e.g., we usually takes, state-of-the-art, We are the first ... method, safely adaptation method, a method which do not ...), unnecessarily repeated sentences or words (e.g., all whole vocab, Sec 3.1), and missing explanations/definitions (e.g., V, \\theta^*). Missing information can be understood by a domain expert, but the paper should be self-contained as much as possible if not citing a prior work. Also, many statements are over-generalized.\n- W4: If the trigger does not appear in the training data, the embedding still wouldn't be updated as Q is computed and applied per sentence according to Eq 4. So, it is unclear how this update is significantly different from just updating the token with its own gradient only. Comparing the gradient and Q can be interesting. It almost looks like the effect is using a larger learning rate, but there was not sufficient analysis on this. This paper proposes a simple defense against backdoor attacks on pre-trained language models. Aside from the large room to improve writing, this paper has many other issues. While the simplicity is fine, this paper failed to show the exact effect of the defense as the rare tokens would be still updated only when it appears in the training data. Also, the evaluation was done for similar tasks where words can be directly grouped into each class. Moreover, the exact poisoning procedure was not explained. Thus, it is not possible to confirm the benefit of the proposed approach.",
                "rating": 3,
                "confidence": 4
            },
            {
                "review_id": "Iaku8O8MGI4",
                "writer": "official_reviewer",
                "reply_to": "iclr_2022_aKZeBGUJXlH",
                "title": "",
                "comment": "This paper identifies an emerging threat for the prevailing pre-trained models -- the inheritance of backdoor attack, and proposes a simple yet effective defense approach: gradient broadcast adaptation (GBA). Instead of the traditional \u201cerasing triggers\u201d, GBA utilizes the \u201cprompt-tuning\u201d as a tool to guide the \u201cperturbed weights\u201d back to the normal state, which helps avoid the degradation of generalization ability. It provides an exciting and novel analysis of why backdoor attacks could be inherited during the pretraining and tuning procedure. Meanwhile, the authors perform an empirical evaluation of the proposed method against four state-of-the-art backdoor attacks. In the security of pre-trained models, the inheritance of backdoor (adapt backdoored models to various downstream tasks) is an important challenge. Injected with the backdoor, the existing model could be further developed for a long life-circle with backdoor survival. This paper examines this interesting and under-explored topic. It locates the most vulnerable component of pre-trained language models \u2013 word embeddings, which is the reason for the ever-lasting backdoor. It shows that by carefully optimizing the word embeddings, and the malicious backdoor could be erased or repaired, taking no effect on further adaptation.\n\nBy taking inspiration from the \u201cerasing backdoor\u201d viewpoint of \u201cNeural Distillation\u201d prior work, it paves a new way of defense instead of \u201cabandon redundant weights\u201d, which obeys the generalization purpose of pre-training. The tool of \u201cprompt-tuning\u201d is also more natural to pre-trained models.\n\nThe paper proposes a much more intuitive optimization strategy that guides \u201cthose perturbed weights\u201d back to the normal state by joining the adaptation stage, thus erasing backdoors while preserving the generalization ability of pre-trained models. Additionally, as a plugin of the optimizer, the usage scenarios of the proposed approach are unlimited.\n\nExperiments show that just by focusing on the word embeddings, one can disable nearly all backdoor attacks. I suggest the author consider this as a \u201cfirewall\u201d to the standard adaptation operations.\n\nComments:\n\n1. The paper is well organized, and the motivation is clearly written.\n2. The authors try to unleash a wave of security concerns in pre-trained models. Many existing works focus on improving the performance of pre-training and adaptation while neglecting the threats of backdoor attacks/adversarial attacks. Following this paper's problem setting and experiment results, it may be easy to plug GBA into the current pipeline. One of the benefits is that we do not need to worry about performance degradation or computation efficiency.\n3. After reading this paper, I can hardly think about a better alternative for the proposed GBA approach. To the best of my knowledge, the root of backdoor attack inheritance only could be the lazy update of rare tokens. This is consistent with the authors\u2019 claim. However, there may be other non-parameterized ways, such as information-entropy smoothed or neighbor cluster, in the repair of backdoored tokens. I am wondering if it can work, but it deserves a trial. I also have some additional questions to discuss with the authors:\nAre all the current backdoor-erasing methods or backdoor-outline methods (mainly proposed in CV) not useful in the case of pre-trained language models?\nCan we view the GBA method as a variant of neural distillation or pruning? So if I were to turn this into a method that outlines the triggers first and then erases them in a blacklist.\nFigure 2 shows a clean acc drop when increasing the data size of clean data from 0% to 1%. Does GBA hurt the model performance under few-shot settings more than the former distill-based method?\n This paper targets an under-explored fundamental challenge in pre-training and adaptation trends. This is a brave and valuable step in the security research of pre-trained models. I enjoy reading this paper, and this is the first time I have seen a reasonable solution to the inheritance of the backdoor phenomenon. Although some problems need to be corrected, I believe authors should be able to take them in hand.",
                "rating": 8,
                "confidence": 4
            }
        ],
        "label": "val",
        "gpt4_judgements": [
            {
                "Content Expression": "defense method (gradient broadcast adaptation) against backdoor attacks on pretrained language models",
                "Sentiment Expression": "introduces",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "prompt tuning to guide the perturbed weights back to a normal state",
                "Sentiment Expression": "proposes",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "Experiments",
                "Sentiment Expression": "are conducted",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Not at all"
            },
            {
                "Content Expression": "The proposed idea",
                "Sentiment Expression": "is well motivated and intuitive",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "Improvement on experiment results",
                "Sentiment Expression": "seems marginal",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "Some technical details of the attack setup",
                "Sentiment Expression": "are unclear",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "Writing of the paper",
                "Sentiment Expression": "needs improvement",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "6lH8nkwKRXV": {
        "paper_id": "iclr_2021_6lH8nkwKRXV",
        "paper_title": "Graph Structural Aggregation for Explainable Learning",
        "paper_abstract": "Graph neural networks have proven to be very efficient to solve several tasks in graphs such as node classification or link prediction. These algorithms that operate by propagating information from vertices to their neighbors allow one to build node embeddings that contain local information. In order to use graph neural networks for graph classification, node embeddings must be aggregated to obtain a graph representation able to discriminate among different graphs (of possibly various sizes). Moreover, in analogy to neural networks for image classification, there is a need for explainability regarding the features that are selected in the graph classification process. To this end, we introduce StructAgg, a simple yet effective aggregation process based on the identification of structural roles for nodes in graphs that we use to create an end-to-end model. Through extensive experiments we show that this architecture can compete with state-of-the-art methods. We show how this aggregation step allows us to cluster together nodes that have comparable structural roles and how these roles provide explainability to this neural network model.\n      ",
        "paper_acceptance": "withdrawn-rejected-submissions",
        "meta_review": "The paper addresses an important unsolved problem, i.e. deriving explainable features for use in graph classification. It does it by providing:\ni)  a simple to implement (local) node aggregation approach;\nii) some theoretical support to the proposed approach;\niii) empirical evidence that the proposed approach could be effective.\n\nNotwithstanding the above merits, the reported work seems to still be in a preliminary phase. In fact:\ni) reference to literature is missing some important recent contributions to the addressed problem (e.g.  Gated Graph Sequence Neural Networks, GNNExplainer);  if possibile, also experimental comparisons vs those approaches is desirable;\nii) experimental results do not provide a solid evidence that the proposed approach can really help to provide a clear explanation of the output, and the overall performance in classification is mostly below SOTA models; adding more datasets could help to give a more solid support to the main statement about explainability/performance;\niii) presentation needs to better highlight the original contribution w.r.t. relevant literature (which is not completely clear in the current version of the paper), to improve the explanation of proofs, to discuss (both from a theoretical and empirical perspective) some important issues, such as computational scalability with the increase of size of local structures, and robustness to noise of the proposed (local) aggregation method.\n\nIn summary, although the proposed approach seems to be of some value, more work is needed to better place the proposed approach in the context of current literature and to gain a stronger experimental support to the main claim of the paper w.r.t. explainability.",
        "meta_review_title": "Final Decision",
        "reviews": [
            {
                "review_id": "J333isNg_89",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "Review comments to Paper 641",
                "comment": "==========Summary==========\n\nIn this paper, the authors investigate how to improve pooling functions in graph neural networks for the purpose of better addressing graph classification problems. The core idea in this paper is to group node representations. In particular, the authors develop StructAgg, a softmax based implementation, to parameterize the grouping process, and the parameters in StructAgg can be learned from downstream supervision signals. Empirical results focus on three aspects: (1) Improvement in graph classification accuracy brought by StructAgg; (2) Visualization on grouped node representations; and (3) Comparison between two variants of StructAgg.\n\n==========Reason for the rating==========\n\nFor the current draft, I am leaning to reject. Although pooling for graph classification is a meaningful problem, it is difficult to see the unique perspective or value in the proposed technique compared with existing approaches. For now, the technical depth in this paper seems to be limited, and more convincing empirical evidences are expected.   \n\n==========Strong points==========\n1. This paper delves into a meaningful problem. Indeed, pooling functions in graph neural networks are critical for graph classification tasks. While existing solutions are simple and efficient, they may be sub-optimal in some cases.\n\n2. The authors propose a grouping-based method to reduce variance in graph representations, which potentially could improve generalization performance.\n\n3. Empirical evidences are provided to confirm the effectiveness and impact from StructAgg.\n\n==========Weak points==========\n1. The idea of \"grouping node representations using softmax for graph classification\" may have been investigated in existing works, e.g., [1]. For the core idea in this paper, the authors may need to discuss its connection with existing literature, and highlight the unique perspective.\n\n2. The technical impact of this paper could be limited. \n    - From the current draft, the proposed StructAgg looks like incremental changes to the existing methods. The authors may provide more theoretical or empirical evidences to motivate the problem, highlight the uniqueness in the technique, and justify their design choices.\n    - The theoretical results in Theorem 1 may need more work. Theorem 1 could aim to answer an important question in StructAgg, but the authors may raise the question first and justify why this is an important/non-trivial question. \n\n3. The empirical evidences could be stronger. \n    - From the current results, StructAgg only performs the best in one of the three datasets. The authors may consider more datasets that can highlight the value of StructAgg.\n    - StructAgg may work with the existing GNNs. The authors may connect StructAgg with the existing GNNs, and demonstrate the incremental gain from StructAgg.\n    - For the evaluation in Table 2, it is unclear why the comparison is limited between GCN and the proposed technique.\n    - For the evaluation in Table 3, the claim on \"node embedding\" may not be sound. Between StructAgg and StructHist, the difference is not just \"node embedding\", and one cannot ignore the impact from \"histogram\". The authors may need to carefully reason the conclusion from the empirical numbers.\n\n==========Questions during rebuttal period==========\n\nPlease address and clarify the weak points above. \n\nIn addition, it will be great if the authors could address the following questions.\n\nQ1. As the authors emphasize the term \"node embedding\", does this paper target on transductive or inductive settings? \n\nQ2. For the entropy minimization discussed in Section 3.3, is it going to bring more overfitting risk?\n\nQ3. For the first condition in Definition 1, it is a bit vague. The authors may give a more accurate mathematical description.\n\nQ4. For the first bullet in Remarks under 3.3, what does \"The structural classes identified by our algorithm are local\" exactly mean?\n\nQ5. In the experiment setup, what do $l_1$ and $l_2$ refer to?\n\n==========Reference==========\n\n[1] Substructure Assembling Network for Graph Classification, AAAI 2018\n\n==========Post rebuttal==========\n\nThe authors' response does not fully address my concerns. I keep the rating as it is. ",
                "rating": 4,
                "confidence": 4,
                "writer": "official_reviewer"
            },
            {
                "review_id": "NREjcrPTEk",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "StructAgg: an ultra-high dimensional node representation in graph neural network that preserves local graph structure for better explanation",
                "comment": "This paper proposed the StructAgg, an aggregation algorithm in convolutional graph neural network that learns the structural roles for nodes in the graph embedding. In this algorithm, a structural representation of node is constructed through concatenation of latest p layers of node presentation in graph neural network, which consists of information from the p-hop neighborhood. The paper is good quality and its demonstration is clear.  The idea behind is original. However, learning embeddings through concatenation of multiple layers of neural representation is not completely new. The contribution of this paper to the community is not ground-breaking as well.  From the experiment results, it is hard to stress its significance as in most of times this method does not beat the state-of-the-art algorithms. \n\nPros: \n1.  As opposed to previous studies, this paper focus on learning node embeddings that preserve local information in a larger p-hop neighborhood. Such idea assumes that the node structural role can be identified via its neighborhood and the node embeddings should carry information regarding its structural roles. To achieve that, the learning algorithms should balance the predictability of the embedding in the node classification as well as the consistency of node representation towards underlying clustering in the graph. \n2.  The proposed representation learned by StructAgg, is able to provide the clustering of nodes as well as the node embeddings that are invariant to the permutation. \n3.  The StructAgg only need to concatenate the existing learned representation in lower layers, thus it is easy to implement. \n\nCons: \n1.  Ulta-high dimensional representation as the size of local neighborhood of interest increases: The concatenation operation expands the dimensionality of embeddings drastically as the local neigborhood expands. As a result, the StructAgg representation can only feasibly represent a structures that are present in small neighborhood graph. In some graph like trees, it is fine. But in complex graphs, with larger local structures, it is not efficient in both storage and computation. \n2. Tradeoff between the classification and clustering:  The proposed method defines the loss function as the combination of supervised node classification loss and unsupervised node cluster assignment loss (in terms of entropy of softmax assignment). The formation of this joint loss function implies that the node label is assigned consistently according to the structural role of the node in the graph with the number of total structural classes in the graph known. It can be imagined that with the number of structural classes increases, the label assignment may not be consistent and will result in decrease of performance. It seems that this method is more suitable for two or three structural classes as shown in the experiment. \n3.  Lack of robustness: The message passing algorithm in small neighborhood tends to preserve not just local information and local noises. In StructAgg, since the aggregation happens only within the same cluster, the noise level in different clusters may varies a lot, depending on the size of cluster. It is more likely to create noisy embeddings in this small clusters and present them in the final embeddings, while in traditional aggregation, such noise level is suppressed due to the involvement of more nodes.\n\nSome question need to clarify:\n1.  In proof of theorem 1, please explain for $\\sum_{i^{'}\\in \\mathcal{N}(i)}\\frac{a_{i,i^{'}}}{\\sqrt{d_{i}d_{i^{'}}}}X_{i^{'}}^{(l)} = \\sum_{j^{'}\\in \\mathcal{N}(j)}\\frac{a_{j,j^{'}}}{\\sqrt{d_{j}d_{j^{'}}}}X_{\\psi(j^{'})}^{(l)}$, what is the relation between coefficients $\\frac{a_{i,i^{'}}}{\\sqrt{d_{i}d_{i^{'}}}}$ and $\\frac{a_{j,j^{'}}}{\\sqrt{d_{j}d_{j^{'}}}}$. Also should it be like $\\sum_{j^{'}\\in \\mathcal{N}(j)}\\frac{a_{\\psi(j),\\psi(j^{'})}}{\\sqrt{d_{\\psi(j)}d_{\\psi(j^{'})}}}X_{\\psi(j^{'})}^{(l)}$ ?\n\n2. In proof of proposition 2, please explain in detail the reason behind $(softmax(P^{T}PX_{struct}p))^{T}P^{T}PX^{L} = (P^{T}softmax(PX_{struct}p))^{T}PP^{T}PX^{L}$. Here i believe it should be $(softmax(P^{T}PX_{struct}p))^{T}P^{T}PX^{L} = (P^{T}softmax(PX_{struct}p))^{T}P^{T}PX^{L}$, with a mistakenly added $P$ in original draft.  The reason is that the softmax is row operation, a row permutation $P^{T}$ does not affect the result of $softmax(\\cdot)$ at each row but their ordering. So  $softmax(P^{T}\\cdot)= P^{T}softmax(\\cdot)$, thus $(softmax(P^{T}PX_{struct}p)) = (P^{T}softmax(PX_{struct}p))$. This leaves \n$(softmax(P^{T}PX_{struct}p))^{T}P^{T}PX^{L} = (P^{T}softmax(PX_{struct}p))^{T}P^{T}PX^{L} \n= (softmax(PX_{struct}p))^{T}PP^{T}PX^{L}  = (softmax(PX_{struct}p))^{T}PX^{L} $",
                "rating": 6,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "LMlllUkBIN",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "This work tries to identify and explore local topological roles in explainable graph learning. Writing needs improvement. Experimental results not conclusive.",
                "comment": "This work tries to identify  local patterns that can be used to provide explainability to GNN models. Different from previous work using simple pooling function to generate graph representation, this work clusters nodes into a predefined number of clusters using their embeddings from L-hop neighbors. Each cluster produces a pooling representation. The representations of all clusters are concatenated to form the representation of the whole graph. However, the manuscript was not well written with some confusing notations and logical problems. It should be beneficial to readers if the manuscript can first clearly demonstrate problems in existing graph classification problems. Some analyses are not solid. The proposed method performed obviously worse than several baseline methods.\n\nThis work assumes that similar node embeddings should have similar structural roles and thus should be clustered together. Nodes with the same local structure may have the same embeddings, but not the other way around may not be true. The structural roles are defined is based on embedding similarity, which doesn\u2019t guarantee to enhance explainability. The author(s) claimed that most GNNs were not explainable. This is not quite true. Recently, the attention mechanism was used in GNNs to enhance explainability. There exist some other works, like GNNExplainer, aiming at explainable GNNs.\n\nThe baseline models compared in this paper were proposed several years ago and not mainly focused on graph readout function. More typical methods for global pooling were missing in comparison. For instance, GlobalAttention (https://arxiv.org/abs/1511.05493) is also an explainable readout function. ASAPooling (https://arxiv.org/abs/1911.07979) is a recent work that also learns a sparse soft cluster assignment for nodes in the pooling phase. In addition, the proposed method performed worse than many baseline models in many cases. For experiments on OGB datasets, it only compares with original GCN model, but there has been a lot of state-of-art models proposed several month ago, achieving much better results than this paper. You can find the latest results on https://ogb.stanford.edu/docs/leader_graphprop/. \n\n\nSome notations need improvements.\n1.\tEquation (2): The trainable weight matrix is missing in GCN(A, X).\n2.\tDefinition 1 denotes two nodes as u and v, but the notation becomes i and j when representing their l-hop neighbors. X is first used as the feature matrix but later used to indicate node vector.\n\nSome claims are not correct.\n1.\tIn Section 3.2, \u201cEmbedding nodes with MP creates embeddings that are close for nodes that are structurally equivalent.\u201d When many GCN layers are stacked together, final node embeddings tend to become similar.\n2.\t \u201cWhen computing the distance between two graphs, if those two graphs have the same distribution of structural roles, they will have embeddings that are close. \u201d This may be true, but the overall graph topology information is omitted. Here structural roles represent local patterns. \n",
                "rating": 3,
                "confidence": 5,
                "writer": "official_reviewer"
            },
            {
                "review_id": "A4DqtLXz4-",
                "reply_to": "iclr_2021_6lH8nkwKRXV",
                "title": "Official Blind Review #3",
                "comment": "This paper focuses on deriving explainable features for use in graph classification. To that end, they propose StructAgg that is essentially an aggregation process based on the structural roles of nodes that is then used in an end-to-end model. Experiments demonstrate the effectiveness of the proposed approach as it provides comparable performance while providing some explainability. This is an important unsolved problem, and this work provides one such approach to obtain more explainable and intuitive features/embeddings for graph classification.\n\nThe example provided in Figure 1 (a) is trivial in the sense that one can assign structural roles to nodes based on degree. For instance, red nodes all have degree 1, orange nodes have degree 3, and yellow nodes have degree 2. Please provide a better example such as the one used in the recent survey paper \u201cOn Proximity and Structural Role-based Embeddings in Networks\u201d (see Figure 1) where this is not the case, and one must look at the actual structural properties to discover roles. Also, I suggest showing actual meaningful features in (b) as this would make the example more complete. There are a few papers related to structural roles that are missing and should be referenced appropriately. For instance, using roles for graph similarity and comparison is discussed in \u201cRole Discovery in Networks\u201d. There are also incorrect references to a paper from 2018 for structural roles that need to be fixed. Overall, this paper is interesting, the problem is important, and the approach is shown to be effective.\n",
                "rating": 7,
                "confidence": 5,
                "writer": "official_reviewer"
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "The paper",
                "Sentiment Expression": "addresses an important unsolved problem",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The paper's approach",
                "Sentiment Expression": "simple to implement, theoretical support, could be effective",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            },
            {
                "Content Expression": "The reported work",
                "Sentiment Expression": "in a preliminary phase",
                "Criteria Facet": "Overall",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Reference to literature",
                "Sentiment Expression": "missing some important recent contributions",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Experimental results",
                "Sentiment Expression": "do not provide a solid evidence",
                "Criteria Facet": "Soundness",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "Presentation",
                "Sentiment Expression": "needs to better highlight, improve",
                "Criteria Facet": "Clarity",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Highly Convincing"
            },
            {
                "Content Expression": "The proposed approach",
                "Sentiment Expression": "seems to be of some value, more work is needed",
                "Criteria Facet": "Advancement",
                "Sentiment Polarity": "Negative",
                "Sentiment Expresser": "Self",
                "Convincingness": "Slightly Convincing"
            }
        ]
    },
    "VAeAUWHNrty": {
        "paper_id": "nips_2022_VAeAUWHNrty",
        "paper_title": "Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising",
        "paper_abstract": "Recent advances in differentiable rendering have enabled high-quality reconstruc\u0002tion of 3D scenes from multi-view images. Most methods rely on simple rendering algorithms: pre-filtered direct lighting or learned representations of irradiance. We show that a more realistic shading model, incorporating ray tracing and Monte Carlo integration, substantially improves decomposition into shape, materials & lighting. Unfortunately, Monte Carlo integration provides estimates with significant noise, even at large sample counts, which makes gradient-based inverse rendering very challenging. To address this, we incorporate multiple importance sampling and denoising in a novel inverse rendering pipeline. This improves convergence and enables gradient-based optimization at low sample counts. We present an efficient method to jointly reconstruct geometry (explicit triangle meshes), materials, and lighting, which substantially improves material and light separation compared to previous work. We argue that denoising can become an integral part of high quality inverse rendering pipelines.",
        "paper_acceptance": "Accept",
        "meta_review": "The paper addresses the task of reconstructing 3D meshes, materials and lighting from multi-view images. To this end Monte Carlo ray tracing is used in combination with denoisers for training efficiency. Experiments show improvement w.r.t. SOTA nvdiffrec and nerfactor.\nReviewers like the combination of denoisers for training and MC rays tracing, the thorough evaluation and ablation studies. All reviewers recommend the paper for acceptance and so do I.",
        "meta_review_title": "",
        "reviews": [
            {
                "review_id": "AB7EbZciig",
                "writer": "author",
                "reply_to": "1nSmiGmD5m",
                "title": "1u76 individual questions",
                "comment": " ## **Evaluations primarily focus on novel rendering/relighting but lacks in individual intrinsic components such as 3D geometry (e.g. depth, chamfer, normal errors etc) and environment maps**\n\nPlease see the common section for additional evaluations.\n\n## **Comparison with recent Monte Carlo inverse rendering method [31,37] especially [37] would strengthen this paper**\n\nAs mentioned, [31] does not optimize geometry, and [37] assumes known lighting (a point light co-located with the camera).\nThe visibility gradients of [31] require efficient silhouette detection, which is non-trivial to extend beyond\nprimary visibility (requires complex 5D data structures for secondary rays, see e.g., \"Unbiased Warped-Area Sampling for Differentiable Rendering\" for a detailed discussion).\n\nOur core contribution is the joint optimization of shape, materials, and environment lighting using a Monte-Carlo renderer.\nTo get a tractable optimization task (in terms of iteration times and noise levels), we restricted the renderer to direct illumination and added (differentiable) denoising to the pipeline.\nWhile there is currently no Monte Carlo path tracing system that tackles the same joint optimization task,\nthe variance-reduction ideas introduced in this paper can be directly applied to future MC inverse rendering pipelines, once the computational resources are available.\n\n## **I wonder to what extent the proposed method could be used to handle inter-reflections and light refraction in e.g. translucent objects**\n\nWhile the paper focuses on direct illumination, we discuss extensions to full path tracing briefly in the Conclusion section. It is a clear avenue for future work, but comes with additional challenges in\nincreased noise-levels, visibility gradients through specular chains, and drastically increased iteration times. The denoising step is applicable as is. Please see common section for further details.\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "K_GwdogxRQZ",
                "writer": "author",
                "reply_to": "3tewi_Yke7p",
                "title": "EEfj individual questions",
                "comment": " ## **The choice of the importance sampling techniques is given with any discussion**\n\nThe three importance sampling techniques we apply are well-proven in the computer graphics community, and applied in most modern production path tracers. Thus, we kept discussion very brief and cited the relevant methods.\nOn request, we can add a more detailed summary of each sampling technique to the supplemental material to make the paper more self-contained.\nApplying these strategies in an inverse rendering pipeline is less explored. We use detached versions of these samplers.\nRecent work by Zeltner et al. [69] studies this problem in detail, and it is a nascent, active research field.\n\n## **How much the work is coupled with NVDIFFREC. Could the suggested techniques be incorporated into methods like PhySG [1] as well?**\n\nOur denoising step works in image space and is directly compatible with any renderer. Similarly, our sampling works with any geometric representation supporting ray tracing (e.g. triangles, SDFs, ...).\nPhySG uses sphere tracing to evaluate the shape (SDF) and Spherical Gaussians to evaluate shading, w/o shadows.\nReplacing the PhySG renderer with a MC tracer was recently done in the paper \"Differentiable Signed Distance Function Rendering\" by Vicini et al.\nIt drastically increases the noise levels compared to low-frequency SG representation of shading, which suggests that our sampling and denoising strategies could be helpful. Vicini et al. did not include results where the environment lighting is jointly optimized with shape and materials.\n\n## **The discussion on the bias introduced (154-162) is unclear. The seems like an important issue that can also promote further research but is very shortly discussed**\n\nThe variance-bias trade-off is indeed a very important topic. We purposely used biased rendering in our pipeline to get a tractable\noptimization problem. Note also that all denoisers inevitably introduce bias.\nWe discuss it a bit further in the supplemental material (Fig 6, Section 4, Fig 10).\n\n## **The effect of the new regularizer is not tested in an ablation study**\n\nThough not a full ablation, please refer to Fig 1 in supplemental material. We additionally show a full breakdown of the NeRF dataset in the newly added Figure 18 in supplemental material.\n\nNote that the regularizer is particularly well suited for scenes with more complex geometry and self-shadowing, such as Hotdog and Lego.\nThe diffuse lighting encodes most of the shadows, while the Kd/Albedo term contains mostly chrominance, as expected.\nOur regularizer is less suited for scenes with simple geometry and complex materials. We still note that the albedo textures look somewhat improved, \ne.g., for the Materials scene. The Chair scene can be considered a failure case, as some material patterns are baked into lighting, still the material parameters look reasonable.\n\nRecent work on image delighting and shadow removal through neural networks is a promising alternative. While such methods will likely win in the long term,\nthey did not perform well enough on our datasets in our initial experiments.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "QiL3qb7ok-s",
                "writer": "author",
                "reply_to": "PAv7OQgQ1Sh",
                "title": "JQ7r individual questions",
                "comment": " ## **The reconstruction accuracy of individual attributes (such as normal or specular components) was not quantitatively evaluated**\n\nPlease see the common section.\n\n## **Therefore, it is very helpful to clarify which kinds of scenes the existing method is not good at and the proposed method is good at handling so that a user can choose the proper method depending on the scene**\n\nIf material & light separation is important, we argue that this approach is preferable (by tracing shadow\nrays, we avoid shadows getting baked into the albedo texture). If view interpolation is the end goal, then NeRF does an excellent job.\n\n## **What does \u201cscale linearly\u201d mean?**\n\nThe computational cost of evaluating shading increases linearly with the number of samples (not counting denoising and geometry evaluation, which is a fixed cost, regardless of sample count). We use GPU ray tracing and measured the overall runtime of the system.  \n\n## **Can the proposed method be applied to the recovery of translucent or transparent objects with some realistic constraints (e.g., known background)?**\n\nIt can likely be extended to support translucency and transparency, but we have left that for future work.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "kp32RWrab_y",
                "writer": "author",
                "reply_to": "IkiMWlRRDvO",
                "title": "Hmte individual questions",
                "comment": " ## **Q1: Multi-bounce**\n\u200b\nPlease see the common section.\n\n## **Q 2 & 4: Light leakage / How robust is the denoiser in early optimization**\n\u200b\nNote that the light leakage term is not strongly related to shadow (silhouettes/boundary) gradients, but is rather a trick to avoid gradient discontinuities in early training, similar to the denoiser footprint schedule.\n\nThe images indeed do not make much sense in early training, and large filters / strong shadows can cause geometry optimization (in particular) to get stuck in a local minima.\nE.g., optimization may fail to carve out geometry, because a strong shadow will temporarily be added until neighboring geometry is removed.\nWe schedule light leakage and filter footprint in tandem, with a simple linear ramp over the first 1750 iterations of the first optimization pass:\\\n$\\mathrm{light\\_leak} = \\max(0, 1 - i / 1750)$, \\\n$\\sigma = \\sigma_{\\mathrm{max}} * \\min(1, i / 1750)$, \\\nwhere $i$ is the iteration number. We used the *exact same schedule* for all scenes in the paper.\nGiven the large diversity of geometric complexity in our scenes, we consider it robust.\n\n## **Q3: Shadow gradients**\n\u200b\nNote that we did not use visibility gradients for shadow rays in the main paper, as we didn't see a clear benefit in our multi-view setting (as discussed in Section 3.1). For the ablation in the supplemental, we used an extension of the nvdiffrast AA test, extended to 3D, to compute shadow gradients, and we expect similar results using other recent gradients options (warped area/edge sampling). A limited evaluation with visibility gradients based on Warped-Area Sampling indicates the same behavior.  \n\u200b\n## **Q5: During testing, can we get high-quality noise-free results by increasing the number of samples without using denoisers?**\n\u200b\nPlease see the common section.\n\n## **Q6: The paper says view interpolation degrades with improved material and lighting estimations. More materials/evidence are needed to support this claim**\n\u200b\nOur claim is too strong and we will reformulate it. We enforce material/light separation through additional regularization. Without the regularizer loss terms we would optimize for the same (PSNR) image loss used for validation, which should yield a better view interpolation result assuming optimization finds the global minimum. Compared to NeRF, all previous work which provides material separation (NeRD, NeRFactor, nvdiffrec, PhySG) reduces quality, but there is no strict correlation between improved material separation and decreased view interpolation quality.",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "9hUbeFhfKW7",
                "writer": "author",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "Common section",
                "comment": " We thank all the reviewers for the great feedback. In this section, we address some common concerns. Please also refer to the individual responses. To help other researchers reproduce our results, we intend to release source code upon acceptance.\n\n## **Multi-bounce results**\n\nOur paper presents results on direct illumination (environment lighting with shadows). As discussed in our conclusions, multi-bounce\npath tracing is a clear (and exciting) avenue for future work, but comes with additional challenges in increased noise-levels, visibility gradients through specular chains, and drastically increased iteration times.\n\nWe leverage importance sampling and denoising techniques from production path tracers, and argue that\nthese techniques will be important building blocks for future high quality inverse rendering pipelines,\ne.g., the denoising step is applicable as is.\nOur approach is a small step towards multi-bounce MC inverse rendering pipelines.  \n\nFurthermore, neural materials pose an engineering challenge. To make optimization tractable in terms of memory consumption, we need to apply \"path replay\",\n(see \"Path Replay Backpropagation: Differentiating Light Paths using Constant Memory and Linear Time\" by Vicini et al. for details) which require fusing the path tracing and MLP kernels, which is non-trivial in the tinycudann system we currently apply for neural material evaluation.\n\n## **Additional terms in evaluation, e.g. depth, chamfer, normal errors, environment maps**\n\nWe present additional quantitative results below for individual terms. Please also refer to Figure 8 in the paper, where we show a visual breakdown of material parameters and normals compared to nvdiffrec for three scenes. We also added visual results to the supplemental material (Fig 17-18)\nto show the geometry quality and regularizer impact.\n\n### **Albedo texture**\n\nAs noted by NeRFactor & nvdiffrec, there is an indeterminable scaling constant per (R, G, B) value between texture albedo and the environment light (bright material and dim light or vice versa), which makes quantitative evaluation of textured albedo quality challenging.\nThis constant can often completely dominate the error. To sidestep this issue, following NeRFactor and nvdiffrec, in the table below we normalize albedo by the average intensity of the reference albedo.\n\nNeRFactor dataset:\n| PSNR (dB)       |  Drums |  Ficus | Hotdog |   Lego |\n|-----------------|--------|--------|--------|--------|\n| nvdiffrec       |   20.7 |   31.6 |   22.8 |   19.1 |\n| Our             |   20.9 |   32.8 |   22.2 |   22.3 |\n\nNeRF dataset:\n| PSNR (dB)       |  Chair | Hotdog |   Lego |    Mic |\n|-----------------|--------|--------|--------|--------|\n| nvdiffrec       |   24.2 |   18.9 |   19.0 |   28.3 |\n| Our             |   25.5 |   18.3 |   21.2 |   27.1 |\n\nThe aforementioned scaling factors make traditional image metrics a poor measure of material quality.\nIn the newly added Figure 18 in supplemental material we show a material component breakdown for the NeRF scenes.\nWhile the PSNR albedo scores for the Hotdog scene are relatively similar, we argue that our desaturated albedo would\nbe easier for an artist to cleanup/edit.\n\n### **Normal**\n\nWe evaluate normal quality in image space based on the rendered normal g-buffer (as shown in Figure 8). For the NeRFactor dataset\nwe note minor differences in normal quality compared to nvdiffrec.\n\nNeRFactor dataset\n| PSNR (dB)       |  Drums |  Ficus | Hotdog |   Lego |\n|-----------------|--------|--------|--------|--------|\n| NVDIFFREC       |   20.0 |   24.8 |   20.9 |   14.5 |\n| Our             |   19.9 |   24.6 |   20.8 |   15.0 |\n\n### **Chamfer loss**\n\nIn the table below we relate our method to Table 8 (supplemental) of the nvdiffrec paper, and note similar geometric quality as nvdiffrec.\nThe Lego scene is an outlier caused by our normal smoothness regularizer. The scene is particularly challenging for geometric smoothing\nsince it contains very complex geometry full of holes and sharp edges.\n\n|                 |  Chair | Hotdog |   Lego |  Mats. |   Mic |\n|-----------------|--------|--------|--------|--------|-------|\n| PhySG           | 0.1341 | 0.2420 | 0.2592 |    N/A | 0.2712|\n| NeRF (w/o mask) | 0.0185 | 4.6010 | 0.0184 | 0.0057 | 0.0124|\n| NeRF (w/ mask)  | 0.0435 | 0.0436 | 0.0201 | 0.0082 | 0.0122|\n| nvdiffrec       | 0.0574 | 0.0272 | 0.0267 | 0.0180 | 0.0098|\n| Our             | 0.0566 | 0.0297 | 0.0583 | 0.0162 | 0.0151|\n\nPlease refer to Figure 17 in the updated supplemental work for a visual comparison.\n\n## **During testing, can we get high-quality noise-free results by increasing the number of samples without using denoisers?**\n\u200b\nWe will update the paper to be more clear. At test time, all view interpolation results are generated **without** denoising at high sample counts. All relighting results are rendered in Blender with moderate-to-high sample counts and using Blenders denoising algorithm (which is different from ours).\n",
                "rating": -1,
                "confidence": -1
            },
            {
                "review_id": "IkiMWlRRDvO",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "",
                "comment": " The paper proposes a method to estimate the geometry, material and lighting \nof an object from a set of multi-view images. It builds on top of previous work \nnvdiffrec. Different from nvdiffrec that applies split-sum approximation for \ndirect lighting, the proposed method applies Monte Carlo ray tracing  and make the rendering more physically accurate. To reduce the \nnoise caused by Monte Carlo integration when\nthere are few samples, the paper proposed to use denoisers to reduce the\nvariance and make the training more efficient. The experiment results show that \nthe proposed method generates better results than baseline methods such as\nnvdiffrec and nerfactor.\n Strengths:\n\nThe paper combines neural inverse rendering and traditional Monte Carlo ray\ntracing, which enjoys the benefits of both world.\nThe idea of using denoisers to reduce variance in Monte Carlo integration \nis interesting and helps make the training more efficient. The paper did\nmultiple experiments to show that it helps improve material, geometry and \nlighting estimation.\n\nWeakness/Questions:\n1. As mentioned in the paper, while using Monte Carlo integration, the paper \ndidn't consider multi-bounces or shadow gradients, which makes the method fail\nto handle indirect illumination. As a stress test, it would be interesting to\nsee how it works when increasing the number of bounces.\n\n2. For shadow gradients, how the light leakage term is set? Is it a fixed\nparameter for all experiments? \n\n3. In Section 3 of the supp, it's not clear to me how the visibility gradients\nare calculated?  Is the paper using the methods based on warped area/edge sampling? \nIt's worth adding a more detailed discussion here to differentiate the referred\nmethods and the method used by the paper.\n\n4. How robust is the denoiser in the optimization? At the early stage of the\noptimization, when the rendering images don't make too much sense, will denoiser\nmake the optimization worse? The paper talks about ramping up the spatial\nfootprint and linearly blending the noisy and denoised images in supp (Line 60), \nhow robust are those operations? Do they need to be fine-tuned for each scene?\n\n5. During optimization, since it minimizes the difference between the denoised\nimages and the GT images, I am wondering whether it's possible that the noisy \nimages have some artifacts that are not caused by low samples but incorrect\nestimations. Such artifacts will be covered by the denoiser, and will be visible\nduring testing. Is such a case possible? During testing, can we get high-quality\nnoise-free results by increasing the number of samples without using denoisers? \n\n\n6. In Line 266, the paper says view interpolation degrades with improved\nmaterial and lighting estimations. More materials/evidence are needed to support this\nclaim. \n\nOverall, I think the idea of combining neural inverse rendering, traditional \nMonte Carlo integration and denoisers is interesting and novel. The paper did \nthorough evaluations on the method and performs detailed ablation study. \nTherefore, I vote for accepting the paper.\n\n See above. The limitations are well discussed.",
                "rating": 6,
                "confidence": 5
            },
            {
                "review_id": "PAv7OQgQ1Sh",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "",
                "comment": " This paper presents an algorithm for the inverse rendering based on the neural fields. Unlike existing neural-fields-based inverse rendering relies on the simple shading model, the paper introduced the ray tracing with Monte Carlo integration that enables the inverse rendering more general and practical. To make the optimization tractable, the paper proposed to introduce multiple importance sampling and denoising for the differentiable ray tracing. The extensive evaluation shows that the proposed method can accurately recover physical attributes in more physically interpretable manner. Strength:\n\n- The introduction of this work is very clear which is supported by an extensive survey of previous studies. \n\n- Utilizing the denoiser and importance sampling for stable and efficient differentiable rendering makes a lot of sense. I expect that this research will lead the computer vision community the active utilization of real-time rendering technics, which are being actively researched and developed in the field of computer graphics.\n\n- Though most of the techniques for forward-rendering in this paper came from previous works, but their application to inverse rendering is not obvious. I think the beauty of this paper is that in every technical component, the reader can learn why it was introduced, including its surroundings thanks to the proper citations.\n\n- The evaluation successfully demonstrated the benefit of the proposed denoising.\n\n- The extensive ablation study justifies the algorithmic choice. It is really helpful to me to see the comparison of denoisers with detailed discussions in the supplementary material.\n\nWeakness:\n\n- As described in the paper, the method still considers the single bounce of light, therefore essentially inter-reflection is still a problem.\n\n- Though qualitatively demonstrated, the reconstruction accuracy of individual attributes (such as normal or specular components) was not quantitatively evaluated.\n\n- As the authors repeatedly mentioned, the training time is not as fast as expected even with the small sample count per pixel.\n\n- Experiments show that the proposed method does a better job of recovering materials and lighting than the existing methods, but on the other hand, the increase in computational cost is an issue. Therefore, it is very helpful to clarify which kinds of scenes the existing method is not good at and the proposed method is good at handling so that a user can choose the proper method depending on the scene.\n\n- The writing of the paper is very good, and the proposed method is undoubtedly very effective in recent inverse rendering based on Neural Fields. On the other hand, however, I had the impression that the paper uses existing techniques at a very high level rather than making innovative proposals, so I have slightly downgraded it from the highest evaluation. - The paper claimed that the proposed method scales linearly with sample count but the Table at line 275 shows the iteration time doesn\u2019t linear to the sample count. What does \u201cscale linearly\u201d mean?\n\n- Can the proposed method be applied to the recovery of translucent or transparent objects with some realistic constraints (e.g., known background)?\n The limitation is properly described.",
                "rating": 8,
                "confidence": 4
            },
            {
                "review_id": "3tewi_Yke7p",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "",
                "comment": " The paper tackles the challenge of learning the shape, light & material properties of a scene from multiview images. The main idea is to consider a general lighting model for the rendering equation, which in turn is approximated using Monte Carlo (MC). As MC introduces large variance gradient estimates for optimization, the paper suggests importance sampling techniques. In addition, an image denoising model is incorporated into the system. The method is evaluated on standard scene datasets. Strengths\n-------------\nThe paper is well written and easy to follow. The introduction is concisely informative.\nThe suggested system seems to improve previous work in learning scene properties decomposition.\nThe evaluation of synthetic data and real data is adequate.\n\nWeaknesses\n----------------\n\nThe paper focuses on the different solutions the proposed system consists of. However, I am missing a discussion on some of the challenges or design choices made in designing such a system. For example, the choice of the importance sampling techniques is given with any discussion. Some questions that arise are: Can the variance reduction be somehow quantified (bounded above)? Are there other existing techniques?\nAnother example of a missed discussion is how much the work is coupled with NVDIFFREC. Could the suggested techniques be incorporated into methods like [1] as well? \n\nIn addition, the discussion on the bias introduced (154-162) is unclear. The seems like an important issue that can also promote further research but is very shortly discussed. \n\nThe paper could be improved by being more self-contained. For example, the sampling techniques used are only referenced.\n\nThe effect of the new regularizer is not tested in an ablation study. \n\n[1]: PhySG: Inverse Rendering with Spherical Gaussians for Physics-based Material Editing and Relighting, CVPR 2021.\n[2]:  Please address the weakness stated above. The paper does not include such a discussion.",
                "rating": 6,
                "confidence": 3
            },
            {
                "review_id": "1nSmiGmD5m",
                "writer": "official_reviewer",
                "reply_to": "nips_2022_VAeAUWHNrty",
                "title": "",
                "comment": " The paper proposes an inverse rendering pipeline for recovering geometry, reflectance and environment map from a set of multi-view images. The key contributions is a denoising step that significantly reduces variance of Monte Carlo light integration at lower sampling rates. The method achieved superior results compared to a direct lighting baseline (nvdiffrec) and Nerfactor. Strengths:\n\n- The idea to denoise Monte Carlo rendering is well motivated and well implemented. To the best of my knowledge this idea is original.\n- Results are convincing. Particularly the comparison with nvdiffrec baseline shows considerable improvement in novel rendering and relighting. Compared to SG and SH representations, the recovered environment map contains high fidelity details, evident in Figure 2 in the appendix.\n- Limitations have been discussed in detail. I appreciate authors honesty in this.\n\n\nWeakness:\n- Evaluations primarily focus on novel rendering/relighting but lacks in individual intrinsic components such as 3D geometry (e.g. depth, chamfer, normal errors etc) and environment maps. \n- Comparison with recent Monte Carlo inverse rendering method [31,37] especially [37] would strengthen this paper. (Although it should be noted [31] does not solve for geometry, while [37] assumes known lighting and does not have full code release).\n- I think the paper is better suited for CV or graphics venues than NeurIPS. There are machine learning components in this paper but the main components (denoising/Monte Carlo rendering) do not seem very well aligned with NeurIPS. I wonder to what extent the proposed method could be used to handle inter-reflections and light refraction in e.g. translucent objects. To me this seems to be one of the biggest advantage of Monte Carlo integration versus SH/SG but I could not find relevant discussions in paper. Yes limitations have been well discussed.",
                "rating": 5,
                "confidence": 3
            }
        ],
        "label": "train",
        "gpt4_judgements": [
            {
                "Content Expression": "the combination of denoisers for training and MC rays tracing, the thorough evaluation and ablation studies",
                "Sentiment Expression": "Reviewers like",
                "Criteria Facet": "Novelty",
                "Sentiment Polarity": "Positive",
                "Sentiment Expresser": "Others",
                "Convincingness": "Not applicable"
            }
        ]
    }
}