{
    "iH1_KBzbwQq": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes an unsupervised domain adaptation method for point cloud detection in the autonomous driving scenario. The method is built on several effective strategies, including BEV feature representation, pseudo labels, instance contrastive alignment, and hard mining. The proposed method achieves significant gains in detection performance on three constructed new benchmarks of autonomous driving datasets.\n\nThe paper provides a detailed explanation of the proposed method and its effectiveness through experiments. However, there are some limitations and potential negative societal impacts that need to be addressed. Some of the reviewers have provided valuable suggestions for improvement.\n\nStrengths:\n\n* The paper proposes an effective unsupervised domain adaptation method for point cloud detection.\n* The method is built on several strategies that have shown good performance in the literature.\n* The paper provides a detailed explanation of the proposed method and its effectiveness through experiments.\n\nWeaknesses:\n\n* The proposed method may not be as effective in other scenarios or environments.\n* The method takes more memory footprint than existing self-training methods at the training time.\n* The potential negative societal impact of the proposed method has not been addressed.\n\nSuggestions for improvement:\n\n* The paper should provide more discussions on the limitations and potential negative societal impacts of the proposed method.\n* The paper should address the issue of memory footprint and propose solutions to improve the efficiency of the proposed method.\n* The paper should consider other scenarios or environments where the proposed method may not be as effective and provide suggestions for improvement.\n\nIn conclusion, this paper proposes an effective unsupervised domain adaptation method for point cloud detection in the autonomous driving scenario. However, there are some limitations and potential negative societal impacts that need to be addressed. The paper should provide more discussions on these issues and propose solutions to improve the efficiency and effectiveness of the proposed method."
    },
    "OgCcfc1m0TO": {
        "generation": "\n\nManuscript Title: Context Optimization for Few-Shot Image Classification with Vision-Language Models\n\nMeta-Review:\n\nThis paper proposes a novel approach named Context Optimization (CoOp) for few-shot image classification with vision-language models. The proposed method aims to automate prompt engineering and improve the efficiency of task-specific transfer for pre-trained vision-language models. The authors conduct extensive experiments on 11 datasets and show that CoOp outperforms linear probing and fine-tuning on most datasets. However, the paper lacks theoretical contributions and the methodology section is quite brief. Moreover, the key concept of context optimization is not well-explained, and the figure quality is poor. Overall, the paper's quality is below the threshold of ICLR standards.\n\nStrengths:\n\n1. The paper proposes a novel approach to few-shot image classification with vision-language models.\n2. The authors conduct extensive experiments on 11 datasets and show the effectiveness of CoOp.\n3. CoOp outperforms linear probing and fine-tuning on most datasets.\n\nWeaknesses:\n\n1. The paper lacks theoretical contributions and the methodology section is quite brief.\n2. The key concept of context optimization is not well-explained.\n3. The figure quality is poor.\n4. The paper's quality is below the threshold of ICLR standards.\n\nRecommendations:\n\n1. The authors should provide more theoretical insights into the context optimization method and its applications in few-shot image classification.\n2. The methodology section should be expanded to provide a more detailed explanation of the CoOp approach.\n3. The figure quality should be improved to better illustrate the framework and the experimental results.\n4. The paper should be revised to meet the ICLR standards for quality and relevance."
    },
    "JYtwGwIL7ye": {
        "generation": "capabilities, and reward misspecifications. The findings are consistent across all environments: as the agent's capabilities increase, the true reward decreases. The paper also identifies \"phase transitions\" where a small increase in capability leads to a qualitative change in behavior that necessitates new monitoring strategies.\n\nThe paper's contribution is significant, as it provides a systematic study of reward hacking phenomenon, which has been a poorly understood problem in the field. The paper's findings are consistent across multiple environments and demonstrate that increasing agent capabilities often leads to lower true reward, which has important implications for real-world applications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "pTZ6EgZtzDU": {
        "generation": "\n\nThis paper presents a novel method for addressing the explore/exploit tradeoff in RL environments, where the task descriptor is leveraged to train an informed policy that can adapt to new environments without requiring explicit task inference. The proposed method, IMPORT, conditioned on the task descriptor during training and uses an RNN policy to adapt to new environments during testing. The experimental results show that IMPORT outperforms other state-of-the-art methods in various environments.\n\nThe main strengths of the paper are its clarity and technical soundness. The authors present a clear and coherent narrative, and the proposed method is technically sound. The experiments are well-designed and provide convincing evidence of the method's effectiveness.\n\nHowever, there are some concerns about the paper's claims and the experimental results. Firstly, the experiments only evaluate IMPORT against simple environments, and it is unclear how well the method would perform in more complex environments that require sophisticated exploration/exploitation strategies. Secondly, the paper does not provide a clear comparison with other state-of-the-art methods, which makes it difficult to assess the novelty and effectiveness of the proposed method. Finally, the auxiliary loss term (C) is not evaluated/ablated in the experiments, which makes it difficult to understand the contribution of this term to the method's performance.\n\nIn conclusion, while the proposed method shows promising results in addressing the explore/exploit tradeoff, the paper could be strengthened by providing more comprehensive and rigorous evaluations in various environments and comparing the proposed method with other state-of-the-art methods. Additionally, a clearer explanation of the auxiliary loss term (C) and its contribution to the method's performance would be helpful."
    },
    "r1gIdySFPH": {
        "generation": "\n\nMeta-Review:\n\nThis manuscript proposes a novel approach to exploration in goal-conditioned reinforcement learning, called Skew-Fit. The proposed method aims to maximize the entropy of the goal distribution, which leads to diverse and novel goals for the RL policy to reach. The manuscript provides a thorough analysis of the algorithm under certain assumptions and demonstrates its effectiveness through simulations and real-world experiments. However, the task setup is relatively simplistic, and it remains unclear how the method would perform in longer-horizon multi-stage control tasks. Additionally, more elaboration is needed to justify the logic jump in the conclusion. Minor issues include broken references in the appendix. Overall, this is a promising work that contributes to the field of reinforcement learning, but further investigation is required to fully establish its effectiveness and generalizability."
    },
    "HFPTzdwN39": {
        "generation": "\n\nTitle: Investigating the Interpretability of Self-Supervised Visual Representations\n\nAuthors: [Authors' Names]\n\nSummary: This paper investigates the interpretability of self-supervised visual representations learned by recent models. The authors propose a method based on quantized reverse linear probing to evaluate the interpretability of the learned representations. They experiment with various models and find that the proposed method can provide valuable insights into the representations learned by these models. However, the method is not without limitations, and the authors identify areas for improvement.\n\nPositive aspects:\n\n1. The paper addresses an important problem in the field of computer vision, which is the lack of understanding of the representations learned by self-supervised models.\n2. The proposed method provides a novel way of evaluating the interpretability of these representations.\n3. The authors experiment with various models and provide valuable insights into the representations learned by these models.\n\nNegative aspects:\n\n1. The method is not without limitations, and the authors identify areas for improvement.\n2. The proposed method is similar to linear probing, which has been criticized for its limitations.\n3. The authors do not provide a thorough comparison with other methods for evaluating interpretability, which would have strengthened their findings.\n\nRecommendations:\n\n1. The authors should consider addressing the limitations of the proposed method and exploring other approaches for evaluating interpretability.\n2. A thorough comparison with other methods for evaluating interpretability would provide valuable insights into the strengths and weaknesses of the proposed method.\n3. The authors should consider exploring the interpretability of other types of representations, such as those learned by supervised or reinforcement learning methods.\n\nIn conclusion, the paper provides valuable insights into the interpretability of self-supervised visual representations learned by recent models. However, the proposed method has its limitations, and the authors identify areas for improvement. Further research is needed to fully understand the interpretability of these representations and to develop effective methods for evaluating their interpretability."
    },
    "CMcptt6nFaQ": {
        "generation": "\n\nThe proposed method is a topology-aware segmentation method that aims to improve the preservation of connectivity structure in semantic segmentation tasks. The method utilizes a new loss function that combines pixelwise cross-entropy with a new distance transform-based loss term to encourage the model to produce a segmentation mask that is close to the ground truth in pixel space without changing the topology. The proposed method is evaluated on several 2D and 3D datasets and shows promising results compared to other state-of-the-art methods.\n\nStrengths:\n\n* The proposed method is simple and intuitive, with a clear and well-motivated algorithm.\n* The method is efficient and delivers strong results on diverse datasets.\n* The paper provides appropriate ablation studies and comparisons to other methods.\n\nWeaknesses:\n\n* The paper could benefit from more discussion on the reasons for the better performance of the proposed method compared to other methods.\n* The paper could provide more examples of failure modes of the proposed method and images of outputs on all datasets.\n* The method could be generalized to multiclass problems and other types of data.\n* The distance sorting heuristic is important for the proposed method, and its simplicity could lead to a decrease in performance.\n* The proposed method only finds a locally optimal mask, and it is unknown whether finding the global optimum is computationally intractable.\n* The method could be further evaluated by sampling each pixel's on/off state from the output distribution instead of thresholding.\n\nIn conclusion, the proposed method is a promising approach for improving the preservation of connectivity structure in semantic segmentation tasks. However, further discussion and evaluation on the reasons for the better performance, failure modes, and generalization to other problems are needed to fully establish the method's effectiveness."
    },
    "3vmKQUctNy": {
        "generation": "\n\nDear Reviewers,\n\nThank you for taking the time to review our paper on fairwashing detection in black-box models. We appreciate your constructive feedback and engagement with our work. Based on your comments, we have addressed the following issues:\n\n1. Clarity: We have revised the paper to improve its clarity, using simpler language and clearer explanations. We have also added more concise sentences to make the paper easier to follow.\n2. Hyperparameter: We have added more discussion on how to choose the hyperparameter \\delta in practical use, taking into account the different scenarios you mentioned.\n3. Problem setting: We have reconsidered the problem setting to address the concern that the detectability of fairwashing is obvious due to accessibility to the original model. We have proposed a new method that detects fairwashing without relying on access to the original model.\n4. Theoretical analysis: We have provided more explicit definitions of the problem setup and the interpretable model, and clarified the construction of the interpretable model.\n5. Empirical evaluation: We have expanded the empirical evaluation to include more models and settings, and provided more guidance on how to choose the threshold \\Delta.\n6. Figure 3: We have clarified the figure and provided more explanation on the dotted line and multiple fidelity values.\n7. Problem motivation: We have added more discussion on the practical relevance of fairwashing and the importance of detecting it.\n\nBased on your feedback, we believe that our paper has improved significantly and addresses most of the concerns raised. We look forward to continuing the discussion and providing further clarification on any remaining issues.\n\nBest regards,\n\n[Your Name]"
    },
    "QevkqHTK3DJ": {
        "generation": "\n\nMeta-Review:\nThis paper proposes a new approach to reduce the dimensionality of the decoder of a pre-trained transformer for text summarization tasks. The proposed approach uses an autoencoder to compress the decoder and achieves a 40% reduction in decoder size without significant loss of accuracy. However, the choice of autoencoder architecture and the way it is trained are not very clear, and the results are only compared with a single baseline system. Additionally, the paper does not provide a comprehensive comparison with existing compression methods, such as pruning and distillation. Therefore, the contribution of the paper is limited, and the results may not be applicable to other tasks or models. Finally, the paper does not address the problem of the ever-increasing size of the pre-trained encoder, which is a major issue in natural language processing. Overall, the paper is not novel enough, and further research is needed to investigate the effectiveness of the proposed approach in other scenarios."
    },
    "SJgVHkrYDH": {
        "generation": "\n=================================================================\n\nThe proposed manuscript presents a novel approach for multi-hop question answering in open-domain settings. The method leverages the Wikipedia graph to construct a reasoning chain, and uses a RNN model to retrieve the candidate reasoning paths. The authors also propose a negative sampling strategy to robustify the sequential extraction process. The manuscript provides thorough experimental results and ablation studies to demonstrate the effectiveness of the proposed method.\n\nStrengths:\n\n* The proposed method has demonstrated strong results on two challenging datasets in open-domain settings.\n* The ablation studies provide valuable insights into the effectiveness of the different components of the proposed method.\n* The negative sampling strategy helps to robustify the sequential extraction process, which is an important contribution of the paper.\n\nWeaknesses:\n\n* The dependence on the Wikipedia graph may limit the generalizability of the proposed method to other domains.\n* The re-ranking step in the reading stage may not provide sufficient inductive bias to improve the performance of the reader model.\n* The limitation of the BERT reader model in terms of the number of positional embeddings may hinder the performance of the proposed method.\n* The running time of the proposed method may be prohibitive for open-domain QA tasks.\n\nOverall, the proposed manuscript presents a novel approach for multi-hop question answering in open-domain settings. The method leverages the Wikipedia graph to construct a reasoning chain, and uses a RNN model to retrieve the candidate reasoning paths. The proposed method has demonstrated strong results on two challenging datasets, and the ablation studies provide valuable insights into the effectiveness of the different components of the proposed method. However, the dependence on the Wikipedia graph may limit the generalizability of the proposed method to other domains, and the re-ranking step in the reading stage may not provide sufficient inductive bias to improve the performance of the reader model. Additionally, the limitation of the BERT reader model in terms of the number of positional embeddings may hinder the performance of the proposed method. Finally, the running time of the proposed method may be prohibitive for open-domain QA tasks."
    },
    "ByeadyrtPB": {
        "generation": "\n\n---\nMeta-Review:\n\nThis paper proposes a novel approach to training deep generative models by stacking multiple Wasserstein autoencoders. The proposed method, called stacked Wasserstein autoencoders (SWAs), is designed to learn hierarchical representations by recursively applying the WAE framework. The key innovation of SWAs is the use of a hierarchical structure to learn multiple levels of representations, which can be used to capture complex patterns in the data.\n\nThe reviews provided are mixed, with some reviewers praising the novelty and elegance of the proposed method, while others are more skeptical about its effectiveness. The main concerns raised by the reviewers include the lack of a clear theoretical justification for the proposed method, the difficulty in comparing the success of the proposed method to existing approaches, and the need for more quantitative evaluation schemes.\n\nIn terms of the manuscript's strengths, the authors provide a clear and concise explanation of the proposed method and its theoretical underpinnings. The experimental results presented in the paper are also impressive, showing that the proposed method can learn hierarchical representations that are useful for tasks such as image generation and denoising.\n\nHowever, the manuscript also has several weaknesses. Firstly, the theoretical justification for the proposed method is not fully developed, which may make it difficult for some readers to understand the underlying principles of the method. Secondly, the comparison to existing approaches is not comprehensive, which may limit the reader's understanding of the novelty of the proposed method. Finally, the manuscript could benefit from more quantitative evaluation schemes, which could help to provide a more comprehensive assessment of the proposed method's effectiveness.\n\nOverall, the paper presents an interesting and novel approach to training deep generative models, but more work is needed to fully establish its effectiveness and theoretical foundations."
    },
    "mk0HzdqY7i1": {
        "generation": "of paper, which questions the validity of a highly influential paper, should be welcomed. \n\nThe paper raises important questions: \n\n* How important is the GNN architecture in the performance of the algorithm? \n* How important are the hyperparameters and how sensitive are they to the performance?\n* What are the fundamental limitations of the algorithm?\n\nThe paper does not answer these questions, but it is an important contribution in the field. \n\n"
    },
    "HyxUIj09KX": {
        "generation": "\n\nThe manuscript \"S-System: A Generalization of Hierarchical Models Including Neural Networks\" presents a new framework for understanding deep learning models, specifically neural networks. The paper proposes a new activation function that can be derived in a principled way and demonstrates that the optimization problem of neural networks has nice properties in certain regimes. However, the reviews indicate that the paper is difficult to follow due to the large number of concepts introduced at once and the lack of clarity in the presentation.\n\nBased on the reviews, I recommend the following actions:\n\n1. Split the paper into multiple parts: The reviews suggest that the paper is too long and complex, and it would be helpful to break it down into smaller, more manageable parts. This would allow the authors to focus on one concept or result at a time, making it easier for the reader to follow.\n2. Provide clearer and more detailed proofs: The reviews indicate that the proofs are difficult to follow, and some reviewers suggest that the authors should provide more detail and clarity in their arguments. This would help to build confidence in the results and make them more accessible to a wider audience.\n3. Use a running example: Some reviewers suggest that using a running example would help to illustrate the implications of the results and make them more accessible to non-experts. This could involve working through a simple example, such as a perceptron or a neural network with a small number of layers and units, to show how the S-System framework applies.\n4. Consider revising the paper for a different journal or conference: One reviewer suggests that the paper is more suitable for a mathematical journal, while another reviewer recommends submitting it to a conference that focuses on deep learning. Considering the feedback from the reviews, it may be helpful to revisit the journal or conference selection to ensure that the paper is submitted to the most appropriate outlet.\n\nIn conclusion, the manuscript presents an interesting and important contribution to the field of deep learning, but it may benefit from some revisions to make it more accessible and easier to follow. By breaking the paper down into smaller parts, providing clearer proofs, using a running example, and considering a different journal or conference, the authors can improve the clarity and impact of their work."
    },
    "Ybx635VOYoM": {
        "generation": "\n\nTitle: Study on Robustness of QA Systems to Misinformation\n\nAuthors: [Authors' Names]\n\nAbstract: \n\nIn this paper, we investigate the robustness of QA systems to misinformation by introducing a new dataset of contradictory contexts for SQuAD. We propose a framework to generate these contradictory contexts and evaluate the performance of state-of-the-art QA models on the new task. Our results show that QA models perform poorly when faced with contradictory information, and we propose a method to detect misinformation using a fake detector model. We also analyze the nature of the edits made by human workers and discuss the potential ethical implications of releasing the dataset.\n\nStrengths:\n\n* The paper addresses an important problem in the field of NLP and QA: the robustness of QA systems to misinformation.\n* The proposed framework for generating contradictory contexts is novel and interesting.\n* The evaluation of state-of-the-art QA models on the new task provides valuable insights into the performance of these models in the presence of misinformation.\n* The proposed fake detector model has the potential to improve the performance of QA models in detecting misinformation.\n\nWeaknesses:\n\n* The paper does not provide a clear explanation of how the fake detector model works and how it can be used to detect misinformation.\n* The evaluation of the fake detector model is limited to a small set of examples and does not provide a comprehensive evaluation of its performance.\n* The paper does not discuss the potential ethical implications of releasing the dataset of contradictory contexts, which could be used to mislead or deceive users.\n* The paper does not provide a comprehensive analysis of the nature of the edits made by human workers, which could be important for understanding the limitations of the proposed framework.\n\nNits:\n\n* In section 4, the authors claim that the proposed framework can generate contradictory contexts that are \"realistic and diverse\". However, the examples provided in the paper do not demonstrate this.\n* In section 5, the authors claim that the proposed fake detector model can improve the performance of QA models in detecting misinformation. However, the evaluation of the model is limited to a small set of examples and does not provide a comprehensive"
    },
    "rkzjUoAcFX": {
        "generation": "\n\nTitle: Adapting TTS for Speakers with Limited Data\n\nAuthors: [insert authors' names]\n\nAbstract: This paper proposes a method for adapting a text-to-speech (TTS) system to a new speaker using limited data. The proposed method leverages a pre-trained WaveNet model and uses a few-shot adaptation technique to adapt the model to the new speaker. The authors evaluate the proposed method using a small number of utterances from the new speaker and show that it outperforms other few-shot adaptation methods.\n\nDiscussion: The paper presents a novel approach for adapting a TTS system to a new speaker using limited data. The proposed method leverages a pre-trained WaveNet model and uses a few-shot adaptation technique to adapt the model to the new speaker. The authors evaluate the proposed method using a small number of utterances from the new speaker and show that it outperforms other few-shot adaptation methods.\n\nThe reviews provided by the reviewers are generally positive, with one reviewer commenting that the paper is well-written and has strong results. However, some reviewers have raised some concerns and suggestions for improvement. For instance, reviewer 2 has suggested that the paper should provide more details about the linguistic features used in the study, while reviewer 3 has suggested that the paper should consider using speaker-dependent linguistic features. Reviewer 5 has also raised a concern about the comparability of the MOS evaluations done in the paper with those done in other studies.\n\nConclusion: Overall, the paper presents a promising approach for adapting a TTS system to a new speaker using limited data. While there are some concerns and suggestions for improvement, the paper has the potential to make a significant contribution to the field of speech synthesis. Therefore, I recommend accepting the paper for publication after addressing the concerns and suggestions raised by the reviewers."
    },
    "dgd4EJqsbW5": {
        "generation": "\n\nMeta-review:\nThis manuscript proposes a new method for policy improvement in reinforcement learning, which combines policy iteration in the latent space with representation learning. The authors provide a theoretical guarantee on the performance of their method, and demonstrate its effectiveness through experiments on several tasks. However, the reviews suggest that the current version of the paper has some limitations.\n\nStrengths:\n\n* The manuscript provides a new and interesting approach to policy improvement in reinforcement learning.\n* The authors provide a theoretical guarantee on the performance of their method, which is a significant contribution to the field.\n* The experiments demonstrate the effectiveness of the proposed method on several tasks.\n\nWeaknesses:\n\n* The reviews suggest that the current version of the paper lacks sufficient experimentation, particularly compared to other state-of-the-art methods.\n* The number of hyperparameters in the loss function is large, which makes it difficult to choose the optimal values.\n* The authors do not provide code for their work, which limits the reproducibility of their results.\n\nRecommendations:\n\n* The authors should consider adding more experiments to demonstrate the effectiveness of their method on a wider range of tasks.\n* The authors should provide more guidance on how to choose the optimal values for the hyperparameters in the loss function.\n* The authors should consider providing code for their work to increase its reproducibility.\n\nOverall, the manuscript provides a promising approach to policy improvement in reinforcement learning, but there are some limitations that need to be addressed through additional experimentation and guidance for practitioners."
    },
    "8OH6t0YQGPJ": {
        "generation": "\n\nMeta-Review:\n\nThe paper proposes a novel framework for multiverse analysis in machine learning, leveraging Gaussian process surrogates and Bayesian experimental design to quantify the exploration of different hyperparameter settings and draw robust conclusions about model performance. The paper demonstrates the effectiveness of the proposed framework through two comprehensive case studies, and addresses potential limitations and environmental concerns.\n\nStrengths:\n\n* The paper provides a principled and practical way to achieve reproducible results and draw robust conclusions in the ML community.\n* The proposed framework is well-demonstrated by two comprehensive case studies, which highlight the effectiveness of the method in understanding the multiverse of hyperparameter settings.\n* The authors address potential limitations and environmental concerns, such as the computational complexity of the framework and the need for efficient surrogate models.\n\nWeaknesses:\n\n* As pointed out by the authors and reviewers, the computation time of the framework will scale with the multiverse size, which may limit its practicality in large-scale ML applications.\n* The effectiveness of the proposed framework is demonstrated mainly through visualization, and there is a need for other quantitative techniques to analyze the results in high-dimensional search spaces.\n* The authors did not provide a clear answer to the question of how to choose the initial design, which may affect the performance of the proposed method.\n\nOverall, the paper provides a valuable contribution to the field of ML by proposing a principled and practical framework for multiverse analysis. However, there are some limitations and concerns that need to be addressed in future work. Specifically, the computational complexity of the framework needs to be carefully considered, and other quantitative techniques may be needed to analyze the results in high-dimensional search spaces. Additionally, the choice of the initial design needs to be addressed in order to ensure the effectiveness of the proposed method."
    },
    "xTYL1J6Xt-z": {
        "generation": "\n\nManuscript Title: FasterRisk: Efficient Risk Score Generation via Sparse Logistic Regression and Star Ray Search\n\nMeta-Review:\n\nThe manuscript presents a novel method for efficiently generating a pool of high-quality risk scores using sparse logistic regression and star ray search. The proposed method, FasterRisk, aims to balance accuracy and efficiency in risk score generation. The authors demonstrate the effectiveness of FasterRisk through extensive experiments on various datasets and provide theoretical guarantees for its quality.\n\nStrengths:\n\n1. The manuscript introduces a novel three-step framework for risk score generation, which combines beam search, sparse logistic regression, and star ray search.\n2. The proposed method is computationally efficient and achieves state-of-the-art performance in many datasets.\n3. The authors provide a thorough analysis of the theoretical guarantees of FasterRisk, which ensures the quality of the generated risk scores.\n\nWeaknesses:\n\n1. The method has some limitations related to the choice of hyperparameters and the tolerance gap level, which could affect the fairness of the generated risk scores.\n2. The authors did not include enough ablation experiments to evaluate the efficiency of the proposed method.\n3. The method only considers logistic regression and does not consider other machine learning models, which may limit its applicability in some scenarios.\n\nRecommendations:\n\n1. The authors should provide more ablation experiments to evaluate the efficiency of FasterRisk in comparison to other methods.\n2. The method should be extended to consider other machine learning models, such as decision trees or neural networks, to increase its applicability.\n3. The authors should provide more detailed discussions on the fairness of the generated risk scores and how to improve it.\n\nIn conclusion, the manuscript presents a novel method for efficient risk score generation using sparse logistic regression and star ray search. The proposed method demonstrates promising results in various datasets and provides theoretical guarantees for its quality. However, the method has some limitations related to the choice of hyperparameters and the tolerance gap level, which could affect the fairness of the generated risk scores. Further research is needed to address these limitations and improve the method's applicability and fairness."
    },
    "_idcJrecij": {
        "generation": "\n\nTitle: Arbitrary Conditional Densities with Energy (ACE)\n\nAuthors: [Author Name]\n\nSummary: The authors propose a simple and effective method for estimating arbitrary conditional densities using a neural network to represent the energy function and importance sampling to estimate the normalizing constant. The approach is demonstrated on several real-world datasets and shows improved performance compared to existing methods.\n\nReviews:\n\n* The authors propose a simple and effective method for estimating arbitrary conditional densities.\n* The approach is based on a neural network to represent the energy function and importance sampling to estimate the normalizing constant.\n* The method is demonstrated on several real-world datasets and shows improved performance compared to existing methods.\n\nWeaknesses:\n\n* The authors do not provide a thorough analysis of the computational complexity of the method.\n* The approach relies on the quality of the neural network used to represent the energy function.\n* The authors do not provide a comprehensive comparison with other state-of-the-art methods.\n\nDiscussion:\n\n* The paper presents a simple and effective method for estimating arbitrary conditional densities.\n* The approach has important implications for a wide range of applications, including missing data imputation and density estimation.\n* The method is well-suited for problems where the conditional density is complex or difficult to model.\n\nRecommendations:\n\n* The authors should provide a thorough analysis of the computational complexity of the method.\n* The authors should conduct a comprehensive comparison with other state-of-the-art methods.\n* The authors should consider applying the method to more complex datasets to demonstrate its capabilities."
    },
    "Yn4CPz_LRKO": {
        "generation": "\n\nMeta-Review:\n\nThis is a manuscript that proposes a new conditional GAN model that employs a discriminative classifier to minimize the reverse KL between joint $Q_{X,Y}$ and $P_{X,Y}$. The proposed ADC-GAN can successfully learn the joint distribution whose conditional marginal classifier is minimizing an undesired divergence. The paper presents interesting analysis of AC-GAN, TAC-GAN, and PD-GAN. The experimental results demonstrate the effectiveness of ADC-GAN on four benchmark datasets.  However, the manuscript has several weaknesses. \n\nStrengths:\n\n1. The paper precisely analyzes the conditional marginal classifier.\n2. The proposed discriminative classifier is interesting to resolve the generator-agnostic issue.\n3. The experimental results demonstrate the effectiveness of ADC-GAN on four benchmark datasets.\n4. The authors provide a simple and interpretable classification model.\n\nWeaknesses:\n\n1. The paper does not fully explain the loss between the joint distribution and the reverse KL between joint $Q_{X, Y$Q_{X,Y} and $P_Y. \n2. The author's method is not be able to learn a conditional GAN to the generator-GAN, which is not minimize the generator-GAN a classifier. \n\n\n3. The author's optimizer is not be able to minimize the generator\n\n4. \n\n\n5. the loss function.\n\n\n\n\n\n\n\n\n\n6.\n\n\n7.\n\n\n\n\n\n\n\n8.\n\n\n\n\n\n\n\n\n9.\n\n\n\n\n9.\n\n\n\n\n\n\n\n\n\n\n\n\n10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "yxafu6ZtUux": {
        "generation": "\n\nMeta-review:\nThis manuscript proposes a new method for online A/B testing with qualitative treatment effects (QTE) detection. The method is based on a scalable algorithm with adaptive randomization, non-asymptotic upper bound on the type-I error, and a maximum number of data peeking times that grows with the number of observations. The authors provide a bootstrap method to determine the stopping boundary and demonstrate the efficiency of the approach through experiments on simulated and real data from Yahoo! The manuscript has received mixed reviews from the reviewers, with some suggesting improvements in the presentation and clarity of the results, while others have raised questions on the novelty and relevance of the proposed method.\n\nThe manuscript has several strong points, including the well-defined problem and the theoretical guarantee on the type-I error. The approach proposed in the manuscript is scalable and adaptive, which makes it suitable for real-world applications. The experiments on simulated and real data provide evidence for the efficiency of the method.\n\nHowever, there are some areas where the manuscript could be improved. Some reviewers have raised concerns about the clarity of the presentation and the lack of real-world examples to illustrate the application of the proposed method. Additionally, some reviewers have questioned the novelty and relevance of the proposed method, particularly in comparison to existing sequential test methods for online A/B testing.\n\nTo address these concerns, the authors could consider revising the manuscript to improve the clarity of the presentation and provide more real-world examples to illustrate the application of the proposed method. Additionally, the authors could provide more comparisons with existing sequential test methods to demonstrate the advantages and limitations of their proposed method. Finally, the authors could consider addressing the questions raised by the reviewers regarding the novelty and relevance of their proposed method.\n\nOverall, the manuscript has the potential to make a valuable contribution to the field of online A/B testing, but it requires further revision and clarification to address the concerns raised by the reviewers."
    },
    "47lpv23LDPr": {
        "generation": "provide any advantage over the invariant-equivariant approach.\n\nRegarding my concern on the need for learning separate encoders, I think the authors could address this by providing a theoretical or empirical comparison between the fully equivariant encoder-decoder and the proposed invariant-equivariant approach. The authors could also explore other equivariant approaches like rotational invariant autoencoder or the invariant-equivariant approach by Mehr and Welling (2021).\n\nRegarding the need for a decoder to have the same structure as the encoder, I think the authors could explore other decoder structures like a linear or affine transformations. \n\nFinally, I believe that the authors should provide more theoretical and empirical analysis of the invariant-equivariant approach, including the comparison with other closely related work."
    },
    "BZ92dxDS3tO": {
        "generation": "\nManuscript Title: OnePose++: Keypoint-free SfM for Accurate Object Pose Estimation. \n\nOnePose++ is a manuscript that proposes a new approach to object pose estimation using keypoint-free SfM. The authors claim that their method outperforms most baselines by a wide margin. However, there are several limitations and potential negative impacts that need to be addressed in future work.\n\nStrengths:\n\n1. The proposed method has the potential to be more accurate than existing methods, especially in low-texture scenarios.\n2. The authors provide a detailed ablation study that analyzes the impact of individual components of OnePose.\n3. The proposed method is well structured and easy to follow.\n\nWeaknesses:\n\n1. The paper assumes that the object videos are given a priori, so one-shot object pose estimation of low-texture objects is not well studied.\n\n2. The proposed method handles low-texture objects and does not require CAD model for training and the keypoint-free SfM.\n\n3. The authors do not provide an evaluation of the proposed method on a real-time.\n4. The method seems to be not well-studied in the object pose estimation.\n\n5. The authors claim that their method is 10 times faster than other methods.\n6. The proposed method has the low-texture objects.\n7. The proposed method is not well-texture.\n\n8.\n\n\n9.\n\n\n10.\n\n\n\n\n\n\n\n\n\n\n10.\n\n\n\n\n\n\n1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "_VjQlMeSB_J": {
        "generation": "\n\nTitle: Chain of Thought Prompting for In-Context Few-Shot Learning\n\nMeta-Review:\nThis paper proposes Chain of Thought (CoT) prompting, a simple and effective method to improve the performance of large language models in complex reasoning tasks. The authors demonstrate that CoT can significantly improve the performance of large language models on arithmetic, commonsense, and symbolic reasoning tasks. The proposed method is well-motivated and well-executed, with comprehensive and detailed analyses to support the claims. The paper also provides insights into the property of the model and the robustness of the proposed approach.\n\nStrengths:\n\n1. Novelty and effectiveness: The proposed method is simple, effective, and generalizable to various tasks. The authors demonstrate that CoT can improve the performance of large language models on challenging tasks.\n2. Comprehensive and detailed analysis: The paper performs comprehensive and detailed analysis to help establish the claims and draw the conclusions with solid evidence and justification. The detailed analyses provide insights and interesting discussions on the property of the model, e.g., the property with regard to language model sizes and complexities of the target problems.\n3. Robustness: The paper shows that the chain-of-thought prompts are robust in different perspectives (e.g., different annotators and various language models).\n\nWeaknesses:\n\n1. Limited originality: The originality of the paper is a bit of concern, as the proposed method is similar to other methods that have been proposed in the literature. However, the paper's contributions are valuable to the community.\n2. Generalizability: The paper claims that the proposed approach is applicable in principle to any tasks that humans can solve via language. This is not well supported in the paper and may need further justification.\n3. Debugging: The paper mentioned that one contribution of the proposed model is facilitating interpretation and debugging of the few-shot learning models. Can the authors add some discussion on how to perform the debug?\n\nIn conclusion, this paper proposes a simple and effective method to improve the performance of large language models in complex reasoning tasks. The proposed method is well-motivated, well-executed, and provides valuable insights into the property of the model. Although there are some limitations and concerns"
    },
    "INBO6h9gtG": {
        "generation": "\n\nMeta-Review:\n\nThis manuscript proposes new DP mean estimation algorithms that improve the sample complexity over previous techniques, and have less stringent assumptions. The authors present two different algorithms for Gaussian and sub-Gaussian settings, which are based on the exponential mechanism with Tukey depth score and the empirically rescaled Gaussian mechanism, respectively. While the proposed methods have a high sample complexity, the authors argue that they are of theoretical interest only and do not have practical applications.\n\nThe reviews are generally positive, with reviewers highlighting the novelty and utility of the proposed methods. However, some reviewers have raised concerns about the computational complexity of the proposed algorithms and the lack of empirical evaluations. Additionally, one reviewer has noted that the paper does not address the issue of catastrophic failure of privacy in approximate-DP frameworks.\n\nOverall, the manuscript presents interesting and novel results in the area of DP mean estimation, but the practical applicability of the proposed methods is limited due to their high computational complexity. Further work is needed to develop efficient and practical algorithms for DP mean estimation."
    },
    "H1ldzA4tPr": {
        "generation": "\n\n\nManuscript Title: Learning Compositional Koopman Operators for Efficient Dynamics Modeling of Non-linear Systems\n\nMeta-Review: This paper proposes a novel approach to learning compositional Koopman operators for efficiently modeling the dynamics of non-linear systems. The proposed method combines graph neural networks with approximate Koopman embedding and reduces the number of parameters by assuming a block-wise structure in the Koopman gain and control matrices. The paper presents experiments on simple toy problems and controlled environments, showing the effectiveness and efficiency of the proposed approach compared to baselines. However, some concerns and suggestions are raised regarding the evaluation and extension of the proposed method to real-world problems.\n\nStrengths:\n\n1. The proposed method builds on a sound theoretical basis of Koopman operator theory, which provides a solid foundation for the development of efficient dynamics modeling techniques.\n2. The use of graph neural networks to encode the state into object-centric embeddings is a novel and interesting approach that can capture complex interactions between objects.\n3. The proposed method is computationally efficient, as demonstrated by the results showing a significant reduction in parameters compared to previous approaches.\n4. The experiments conducted on simple toy problems and controlled environments provide a good understanding of the effectiveness and efficiency of the proposed method.\n\nWeaknesses:\n\n1. The evaluation of the proposed method is limited to synthetic datasets, which may not provide a comprehensive understanding of its performance on real-world problems.\n2. The lack of a benchmark or web-based evaluation methodology hinders the ability to compare the proposed method with other approaches in the literature.\n3. The assumption of a block-wise structure in the Koopman gain and control matrices may not be applicable to systems with varying properties, such as objects with different stiffness or damping.\n4. The control experiment is limited to 32 steps, which may not be sufficient to fully evaluate the performance of the proposed method in control tasks.\n\nSuggestions for Improvement:\n\n1. Extend the proposed method to real-world problems by conducting experiments on more complex and diverse systems.\n2. Develop a benchmark or web-based evaluation methodology to facilitate comparisons between different approaches in the literature.\n3. Consider relaxing the assumption of a block-wise structure in the Koopman gain and control"
    },
    "BJl6bANtwH": {
        "generation": "\n\nThe manuscript proposes a novel method for detecting underdetermination in deep neural networks, which is a key problem in extrapolating to unseen data. The method uses a local ensemble of neural networks to estimate the uncertainty of predictions, and the authors demonstrate its effectiveness through theoretical analysis and experimental evaluation.\n\nThe main contribution of the paper is the introduction of a new measure of extrapolation score, which is based on the variance of predictions across a local ensemble of neural networks. The authors show that this measure is proportional to the standard deviation of predictions across a model ensemble with similar training loss, providing a tractable quantity for quantifying uncertainty in predictions.\n\nThe paper also provides a theoretical analysis of the extrapolation score, showing that it is closely related to the geometry of the loss surface. Specifically, the authors prove that the extrapolation score is proportional to the first-order approximation to the change in prediction under a perturbation of the parameters with minimal change in loss.\n\nThe experimental evaluation of the method is also strong, with the authors demonstrating its effectiveness on several benchmark datasets. The results show that the proposed method outperforms several state-of-the-art methods for extrapolation, including MC-dropout and Bayes by Backprop.\n\nHowever, there are some limitations to the paper that should be addressed. Some of the reviewers have raised concerns about the theoretical foundation of the method, particularly in terms of the connection between the extrapolation score and the geometry of the loss surface. Additionally, the experimental evaluation is limited to a few benchmark datasets, and it would be useful to explore the method's performance on a wider range of datasets.\n\nOverall, the manuscript provides a promising contribution to the field of deep learning, and it has the potential to be a valuable tool for practitioners and researchers working on extrapolation and uncertainty in deep neural networks. However, further theoretical and experimental analysis is needed to fully establish the method's effectiveness and robustness."
    },
    "BJeapjA5FX": {
        "generation": "\n\n\nMeta-Review:\nThe paper proposes a novel approach to adversarial robustness by combining BNP-MFA with a feature extraction and classification stage. While the authors demonstrate promising results in the experiments, there are several concerns that need to be addressed.\n\nFirstly, the reviewer questions the choice of using unsupervised conditional kernel density estimates (KDE) for feature extraction, as it is unclear how well these features will generalize to unseen data. Additionally, the reviewer wonders whether the soft label encoding helps increase the robustness, as there are existing works that have shown similar results using simpler models.\n\nSecondly, the reviewer notes that the proposed model is not robust to gradient-based noise and outliers, as there is no direct attack evaluation provided in the paper. The reviewer suggests that the authors should conduct thorough robustness evaluations using various attack methods to support their claims of robustness.\n\nFinally, the reviewer raises the question of scalability, as the proposed model is computationally expensive and may not be practical for large-scale datasets. The reviewer suggests that the authors should provide more technical details and experiments to address these concerns.\n\nIn conclusion, while the paper proposes an interesting approach to adversarial robustness, there are several concerns that need to be addressed before the paper can be considered robust. The authors should provide more technical details, conduct thorough robustness evaluations, and address the issue of scalability to support their claims."
    },
    "8pOPKfibVN": {
        "generation": "\n\nMeta-Review:\nThe paper proposes a new method for adapting neural networks at inference time to minimize some given unsupervised loss. The method, called tailoring, and its extension, meta-tailoring, show promising results in improving robustness and reducing the train-test generalization gap. However, there are some limitations and open questions regarding the practical implementation of the method, such as the need for careful tuning of hyperparameters, and the potential impact on privacy. The authors provide extensive examples of inductive biases and conduct extensive experiments to demonstrate the effectiveness of the proposed method.\n\nStrengths:\n\n* The proposed method, tailoring, provides a novel and interesting perspective on reducing the generalization gap.\n* The extension of the method, meta-tailoring, demonstrates the potential for improving robustness and reducing the train-test generalization gap.\n* The authors provide a wide variety of experiments to show the effectiveness of the method across different datasets and inductive biases.\n* The theoretical discussion and empirical results provide a solid foundation for the proposed method.\n\nWeaknesses:\n\n* The practical implementation of the method requires careful tuning of hyperparameters, which can be time-consuming and may not be scalable.\n* The potential impact on privacy is not fully addressed, and further research is needed to understand the implications of using tailoring in practical applications.\n* The method is not limited to any specific domain or application, but rather a general framework that could be applied to a wide range of tasks.\n\nOpen Questions:\n\n* How can the practical implementation of tailoring be made more scalable and efficient?\n* How can the potential impact on privacy be better understood and addressed?\n* How can the method be extended to other domains or applications, such as computer vision or natural language processing?\n* How can the method be combined with other techniques, such as adversarial training or transfer learning, to further improve robustness and generalization?\n\nIn conclusion, the paper provides a promising new method for adapting neural networks at inference time to minimize some given unsupervised loss. The method shows potential for improving robustness and reducing the train-test generalization gap, but further research is needed to address the practical implementation and potential impact on privacy. The proposed method provides a solid foundation for future research in this area, and could have"
    },
    "qRDQi3ocgR3": {
        "generation": "\n\nMeta-Review:\n\nThe paper presents an interesting analysis of the preference of deep neural networks for different visual cues, such as color, shape, and ethnicity. The study uses a novel training setup that controls the prevalence of cues and shows that networks tend to prefer simpler cues, such as color. The paper also provides evidence that easy-to-learn cues tend to converge to relatively flat minima and models that prefer these cues are more abundant in parameter space. The analysis is insightful, and the results are interesting. However, there are some minor concerns and suggestions for improvement that could make the paper even more impactful.\n\nFirstly, the reviewer notes that the problem statement of the paper could be improved. Specifically, the reviewer suggests that the authors should provide more insight into what they mean by \"bias\" in the context of inductive learning. Additionally, the reviewer points out that the experiment setup is sound, but the presented answers that networks tend to learn simpler cues, where simpler can be measured by Kolmogorov complexity, are not overly surprising or counter-intuitive.\n\nSecondly, the reviewer raises a methodical weakness of the paper, which is the lack of a truth/label assignment on the off-diagons (e.g. indicating that blue small triangle is not supposed to be class 1). Without this truth assignment, the reviewer questions whether the bias of the model can be truly evidenced.\n\nThirdly, the reviewer suggests that the paper could be improved by providing more insight into the potential impact of the analysis. Specifically, the reviewer asks what the authors can now see different, do different, etc. after reading the paper. Additionally, the reviewer notes that the paper addresses an important topic, but the overall impact, measured by depth of presented contributions, is too shallow.\n\nFinally, the reviewer raises some minor comments and suggestions for improvement. For example, the reviewer notes that the term \"averted solution\" is not defined before being used in section 3.2, and suggests that the authors provide more clarity on the difference between averted and preferred solutions. Additionally, the reviewer suggests that the authors depict several plots per solution to cover more directions of variation in figure 4, and provide more insight into the potential impact of large models on model preferences for simpler"
    },
    "SkgVRiC9Km": {
        "generation": "\n\nMeta-Review:\n\nThis manuscript presents a new approach to defending against adversarial attacks on neural networks using a fortified network model. The proposed method, which extends a denoising autoencoder to the hidden layers, shows improvements in adversarial robustness on three datasets. However, the reviewers have raised several concerns regarding the evaluation of the defense and the motivation behind the proposed method.\n\nFirstly, the reviewers have noted that the authors primarily evaluate their defense using FGSM, particularly on CIFAR, and have not benchmarked against state-of-the-art defenses such as PGD. It is important to establish the merit of a new defense by comparing it to the best available defenses in the literature. Additionally, the reviewers have pointed out that the epsilon values used for the PGD attacks are relatively small, and the authors should report accuracies to a range of epsilon values for the PGD attack.\n\nSecondly, the reviewers have questioned the motivation behind using a denoising autoencoder in the proposed method. While it is not clear that adversarial examples lie off the data manifold, and a simple regularization loss based on the error between hidden layer representations to natural examples and the corresponding adversarial example may be a more straightforward approach. The authors should motivate the use of denoising autoencoders by comparing to such a simple baseline.\n\nFinally, the reviewers have suggested that the proposed method may not be robust to attacks that target the fortified layers. Ian Goodfellow has pointed out that a more specifically designed algorithm could attack the network described in the paper as the old way, and the authors should address this concern.\n\nIn conclusion, while the proposed method shows improvements in adversarial robustness, further evaluation and analysis are needed to establish its effectiveness against a wider range of attacks and to understand the limitations of the proposed approach. The authors should also address the concerns raised by the reviewers regarding the motivation and robustness of the proposed method."
    },
    "rC3zu-OqnII": {
        "generation": "\n\nRebuttal:\nThank you for your thoughtful and detailed reviews. I appreciate your time and effort in evaluating our manuscript. Your feedback is invaluable in improving the quality of our work.\n\nRegarding the first reviewer's comments, we agree that the paper's contribution is significant and novel, and we have added a paragraph to the introduction to highlight the limitations of existing works and the gap in the literature that our paper fills. We have also clarified the notation and the limiting regime in the theorems, and provided additional examples to illustrate the results.\n\nWe appreciate the second reviewer's feedback on the quality of the paper. We have made several revisions to improve the clarity and readability of the manuscript, including revising the math notation and the exposition. We have also added more descriptions of the diffusion scaling technique and its limitations to address the reviewer's concern.\n\nRegarding the third reviewer's comments, we agree that the paper's results are primarily theoretical and do not directly address practical applications. However, we believe that understanding the behavior of bandit algorithms is an important fundamental problem in machine learning, and our results provide a better understanding of the asymptotic behavior of UCB and Thompson sampling. We have added a paragraph to the conclusion to discuss the potential implications of our results for practical applications.\n\nFinally, we appreciate the fourth reviewer's comments on the limitations of the paper and the need for further research in the multi-armed bandit setting. We have added a paragraph to the conclusion to highlight the directions for future research.\n\nIn conclusion, we believe that our manuscript provides a significant and novel contribution to the field of bandit algorithms, and we have addressed the reviewer's comments and concerns to improve the quality of the paper. We hope that the manuscript will be accepted for publication.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "HJgkx2Aqt7": {
        "generation": "\n\nMeta-review:\n\nThe paper proposes a novel method for automatically tuning the parameters of a simulation engine to maximize the performance of a model trained using this simulation engine. The method uses policy gradients to optimize the simulation parameters, and the experiments demonstrate the effectiveness of the proposed method on several computer vision tasks. However, the paper is not without its limitations.\n\nFirstly, while the method is sound and well-motivated, the paper fails to provide a thorough comparison with related work. This is a critical weakness, as the proposed method bears a striking resemblance to previous work in the field. A more comprehensive comparison would have provided valuable insights into the strengths and limitations of the proposed method.\n\nSecondly, the experiments are relatively limited in scope, with only a few tasks and datasets considered. While the results are promising, it is unclear how the proposed method would perform on more complex tasks or datasets. A larger and more diverse set of experiments would be needed to fully assess the effectiveness of the proposed method.\n\nThirdly, while the method is presented as orthogonal to domain adaptation and other sim-to-real techniques, it is not clear how the proposed method would perform in these contexts. Given the growing interest in sim-to-real transfer learning, it is important to explore how the proposed method can be integrated with these techniques to further improve performance.\n\nDespite these limitations, the paper makes an important contribution to the field of simulation-based training and transfer learning. The proposed method has the potential to significantly improve the performance of models trained using simulation engines, and its orthogonality to other sim-to-real techniques makes it a valuable addition to the field.\n\nIn conclusion, the paper presents a novel method for automatically tuning the parameters of a simulation engine to maximize the performance of a model trained using this simulation engine. While the paper is not without its limitations, it makes an important contribution to the field of simulation-based training and transfer learning. Further work is needed to fully assess the effectiveness of the proposed method and to explore its potential integration with other sim-to-real techniques."
    },
    "BJx040EFvH": {
        "generation": "\n\nThis manuscript presents a novel approach to training robust neural networks against strong adversarial attacks. The proposed method combines the Random+FGSM method with optimization tricks such as cyclic learning rate and mixed precision to achieve fast and robust training. The experimental results show that the proposed method is effective in defending against strong PGD attacks and achieves better performance than previous methods in terms of both accuracy and training time.\n\nThe reviews from the referees are generally positive, with one reviewer expressing some concerns about the experimental results. However, the authors have addressed these concerns in their rebuttal and provided additional experiments to alleviate the issues.\n\nThe main contribution of the paper is the demonstration that the Random+FGSM method can be used to train robust neural networks against strong adversarial attacks. The proposed method combines the Random+FGSM method with optimization tricks such as cyclic learning rate and mixed precision to achieve fast and robust training. The experimental results show that the proposed method is effective in defending against strong PGD attacks and achieves better performance than previous methods in terms of both accuracy and training time.\n\nOne concern raised by one reviewer is that the success of the proposed method relies on early stopping, which may not be unique to the proposed method. However, the authors have addressed this concern by providing additional experiments to show that the proposed method can achieve good performance without early stopping.\n\nAnother concern raised by one reviewer is that the authors have not compared their method with other state-of-the-art methods for defending against adversarial attacks. However, the authors have addressed this concern by providing a comparison with other methods in their rebuttal.\n\nOverall, the paper provides a novel approach to training robust neural networks against strong adversarial attacks, and the experimental results demonstrate the effectiveness of the proposed method. The paper is clearly written and easy to follow, and the authors have addressed the concerns raised by the referees in a satisfactory manner. Therefore, I recommend acceptance of the manuscript."
    },
    "bJz3cFePTna": {
        "generation": "\n\nThis is a well-written and well-structured paper that proposes a new method for mixture proportion estimation and PU learning. The proposed method, TED, combines BBE and CVuO and shows promising results in experiments. However, there are some minor suggestions and limitations that could be addressed in future work.\n\nThe paper provides a clear and detailed explanation of the proposed method and its theoretical analysis, which is well-organized and easy to follow. The experimental results are convincing and show that the proposed method outperforms baselines in most cases.\n\nHowever, there are some limitations and concerns that could be addressed in future work. Firstly, the method relies on milder conditions, but a clearer explanation is needed to understand the weaknesses of the prior methods. Secondly, the procedure of the proposed method is relatively simple but consists of some parts, which could bring extra computational consumption. Finally, the method consists of some hyper-parameters that need to be determined, and their influence in experiments should be analyzed.\n\nIn conclusion, this paper provides a significant contribution to the field of mixture proportion estimation and PU learning. The proposed method, TED, shows promising results and has the potential to be applied in real-world scenarios. However, some minor improvements and analyses are needed to further strengthen the method and address the limitations."
    },
    "iBBcRUlOAPR": {
        "generation": "\n\nTitle: Exploring the Scaling Law of Training Transformer-Based Large Language Models\n\nAuthors: [Author's Name]\n\nAbstract: This paper explores the scaling law for training Transformer-based large language models (LLMs). The authors train various LLMs by varying the model size and training tokens, and estimate the power-law relationship between them. They find that the optimal scaling way is to increase the size of training data equally while enlarging the model size. They also train a new model called Chinchilla, which uses more data but the same compute as a state-of-the-art model and significantly outperforms the latter on multiple downstream tasks.\n\nStrengths:\n\n* The paper provides experimental results that contradict previous findings and might be quite consequential for the future of large language models.\n* The experiments are quite extensive, and the authors address the limitations of their work in Section 5.\n* The paper is clearly written, and the conclusion is easy to understand and implement.\n* The topic of the paper is very important but under-explored, and the results will be useful for years to come.\n\nWeaknesses:\n\n* The paper does not provide any code or models for reproducibility, which is a major concern.\n* The authors do not discuss whether all tokens are created equal, or if there are specific domains that are more beneficial for downstream tasks.\n\nSocietal Impact:\n\n* The paper's findings could have a positive impact on the development of large language models, as it provides a useful scaling law to save computational budget.\n* However, the authors did not address the potential negative societal impact of their work, such as the deployment of biased language models.\n\nOverall, the paper provides valuable insights into the scaling law of training Transformer-based LLMs, and the results are significant and easy to implement. However, the lack of code and models release, and the potential negative societal impact, are major concerns that need to be addressed in future works."
    },
    "rRFIni1CYmy": {
        "generation": "\n\nMeta-review:\n\nThe proposed paper, \"Egocentric Spatial Memory Networks (ESMN) for Learning Spatial Memory in Robotics,\" presents a novel approach to learning spatial memory in robotics. The paper proposes a new architecture for encoding spatial memory in a sphere representation, which can be used for various downstream applications such as semantic segmentation and action learning. The proposed approach builds on a Kalman Filter setting and encodes memory in a spherical structure, which makes it different from existing approaches.\n\nThe reviews from the reviewers are generally positive, with some minor concerns and suggestions for improvement. Reviewer 1 notes that the method section is confusing and lacks clarity, while Reviewer 2 raises concerns about the experimental evaluation and the choice of baselines. Reviewer 3 provides detailed feedback on the experimental evaluation and suggests that the paper should provide more details on the experimental setup and baselines.\n\nTo address these concerns, the authors could consider revising the method section to provide more clarity and detail on the proposed approach. Additionally, the authors could provide more information on the experimental setup and baselines in the paper to enable proper evaluation and comparison.\n\nOverall, the proposed paper presents a novel approach to learning spatial memory in robotics, which has the potential to make a significant contribution to the field. However, to ensure the quality and impact of the paper, the authors should address the minor concerns and suggestions for improvement raised by the reviewers."
    },
    "rylDzTEKwr": {
        "generation": "\n\nMeta-review:\n\nThe proposed paper presents an end-to-end variational hashing-based collaborative filtering scheme with self-masking to address the efficiency issue of large-scale recommender systems. The authors propose a novel self-masking technique to differentiate the importance of binary bits in the Hamming distance computing process. The comparative experiments on several datasets demonstrate the superior performance of the proposed method. However, there are some issues with the experimental results and the reported results are not convincing. The reported results are inconsistent with the results in the original paper, and the efficiency comparison with other methods is not well conducted. The motivations of the designed hashing method are vague. Therefore, I tend to reject this paper."
    },
    "BkSDMA36Z": {
        "generation": "\n\nMeta-review:\n\nThe proposed method for text classification is well-motivated and well-executed. The authors introduce a simple and effective approach to learning vector representations of fixed-size text regions, which leads to state-of-the-art results on several benchmark datasets. The method is based on a combination of word embeddings and local context units, which are learned using a simple optimization objective. The authors provide a detailed analysis of the influence of different parameters on the model's performance, which helps to further understand the strengths and limitations of the approach.\n\nHowever, there are some minor issues with the writing and formatting of the paper, which could be improved. Some of the typos and grammatical errors listed in the reviews could be addressed to make the paper more readable. Additionally, the authors could provide more discussion on the relationship between their method and recent advances in attention-based models, which seem to be closely related.\n\nOverall, the paper presents a valuable contribution to the field of text classification and provides a simple and effective approach to learning vector representations of fixed-size text regions."
    },
    "vuFJO_W85VU": {
        "generation": "\n\nMeta-Review:\n\nThe proposed work, Iterative Amortized Policy Optimization, presents a novel approach to policy optimization in reinforcement learning (RL) that leverages the connection between amortized variational inference and policy-based RL with function approximation. The approach adds an additional optimization procedure over the distribution parameters themselves, rather than those of the policy network, to improve the performance of the policy. The work demonstrates the benefits of this approach through extensive experimental results on MuJoCo environments and shows that it can improve performance, accuracy, and flexibility in the optimization process.\n\nThe reviews of the paper are generally positive, with reviewers highlighting the novelty and potential of the proposed approach. However, some reviewers have raised some concerns and suggestions for improvement, such as investigating the tradeoffs inherent in iterative amortization, discussing the properties of environments that make iterative amortization more or less useful, and providing more theoretical analysis to develop a deeper understanding of the approach.\n\nOverall, the paper provides a valuable contribution to the field of RL by introducing a novel approach to policy optimization that leverages the connection between amortized variational inference and policy-based RL with function approximation. The work demonstrates the potential of the proposed approach through extensive experimental results and provides a good starting point for future research in this area.\n\nHowever, it is important to address the concerns and suggestions raised by the reviewers to further improve the paper and provide a more comprehensive understanding of the proposed approach. Specifically, the paper should provide more theoretical analysis to develop a deeper understanding of the approach and its limitations, and investigate the tradeoffs inherent in iterative amortization to better understand its potential and limitations.\n\nIn conclusion, the proposed work provides a valuable contribution to the field of RL by introducing a novel approach to policy optimization that leverages the connection between amortized variational inference and policy-based RL with function approximation. While the paper has some limitations and suggestions for improvement, it provides a good starting point for future research in this area and has the potential to make a significant impact in the field of RL."
    },
    "XQu7UFSbzd2": {
        "generation": "\n\nThe proposed work in the manuscript tackles the problem of event sequence modeling and forecasting in the presence of distribution shift. The authors propose a variational learning algorithm that encodes the context embeddings, which model the changing data distribution of past event sequences, and motivate it as a structural causal model where the causal relation between context and future event sequence is deconfounded. The proposed approach is evaluated on several benchmarks, showing improvement in some of them. However, there are some limitations and open questions that need to be addressed.\n\nFirstly, the reviewers have raised concerns about the assumption of the proposed approach, which relies on the backdoor adjustment to deconfound the context and events causal relationship. It is not clear how effective this approach will be in real-world scenarios where the distribution shift may be more complex and non-stationary. Moreover, the reviewers have pointed out that the model does not adapt to novel contexts and there is no evidence of good uncertainty estimation in unknown scenarios.\n\nSecondly, the reviewers have highlighted some limitations of the proposed approach, such as the sensitivity of the model to the number of contexts and the lack of evaluation on datasets with known context shifts. Additionally, the reviewers have suggested that the proposed approach may not be robust to novel or derivative contexts.\n\nFinally, the reviewers have recommended providing more details on the model's ability to adapt to uncertain situations and its ability to provide good uncertainty bounds. They have also suggested evaluating the model on a benchmark where changes in context are known to test if the learned contexts map to domain knowledge.\n\nIn conclusion, while the proposed work shows promising results in addressing the problem of event sequence modeling and forecasting in the presence of distribution shift, there are still some limitations and open questions that need to be addressed. The authors should consider these suggestions and provide more insights into the model's ability to adapt to uncertain situations and provide good uncertainty bounds."
    },
    "yqPnIRhHtZv": {
        "generation": "better to\n  other languages.\n\n- The paragraphs could benefit from a slightly more consistent\n  structure. Sometimes, there are three or four sentence-long\n  clauses in a row, which can make the text hard to follow.\n  Alternatively, it would be great if the authors could use a\n   more consistent paragraph structure, with each sentence forming\n   a coherent thought.\n\n# Positive aspects\n\nThis is a very interesting approach, which could potentially\nlead to more accurate and robust topological\nrepresentations. I appreciate the authors' effort in\npresenting the material in a more general and intuitive way, and I\nbelieve that this could make a strong contribution to the field.\n\n# Negative aspects\n\nI have some minor confusions and concerns regarding the proposed\napproach, which I will discuss below. Firstly, I would like to see more\n  details about the learned hyperbolic representation for the persistence diagrams and image\nclassification tasks. classification.\n\n1. The authors show that the persistence diagrams as a good and robust and more robust and more robust and practical applications robustness of the robust and meaningful and robustness of the robustness of the robustness of the proposed method is not only some of the robustness of the robustness of the robustness of the persistence diagrams and classification tasks."
    },
    "eNB4WXnNczJ": {
        "generation": "\n\nTitle: Compressed and Accelerated Gradient Method for Distributed Optimization\n\nAuthors: [Author's Name]\n\nAbstract:\n\nThis paper proposes a novel distributed optimization method, called CANITA, which combines the advantages of compressed gradient passing and acceleration techniques. The proposed method achieves a faster convergence rate than existing state-of-the-art methods in non-strongly convex problems. Theoretical analysis and numerical experiments demonstrate the effectiveness of the proposed method.\n\nReviews:\n\n1. The paper proposes a compressed and accelerated gradient method called CANITA for distributed optimization. The main contribution is the (near) optimal algorithm and its theoretical analysis. Originality: The paper considers a unexplored topic, how to fuse acceleration technique and unbiased compression operator organically. Though the algorithm (CANITA) is a combination of accelerated ANITA method and compressed DIANA method, the analysis of the resulting algorithm has its own difficulty, i.e., how to analyze the error introduced by the compression operator while not ruining the delicate structure of acceleration for convex problems.\n\n2. The paper improves the state-of-the-art result of non-accelerated compressed algorithms. I think the work might give insights on how to combine compression and acceleration optimally. Quality: The theoretical analysis is valid. The author also provides a proof sketch to illustrate how to construct a potential function for convergence analysis. Clarity: The paper is well written and clear. Significance: The paper improves the state-of-the-art result of non-accelerated compressed algorithms.\n\n3. The paper considers a distributed GD setting where multiple nodes need to jointly minimize the average of individual convex objectives by computing and communicating its compressed local gradients. The proposed algorithm is shown to achieves an accelerated convergence rate with compressed gradient passing. This paper is clearly written and well organized. The proven convergence with the proposed algorithm is faster than a few earlier works. My major concern is the usefulness of the studied setting. Requiring each node to compute its gradient (instead of stochastic/sampled gradient) in a distributed optimization does not seem practical.\n\n4. This paper proposes a novel distributed methods with acceleration and communication compression. Authors proof the convergence and show that"
    },
    "-geBFMKGlkq": {
        "generation": "\n\nTitle: A New Density Function for Density-Based Clustering\n\nAuthors: [insert authors' names]\n\nManuscript: [insert manuscript's title and reference number]\n\nReviewers: [insert names of reviewers]\n\nDiscussion:\n\nThe proposed manuscript presents a new density function for density-based clustering methods, specifically for DBSCAN and DPC. The authors propose a diffusion-based approach to construct the density function, which captures the local characteristics of the dataset. The manuscript provides experimental results to demonstrate the effectiveness of the proposed method. However, there are some concerns and suggestions raised by the reviewers that need to be addressed before the manuscript can be accepted.\n\nKeywords: density-based clustering, DBSCAN, DPC, density function, diffusion process.\n\n1. Theoretical Justification:\n\nReviewer 1 raises a valid concern about the theoretical justification of the proposed density function. The authors need to provide a more convincing argument to demonstrate that the proposed density function can capture the local characteristics of the dataset.\n\n2. Experimental Validation:\n\nReviewer 2 highlights the importance of experimental validation. The authors need to provide more detailed information about the experimental setup, including the number of clusters, the value of hyperparameters, and the computational cost of the proposed method.\n\n3. Hyperparameter Tuning:\n\nReviewer 3 suggests that the authors should provide NMI values for the datasets of Table 1. Moreover, the authors need to address the issue of hyperparameter tuning, which is a common problem in density-based clustering methods.\n\n4. Dimensionality:\n\nReviewer 4 raises a concern about the dimensionality of the face datasets. The authors need to provide information on the dimensionality of the datasets and the effect of dimensionality on the proposed method.\n\n5. Small Datasets:\n\nReviewer 5 highlights the issue of small datasets of relatively high dimensionality. The authors need to address this concern by providing a more detailed analysis of the performance of the proposed method on small datasets.\n\nConclusion:\n\nThe proposed manuscript presents a new density function for density-based clustering methods. While the proposed method shows promising results, there are some concerns and suggestions raised by the reviewers that need to be addressed before"
    },
    "Ehhk6jyas6v": {
        "generation": "m not sure if it's the mean across all tasks, or a single task\n- some of the tables have very sparse entries - it would be nice to have some indication of how many datasets/methods were used for each table\n\nThe paper proposes a new framework for understanding and evaluating methods for unsupervised and semi-supervised disentanglement learning and concept learning. The proposed metrics are related to the quality of the learned representations and the performance of the model. The results are quite interesting and give insights into how the two types of methods perform on different downstream tasks. The authors have put decent efforts to unify the two types of methods. The paper has some limitations, as discussed below."
    },
    "ZDaSIkWT-AP": {
        "generation": "\nThe manuscript proposes a novel approach to improving text-based game AI by combining case-based reasoning (CBR) and reinforcement learning (RL). The authors present a detailed evaluation of their approach on two challenging text-based games, demonstrating significant improvements in performance compared to baseline RL agents. The manuscript is well-written and easy to follow, with clear explanations of the proposed approach and experimental results. However, there are some points of concern and suggestions for improvement that are raised by the reviewers, which are summarized below:\n\nStrengths:\n\n* Novel approach to improving text-based game AI by combining CBR and RL\n* Detailed evaluation of the proposed approach on two challenging games\n* Clear and well-structured manuscript\n\nWeaknesses:\n\n* Limited novelty of the proposed approach, as similar approaches have been proposed in the past\n* Need for more thorough comparison with other approaches (e.g., based on transformers) that make use of external knowledge to generalize better\n* Some design choices (e.g., retrieval mechanism, discretization scheme) could benefit from more analysis and justification\n* Arbitrary choice of threshold in the retrieval mechanism\n* Need for more ablation studies to confirm the effectiveness of the proposed approach\n\nSuggestions for improvement:\n\n* Provide more detailed analysis of the design choices and their impact on performance\n* Conduct thorough comparisons with other approaches that make use of external knowledge to generalize better\n* Address the concerns raised by the reviewers regarding the retrieval mechanism and discretization scheme\n* Provide more ablation studies to confirm the effectiveness of the proposed approach\n\nIn conclusion, the manuscript presents a novel approach to improving text-based game AI by combining CBR and RL. While the proposed approach shows promising results, there are some points of concern and suggestions for improvement that are raised by the reviewers. Addressing these points will help to strengthen the manuscript and demonstrate the effectiveness of the proposed approach."
    },
    "GjWDguPZRmr": {
        "generation": "\n\nThe manuscript proposes a unified solution to two problems that can occur when training VAEs: the hole problem (where the aggregated variational distribution fails to fit the prior) and posterior collapse (where the variational distribution becomes the same for every data point and is therefore uninformative). The proposed solution is a modification to the VAE's training objective, where the per-data-point KL divergence term is replaced by the KL divergence from the aggregated posterior to the prior. The authors compare their method to various baselines, all of which were designed to solve the posterior collapse issue. The proposed method appears to address the posterior collapse problem, while suffering the hole problem to a lesser extent than the baselines.\n\nThe main strengths of the paper include its simplicity and effectiveness in solving both the hole problem and posterior collapse. The authors do a good job at explaining these problems and the proposed method's advantages. The empirical results appear to outperform the baselines in terms of the latent representations and observations having a high amount of mutual information.\n\nHowever, there are some weaknesses in the paper. The explanation of the proposed method is sometimes difficult to follow, and some of the notation is confusing. Additionally, the evaluation in the experiments section is not entirely convincing, and the limitations of the method are not discussed.\n\nOverall, the paper provides a valuable contribution to the field of VAEs by proposing a unified solution to two problems that can occur during training. However, there are some areas where the paper could be improved, such as clarifying the notation and providing more detailed explanations of the method and its advantages."
    },
    "_A4-JP8d_f": {
        "generation": "\n\nTitle: A Framework for Analyzing Variational Inference in f-Divergence Families\n\nAuthors: [Author1, Author2, Author3]\n\nThis paper presents a framework for analyzing variational inference in f-divergence families, which can be used to guide practitioners in selecting appropriate methods for different problem scenarios. The authors propose a novel approach based on generalized Pareto distributions to model the density ratio in the pre-asymptotic regime, which allows for a more accurate prediction of the behavior of different variational inference methods. The paper provides a comprehensive analysis of the three predictions made in the framework, and the results are supported by experimental validation using several commonly used models. The authors also highlight some limitations of the framework and suggest directions for future research.\n\nThe reviewers have provided valuable feedback on the manuscript, including suggestions for improving the presentation of the results and the discussion of the limitations of the framework. The authors should consider addressing these points in the revision process.\n\nOverall, the paper provides a useful contribution to the field of variational inference by providing a framework for analyzing the behavior of different methods in f-divergence families. The proposed approach has the potential to guide practitioners in selecting appropriate methods for different problem scenarios, and the results of the analysis are supported by experimental validation. However, there are some areas where the presentation could be improved, and the authors should address these points in the revision process."
    },
    "zgMPc_48Zb": {
        "generation": "\n\nMeta-review:\n\nThe paper proposes a novel approach to learning differentially private generative models using the Sinkhorn divergence. The proposed method takes advantage of the Sinkhorn divergence to achieve robustness against the hyperparameters' choices. The authors also introduce a cost function enabling the generative model to generate images associated with a specific class label. However, the paper faces several challenges and limitations.\n\nFirstly, the novelty of the proposed method is minimal as the Sinkhorn GANs have previously been proposed. The method merely adds Gaussian noise to gradients, and the DP analysis is not significantly different. Secondly, the paper's experimental results are not strong, with poor performance in both MNIST and CelebA datasets compared to existing methods. The paper fails to provide a thorough comparison with other differentially private GANs, which is a significant limitation.\n\nThirdly, the concatenation of data and labels in the cost function may not be a good strategy, as the data and label spaces are different. This may lead to a weak optimization strategy. Fourthly, the relationship to existing work, particularly Chen et al., is not clearly handled. It is unclear how to prioritize the papers, and the authors' contribution is not well-defined.\n\nFinally, the paper's focus on accuracy measures such as FID score is limited, as these measures may not accurately capture the quality of the generated samples. The paper does not adequately address the challenge of adapting the framework to task-specific accuracy measures.\n\nIn conclusion, while the paper presents an interesting approach to learning differentially private generative models, it faces several challenges and limitations. The novelty of the proposed method is minimal, the experimental results are not strong, and the relationship to existing work is not clearly handled. Therefore, the paper is borderline and requires significant revision before it can be accepted for publication."
    },
    "SkxxIs0qY7": {
        "generation": "\n\nMeta-Review:\n\nThe manuscript \"CoT: A Novel Framework for Learning Sequence Distributions\" presents a new approach for learning sequence distributions using a mediator-based generative model. The proposed method, called CoT, replaces the discriminator in a GAN with a mediator that estimates the density of the mixture of the data and the generator's distribution. The main advantage of CoT is that it can achieve lower Jensen-Shannon divergence values compared to existing methods for discrete sequential data. The manuscript provides promising experimental results on a synthetic Turing test and text generation from EMNLP's 2017 news dataset. However, there are some minor errors and odd formulations in the manuscript that need to be addressed.\n\nPros:\n\n* The proposed method, CoT, is novel and has the potential to improve the learning of sequence distributions.\n* The numerical experiments presented in the manuscript are promising.\n* The manuscript provides a clear explanation of the proposed method and its theoretical convergence properties.\n\nCons:\n\n* There are some minor errors and odd formulations in the manuscript that need to be addressed.\n* The manuscript does not provide a clear link to the code used for the experiments, which makes it difficult to reproduce the results.\n* The manuscript does not discuss the consistency of the proposed method with respect to other metrics, which is an important aspect of any generative model.\n\nQuestions:\n\n* How does CoT extend to other types of divergences that have proved superior to Jensen-Shannon in some continuous settings?\n* What is the maximum entropy solution mentioned in Equation 14, and how does it relate to the optimization problem?\n* How do the (b) and (c) plots in Figure 2 relate to each other?\n\nRecommendations:\n\n* Address the minor errors and odd formulations in the manuscript.\n* Provide a clear link to the code used for the experiments.\n* Discuss the consistency of the proposed method with respect to other metrics.\n* Extend the proposed method to other types of divergences.\n* Provide a more detailed explanation of the maximum entropy solution.\n\nOverall, the manuscript provides a promising approach for learning sequence distributions using a mediator-based generative model. With some minor revisions, the manuscript can be further improved to"
    },
    "5dHQyEcYDgA": {
        "generation": "\n\nThe manuscript proposes a novel method for improving the interpretability of attention-based MIL models in digital pathology. The proposed method, called additive MIL, combines the strengths of attention-based MIL and additive attention models to provide a more interpretable and accurate model. The authors demonstrate the effectiveness of the proposed method through experiments on three publicly available histopathology datasets.\n\nStrengths:\n\n* The method provides a straightforward way to assign credit to different regions in an image, allowing for more interpretable and accurate predictions.\n* The authors provide a theoretical guarantee that the per-instance prediction represents the marginal prediction, which is a significant contribution to the field.\n* The method is simple and general, and can be applied to any MIL method that includes a pooling operation.\n* The authors provide a comprehensive evaluation of the proposed method through experiments on three publicly available datasets, demonstrating its effectiveness in improving interpretability and accuracy.\n\nWeaknesses:\n\n* The method is based on the assumption that the attention coefficients in attention-based MIL models are not interpretable, which may not always be the case.\n* The authors do not provide a detailed analysis of the computational complexity of the proposed method, which may be a concern for large-scale applications.\n* The method is only evaluated on three datasets, and it is unclear how well it will perform on other datasets or MIL methods.\n* The authors do not provide a comprehensive comparison of the proposed method with other state-of-the-art methods for interpretability, which may limit the generalizability of their findings.\n\nIn conclusion, the proposed method provides a novel and effective way to improve the interpretability of attention-based MIL models in digital pathology. The method combines the strengths of attention-based MIL and additive attention models, and provides a more interpretable and accurate model. However, the method is based on certain assumptions and may have limitations in terms of computational complexity and generalizability. Further research is needed to fully evaluate the effectiveness and limitations of the proposed method."
    },
    "0RDcd5Axok": {
        "generation": "\n\nMeta-review:\n\nThis paper provides a unified framework for parameter-efficient NLP, an important task when serving models at scale. The authors analyze recent parameter-efficient methods and propose a unified framework that subsumes a number of existing approaches. They then compare their performance across four NLP tasks, with varying level of difficulty and amount of available resources. The paper is well-organized, well-motivated, and well-written, with clear theoretical analysis and proof. The experiments are comprehensive and provide new insights into the performance of existing methods. The proposed method Scaled PA consistently shows its advantages against the other baselines. The paper also proposes a novel MAM adapter module that leads to further gains in every task. Overall, this paper is well-written and well-motivated, and it is likely to attract lots of researchers and inspire future works in parameter-efficient training. However, the lack of standard deviations and the focus on generative tasks / enc-decoder models limits the robustness and generality of the results.\n\nStrengths:\n\n* A new perspective of parameter-efficient methods within a unified framework\n* A number of controlled studies which give insights into which components are meaningful and in which cases\n* The released code base can help boost future research in this area\n* Clear and well-written\n\nWeaknesses:\n\n* No major weaknesses identified\n\nQuestions:\n\n* It would be interesting to see the variance observed when applying parameter-efficient methods\n\nTypos:\n\n* In Figure 2, the performance of Adapter is 21.00, not 20.46\n* The \"v\" subscript is missing in Eq. 5"
    },
    "4azYdmhHCG": {
        "generation": "\n\nTitle: Calibrating Bias-Only Models for Improved Debiasing Performance\n\nAuthors: [Author Name]\n\nManuscript: This manuscript presents a novel approach to improving debiasing performance in Ensemble-based Debiasing (EBD) methods by calibrating the bias-only models used in these methods. The authors provide theoretical and empirical analysis demonstrating the effectiveness of this approach and highlighting the importance of calibrating bias-only models for EBD methods.\n\nStrengths:\n\n1. Novelty: The manuscript presents a novel approach to improving debiasing performance by calibrating the bias-only models used in EBD methods. This is an important contribution to the field, as previous work has primarily focused on developing new debiasing methods without considering the uncertainty of the bias-only models.\n2. Clarity: The manuscript is well-written and easy to follow, with clear explanations of the theoretical concepts and experimental results.\n3. Empirical results: The authors provide thorough empirical results demonstrating the effectiveness of their approach across multiple datasets and calibration methods.\n\nWeaknesses:\n\n1. Limited impact: The empirical gains obtained by calibrating the bias-only models are relatively small, especially compared to the gains yielded by standard EBD methods. This may limit the practical impact of the manuscript.\n2. Limited generalizability: The manuscript focuses primarily on NLP tasks, and it is unclear how well the proposed approach would generalize to other domains.\n3. Lack of comparison to state-of-the-art methods: The authors do not provide a comprehensive comparison of their approach to state-of-the-art debiasing methods, which would help to further demonstrate the effectiveness of their approach.\n\nRecommendations:\n\n1. Increase the empirical gains: The authors could explore ways to further improve the debiasing performance, such as using different calibration methods or incorporating additional information into the debiasing process.\n2. Explore generalizability: The authors could investigate the applicability of their approach to other domains and datasets to better understand its limitations and potential for improvement.\n3. Provide a comprehensive comparison: The authors could conduct a thorough comparison of their approach to state-of-the-art debiasing methods to"
    },
    "2zCRcTafea": {
        "generation": "\n\nMeta-review:\nThis paper proposes a new attention mechanism called focal attention for vision transformers to enable efficient long-range interactions. The proposed module shows better performance than state-of-the-art approaches on both image classification and object detection tasks. However, there are some concerns about the inference speed and scalability of the proposed method, which need to be addressed in future works.\n\nStrengths:\n\n* Novel attention mechanism for efficient long-range interactions in vision transformers\n* Improved performance on image classification and object detection tasks compared to state-of-the-art methods\n* Extensive ablation studies and comparisons with other methods\n\nWeaknesses:\n\n* Limited discussions on model efficiency and scalability\n* The proposed module may introduce additional computational overhead due to the need for multi-scale windows and region partition operations\n* The number of levels in the attention mechanism is fixed and may not be optimal for all tasks\n\nRecommendations:\n\n* Include more experimental comparisons with other efficient transformer methods to evaluate the performance and efficiency of the proposed module\n* Conduct thorough evaluations of the scalability and computational overhead of the proposed module\n* Explore the optimal number of attention levels for different tasks and models to improve the performance and efficiency of the proposed module."
    },
    "TlS3LBoDj3Z": {
        "generation": "\n\nMeta-review:\n\nThis paper presents an improved version of QTRAN, a theoretically justified algorithm for cooperative multi-agent learning. The authors propose several modifications to the original algorithm, including a modified loss function and a new architecture for the action-value estimator. The paper demonstrates the effectiveness of QTRAN++ on the SMAC environments, outperforming several baselines in terms of data efficiency and final performance. However, some reviewers have raised concerns about the complexity of the proposed algorithm and the lack of comparison with other state-of-the-art methods.\n\nThe main strengths of the paper are its theoretical foundation and the empirical performance of the proposed algorithm. QTRAN has a solid theoretical basis, and the authors provide a clear explanation of how the proposed modifications improve the performance of the algorithm. The experimental results demonstrate the effectiveness of QTRAN++ on several benchmark environments, outperforming several baselines.\n\nHowever, some reviewers have raised concerns about the complexity of the proposed algorithm and the lack of comparison with other state-of-the-art methods. The authors have proposed several modifications to the original QTRAN algorithm, which may make the proposed algorithm more difficult to understand and implement. Additionally, the paper does not provide a comprehensive comparison with other state-of-the-art methods, which could help to further validate the effectiveness of QTRAN++.\n\nOverall, the paper presents a promising approach to cooperative multi-agent learning, but further work is needed to fully validate the effectiveness of the proposed algorithm and to provide a more comprehensive comparison with other state-of-the-art methods."
    },
    "SJw03ceRW": {
        "generation": "\n\nMeta-review:\n\nThe paper proposes a method for adapting a pre-trained network to incorporate novel classes for doing classification, especially when the novel classes only have a few training examples available. The method uses a 'hard' distillation approach, where new nodes and parameters are introduced to the network to add the new classes, but only the new networks are fine-tuned without modifying the original parameters. This ensures that class confusions will only be between the old and new classes and not between the old classes. The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation (iCARL) in low-shot settings and with a large number of training examples of the novel classes available. The proposed method performs better in low-shot settings and comparably when large number of training examples of the novel classes are available.\n\nThe paper is generally well-written and the idea is simple and novel. However, the validation is limited to a small dataset size (O(10) classes) and far from any realistic use. The method is not convincing, especially when the number of classes is large. It is necessary to curate subsets for the settings proposed to make the method more convincing.\n\nThe main criticism is the limited dataset size on which the method is validated. The method is compared with other methods, but the comparison is not convincing as the methods are not tested on the same dataset. The paper should provide more convincing results by testing the method on larger datasets.\n\nThe paper proposes a simple and novel method for adapting a pre-trained network to incorporate novel classes for doing classification, especially when the novel classes only have a few training examples available. The method uses a 'hard' distillation approach, where new nodes and parameters are introduced to the network to add the new classes, but only the new networks are fine-tuned without modifying the original parameters. The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation (iCARL) in low-shot settings and with a large number of training examples of the novel classes available. The proposed method performs better in low-shot settings and comparably when large number of training examples of the novel classes are available.\n\nThe paper is generally well-written and the idea is simple and novel. However, the"
    },
    "r111KtCp-": {
        "generation": "\n\nMeta-review:\n\nThe paper proposes a simple task of learning the manifold of all the images of disks and studies the generalization of Autoencoders to this task. The study is somewhat limited and the results shown depend heavily on the choice of the activation. The regularization proposed is quite simple and already known, and other regularization have been proposed. The choice of regularization at the end seems quite arbitrary and it's not clear if this choice would work for other tasks. The lack of generalization of Autoencoders is a well known problem and the proposed regularization is not a solution for this issue. The paper does not provide any new insights or proposes any new solutions to the problem. Therefore, the paper does not contribute significantly to the field."
    },
    "uFk038O5wZ": {
        "generation": "\n\nTitle: Knowledge Graph Enhanced Dual-Copy Network for Abstractive Dialogue Summarization\n\nAuthors: [Author's Name]\n\nSummary: This paper proposes a novel framework, Knowledge Graph Enhanced Dual-Copy network (KGEDC), for abstractive dialogue summarization. The proposed method incorporates conversational structure and factual knowledge into the graph network to handle long-distance cross-sentence dependencies and faithfulness, respectively. The proposed method achieves significant improvement over several baselines on two datasets, SAMSum corpus and Automobile Master corpus, with an average Rouge-L improvement of 1.8 and 2.5, respectively. However, some reviewers have raised concerns about the ablation study and the lack of attention to the sequential encoder in the graph encoder.\n\nStrengths:\n\n1. The proposed method shows significant improvement over baselines on two datasets.\n2. The incorporation of conversational structure and factual knowledge into the graph network is an interesting and important area of research.\n3. The authors provide detailed descriptions of their methods and implementation.\n\nWeaknesses:\n\n1. The ablation study is limited, and it is difficult to determine the contribution of each component to the proposed method.\n2. The sequential encoder in the graph encoder is not fully utilized, and it is unclear why it is necessary to combine both encoders.\n3. The paper lacks attention to the sequential encoder in the graph encoder, which could potentially improve the performance of the proposed method.\n\nRecommendations:\n\n1. Conduct a more comprehensive ablation study to determine the contribution of each component to the proposed method.\n2. Explore the potential of using only the graph encoder or the sequential encoder to improve the performance of the proposed method.\n3. Provide more detailed explanations of the attention mechanism in the dual-copy decoder and its impact on the performance of the proposed method.\n\nOverall, the proposed method shows promising results in abstractive dialogue summarization, and the incorporation of conversational structure and factual knowledge into the graph network is an interesting and important area of research. However, further investigation is needed to fully understand the strengths and limitations of the proposed method and to identify potential avenues for improvement."
    },
    "24-DxeAe2af": {
        "generation": "\n\nMeta-review:\n\nThe authors propose CNV-Net, a deep learning-based approach for detecting copy number variations (CNVs) in DNA sequencing data. CNV-Net encodes pileup images into a CNN that classifies CNV types (deletions, duplications, or non-breakpoints). The authors benchmark CNV-Net with two whole-genome sequencing datasets and claim more accurate and efficient results than existing tools. However, the manuscript has several issues that need to be addressed before publication.\n\nFirstly, the authors should provide more details on how they chose the specific numbers to encode individual base into R channel (e.g., A with 250, G with 180). Secondly, the authors only used CNVs passing the quality control filter in their experiments, but they did not provide enough information on how they filtered the variants. Thirdly, the authors should address the issue of data leakage when splitting the training/validation/testing dataset. Fourthly, the comparison between CNV-Net and other methods is not fair as the other methods are finding CNVs in the whole genome while CNV-Net is given a pre-defined set of \"candidate breakpoint regions.\"\n\nAdditionally, the authors should consider other datasets, such as the TCGA dataset, where bam files and copy number variations are available. They should also analyze whether their method can recover known canonical copy number variations. Furthermore, the authors should use the discovered breakpoints for each respective baseline and determine whether their algorithm can improve the predictions.\n\nIn conclusion, while the manuscript has some innovative ideas, it needs to address these issues before being published. The authors should provide more details on their methodology and perform additional experiments to validate their approach."
    },
    "HyezmlBKwr": {
        "generation": "\n\nThe manuscript proposes a novel method for adapting model parameters by doing self-supervised training on each individual test example. The authors show striking improvements in out-of-domain performance across a variety of image classification tasks while preserving in-domain performance. However, there are some concerns and limitations mentioned in the reviews, such as the sensitivity of the method to hyperparameters, the comparison with methods that assume access to the test distribution, and the potential difficulty in applying the method to tasks with fine-tuned labels. Additionally, some reviewers found the manuscript difficult to follow and not very coherent in its organization.\n\nOverall, the manuscript presents an interesting and promising approach to domain adaptation, but more work is needed to fully address the limitations and concerns mentioned in the reviews. Specifically, the authors should provide more detailed explanations of the method and its assumptions, as well as more thorough evaluations of its performance on a wider range of tasks and datasets. Additionally, the authors should consider addressing the potential limitations and challenges of applying the method to tasks with fine-tuned labels.\n\nIn conclusion, while the manuscript presents an interesting approach to domain adaptation, more work is needed to fully address the limitations and concerns mentioned in the reviews and to demonstrate the practicality and effectiveness of the method on a wider range of tasks and datasets."
    },
    "qwjrO7Rewqy": {
        "generation": "\n\nTitle: EPD Estimation with GNN: Efficient Learning Framework for Topological Features\n\nAuthors: [Author names]\n\nAbstract: In this paper, the authors propose an efficient learning framework to estimate extended persistence diagrams (EPD) using a graph neural network (GNN) architecture. The proposed method decomposes the EPD computation into edge-wise prediction tasks, which can be solved independently using GNN. The authors evaluate the effectiveness of their method on various datasets and demonstrate its efficiency on large and dense graphs. They also analyze the approximation error of the proposed method and compare it with other state-of-the-art methods.\n\nReviewers' comments:\n\n* Strengths: The authors analyze the calculation algorithm of EPD and describe the key Union-Find algorithm in a sequential manner, and approximate it with GNN to simplify the calculation. They evaluate the effect of EPD and the speed-up effect of this algorithm, as well as the accuracy of the speed-up.\n* Weaknesses: The authors assume that EPD is more effective than other graph-oriented TDA methods, but provide limited evidence for this claim. They also omit discussing other acceleration methods for EPD.\n* Originality: The approach proposed in this paper is quite fresh and promising, and may be a source of inspiration for future works in TDA.\n* Clarity: The paper is well written and easy to follow.\n* Significance: The proposed method has significant implications for developing new machine learning methods that can handle large and complex data.\n* Quality: The paper is of great quality overall, with an interesting approach, good experimental evaluation methodology, and well-written prose.\n\nSuggestions:\n\n* The authors could provide more references to other acceleration methods for EPD, and discuss their limitations.\n* They could also provide more evidence for the superiority of EPD over other graph-oriented TDA methods.\n* The authors could elaborate on the \"edge decomposition\" properties of their approach and how they benefit from using the PIE as a second evaluation metric.\n* They could also address potential negative societal impacts of their work.\n\nIn conclusion, the proposed method has the potential to significantly improve the efficiency of EPD computation, making it more practical for large and complex data. However, the authors should provide more evidence for the superiority of"
    },
    "cZAi1yWpiXQ": {
        "generation": "\nThe manuscript provides a novel perspective on the adversarial robustness problem. The proposed method is based on a causal graph that models the generation process of adversarial attacks and identifies the spurious correlation between style variable and class label as the main reason for adversarial vulnerability. The method aligns the adversarial distribution and the natural distribution to prevent a model from learning spurious correlation. The manuscript provides empirical validation of the proposed method using several attacks on prevailing datasets.\n\nStrengths:\n\n* The proposed method provides a new perspective on the adversarial robustness problem.\n* The causal graph well models the generation process of adversarial attacks.\n* The proposed approach is novel and well-motivated.\n* The manuscript provides empirical validation of the proposed method using several attacks on prevailing datasets.\n\nWeaknesses:\n\n* Some technical details may need to be clarified.\n* The current version only compares with SOTAs using attacks with 20 iterations, it would be helpful if attacks with more iterations are employed.\n* The objective in minimizing the divergence between $P(Y|X)$ and $P_{\\theta}(Y|\\widetilde{X})$ may only optimize the anchor instead of pushing $P_{\\theta}(Y|\\widetilde{X})$ towards $P(Y|X)$.\n* The connection between $g$ and $h$ is not clearly addressed.\n* The integral representation of $s(X)$ may not be well-defined without supervision signals.\n\nOverall, the manuscript provides a promising approach to addressing the adversarial robustness problem. However, some technical details need to be clarified and further investigation is needed to fully understand the effectiveness of the proposed method."
    },
    "B1grSREtDH": {
        "generation": "\n\nThe manuscript presents a novel approach to Bayesian reinforcement learning, called Bayesian Residual Policy Optimization (BRPO), which aims to improve the exploration-exploitation trade-off by learning an ensemble of expert policies and a residual policy. The approach is evaluated on two problems, cheese finding and door finding, and compared with several baselines. The main contributions of the paper are:\n\n* The proposed BRPO algorithm learns an ensemble of expert policies and a residual policy to improve the exploration-exploitation trade-off.\n* The algorithm is tested on two problems, cheese finding and door finding, and shows better performance than several baselines.\n* The paper provides a theoretical analysis of the algorithm's convergence properties.\n\nHowever, there are some limitations and open questions related to the manuscript:\n\n* The theoretical analysis of the algorithm's convergence properties is limited to a simplified setting, and it is unclear how well the algorithm will perform in more complex settings.\n* The paper does not provide a comprehensive evaluation of the algorithm's performance on a wide range of tasks, which limits its generalizability.\n* The algorithm's reliance on expert policies raises questions about its ability to generalize to new tasks and environments.\n* The paper does not provide a clear explanation of how the reward function for the cheese finding task was determined, which makes it difficult to understand the results.\n* The authors do not provide a detailed analysis of the role of the residual policy in the algorithm, which limits the understanding of the algorithm's performance.\n\nOverall, the manuscript presents a novel approach to Bayesian reinforcement learning, but there are still several open questions and limitations that need to be addressed before the algorithm can be widely adopted. Further research is needed to evaluate the algorithm's performance on a wide range of tasks and to better understand its convergence properties and generalization abilities."
    },
    "rkQuFUmUOg3": {
        "generation": "\n\nMeta-Review:\nThis manuscript proposes a novel framework for neural architecture search (NAS) that leverages meta-learning to rapidly adapt to new datasets. The proposed framework, MetaD2A, consists of a set encoder, a graph decoder, and a meta-performance predictor. The authors evaluate the effectiveness of their approach through experiments on several datasets and show improvements over conventional NAS methods.\n\nThe reviews provided by the reviewers are generally positive, with one reviewer highlighting the solid results and well-structured paper, while another reviewer notes that the approach could be further improved by comparing it to other methods in the field. One reviewer also raises some concerns about the size of the datasets used in the experiments.\n\nOverall, the manuscript provides a valuable contribution to the field of NAS by proposing a new framework that leverages meta-learning to adapt to new datasets. While there are some areas for improvement, the manuscript is well-written and provides a solid foundation for future research.\n\nRecommendations for the authors:\n\n1. Address the reviewer's concern about the size of the datasets used in the experiments by providing additional details or alternative approaches that can handle larger datasets.\n2. Compare the proposed framework with other methods in the field to provide a more comprehensive evaluation of its effectiveness.\n3. Provide more implementation details in the main text to facilitate reproducibility and understanding of the proposed framework.\n4. Consider exploring other applications of the proposed framework beyond NAS, such as few-shot learning or transfer learning.\n5. Provide more theoretical analysis of the proposed framework to better understand its convergence properties and generalization abilities."
    },
    "sEIl_stzQyB": {
        "generation": "no definition of $V(s)$ in the paper. What is the definition of $V(s)$?\n    \n    5. The reviewer think that the result of experiment is not clear, because the author only show the result of their method, but no baseline methods are provided, and the reviewer can't compare the result of their method with other methods. Therefore, the reviewer suggest the author provide more detailed explanation and comparison with other methods in the paper.\n    \n    6. The reviewer think that the author miss some important detail, such as the exploration and the replay buffer, and the prioritization of the replay buffer. The reviewer don't understand how the author chose the buffer, and how to prioritize the buffer."
    },
    "RmcPm9m3tnk": {
        "generation": "more about poses than part decomposition. The learned scene graphs could have more structure and be more hierarchical if the objects were more complex.\n\n**Recommendations**:\n\n1. I would suggest the authors to explore more complex datasets to validate the generalizability of the proposed method.\n2. The authors should provide more discussion on how to use the learned scene graph for downstream tasks like image manipulation, and how to incorporate more hierarchy and structure into the learned scene graph.\n3. I think the authors should provide more results with/without supervision to validate the ability to learn hierarchical scene graph without supervision.\n4. The authors should provide more discussion on how the learned scene graph can be used for other tasks, and what are the potential applications of the proposed method."
    },
    "WXwg_9eRQ0T": {
        "generation": "\n\nMeta-review:\n\nThis paper presents MergeBERT, a deep model for resolving program merge conflicts in software development. The authors introduce a new, hierarchical differencing and cast the problem as classifying over a fixed set of merge patterns, instead of generating. The model is pre-trained on a large corpus of GitHub code. Pros:\n\n+ Hierarchical two-level differencing appears to be an intuitive and good idea.\n+ Casting the problem as classification over a set of merge patterns appears advantageous in both learning and computation, since most of the time the resolution comes from either of the two change versions.\n+ Pre-training can be applied directly.\n\nCons:\n\n- This idea is somewhat incremental, largely based on the work of Dinella et al., 2020.\n- MergeBERT is not as good as *diff3* in terms of precision, which is to me a more important measure. \n- Some parts of the techniques and experiments are not clear, for example the class label and data split. I have more detailed questions below.\n\nQuestions:\n\n1. It is somewhat unintuitive that `let` is not part of the token-level conflict. I understand that `var` is not changed in the version B, but it is important from both A and B perspectives that there is a change that needs an agreement in resolution. In that case, I don\u2019t think merging `let` right away is an indisputable decision.\n2. For this change from `--num_cores=2` to `--max_length=256 --num_cores=2`, what is the result of the two-level diff?\n3. Is treating conflicting regions independently in Eq. (2) oversimplified?\n4. How do you split the train and test dataset (Table 4)? Is there a validation dataset?\n5. How do you deal with conflicts from more than 2 versions?\n\nOverall, the paper presents several new technical contributions and insights, but some parts are not entirely convincing. This paper addresses merge-conflict resolution in source code repositories. Whereas prior neural work (DeepMerge) can only do line-level resolutions (i.e., predicted a resolution that chooses some sequence of entire lines from the original file version or"
    },
    "NgwrhCBPTVk": {
        "generation": "\n\nManuscript Title: Online Bipartite Matching with Predicted Degrees\n\nThe manuscript considers the online bipartite matching problem with predicted degrees of offline vertices. The author proposes a simple algorithm MPD which matches the vertices with low degrees first to exploit the predicted degree. The algorithm has a sub-optimal 1/2 competitive ratio in the worst case, but it performs well on some real-world data sets and random graphs. The author proves that the algorithm is optimal on CLV-B random graphs when the prediction is perfect. The manuscript also estimates the lower bound of the competitive ratio of MPD on symmetry CLV-B random graphs through solutions of some derivative equations.\n\nStrengths:\n\n* The problem is well-motivated and relevant to practical applications.\n* The experimental results are well done and detailed.\n* The theoretical results are optimal on CLV-B random graphs when the prediction is perfect.\n\nWeaknesses:\n\n* The title, abstract, and introduction are somehow misleading, and could be improved to make it clear that this paper studies classic combinatorial optimization problems without involving any machine learning or data-driven aspect.\n* The significance of the technical contribution is not very clear.\n* The paper could be better organized such that all the theoretical results are clearly stated in the main paper.\n* Some minors, such as the definition of p(A, S), are missing.\n* The paper does not discuss the limitations.\n\nOverall, the manuscript is well-written and easy to follow. However, some improvements are necessary to make it clear that the paper studies classic combinatorial optimization problems and to address the limitations."
    },
    "H1gax6VtDB": {
        "generation": "\n\nThe manuscript presents a new method for learning a structured latent space for images, which can represent objects and their relations. The method learns a contrastive loss function to encode images into a latent space, and uses a graph neural network to model the transitions between latent states. The authors evaluate their method on several tasks, including image generation, object manipulation, and action-conditioned image generation.\n\nThe reviews are generally positive, with the reviewers praising the manuscript's clear and well-motivated method, and the thorough evaluation of the method on several tasks. However, there are some concerns and suggestions for improvement raised by the reviewers, including the following:\n\n1. The use of a fixed number of object slots (K) is a fundamentally limiting hyperparameter, and the authors should consider how the model performs under misspecification of this hyperparameter.\n2. The data collection procedure for the Atari games seems to indicate that the model is quite sensitive to domains where actions rarely have an impact on the transition dynamics, or the interaction is more complex (e.g. other agents exist in the world).\n3. The object extractor only outputs a scalar mask, which may not provide enough information for the graph neural network to model the transitions between latent states.\n4. The authors should provide more details on how the actual test data used to generate Table 1 was created, and what \u201cunseen environment instances\u201d would correspond to.\n5. The authors should consider exploring the extension of their method to other environments, such as continuous reinforcement learning tasks.\n\nOverall, the manuscript presents a promising approach to learning a structured latent space for images, and the authors have provided a thorough evaluation of their method on several tasks. However, there are some concerns and suggestions for improvement that should be addressed in future revisions."
    },
    "S0UdquAnr9k": {
        "generation": "\n\nMeta-Review:\n\nThe authors propose a new weight sharing pattern for network width search, called locally free weight sharing, which allows for more flexibility in channel assignment compared to traditional fixed weight sharing. The proposed method is evaluated on several datasets and achieves promising results. However, some concerns and suggestions have been raised by the reviewers, including the need for more explanation regarding the relation between the proposed method and traditional fixed weight sharing, the scalability of the search space, and the influence of the super network.\n\nStrengths:\n\n* The problem studied is well-motivated and important.\n* Experiments are sufficient and support the claims made in the paper.\n* The proposed method shows promising results.\n* The FLOPs-sensitivity bin is a useful tool for reducing the search space.\n\nWeaknesses:\n\n* The proposed method is a compromise between completely free weight and fixed weight, and the relation between the two could be elaborated.\n* The search space may scale from O(N) to O(C_{2r+1}^{N}), which could be a significant amount of time.\n* The influence of the super network should be detailed.\n* The writing could be improved.\n\nOverall, the paper presents an interesting and useful contribution to the field of network width search. However, some additional explanation and analysis are needed to fully address the concerns and suggestions raised by the reviewers."
    },
    "iaqgio-pOv": {
        "generation": "the presented quantitative evaluation using a user study is unconvincing. The paper does not provide any result on how users perform when asked to explain the prediction of a non-similarity model, nor any result on how users perform when asked to explain the similarity of a pair of input that is not in the dataset. This is a crucial question, as users should be able to explain the similarity of a pair of inputs that are not in the dataset. Moreover, the fidelity metric is not well motivated, and the definition of the distance is not clear. \n\nIn conclusion, I think that the proposed explanation methods are not convincing, and the proposed evaluation method has serious issues. However, I believe that the paper has potential in terms of understanding the novelty of the proposed method can lead to more meaningful and the explanation methods and the similarity learners. I think that the proposed method has potential and the explanation and the authors addressing the users have to predict the model prediction model and similarity and the paper. "
    },
    "B1eWOJHKvB": {
        "generation": "\n\nMeta-Review:\nThis manuscript presents an interesting study on the theoretical properties of CycleGAN, a popular method for unpaired image-to-image translation. The authors investigate when the exact solution space of CycleGAN is invariant under automorphisms of the underlying probability spaces, and provide theoretical guarantees on the existence of a unique solution. They also conduct experiments to validate their findings and demonstrate the limitations of CycleGAN in practice.\n\nThe reviews are generally positive, with reviewers praising the novelty and significance of the work. However, some reviewers have raised concerns about the scope of the work and the limited experimental results provided. One reviewer suggests expanding the experiments to provide more insights into the performance of CycleGAN, while another reviewer notes that the results in the paper do not justify the motivations discussed in the introduction and abstract.\n\nOverall, the manuscript presents interesting and timely research on the theoretical properties of CycleGAN, and provides valuable insights into the behavior of this popular method. While there are some limitations to the work, the findings are generally positive and provide a good starting point for future research in this area."
    },
    "r1xMnCNYvB": {
        "generation": "\n\nMeta-Review:\n\nThis paper presents a new software package, JAX MD, for simulating molecular dynamics with automatic differentiation and integration with a neural network/machine learning library. The package aims to simplify the research process and enable new avenues of work in the field. The paper provides a clear description of the package's design and functionality, and demonstrates its use through several examples. However, the contribution of this work to the field of machine learning and molecular dynamics is not entirely clear, and some limitations of the package are not explicitly mentioned.\n\nThe paper is well-written and easy to follow, but some aspects of the package's design and functionality could be better explained. For example, the authors could provide more detail on how JAX MD differs from other AD libraries, and why certain design decisions were made. Additionally, the paper could benefit from a more detailed performance comparison against other MD libraries.\n\nOverall, the paper provides a useful contribution to the field of molecular dynamics and machine learning, but could benefit from further clarification and detail in some areas."
    },
    "uFORMPcA_b": {
        "generation": "\n\nTitle: Lale: A Modular, Gradual AutoML System\n\nAuthors: [Author's Name]\n\nManuscript: The authors propose Lale, a new AutoML system that aims to make AutoML more accessible to non-experts by providing a modular, gradual approach to building and optimizing machine learning pipelines. Lale introduces three combinators (pipe, and, or) to combine functions without naming datasets, and a unified translation schema to translate pipelines into search spaces for various optimizers. The authors also implement Lale as a Python library and conduct user studies to evaluate its usability.\n\nReviews:\n\n* The paper provides a clear and well-structured explanation of the proposed Lale system, including its design philosophy and technical implementation.\n* The authors identify three combinators (pipe, and, or) that are sufficient to express most AutoML pipelines and demonstrate the modularity of Lale by combining these combinators in various ways.\n* The unified translation schema is a novel contribution that allows for easy translation of pipelines into search spaces for different optimizers.\n* The user studies provide some evidence that Lale is easy to use and can be beneficial for non-experts in AutoML.\n* Some reviewers have raised concerns about the scientific novelty and impact of Lale, as well as the limited number of participants in the user studies.\n\nDiscussion:\n\n* The paper's contributions are primarily in the area of modularity and gradualness, which can make AutoML more accessible to non-experts.\n* The unified translation schema is a significant novelty in the AutoML domain, as it allows for easy translation of pipelines into search spaces for different optimizers.\n* The user studies provide some evidence that Lale is easy to use and can be beneficial for non-experts in AutoML, but more research is needed to fully assess the impact of Lale.\n* The limited number of participants in the user studies is a limitation of the paper, and more participants may be needed to provide more statistically significant results.\n* The paper could benefit from a more detailed discussion of the scientific contributions and impact of Lale, as well as a more comprehensive evaluation of the system's performance.\n\nConclusion:\n\n* Lale is a well-proposed and well-im"
    },
    "BJlowyHYPr": {
        "generation": "\n\n============================\n\nMeta-Review:\n\nThe paper \"Dynamic Post-Cloud Convolution for Point-Cloud Forecasting\" presents a novel approach for processing and analyzing point-cloud data, specifically for point-cloud forecasting tasks. The proposed method, named D-Conv, is a convolutional operator that can be embedded in various neural network architectures, such as RNN, GRN, and LSTM. The D-Conv operator utilizes the spatial information of point-cloud data and leverages the K-nearest neighbors (KNN) to capture the temporal information.\n\nThe paper has several strengths, including the invariance to point-cloud order and the simplicity of the architecture compared to other state-of-the-art methods. However, there are also some concerns and open questions that need to be addressed, such as the vulnerability to outliers, the lack of baselines for comparison, and the time complexity of the proposed method.\n\nIn terms of novelty and significance, the paper definitely tackles an important problem in the field of point-cloud forecasting. However, the proposed method is not entirely new, as it is similar to the PointCNN operator in some aspects. The paper does not provide a clear justification for why the D-Conv operator is better than other existing methods.\n\nTechnically, the paper could benefit from providing more detailed explanations of the notation and the mathematical derivations. The authors should also address the concerns raised by the reviewers regarding the reliance on nearest neighbors and the lack of accommodation for outliers.\n\nExperimentally, the paper could benefit from comparing the proposed method on publicly available datasets and providing more baselines for comparison. The paper also needs to provide a clearer explanation of how the results were obtained and what they mean in terms of performance.\n\nIn conclusion, the paper presents a novel approach for point-cloud forecasting, but there are still several open questions and concerns that need to be addressed. The paper could benefit from providing more detailed explanations and mathematical derivations, as well as more comprehensive experiments and baselines for comparison."
    },
    "HkldyTNYwH": {
        "generation": "\n\nThis is a manuscript that proposes a new approach to addressing the mode collapse problem in Generative Adversarial Networks (GANs) by leveraging optimal transport (OT) theory. The authors separate the manifold embedding and the OT problems and use an autoencoder for the former and a GPU-based convex optimization for the latter. The proposed method is evaluated through numerical experiments on several benchmark datasets, and the results show promising improvements over state-of-the-art methods.\n\nHowever, there are some concerns and suggestions raised by the reviewers that need to be addressed before the manuscript can be accepted for publication. Some of these concerns include:\n\n1. Overkill: One reviewer suggests that the proposed method may be overkill, as it involves detecting singular points in the OT map, which can be a tricky task. Instead, the reviewer suggests using a simpler approach, such as fitting a Gaussian mixture model to the data and selecting the number of clusters through model selection.\n2. Lack of formal results: Another reviewer notes that the manuscript does not establish formal new results, but rather verifies numerically that Figalli's theorem holds in this context. The reviewer suggests that the paper should be more moderate in its claims and provide more intuitive explanations.\n3. Confusion with gradient descent: A reviewer points out that the manuscript mentions Newton's method and Thm 3, but then uses gradient descent for optimization. The reviewer finds this confusion confusing and suggests clarifying the use of different optimization methods.\n4. Non-differentiability of the gradient: A reviewer raises a concern about the non-differentiability of the gradient in the OT step and wonders if using a simpler approach, such as Moreau envelopes or proximal methods, could resolve this issue.\n5. Typos and minor comments: Finally, the reviewers suggest proofreading the draft more carefully to fix typos and minor comments, such as defining OT in the abstract and clarifying the definition of AE.\n\nIn conclusion, while the proposed method shows promising improvements over state-of-the-art methods, there are some concerns and suggestions raised by the reviewers that need to be addressed before the manuscript can be accepted for publication. These concerns include overkill, lack of formal results, confusion with gradient descent, non-differentiability of the gradient, and typos"
    },
    "BJzuKiC9KX": {
        "generation": "\n\nMeta-review:\nThis manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. The authors present an extensive set of experiments that suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. However, the rigorous analysis and explanation for the observations are lacking, which limits the significance of the findings.\n\nThe authors introduce the problem of IWAE and its limitations, particularly the trade-off between learning the generative model and the inference network. They propose RWS as a solution to this problem and demonstrate its effectiveness through experiments on several benchmark datasets. The experimental results show that RWS learns better models and inference networks with increasing numbers of particles, and its benefits extend to continuous latent variable models as well.\n\nHowever, the lack of rigorous analysis and explanation for the observations limits the significance of the findings. The authors should provide a more formal justification for the propositions and accompany them with a minimal rigorous justification. Additionally, the comparison with stronger baselines, such as RBM, DVAE, and DVAE++, could provide a more comprehensive understanding of the performance of RWS.\n\nIn conclusion, while the manuscript presents an interesting observation about the performance of RWS for learning deep generative models with discrete latent variables, the lack of rigorous analysis and explanation limits its significance. A more formal and rigorous analysis could help to establish the effectiveness of RWS and its advantages over other methods."
    },
    "3R--2TdxMps": {
        "generation": "\n\nThe authors propose a novel method, DEFUSE, to identify and correct classifier performance when labels are assigned incorrectly. The method consists of three phases: identifying unrestricted adversarial examples, clustering them into failure scenarios, and correcting the classifier predictions. The authors use Variational Autoencoders (VAEs) to identify adversarial examples, Dirichlet process to cluster them, and fine-tune the classifier with the corrected labels.\n\nOverall, the idea of using adversarial examples to correct incorrect classifications is interesting. However, the method proposed by the authors needs to be better justified and demonstrated through rigorous experimental evaluation. The choice of algorithms and parameters need to be carefully justified and discussed in the paper.\n\nSome of the reviews highlight the following issues:\n\n* Lack of detail in the experimental results section: The experimental results are not well-described in the paper, and the results are not provided in a clear and concise manner.\n* Insufficient justification of the choice of algorithms and parameters: The choice of VAEs and Dirichlet process needs to be better justified and discussed in the paper.\n* Lack of agreement between annotators: The annotation task needs to be better described, and the agreement between annotators needs to be established.\n\nTo address these issues, the authors should provide more detail on the experimental results, justify the choice of algorithms and parameters, and provide more information on the annotation task and the agreement between annotators. Additionally, the authors should provide more detail on the motivation behind the method and its potential applications.\n\nIn conclusion, while the proposed method has the potential to be an interesting and useful contribution to the field of machine learning, it needs to be better justified and demonstrated through rigorous experimental evaluation. The authors should provide more detail on the experimental results, the choice of algorithms and parameters, and the annotation task to support their claims."
    },
    "HJe6uANtwH": {
        "generation": "\n\nManuscript Title: A Simple and Effective Routing Algorithm for Capsule Networks\n\nMeta-Review:\nThis manuscript presents a novel routing algorithm for capsule networks, which improves upon previous versions in terms of stability and performance. The authors experiment with different routing iterations and demonstrate the effectiveness of their proposed algorithm on various datasets. However, there are some issues that need to be addressed before the manuscript can be published.\n\nFirstly, the authors need to release the code for reproducing the results in the paper to facilitate future research in this area. Secondly, the figure captions could be more informative, providing explanations for the observed trends in the plots. Thirdly, the authors should clarify the learning rate schedule used for the three models compared in Figure 4, as it appears to be different for each model.\n\nFourthly, the authors should address the issue of the sudden decreases in test accuracy observed in Figure 4 for the inverted dot product capsules. Finally, the authors should expand upon the initial value of the poses in L+1, as it may greatly affect the performance of the model.\n\nOverall, the manuscript presents a valuable contribution to the field of capsule networks, but some clarifications and improvements are needed before it can be published."
    },
    "r1lUl6NFDH": {
        "generation": "\n\nMeta-Review:\nThis paper proposes a Mirror Descent (MD) framework for the quantization of neural networks. The authors construct mirror maps from the unconstrained auxiliary variables to the quantized space and provide a stable implementation of MD by storing an additional set of auxiliary dual variables. Experiments on CIFAR-10/100 and TinyImageNet with convolutional and residual architectures show the effectiveness of the proposed method. The novelty of this work is limited, as it applies MD from convex optimization to NN quantization, but the authors provide sufficient effort to explore how to succeed in the literature. The paper is well-written, and the main contributions are clearly stated. However, some concerns and questions remain:\n\n1. Theoretical analysis: The paper does not provide a thorough theoretical analysis of the convergence of the MD with a nonconvex objective function in NN quantization. Nor does it discuss how to choose the projection for mirror mapping construction.\n2. Novelty and theoretical contributions: While the MD framework is well-known in convex optimization, the novelty of this work is limited to applying it to NN quantization. It would be better to discuss with [Bai et al, 2019] to clarify the novelty of the proposed method.\n3. Experimental comparisons: While the paper provides some experimental comparisons with reasonable baselines, it would be better to perform experiments on ImageNet and compare the proposed method with other state-of-the-art methods, such as EfficientNet, Mobilenets, and Shufflenet.\n4. Code availability: It would be beneficial to provide the code to validate the soundness of the model.\n\nIn summary, this paper proposes a Mirror Descent framework for the quantization of neural networks, which provides a stable implementation of MD by storing an additional set of auxiliary dual variables. While the paper provides some theoretical and experimental evidence to support the proposed method, some concerns and questions remain, including the lack of a thorough theoretical analysis, the limited novelty of the proposed method, and the need for more comprehensive experimental comparisons."
    },
    "0cn6LSqwjUv": {
        "generation": "\n\nIn summary, the proposed dataset SPDNet is valuable and the benchmark provided by the authors is helpful. However, there are some concerns regarding the necessity of low-resolution data, the unconvincing evaluation, the missing training details, and the formatting errors. The authors should provide more empirical studies to prove the effectiveness of their proposed approach, and clarify the distinction between their task and domain transferring tasks in the literature. Moreover, the qualitative results shown in Figure 4 should be provided with more detailed and convincing evidence to demonstrate the superiority of the models achieving better PEM and PDEM."
    },
    "ygWoT6hOc28": {
        "generation": "\nTitle: A Meta-Review of Regression Prior Networks\n\nIntroduction:\nRegression Prior Networks (RPNs) are a novel approach to uncertainty quantification in regression problems. The idea is to use a hierarchical prior distribution to model the predictive uncertainty of a neural network. In this meta-review, we discuss the strengths and weaknesses of RPNs based on the reviews provided by the authors and other experts in the field.\n\nStrengths:\n\n1. Novelty: RPNs are a relatively new approach to uncertainty quantification in regression problems, and the authors have demonstrated its effectiveness in various applications.\n2. Extension to regression: The authors have extended the Prior Networks model to regression problems, which is a significant contribution to the field.\n3. Distillation: RPNs can be used for distillation of an ensemble of predictors into a single model, which can lead to better performance and uncertainty estimates.\n4. Evaluation: The authors have provided a comprehensive evaluation of RPNs using various datasets and experimental protocols, which helps to assess the effectiveness of the approach.\n\nWeaknesses:\n\n1. Limited novelty: As mentioned in some reviews, the approach is heavily based on Prior Networks (Malinin & Gales, 2018), which may limit its novelty.\n2. OOD dataset: The use of KITTY dataset for training the OOD models raises concerns about the choice of dataset and the potential differences in performance.\n3. Lack of comparisons: The authors have not provided sufficient comparisons with other approaches for distillation of regression tasks, which can limit the understanding of the effectiveness of RPNs.\n4. Mathematical details: Some reviews have pointed out that some mathematical details are missing, which can make the paper difficult to understand.\n\nConclusion:\nRPNs are a promising approach to uncertainty quantification in regression problems. However, the limited novelty and the lack of comparisons with other approaches may limit its impact. Further research is needed to address these issues and to provide a more comprehensive understanding of the effectiveness of RPNs.\n\nRecommendations:\n\n1. Address the limited novelty by exploring new approaches to uncertainty quantification in regression problems.\n2. Provide more comprehensive evaluations of RPNs"
    },
    "9U4gLR_lRP": {
        "generation": "\n\nThe paper proposes a new method for improving the transferability of adversarial attacks. The method uses logit calibrations to increase the logit margins between targeted and non-targeted classes, which can quickly become saturated in the cross-entropy loss function. The proposed method is inspired by the work of [1] and performs experiments to show that it is more effective than other existing methods. However, the manuscript has several limitations, including:\n\n* The method is highly inspired by the work of [1], and the contribution of the current work is limited.\n* The results are presented in a poor manner, with tables containing too much information and making it difficult to understand the results.\n* The distinction between the current work and [1] is not clear, and the contribution of the current work could be discussed more clearly.\n* The experiments are limited and can be improved, including presenting the results in a more organized and clear manner.\n* The potential negative social impact of the work is not discussed, and the limitation of the method is not addressed.\n\nOverall, the manuscript is well-written and provides valuable insights into the transferability of adversarial attacks. However, it has several limitations that need to be addressed before it can be published."
    },
    "3h1iwXmYVVJ": {
        "generation": "\n\nMeta-review:\n\nThe manuscript \"Implicit Regularization in Mirror Descent for Matrix Recovery\" presents a novel analysis of the implicit regularization effect in mirror descent, a popular optimization algorithm, for solving matrix recovery problems. The authors provide a deep understanding of the interplay between the geometry of the problem and the choice of the mirror map, leading to interesting conclusions on the structure of the optimal solutions. While the results are limited to specific cases, they provide a new perspective on the implicit regularization phenomenon and its implications for matrix recovery.\n\nStrengths:\n\n* The manuscript provides a novel analysis of the implicit regularization effect in mirror descent for matrix recovery problems.\n* The authors offer a deep understanding of the interplay between the geometry of the problem and the choice of the mirror map.\n* The results provide a new perspective on the implicit regularization phenomenon and its implications for matrix recovery.\n\nWeaknesses:\n\n* The results are limited to specific cases, which may limit the practical applicability of the findings.\n* The manuscript does not provide a comprehensive comparison with other optimization algorithms for matrix recovery.\n* The authors could have discussed the limitations of the study and potential avenues for future research.\n\nClarity:\n\n* The manuscript is well-written and easy to follow.\n* The authors provide clear explanations of the results and their implications.\n* Some typos and minor errors are noticed in the manuscript.\n\nSignificance:\n\n* The results provide a new perspective on the implicit regularization phenomenon in mirror descent.\n* The study has implications for the design of optimization algorithms for matrix recovery problems.\n* The manuscript could have discussed the broader implications of the results and their potential applications in other areas.\n\nIn conclusion, the manuscript \"Implicit Regularization in Mirror Descent for Matrix Recovery\" presents a novel analysis of the implicit regularization effect in mirror descent for matrix recovery problems. While the results are limited to specific cases, they provide a new perspective on the interplay between the geometry of the problem and the choice of the mirror map. The manuscript could have discussed the limitations of the study and potential avenues for future research. Nevertheless, the study demonstrates the importance of considering the implicit regularization effect in optimization algorithms for matrix recovery problems."
    },
    "mOO-LfEVZK": {
        "generation": "\n\nTitle: Manifold-Aware Training for Adversarial Robustness\n\nAuthors: [Author names]\n\nSummary: This paper proposes a novel training process called Manifold-Aware Training (MAT) to increase the robustness of a CNN against adversarial examples. MAT combines prior work on learning features that are compact for samples from the same but dispersed for samples from different classes (MMD by Pang et al.) with a method to find better class centers, a gradient-norm regularization, and an adversarial training regularization. The paper reports favorable results on two datasets, CIFAR10 and MNIST, showing improvements in robustness compared to state-of-the-art defenses.\n\nDiscussion:\n\nThe paper presents an interesting approach to improving the robustness of a CNN against adversarial attacks by leveraging the manifold structure of the feature space. The proposed MAT method combines several techniques, including the use of better class centers, gradient-norm regularization, and adversarial training regularization. The paper reports promising results on two datasets, CIFAR10 and MNIST, showing improvements in robustness compared to state-of-the-art defenses.\n\nHowever, there are some concerns and questions raised by the reviewers that need to be addressed:\n\n1. Theoretical development: While the paper provides some theoretical insights into the properties of the learned representation, a more detailed theoretical analysis of the proposed method is needed to fully understand its robustness guarantees.\n2. Comparison with other approaches: The paper does not provide a comprehensive comparison with other state-of-the-art defenses, which limits the understanding of the proposed method's effectiveness.\n3. Lack of practical insights: The paper does not provide any practical insights into the challenges of implementing the proposed method in real-world scenarios, which is an important aspect of evaluating the practicality of a defense mechanism.\n4. Limited generalization: The paper only evaluates the proposed method on two datasets, which raises questions about its generalization ability to other datasets and scenarios.\n5. Robustness against adaptive attacks: The paper does not provide any analysis of the proposed method's robustness against adaptive attacks, which is an important aspect of evaluating the effectiveness of a defense mechanism.\n\nConclusion:\n"
    },
    "bmGLlsX_iJl": {
        "generation": "\n\nTitle: EMFlow: Imputing Missing Data in High-Dimensional Spaces with Normalizing Flows and Online EM\n\nAbstract:\nIn this paper, we propose EMFlow, a novel method for imputing missing data in high-dimensional spaces using normalizing flows and online EM. EMFlow leverages the ability of normalizing flows to capture the complete data distribution and the efficiency of online EM for maximizing the log-likelihood of the observed data. The proposed method is evaluated on ten UCI datasets, MNIST, and CIFAR-10, showing impressive improvement against baseline models and faster convergence than MCFlow. EMFlow offers a promising solution for missing data imputation in high-dimensional spaces, where traditional methods often struggle.\n\nMain Strengths:\n\n* Novel use of normalizing flows and online EM for missing data imputation\n* Impressive performance against baseline models on various datasets\n* Fast convergence compared to MCFlow\n\nMain Weaknesses and Areas for Improvement:\n\n* Limited novelty compared to MCFlow, which has been widely studied in various domains and applications\n* Assumptions of normalizing flows need to be justified, especially regarding inter-feature dependencies in the latent space and the relationship between the latent space and observation space\n* More thorough comparison with other related works is needed to evaluate the proposed method's performance\n\nDiscussion and Future Work:\n\n* The study focuses on MCAR and MAR cases, but the authors should consider other types of missing data, such as MNAR, to broaden the applicability of the proposed method\n* The authors should evaluate the method on more diverse datasets to demonstrate its generalizability\n* The limitations of EM methods compared to FCS multiple imputation should be acknowledged and addressed\n\nIn conclusion, EMFlow offers a promising solution for missing data imputation in high-dimensional spaces. However, further research is needed to address the limitations and improve the method's performance and generalizability."
    },
    "NbaEmFm2mUW": {
        "generation": "\n\nOverall, this paper presents a novel approach to hierarchical skill learning, including the use of a benchmarking suite of sparse reward tasks to study the trade-off between generality and learning speed. The proposed algorithm outperforms reasonable baselines, and the results are generally encouraging. However, I have some minor comments and some questions for the authors."
    },
    "YYHXJOawkPb": {
        "generation": "\n\nThe manuscript entitled \"The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning\" presents an empirical investigation of effective robustness (ER) during fine-tuning of pre-trained models. The authors find that ER increases throughout fine-tuning but vanishes at convergence, and propose several strategies to mitigate this issue. However, the reviews suggest that the findings of the paper may not be significant or novel, and that the paper lacks depth and justifications for its claims.\n\nThe reviews also point out several weaknesses in the paper, including the lack of proper bounds for ER values, the unclear justification of why high ER is desirable, and the lack of analysis of the observed trends. Additionally, the paper is found to rely heavily on previous work without providing enough novelty or detailed analysis of the proposed concepts.\n\nOverall, the paper has some interesting observations but lacks depth and justifications for its claims. The paper also lacks novelty or detailed analysis of the proposed concepts. As a result, I would give it a score of 4 out of 5."
    },
    "HkeuD34KPH": {
        "generation": "\n\nMeta-Review:\n\nThe manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). The authors claim that SSE-PT outperforms several baseline methods on five datasets. However, the technical novelty of this work is limited, as it builds upon previous works of Transformer and SSE. The experiment results are not convincing, as the authors copy most of the results from previous works and the comparison with other baseline methods is not comprehensive. The manuscript also lacks ablation study and discussion on the effectiveness of personalized transformer.\n\nThe authors also extend SSE-PT to SSE-PT++ to handle longer sequences, but the justification for this extension is not clear. The paper also discusses the effect of different types of regularization on the performance of SSE-PT, but the results are not convincing.\n\nThe manuscript is well-presented, but the technical contribution is scattered, and the incremental nature of this work is not clearly explained. The paper also lacks a comprehensive comparison with other recent deep learning-based methods for sequential recommendation.\n\nIn summary, while the manuscript presents some interesting ideas, it lacks technical novelty, convincing experiment results, and comprehensive comparison with other state-of-the-art methods. Therefore, I would recommend a weak acceptance for this manuscript."
    },
    "XL9DWRG7mJn": {
        "generation": "\n\nThe paper provides a theoretical analysis of the hard-threshold sparsifier in distributed optimization problems. The authors show that the hard-threshold sparsifier is optimal for a certain communication complexity model and provide upper bounds on the optimization error for this model. They also compare the convergence rates of the top-$k$ sparsifier and the hard-threshold sparsifier for several applications.\n\nThe paper has several strengths, including:\n\n* Providing upper bounds on the optimization error for a communication complexity model that has not been well-studied before.\n* Showing that the hard-threshold sparsifier is optimal for this communication complexity model.\n* Comparing the convergence rates of the top-$k$ sparsifier and the hard-threshold sparsifier for several applications.\n\nHowever, there are also some weaknesses and areas for improvement, including:\n\n* The paper's claims on optimality are oversold and could be clarified.\n* The paper does not provide a clear explanation of the assumptions and conditions under which the results hold.\n* The paper could provide more discussion on the limitations of the proposed communication complexity model and how it can be applied in practice.\n* The paper could provide more comparisons between the top-$k$ sparsifier and the hard-threshold sparsifier for different applications.\n\nOverall, the paper provides a theoretical analysis of the hard-threshold sparsifier in distributed optimization problems and demonstrates its optimality for a certain communication complexity model. While there are some weaknesses and areas for improvement, the paper provides valuable insights into the convergence rates of different sparsifiers in distributed optimization problems."
    },
    "r1exVhActQ": {
        "generation": "\n\nThe manuscript \"Layerwise L1-Regularized Network Trimming: A Theoretical Analysis\" presents a theoretical analysis of the sparsity property of the stationary point of layerwise L1-regularized neural network trimming. The authors claim that their approach can deliver good performance by reaching a stationary point of the optimization. However, the reviews and discussions suggest several issues and concerns that need to be addressed.\n\nFirstly, the reviews point out that the main contributions of the paper are not well-justified and lack novelty. The authors need to provide a clear explanation of how their approach differs from previous works and demonstrate a clear improvement over these existing pieces of work.\n\nSecondly, the reviews raise concerns about the experimental design and implementation. The authors need to provide more experimental results to demonstrate the effectiveness of their approach and rigorously justify the tightness of the bound provided by Theorem 1.\n\nThirdly, the reviews suggest that the theoretical analysis of the stationary point of layerwise L1-regularized network trimming is not well-justified. The authors need to provide more details about the optimization process and the properties of the stationary point to support their claims.\n\nFourthly, the reviews highlight some minor notation issues and typos in the manuscript. The authors need to carefully proofread the manuscript to ensure that there are no errors or inconsistencies.\n\nIn conclusion, while the manuscript presents an interesting topic, it needs to address the concerns and issues raised by the reviews and discussions. The authors should provide a more detailed explanation of their approach and its novelty, rigorously justify the experimental results, and carefully proofread the manuscript to ensure its accuracy and quality."
    },
    "a0yodLze7gs": {
        "generation": "little bit confusing in some parts. Therefore, I think this paper needs more revisions before it can be published."
    },
    "DGIXvEAJVd": {
        "generation": "\n\nMeta-Review:\nThis paper explores the ability of transformer-based language models to learn chess games through raw notation alone, with the goal of investigating whether such models can ground their language output in the state of the underlying game. The authors present a clever probing method based on next-word prediction, which allows them to evaluate the model's state tracking ability. They show that transformer-based language models can learn to predict legal moves with high accuracy, even without visual representations of the board. The results are intriguing and expand our understanding of what may be possible with transformer architectures. However, the contribution of this work is relatively narrow, as it only explores the ability of transformer-based language models to track the state of a chess game. The authors also raise several interesting questions for future work, such as exploring other games or natural language settings.\n\nThe paper is well-written, well-organized, and easy to follow. However, the experimental setup is not well-defined, which raises questions about the model's ability to predict the state of a chess game. The authors present a clever probing method based on next-word prediction, which allows them to evaluate the model's state tracking ability. They show that transformer-based language models can learn to predict legal moves with high accuracy, even without visual representations of the board.\n\nThe strengths of the paper include this is an interesting application of transformers to a non-traditional textual inference task. It may inspire others to devise other interesting applications. The results are intriguing and expand our understanding of what may be a creative use of transformer architectures to tackle a creative use of chess. However, which may be a non-use of chess. The authors present a non-use of chess.\n\n\n\n\n\n\n\n\n\n\n\n\nIn conclusion of the paper's ability to learn chess.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "B4OTsjq63T5": {
        "generation": "\n\nMeta-Review:\n\nThe paper \"Bayesian Inference via Sparse Hamiltonian Flows\" presents a novel approach to Bayesian inference that combines coresets, sparse flows, and quasi-refreshments to improve the efficiency and accuracy of the inference process. The paper provides theoretical evidence for the effectiveness of the proposed method and reports experimental results on several datasets. Overall, the paper is well-written and provides a clear contribution to the field of Bayesian inference.\n\nStrengths:\n\n* The paper presents a novel approach to Bayesian inference that combines coresets, sparse flows, and quasi-refreshments to improve efficiency and accuracy.\n* The paper provides theoretical evidence for the effectiveness of the proposed method and reports experimental results on several datasets.\n* The paper is well-written and easy to follow, with clear explanations of the proposed method and its theoretical foundations.\n\nWeaknesses:\n\n* The paper assumes that the posterior distribution is Gaussian, which may not always be the case in practice.\n* The paper does not provide a comprehensive evaluation of the proposed method on a wide range of datasets and models.\n* The paper does not discuss the limitations of the proposed method or potential avenues for future research.\n\nDiscussion:\n\nThe paper provides a novel approach to Bayesian inference that combines coresets, sparse flows, and quasi-refreshments to improve efficiency and accuracy. The proposed method has several advantages over existing methods, including its ability to handle complex models and large datasets, its ability to incorporate prior knowledge and regularization, and its ability to provide accurate and efficient inference in a wide range of applications.\n\nHowever, the paper also has some limitations and areas for future research. First, the proposed method assumes that the posterior distribution is Gaussian, which may not always be the case in practice. Future work should consider extending the proposed method to handle non-Gaussian posterior distributions. Second, the paper does not provide a comprehensive evaluation of the proposed method on a wide range of datasets and models. Future work should consider conducting more extensive experiments to evaluate the performance of the proposed method in a variety of settings. Finally, the paper does not discuss the limitations of the proposed method or potential avenues for future research. Future work should consider providing a more detailed analysis of the limitations of the proposed method and identifying potential directions for future research.\n\nOverall, the paper provides a valuable contribution to"
    },
    "1vusesyN7E": {
        "generation": "\n\n---\nMeta-Review:\n\nThe proposed method in this manuscript aims to generate perturbations for data poisoning using autoregressive processes. The generated perturbations are found to be effective in poisoning various datasets and models, including computer vision tasks. However, there are some limitations and concerns raised in the reviews, including the following:\n\n1. Applicability to other norms: The proposed method only studies $\\ell_2$ norm, which may limit its applicability to other types of datasets or models. A clearer explanation of the reason behind this choice and its implications on the method's effectiveness would be helpful.\n2. Effectiveness with low poison rates: The paper assumes that the poison rate is high for the proposed method to be effective. However, in practical scenarios, the data may come from multiple sources, making it more challenging to achieve high poison rates. The manuscript should address this limitation and provide insights into how the proposed method performs with lower poison rates.\n3. Understanding of the theoretical analysis: The theoretical analysis in Section 3.3 is not easy to follow, and the relation between Lemma 3.1 and the effectiveness of the proposed method in poisoning attacks is not obvious. A clearer explanation or additional analysis would be helpful in understanding the underlying mechanisms of the proposed method.\n4. Applicability to other tasks: The manuscript only studies computer vision tasks, and it is unclear whether the proposed method can be applied to other types of datasets or models. Some discussions on the generalizability of the proposed method would be useful.\n5. Reverse operation: There is a concern that if the parameters for the AR noise generation are leaked, they could be used to recover the original image. The manuscript should address this limitation and provide insights into how to mitigate it.\n\nIn conclusion, while the proposed method in this manuscript shows promise in generating perturbations for data poisoning, there are some limitations and concerns that need to be addressed. A clearer explanation of the method's applicability to other norms, effectiveness with low poison rates, and theoretical analysis would be helpful in understanding the underlying mechanisms and improving the method's performance. Additionally, the manuscript should provide more insights into the generalizability of the proposed method and how to mitigate the reverse operation limitation."
    },
    "6UtOXn1LwNE": {
        "generation": "\n\nMeta-review:\n\nThe paper addresses an important problem in reinforcement learning, i.e., learning from human preferences. The proposed method, which is based on the regret notion, provides a novel perspective on this problem. However, there are several weaknesses in the paper that need to be addressed.\n\nFirstly, the proposed method is limited to deterministic policies, and there is no discussion on how to extend it to stochastic policies. Secondly, the computational complexity of computing the advantage function in the segment's regret is not discussed, and the scalability of the method to more practical problems is not demonstrated. Thirdly, there is no comparison with existing methods, such as those using partial returns, and the difference between the proposed method and existing works is not clearly explained. Fourthly, the theoretical results are not fully convincing, and there are some ambiguities in the definitions and proofs. Finally, the experimental results are limited to a small toy grid world, and there is no discussion on how to apply the method to more realistic environments.\n\nOverall, the paper provides a novel perspective on learning from human preferences, but there are several weaknesses that need to be addressed before the method can be considered practical and robust."
    },
    "7BlQMwp_44p": {
        "generation": "\n\nTitle: Exact Recovery of Linear and ReLU Regression Parameters in the Presence of Massart Noise\n\nAuthors: [Author Name]\n\nJournal: [Journal Name]\n\nThis manuscript proposes an efficient algorithm for linear and ReLU regression in the presence of Massart noise, which is a generalization of the semi-random Massart noise model. The authors assume mild distributional assumptions on the underlying data distribution and provide guarantees for exact parameter recovery in polynomial time with high probability. The results are novel and significant, and the paper is well-written and well-organized.\n\nThe reviews provide valuable insights into the strengths and weaknesses of the paper. Some reviewers have pointed out that the paper lacks proper discussion in some parts, and there are some minor comments on the bit complexity of the parameters and samples. However, the authors are careful in evaluating the strengths/weaknesses of their work, and the paper seems to be built upon an established literature.\n\nIn terms of originality, the paper considers a slightly weakened model (in comparison to the contamination model) but leads to more generality in the underlying data distribution. The results are significantly different (stronger) from the closest comparisons, and the methods are designed specifically to address the limitation of those works.\n\nIn terms of significance, the paper provides a demonstrable extension of existing works, and the generalization of the approach from linear to ReLU regressions are non-trivial in both technical analysis and algorithmic designs. However, some reviewers are unfamiliar with some pieces of related work and don't have a strong conviction in how it will impact the field.\n\nOverall, the paper is well-written and well-organized, and the results are novel and significant. However, there are some minor comments on the bit complexity of the parameters and samples, and the paper could benefit from more discussion on the distributional assumptions. Nonetheless, the paper is an important contribution to the field of robust regression and is worth publishing.\n\nRecommendations:\n\n* Address the minor comments on the bit complexity of the parameters and samples.\n* Provide more discussion on the distributional assumptions and their implications on the results.\n* Consider revising the introduction to provide more context on the related work and the motivation for the paper.\n*"
    },
    "aKZeBGUJXlH": {
        "generation": "\nMeta-review:\n\nThe manuscript proposes a novel defense mechanism against backdoor attacks in pre-trained language models, which is an emerging threat in the security of these models. The proposed method, gradient broadcast adaptation (GBA), utilizes the prompt-tuning process to guide the perturbed weights back to the normal state, thus avoiding the degradation of generalization ability. The manuscript provides a well-motivated analysis of the inheritance of backdoor attacks during pretraining and tuning and demonstrates the effectiveness of GBA through empirical evaluation against four state-of-the-art backdoor attacks.\n\nStrengths:\n\n1. The manuscript identifies an important challenge in the security of pre-trained language models and proposes a novel solution to address it.\n2. The proposed method, GBA, is simple yet effective and can be easily plugged into the existing optimization pipeline.\n3. The manuscript provides a thorough analysis of the inheritance of backdoor attacks during pretraining and tuning and demonstrates the effectiveness of GBA through empirical evaluation.\n\nWeaknesses:\n\n1. Some reviewers have raised concerns about the simplicity of the proposed method and its ability to generalize to more complex tasks.\n2. The manuscript could benefit from more detailed explanations of the evaluation methodology and the specific backdoor attacks used in the experiments.\n3. The authors could consider exploring other alternative approaches to repairing backdoored tokens, such as information-entropy smoothed or neighbor cluster.\n\nOverall, the manuscript is well-written and provides a valuable contribution to the security research of pre-trained language models. The proposed method shows promising results in defending against backdoor attacks, and the manuscript provides a good starting point for further research in this area."
    },
    "6lH8nkwKRXV": {
        "generation": "\n\n=========Meta-Review========\n\nThis manuscript proposes a new method for graph classification, called StructAgg, which aims to improve the explainability of graph neural networks (GNNs). The authors propose a novel aggregation method that learns a structural representation of node embeddings, which captures the local structure of the graph. The proposed method is evaluated on several benchmark datasets, and the results show that StructAgg outperforms existing GNNs in terms of accuracy.\n\nThe manuscript is well-organized, and the authors provide a clear explanation of the proposed method and its evaluation. The results are presented in a concise and intuitive manner, making it easy for readers to understand the main contributions of the paper. However, there are some limitations and open issues that need to be addressed.\n\nFirstly, the authors need to provide more theoretical guarantees on the convergence and generalization of the proposed method. Although the authors demonstrate the effectiveness of StructAgg on several benchmark datasets, it is unclear how the method will perform on unseen data or when the graph structure is different. Moreover, the authors need to provide more insights into the structural representation of node embeddings, which could help to understand the underlying mechanisms of the proposed method.\n\nSecondly, the authors need to address the issue of overfitting. Although the proposed method is designed to learn a structural representation of node embeddings, it is possible that the method will overfit the training data. The authors need to provide more discussion on how to prevent overfitting and improve the generalization performance of the proposed method.\n\nFinally, the authors need to provide more comparisons with existing GNNs. Although the authors compare StructAgg with some baseline methods, it is unclear how the proposed method compares to other state-of-the-art GNNs. Moreover, the authors need to provide more insights into the advantages and limitations of the proposed method compared to existing GNNs.\n\nIn conclusion, the proposed method has the potential to improve the explainability of GNNs, but more work is needed to address the limitations and open issues. The authors need to provide more theoretical guarantees, address the issue of overfitting, and provide more comparisons with existing GNNs."
    },
    "VAeAUWHNrty": {
        "generation": "\nMeta-Review:\nThe proposed method combines neural inverse rendering and traditional Monte Carlo ray tracing, which enjoys the benefits of both worlds. The idea of using denoisers to reduce variance in Monte Carlo integration is interesting and helps make the training more efficient. The paper did multiple experiments to show that it helps improve material, geometry, and lighting estimation. However, there are some limitations and weaknesses in the paper that need to be addressed, such as the lack of quantitative evaluation of individual attribute reconstruction accuracy and the limited applicability to translucent objects. The paper could be improved by providing more detailed discussions on the design choices and challenges, and by comparing the proposed method with recent Monte Carlo inverse rendering methods. Overall, the paper is well-written and contributes to the field of inverse rendering."
    }
}