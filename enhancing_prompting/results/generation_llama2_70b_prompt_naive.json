{
    "iH1_KBzbwQq": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a novel unsupervised domain adaptation method for 3D point cloud object detection, named 3D-CoCo, which consists of a shared Bird-Eye-View (BEV) transformation module, a domain-specific 3D encoder, and an augmented contrastive alignment scheme. The proposed method is evaluated on three large-scale datasets collected by heterogeneous LiDAR sensors and shows improved performance compared to recent SOTA methods. The reviewers generally praise the novelty and significance of the paper, but also raise some concerns and suggestions for improvement.\n\nOne reviewer suggests considering other feature extraction approaches and discussing the sensitivity of the model to hyperparameters. Another reviewer raises questions about the proposed augmentation strategy, such as how to distinguish between easy and hard samples, how many points are discarded, and how to calculate the viewpoint of a certain sample. They also suggest considering related works from semi-supervised learning, such as consistency regularization.\n\nOne reviewer notes that the full name of BEV should be mentioned when it is first introduced, and that a brief description of BEV and its related references would be helpful for a wider range of audiences. They also suggest comparing the high-level 3D features with the BEV features and discussing the information loss caused by mapping the 3D domain shift into the 2D one.\n\nAnother reviewer points out that the proposed method is similar to [b, c, d] for 2D UDA and suggests considering them as related works and making comparisons and discussions appropriately. They also suggest approximately quantifying the distribution discrepancy across domains and evaluating the cross-domain performance on the target domain.\n\nOne reviewer suggests that the \"co-training\" in this paper means two individual encoders for the source and target domains, or the training strategy, and that the difference should be made clear. They also suggest discussing the sensitivity of the model to the temperature and hyperparameter.\n\nOverall, the reviewers agree that the paper has novelty and significance, but also have concerns about the proposed method and suggest improvements and further discussions. The authors should address these concerns and suggestions in the revision.\n\nThe potential negative societal impact of their work has not been mentioned."
    },
    "OgCcfc1m0TO": {
        "generation": "\nThis paper proposes a novel approach named context optimization (CoOp) for prompt engineering of vision-language pre-training models. The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots.\n\nReviewers generally agree that the paper presents a simple yet effective approach to improve few-shot image classification performance using CLIP. They also appreciate the thorough experiments and ablation studies conducted by the author. However, some reviewers have raised concerns about the novelty of the proposed method, citing similarities with existing works such as Li and Liang (2021) and Lester et al. (2021). One reviewer also questions the effectiveness of CoOp in leveraging more computation when adapting to tasks, as the additional embeddings are only optimizing a small number of parameters.\n\nAnother reviewer raises several technical concerns, including the lack of details for the linear probe and the regularization parameter chosen, and the absence of a comparison between CoOp and fine-tuning all the parameters of CLIP. Additionally, there are presentation concerns regarding the potential misleading nature of Figures 1 and 5b, which could be improved by including a comparison to linear probe CLIP and zero-shot CLIP, respectively.\n\nOne reviewer also suggests that the paper lacks theoretical contribution, with a rough description of the context optimisation method and unclear explanations of concepts such as \"unified context\" and \"class-specific context\". They also find Figure 2 to be not helpful in understanding the framework and of low quality.\n\nIn summary, while the paper presents a simple and effective approach to improve few-shot image classification performance, the novelty of the method is questionable, and the paper could benefit from more thorough explanations and theoretical contributions. Additionally, the presentation of certain figures could be improved for better clarity.\n\nMeta-Review:\n\nStrengths:\n\n* Simple yet effective approach to improve few-shot image classification performance using CLIP\n* Thorough experiments and ablation studies"
    },
    "JYtwGwIL7ye": {
        "generation": "capabilities, and reward misspecifications. The results demonstrate that in many cases, increasing agent capabilities leads to exploitation of the misspecified reward, resulting in a significant decrease in the true reward. Additionally, the authors find instances of phase transitions, where a small increase in agent capability leads to a drastic change in behavior and a sharp decrease in the true reward. To address this issue, they propose setting up an anomaly detection task to identify when policies from a different model are satisfactory for the true reward. The authors provide several baseline anomaly detectors and show their performance on different tasks. \n\nThe paper is well-written and easy to follow, and it sheds light on a crucial problem in AI safety. The findings are clearly useful to the research community, and the paper provides a valuable contribution to the field. The paper's contributions could be further strengthened by addressing the following minor concerns:\n\n1. It would be beneficial to include a more detailed related work section that discusses the current state of research on reward hacking and the novel aspects of this paper's approach.\n\n2. The paper could provide more intuition on why the proposed anomaly detection approach could be effective in mitigating reward hacking. While the paper shows that the baseline anomaly detectors perform well on some tasks, it remains unclear why this approach should generally work. \n\n3. The paper uses the term \"trusted model\" without much explanation. It would be helpful to elaborate on what constitutes a trusted model and how to obtain one.\n\n4. It would be beneficial to provide more information on the correlation between the true and proxy rewards. Understanding the relationship between the true and proxy rewards is crucial to reward hacking.\n\n5. The paper uses the term \"resolution\" to refer to action noise, which may be confusing. It would be beneficial to use more standard terminology to avoid confusion.\n\n6. It would be helpful to include more discussion on the limitations of the proposed approach and potential avenues for future work. \n\n7. The paper could benefit from providing more concrete examples of how the proposed approach could be applied in real-world scenarios.\n\n8. The paper could benefit from providing a more detailed analysis of the computational complexity of the proposed approach.\n\n9. It would be beneficial to include a discussion on the potential risks associated with the"
    },
    "pTZ6EgZtzDU": {
        "generation": "\n\nMeta-Review:\nThis paper proposes a method called IMPORT that leverages task descriptors for multi-task learning in reinforcement learning. The method consists of training an informed policy that takes in the task descriptor and the state to maximize the expected return, and a RNN policy based on the history of states and actions that imitates the behavior of the feature extraction layers of the informed policy. The experimental results show that IMPORT outperforms baseline methods that leverage task descriptors.\n\nThe reviewers generally found the paper to be well-written and the proposed method to be novel and promising. However, there were some concerns and suggestions for improvement.\n\nOne reviewer asked for clarification on the experiment procedure, specifically how the policy is adapted for test tasks and whether the informed and RNN policies are further trained on test tasks. They also recommended adding the learning curve on the test tasks to show the performance of adaptation.\n\nAnother reviewer suggested that the paper frame the method more generally and show its usefulness on a broader range of domains that require adaptation or exploration/exploitation strategies. They also mentioned that using an interchangeable embedding and RNN state is a good way to avoid the challenges of conditional architectures.\n\nA third reviewer expressed some concerns about the experiments, such as the lack of clear substantiation of the claims made in the main text, the performance improvement from IMPORT being fairly modest, and the TS baseline seeming strangely implemented. They recommended that the authors carefully select experiments that better evaluate IMPORT's ability to perform sophisticated exploration.\n\nOverall, the reviewers found the proposed method to be promising but suggested that the authors address the concerns and improve the experiments to better substantiate the claims made in the paper."
    },
    "r1gIdySFPH": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a new exploration strategy for goal-conditioned reinforcement learning called Skew-Fit, which maximizes the entropy of the distribution of goals. The authors claim that this approach can improve state coverage and lead to diverse exploration behavior. The paper provides a theoretical analysis of the algorithm and demonstrates its effectiveness through simulations and real-world robotic manipulation tasks.\n\nThe reviewers have raised several concerns and suggestions for improvement. Reviewer 1 questions the assumption that the entropy of the resulting state distribution is no less than the entropy of the goal distribution, and suggests clarifying this point. Reviewer 2 suggests discussing why maximizing the entropy of the state visitation frequency is a good exploration objective, and providing more theoretical insights and details to support this claim. Reviewer 3 suggests evaluating the method on longer-horizon multi-stage control tasks and exploring how it performs with different goal-conditioned RL algorithms.\n\nOverall, while the paper presents an interesting and promising approach to exploration in goal-conditioned reinforcement learning, the reviewers have raised several valid concerns that should be addressed before the paper can be accepted. The authors should provide a more thorough analysis of the algorithm's assumptions and limitations, and demonstrate its effectiveness in a wider range of tasks and settings."
    },
    "HFPTzdwN39": {
        "generation": "The manuscript proposes a method for interpreting the representations learned by self-supervised models. The proposed method is based on reverse linear probing, which is a post-hoc method that aims at predicting a quantized version of the internal representation learned by a given model, and evaluates the interpretability of the representation using a normalized mutual information score. The manuscript compares the proposed method with linear probing, and shows that it has some advantages over linear probing. The manuscript also presents an analysis of the interpretability of the representations learned by various self-supervised models, and finds that some models are better at capturing certain types of concepts than others. The paper also discusses the limitations of the proposed method and suggests directions for future work.\n\nOverall, the reviewers find the paper to be well-written and the proposed method to be interesting and relevant to the field. However, some reviewers have concerns about the novelty and effectiveness of the proposed method, as well as its relationship to related work. Some reviewers also suggest that the paper could be improved by providing more insightful analysis of the results and addressing some of the limitations of the proposed method.\n\nOne reviewer suggests that the paper could benefit from a clearer positioning with respect to related work, particularly in the area of topic models and NLP. Another reviewer notes that the proposed method is very similar to linear probing and that it is not clear how it can better handle combinations of concepts. This reviewer also suggests that the paper could provide more analysis of the results and discuss the limitations of the proposed method in more detail.\n\nAnother reviewer raises some minor comments, such as the choice of K for the number of clusters, the ability of the proposed method to capture non-linear relationships between image attributes, and the sensitivity of the rankings of the evaluated self-supervised methods to the metrics used.\n\nBased on these reviews, it is clear that the manuscript has some strengths, such as its well-written style and its relevance to the field. However, there are also some concerns about the novelty and effectiveness of the proposed method, as well as its relationship to related work. In order to improve the manuscript, the authors may want to address these concerns by providing more analysis of the results and discussing the limitations of the proposed method in more detail. Additionally, the authors may want to consider providing more context and comparison to related work in the"
    },
    "CMcptt6nFaQ": {
        "generation": "\n\nThis paper proposes a new method for image segmentation that incorporates topological accuracy and efficiency. The method uses a distance-ordered homotopy warping algorithm to identify topologically critical locations and a homotopy warping loss to train the segmentation network. The authors claim that their method achieves better topological accuracy and efficiency compared to previous methods.\n\nThe reviewers generally find the proposed method and its motivation well-justified and intuitive. They also appreciate the thorough experimental evaluation and comparisons with other methods. However, some reviewers point out some limitations and areas for improvement.\n\nOne reviewer suggests that the method may not be robust to noisy segmentation outputs, and the improvement in Dice score may be subtle. Another reviewer raises concerns about the choice of terminology and the definition of \"topologically critical locations.\" They also suggest that the method may not be guaranteed to identify the real foreground pixels, and the warping operation may not be unique.\n\nAdditionally, reviewers suggest that the authors could provide more discussion on the computational complexity of the proposed method compared to other methods, especially TopoNet. They also request more examples of failure modes and outputs on all datasets studied.\n\nFinally, one reviewer raises a question about the global optimum of the proposed heuristic and suggests evaluating warping only in one direction. They also suggest sampling each pixel's on/off state from the output distribution instead of thresholding.\n\nOverall, the reviewers find the proposed method promising, but some concerns and suggestions for improvement need to be addressed. The authors should provide more discussion and analysis to strengthen their claims and improve the method's robustness and generalizability."
    },
    "3vmKQUctNy": {
        "generation": "\n\nMeta-Review\n\nThis manuscript focuses on the problem of fairwashing detection in black-box models. The authors argue that fairwashing is a technique used to fool model explanation methods to produce deceptive outputs for unfair models, and propose a novel framework FRAUD-Detect to detect fairwashed models. They also present empirical results and theoretical analysis to show that fairwashing is unlikely to be avoided.\n\nThe reviewers generally agree that the paper investigates an important problem and provides a simple but effective method for detecting dishonest interpretation models. They also appreciate the thorough evaluation of the proposed detection method, including its robustness towards an informed adversary.\n\nHowever, some reviewers have raised concerns about the paper's readability and the choice of the hyperparameter \\delta in practical use. One reviewer points out that the authors make an impractical assumption about the defender's knowledge and lack some important references. Another reviewer suggests that the theory is missing explicitness in several places and there are gaps in the exact setting.\n\nTo address these concerns, the authors have provided a detailed response to the reviewers' comments. They have clarified the problem setup and the construction of the interpretable model, and provided more guidance on choosing the hyperparameter \\delta. They have also addressed the concern about the method's applicability to other fairness criteria.\n\nOverall, the reviews suggest that the paper provides a valuable contribution to the field of fairness in machine learning. The proposed method is effective and well-evaluated, and the paper provides a thorough analysis of the problem of fairwashing. However, the authors should address the concerns about readability and explicitness, and provide more guidance on practical applications of the method.\n\nRecommendation:\n\n* The authors should revise the paper to address the concerns about readability and explicitness.\n* The authors should provide more guidance on practical applications of the method, including choosing the hyperparameter \\delta and adapting the method to other fairness criteria.\n* The authors should address the concern about the assumption of the defender's knowledge and provide more references to related work.\n\nIf the authors address these concerns and revise the paper accordingly, it has the potential to be a valuable contribution to the field of fairness in machine learning."
    },
    "QevkqHTK3DJ": {
        "generation": "\n\nMeta-Review\n\nThe paper proposes a novel approach to reduce the dimensionality of the decoder in transformer-based text summarization models by utilizing pre-trained autoencoders. The authors conduct extensive experiments to find the best trade-off between compression ratio and model performance. The paper presents generation results and additional experiments in the appendix.\n\nStrengths:\n\n* The paper is well-written, and the proposed approach is novel.\n* The authors conducted thorough experiments to find the ideal trade-off between compression ratio and model performance.\n* The paper presents generation results and additional experiments in the appendix.\n\nWeaknesses:\n\n* The results of the transformer baseline are far from previous literature, which raises concerns about the validity of the comparison.\n* The paper lacks comparisons with existing compression methods, such as pruning and distillation.\n* The choice of a purely linear network for the autoencoder is not well-motivated, and it is not clear why a standard MLP bottleneck autoencoder was not used.\n* The way the autoencoder is used is not very clear, and it is not obvious why the CNN and LSTM autoencoders were chosen.\n* There appears to be a problem with Table 4, and it is not clear why the AE S variant performs worse than the AE variant.\n* The approach only adjusts the size of the decoder and does not address the problem of the ever-increasing size of the pre-trained encoder.\n* The results may not be generalizable to different tasks with more training data.\n\nOverall, the paper presents an interesting approach to reducing the dimensionality of the decoder in transformer-based text summarization models. However, the lack of comparisons with existing compression methods and the unclear motivation for some design choices limit the impact of the work. The paper could benefit from additional experiments and analysis to address these weaknesses."
    },
    "SJgVHkrYDH": {
        "generation": "\n\nMeta-Review\n==========\n\nThe paper proposes a graph-based recurrent retrieval model for retrieving evidence documents in a multi-hop reasoning question answering task. The main idea is to use the graph formed by Wikipedia links between passages as a constraint for constructing reasoning chains and to use the joint encoding of the question and current passage to retrieve a subsequent passage in the reasoning chain. The paper describes a model for implementing the above retrieval system and how they jointly train with a reading comprehension model. They demonstrate the effectiveness of the system on HotPotQA and SQuaD-Open, showing improvements over previously published models and competitive results.\n\nThe reviewers have provided detailed comments on the paper, pointing out both strengths and weaknesses. One reviewer notes that the paper presents a well-designed system for handling multi-hop queries, and the explicit recurrent state is a nice contribution to the IR model. However, the same reviewer also points out that the paper lacks clarity on some formal definitions and the definition of the graph, making it hard to understand the content precisely. They also mention that the results are somewhat strong, but the ideas presented are small variations on existing systems.\n\nAnother reviewer raises several concerns, including the reliance on linked documents for constructing the retrieval system, the lack of clarity on what E denotes, the representation of the starting retrieval, and the definition of C_t. They also suggest that the paper should discuss the scalability of the method.\n\nThe authors have addressed all the comments and improved the results since. They have provided a clear and well-written paper that presents a well-designed system for handling multi-hop queries. The proposed method has demonstrated strong results on two datasets in challenging open-domain settings. The ablation results are helpful, and the paper is clearly written and easy to follow.\n\nHowever, some weaknesses remain, such as the reliance on Wikipedia graph, which limits the method's ability to work without a hyperlink graph. The paper should mention this limitation and possible remedies. The authors should also discuss the scalability of the method and provide running time comparisons.\n\nIn conclusion, the paper presents a well-designed system for handling multi-hop queries. The proposed method has demonstrated strong results on two datasets in challenging open-domain settings. The authors have"
    },
    "ByeadyrtPB": {
        "generation": "\n\nThe manuscript presents a deep generative model based on stacked Wasserstein autoencoders. The model is novel and elegant, and the authors demonstrate its effectiveness through experimental results on various image datasets. However, some reviewers have raised concerns about the lack of sufficient argumentation regarding the differentiation of the proposed method from earlier attempts to nest Wasserstein distances. Additionally, there is a lack of quantitative evaluation schemes for the success of the proposed method.\n\nOne reviewer suggests that the proposed method is a straightforward extension of the existing Wasserstein Auto-Encoder (WAE) approach, and that the authors should provide a more thorough comparison with previous work. Another reviewer suggests that the advantages of the proposed model over WAE and VLAE are not immediately obvious, and that the paper could benefit from additional experiments and analysis to demonstrate the benefits of the proposed approach.\n\nOverall, the reviews suggest that the manuscript has potential but could benefit from additional work to address the concerns raised by the reviewers. The authors could consider providing a more thorough comparison with previous work, as well as additional experiments and analysis to demonstrate the effectiveness and advantages of the proposed method.\n\n---\n\nCould you please provide a review of the meta-review?\n\nSure, here's a review of the meta-review:\n\nThe meta-review provides a good summary of the strengths and weaknesses of the manuscript. It highlights the novelty and elegance of the proposed method, while also pointing out the lack of sufficient argumentation regarding its differentiation from earlier attempts to nest Wasserstein distances. Additionally, it notes the lack of quantitative evaluation schemes for the success of the proposed method.\n\nThe meta-review also suggests that the proposed method may be a straightforward extension of existing Wasserstein Auto-Encoder (WAE) approach, and that the authors should provide a more thorough comparison with previous work. It also suggests that additional experiments and analysis may be needed to demonstrate the benefits of the proposed approach.\n\nOverall, the meta-review provides a fair and balanced assessment of the manuscript, and provides constructive feedback for the authors to improve their work."
    },
    "mk0HzdqY7i1": {
        "generation": "of research should ideally be replicated by others. \n\nAnother potential risk is that the paper can be considered overly negative. While the paper provides the necessary scientific scrutiny, it can be considered overly focused on one specific paper. It can be argued that the paper does not provide an overall view of the field, nor does it offer a solution to the problem. \n\nMajor revisions required:\n\n1. The paper should provide a more positive view of the field. The paper can provide a brief overview of the field, provide a historical context, and provide an overview of what is known to work and what not. It can also provide a discussion on what are the known challenges of this field and how the community is trying to overcome them.\n\n2. The paper can provide some possible solutions to the problem. One of the reasons for lack of replicability of results can be that the problem is ill-defined. Providing a clear problem definition and/or a clear methodology can help future research. If the authors have some specific ideas on how to improve the GNN model, it can be included in the paper.\n\n3. A more detailed analysis of the results of Ahn et al. 2020. The paper presents a detailed analysis of the GNN model of Li et al. but a much less detailed analysis of the model of Ahn et al. A more detailed analysis can help to convince the reader that the results of Ahn et al. are indeed not replicable.\n\n4. The authors should consider to provide some results of their own. While the paper provides a negative result that is interesting, it can also provide a positive result. For example, they can take one of the best models and perform a thorough evaluation of it.\n\n5. The authors should consider to provide a more thorough literature review. For example, they can consider to include a subsection on survey papers in this area, a subsection on GNN models in combinatorial optimization, a subsection on other attempts to replicate important results in the field.\n\n6. The authors should consider to provide a more thorough discussion of the limitations of the study. For example, they can consider to include a subsection on potential issues with their implementation, potential issues with the data set, potential issues with the evaluation methodology, and potential issues with the computational resources.\n\n7. The authors should consider to provide a more"
    },
    "HyxUIj09KX": {
        "generation": "\n\nMeta-review:\n\nThis paper presents a new framework, called the \"S-System,\" which generalizes hierarchical models, including neural networks. The paper derives activation functions used in practice in a principled way and studies the optimization problem of neural networks with a large number of input/hidden units and diverse neurons. The paper has interesting ideas that can potentially lead to a deeper understanding of deep learning. However, the presentation is difficult to follow, with too many concepts introduced at once, and the proofs are impossible to understand.\n\nThe reviewers suggest that the paper be split into multiple parts with clear-cut statements and detailed proofs and submitted to appropriate journals or conferences. The paper would be better appreciated by an audience of a mathematical journal. The authors are encouraged to simplify the content using a running example to explain the implications and assumptions.\n\nOverall, the paper has interesting ideas, but the presentation needs to be improved for a non-expert to understand the results. The paper is rejected for now, and the authors are encouraged to revise and resubmit."
    },
    "Ybx635VOYoM": {
        "generation": "\n\nMeta-Review:\n\nThis paper presents a new dataset and framework for evaluating the robustness of question-answering models to contradictory information. The dataset, called ContraQA, contains human and machine-generated contradictory contexts for SQuAD 1.1. The authors propose a misinformation-aware framework that combines the score of the QA model with a trust score output by a fake detector. They evaluate the performance of several QA models on the new dataset and show that their framework can improve the performance of the models.\n\nThe reviewers have raised several strengths and weaknesses of the paper. The strengths include the introduction of a new task for evaluating the robustness of QA models, the creation of a new dataset that includes contradictory contexts, and the proposal of a framework for mitigating the problem of contradictory information. The weaknesses include the lack of consideration of the problem statement, the claims not well supported by the results, and ethical concerns related to the release of the trained model.\n\nOne of the reviewers has pointed out that the problem of misinformation has been explored in the literature, and many works treat the task as a fact-checking problem. The reviewer suggests that the problem addressed in this work is different from fact-checking, and the authors should have discussed this in the paper. Another reviewer has also pointed out that the authors have not provided a clear definition of contradicting examples, and it remains unclear how the produced contexts are different from context generated for adversarial attacks of QA models.\n\nOne reviewer has also raised concerns about the ethical implications of releasing the trained model, BERT-FG, as it can be used to create fake contexts that can mislead people. The reviewer suggests that the authors should have discussed this issue in the paper and provided a clear justification for releasing the model.\n\nOverall, the reviewers have provided constructive feedback on the paper, highlighting both the strengths and weaknesses of the work. The authors should address the concerns raised by the reviewers, particularly those related to the problem statement, ethical implications, and the lack of consideration of the existing literature on fact-checking. The authors should also provide a clear definition of contradicting examples and justify the release of the trained model.\n\nThe paper presents a valuable contribution to the field"
    },
    "rkzjUoAcFX": {
        "generation": "\n\nMeta-Review\n\nThis manuscript has been reviewed by three experts in the field, and their comments and suggestions have been summarized in this meta-review. The manuscript proposes a method for customizing a text-to-speech synthesis system to a new speaker using a small amount of data from that speaker. The proposed method is based on adapting a pre-trained WaveNet model to the new speaker's voice, and the authors evaluate the effectiveness of their approach using several objective and subjective measures.\n\nThe reviewers generally agree that the paper is well-written and the results are strong, indicating high quality, naturalness, and similarity with real speech from a speaker. However, there are some concerns and suggestions for improvement.\n\nReviewer 1 suggests that the paper should provide more details about the experimental setup, such as the type of linguistic features used and how they are acquired. The reviewer also mentions that the speaker-dependent linguistic features are not speaker-adapted, which may limit the effectiveness of the proposed method.\n\nReviewer 2 suggests that the paper should provide more details about the early termination criterion used in the training process. The reviewer also suggests that the MOS evaluations should be done by a more diverse group of people to make the results more representative.\n\nReviewer 3 suggests that the paper should provide more details about the speaker verification model used in the evaluation. The reviewer also mentions that the proposed method is similar to the work in Arik et al. (2018), but with a different front-end and back-end setup.\n\nOverall, the reviewers agree that the paper presents a valuable contribution to the field of text-to-speech synthesis, but there are some areas that need improvement. The authors should address the concerns and suggestions raised by the reviewers to strengthen their paper and make it more impactful."
    },
    "dgd4EJqsbW5": {
        "generation": "\n\nMeta-Review\n\nThis paper proposes an algorithm for learning controllable embeddings (LCE) based on policy iteration in the latent space, with a focus on high-dimensional sensory observations. The authors provide a theoretical analysis of the policy improvement error, which is used to develop three algorithmic variations. The paper has received three reviews, which generally acknowledge the paper's contributions but also raise concerns regarding the theoretical foundations, experimental design, and clarity of presentation.\n\nReview 1 raises questions about the theoretical foundation of the paper, specifically regarding the proof of Theorem 1. The reviewer suggests that the implication T^2 VE(x) < T VE(x) + \u03b3 Delta(x) for all x, would not hold and that a bound that depends on L-inf norm would be inappropriate. The reviewer also suggests that the algorithm's dependence on hyperparameters and the lack of guidance on choosing them is a concern.\n\nReview 2 provides a weak reject recommendation, citing concerns about the completeness of the results and the need for more comprehensive experiments. The reviewer suggests that the methodological sections are difficult to follow, and that there is a lack of guidance on picking hyperparameters. The reviewer also notes that the proposed approach is not properly compared to other control algorithms in the latent space.\n\nReview 3 provides a weak accept recommendation, acknowledging the quality of the theoretical work but expressing concerns about the experimentation and large number of hyperparameters. The reviewer suggests that the authors should provide guidelines for choosing hyperparameters and compare their approach to other offline RL methods.\n\nFollowing discussions with the authors, Review 1 has increased their score to weak accept, while Reviews 2 and 3 maintain their original scores.\n\nBased on the reviews, the manuscript's strengths include its focus on high-dimensional sensory observations, the provision of a theoretical analysis of policy improvement error, and the development of algorithmic variations. However, the manuscript's weaknesses include concerns about the theoretical foundations, the lack of comprehensive experiments, and the need for better guidance on choosing hyperparameters.\n\nTo improve the manuscript, the authors could address the concerns regarding the theoretical foundations and provide more comprehensive experiments to demonstrate the effectiveness of their approach. Additionally, the authors could provide better"
    },
    "8OH6t0YQGPJ": {
        "generation": "\n\n### Introduction \nThis manuscript presents a novel approach to systematically study the credibility of claims and robustness of conclusions in machine learning by adopting the concept of 'Multiverse Analysis' from psychology. The authors propose a framework that leverages Bayesian optimization and Gaussian process surrogate models to quantify and emulate the relationship between interests of study and variance in layouts. The framework is evaluated through two motivating case studies, one on adaptive optimizers and the other on the large batch generalization gap. \n\nThe reviewers generally agree that the paper is well-written, and the proposed framework is novel and practical for drawing robust conclusions in machine learning. However, some reviewers have raised concerns about the computational scalability of the framework, the choice of the initial design, and the significance of model comparison. \n\n### Strengths \n- The paper presents a novel approach to systematically study the credibility of claims and robustness of conclusions in machine learning. \n- The proposed framework is practical for drawing robust conclusions and can be applied to various machine learning models. \n- The case studies demonstrate the effectiveness of the proposed framework in exploring the multiverse of hyperparameter settings. \n\n### Weaknesses \n- The computational scalability of the framework is a concern, as it scales with the size of the multiverse. \n- The choice of the initial design and its impact on the performance of the proposed method needs further discussion. \n- The significance of model comparison, especially when the Bayes factor is close to 1, needs further investigation. \n\n### Recommendations \n- Addressing the scalability concern by exploring efficient surrogate models that can utilize existing experiments' results. \n- Providing a more detailed discussion on the choice of the initial design and its impact on the performance of the proposed method. \n- Investigating the significance of model comparison, especially when the Bayes factor is close to 1. \n\nOverall, the paper presents a promising approach to drawing robust conclusions in machine learning. However, the authors need to address the concerns raised by the reviewers to ensure the practicality and reliability of the proposed framework."
    },
    "xTYL1J6Xt-z": {
        "generation": "\n\nMeta-Review\n\nThis paper proposes a novel method, FasterRisk, for efficiently generating a pool of sparse risk scores by combining a beam search algorithm, diverse high-quality solutions, and a star search technique. The authors evaluate their method and baselines along both accuracy and speed metrics and provide extensive additional experiments in their supplement. However, some reviewers have raised concerns about the scope, quality, and significance of the study.\n\nReviewer 1 suggests that the authors describe an interesting method for quickly identifying risk scores in a diverse range of settings, but questions the usefulness of the speed improvement in real-world offline settings. They also mention that the authors do not consider societal impact, which is an important factor given the suggested use cases. Additionally, they find the figure displaying the speed and accuracy trade-off frustrating, as it doesn't allow for a proper assessment of the baseline method's scaling.\n\nReviewer 2 points out that the authors' algorithm often produces a pool of solutions, but the number of solutions produced and their differences from each other are not discussed. They also raise concerns about the potential negative societal impacts of the approach, which the authors have not addressed.\n\nReviewer 3 describes the proposed method as novel and efficient, achieving SOTA performance with less time cost in some datasets. However, they also note that the examples of scoring systems in the introduction are outdated, and there is little discussion of the traditional framework of scoring systems and their differences in methodology and performance with the proposed method. Additionally, they suggest that the model AUC should be assessed for consistency between predicted scores and actual risk, and the feasibility of the generated scoring system should be discussed.\n\nReviewer 4 raises several methodological concerns, such as the choice of hyperparameters, the tolerance gap level, and the discussion of alternative approaches. They also suggest that the authors make use of the pool of \"equally good\" scores to help users balance performance and fairness.\n\nOverall, the reviewers have provided valuable feedback on the manuscript, highlighting both its strengths and weaknesses. While the method proposed in the paper shows promise, the reviewers have raised important concerns that need to be addressed before it can be widely adopted. The authors should consider addressing these concerns in a revised version of the manuscript."
    },
    "_idcJrecij": {
        "generation": "\n\nMeta-review:  This paper proposes a method for estimating arbitrary conditional distributions using an energy-based framework and importance sampling.  The approach is well-motivated, and the authors provide a thorough analysis of the proposed method.  The method is shown to be effective on several data sets, and the authors provide a thorough comparison with other methods.  Some reviewers have raised concerns about the limitation of the method, such as the sensitivity to the choice of order in which the product rule is applied and the potential for accumulation of errors in the dimension of the distribution being estimated.  Additionally, some reviewers have suggested that the authors could provide more discussion on the societal implications of their work and provide more concrete examples of what cannot be modeled/is difficult to model with flows.  However, overall, the reviews suggest that the paper is well-written, and the proposed method is a valuable contribution to the field of density estimation. \n\nThe authors have proposed a novel approach for estimating arbitrary conditional distributions using an energy-based framework and importance sampling.  The approach is shown to be effective on several data sets and outperforms other methods in some cases.  However, some limitations of the method have been pointed out by the reviewers, such as the sensitivity to the choice of order in which the product rule is applied and the potential for accumulation of errors in the dimension of the distribution being estimated.  Additionally, some reviewers have suggested that the authors could provide more discussion on the societal implications of their work and provide more concrete examples of what cannot be modeled/is difficult to model with flows.  Overall, the reviews suggest that the paper is well-written, and the proposed method is a valuable contribution to the field of density estimation. \n\nThe authors have proposed a method for estimating arbitrary conditional distributions using an energy-based framework and importance sampling.  The approach is well-motivated, and the authors provide a thorough analysis of the proposed method.  The method is shown to be effective on several data sets, and the authors provide a thorough comparison with other methods.  However, some reviewers have raised concerns about the limitation of the method, such as the sensitivity to the choice of order in which the product rule is applied and the potential for accumulation of errors in the dimension of the distribution being estimated.  Additionally, some reviewers have suggested that the authors could provide more discussion on the societal implications of"
    },
    "Yn4CPz_LRKO": {
        "generation": "\n\nMeta-Review:\n\nThis manuscript proposes a new framework called ADC-GAN to improve the performance of classifier-based cGANs. The authors claim that the proposed method can learn the joint distribution of real and fake images and their corresponding class labels, and demonstrate the effectiveness of ADC-GAN through experiments on several benchmark datasets. \n\nThe reviewers have raised several concerns and suggestions regarding this manuscript. One reviewer points out that the proposed method is similar to TAC-GAN, and wonders whether the difference in performance is due to the difference in implementation or hyperparameters. Another reviewer raises questions about the theoretical analysis, suggesting that the authors should provide a better overview of the progress in this direction. \n\nAdditionally, the reviewers also point out that the reported numbers seem inconsistent with other works, and that some descriptions are not accurate. One reviewer also suggests removing Proposition 1 and citing TAC-GAN for the analysis. \n\nOverall, while the proposed method shows promising results, the reviewers have raised several valid concerns that need to be addressed before the manuscript can be accepted. The authors should provide a thorough response to the reviewer comments and make necessary revisions to the manuscript before it can be considered for publication. \n\nIn particular, the authors should clarify the differences between their method and TAC-GAN, provide a thorough analysis of the proposed method's theoretical foundations, and ensure that the reported numbers are consistent with other works in the field. Additionally, they should address the concern regarding the bias issue of ACGAN and provide a better overview of the progress in this direction."
    },
    "yxafu6ZtUux": {
        "generation": "\n\nThis paper proposes a new method for A/B testing called BAT, which stands for Bandit-based Adaptive Testing. BAT is designed to detect qualitative treatment effects (QTE) in online experiments. It uses an adaptive randomization algorithm that adjusts the probability of assigning a subject to a treatment based on the observed rewards. The method also includes a sequential monitoring and online updating procedure to control the Type-I error. The paper presents a theoretical guarantee on the Type-I error and experiments on real data from Yahoo.\n\nThe reviewers generally find the paper well-written and the method interesting, but they also raise several concerns.\n\nOne reviewer asks for more details on how BAT relates to Bandits tests and whether it is better to use BAT or Bandits tests for A/B testing.\n\nAnother reviewer raises several questions, including the difference between ATE and QTE, how QTE can be applied in tech companies where ATE fails, and why the experiment on Yahoo! Today Module is not a good example. This reviewer also suggests moving the theorems to Section 3.2 and providing more real-world motivating applications.\n\nA third reviewer, who is a statistician, raises several questions about the assumptions used in the paper and the strategy to choose the arms. This reviewer also asks for clarification on Equation (1), the meaning of Equations (6) and (7), and how the results compare to multi-armed bandit testing with online FDR control.\n\nBased on these reviews, it seems that the paper presents an interesting and potentially useful method for A/B testing, but there are some concerns about the theoretical underpinnings and the presentation of the results. The authors may want to address these concerns by providing more details on the relationships between BAT and other methods, improving the clarity of the writing, and providing more real-world examples and comparisons."
    },
    "47lpv23LDPr": {
        "generation": "work, and why. The authors' choice to have separate encoders for equivariant and invariant representations does not become clear to me from the provided explanations. I would recommend the authors to provide a more detailed explanation why their approach is necessary.\n\nThe authors have addressed my concerns regarding the comparison with the QAE baseline, by including a comparison with a non-equivariant baseline. I still would like to see a comparison with a fully equivariant baseline, as I think this would further show the benefits of their approach.\n\n\n\n\n\n\n"
    },
    "BZ92dxDS3tO": {
        "generation": "\n\nThis manuscript presents a novel approach to object pose estimation that uses a keypoint-free SfM framework to handle low-texture objects. The proposed method, OnePose++, is an extension of the recently proposed OnePose method, and it uses the LoFTR feature extractor to establish 2D-3D correspondences. The authors also introduce a new dataset, OnePose-HARD, which is specifically designed to evaluate the performance of object pose estimation methods on low-texture objects.\n\nThe reviewers generally agree that the proposed method is well-motivated and addresses a significant problem in object pose estimation. They also acknowledge that the method is well-described, and the results are well-presented. However, there are some concerns regarding the novelty of the proposed approach, the choice of baselines, and the evaluation metrics used.\n\nOne reviewer points out that the proposed method is not entirely keypoint-free, as it still uses keypoints from LoFTR. The reviewer suggests that the authors should provide a more thorough discussion of the differences between their approach and other keypoint-free methods. Additionally, the reviewer suggests that the authors should provide a comparison with other state-of-the-art methods, such as Patch2Pix and Dual RCNet, to demonstrate the superiority of their approach.\n\nAnother reviewer raises concerns about the evaluation metrics used in the paper. The reviewer suggests that the authors should provide a more thorough analysis of the results, including a breakdown of the failure cases and a discussion of the limitations of the proposed method.\n\nFinally, one reviewer suggests that the authors should provide more information about the OnePose-HARD dataset, including example images and a description of the annotation process. The reviewer also suggests that the authors should clarify the source of the results of other methods that they compare their method to.\n\nIn summary, the reviewers generally agree that the proposed method is well-motivated and well-described, but there are some concerns regarding the novelty of the approach, the choice of baselines, and the evaluation metrics used. The reviewers suggest that the authors should provide more information about the OnePose-HARD dataset and clarify the source of the results of other methods.\n\nIn conclusion, this meta-review suggests that the authors should address the concerns raised by the reviewers, particularly those related to the"
    },
    "_VjQlMeSB_J": {
        "generation": "\n\nThis paper proposes a simple and effective method to improve the ability of language models to perform complex reasoning. The proposed method, called chain of thought prompting, decomposes reasoning sub-procedures into natural language instructions that guide the language models to generate intermediate reasoning steps leading to the final output. The authors evaluate the effectiveness of the proposed method on arithmetic reasoning, commonsense reasoning, and symbolic reasoning tasks, and demonstrate that it can significantly improve the performance of language models. They also perform comprehensive analyses to understand the effectiveness of the proposed method and provide insights into the property of language models.  The paper has several strengths. First, the proposed method is novel, simple, and effective. It can significantly improve the performance of language models on various reasoning tasks. Second, the paper provides comprehensive and detailed analysis to help establish the claims and draw the conclusions with solid evidence and justification. The detailed analyses provide insights and interesting discussions on the property of language models.  However, the paper also has some limitations. First, the originality and novelty of the paper, particularly from the methodology perspective, is limited. The proposed method is built on the existing few-shot learning framework and does not introduce new techniques or methodologies. Second, the paper claims that the proposed approach is applicable in principle to any tasks that humans can solve via language, but this claim is not well supported in the paper and may need further justification. Third, when annotator A, B, and C wrote chain of thought, their strategies are not discussed in the paper.  In summary, the paper proposes a simple and effective method to improve the ability of language models to perform complex reasoning. The proposed method can significantly improve the performance of language models on various reasoning tasks, and the paper provides comprehensive and detailed analysis to support the claims. However, the paper also has some limitations, such as the limited originality and novelty and the lack of justification for the claim of generalizability. Overall, the paper is a valuable contribution to the field of natural language processing and has the potential to inspire more studies on improving the ability of language models to perform complex reasoning.  This paper was evaluated based on the following reviews:\n\n* Review 1: Strengths: 1, 2, 3. Weaknesses: 1, 2.\n* Review 2: Strengths: 1, "
    },
    "INBO6h9gtG": {
        "generation": "\n\nThe manuscript under consideration is a theoretical paper that explores the problem of estimating the mean of a high-dimensional Gaussian (and sub-Gaussian) distribution under the constraint of differential privacy (DP). The paper proposes two algorithms, one for Gaussian data and another for sub-Gaussian data, that aim to achieve accurate mean estimation while maintaining privacy.\n\nThe reviewers generally agree that the paper presents novel and important results that advance the state of the art in DP statistics. The proposed algorithms are based on clever combinations of existing DP techniques, such as the exponential mechanism, Tukey depth, and propose-test-release (PTR) framework. The paper also provides rigorous privacy and utility analyses for both algorithms.\n\nReviewer 1 notes that the paper's contributions are primarily theoretical, and the proposed algorithms are not computationally efficient. However, the reviewer acknowledges that the results could provide building blocks for future researchers to design efficient DP algorithms for other problems. Reviewer 2 also mentions that the paper's results are of theoretical interest, but the lack of a practical algorithm limits its usefulness.\n\nReviewer 3 raises a concern about the paper's assumption of approximate-DP, which allows for a catastrophic failure of privacy with a small probability. The reviewer suggests that the results cannot be analyzed using Renyi-DP, which is a more robust DP framework.\n\nReviewer 4 provides a detailed summary of the paper's contributions and techniques. The reviewer notes that the proposed method does not estimate the covariance matrix, which is a key distinction from some prior methods. The reviewer also highlights the novel use of Tukey depth and the propose-test-release framework to achieve privacy.\n\nOverall, the reviewers agree that the paper presents important and novel results in the field of DP statistics. However, some reviewers express concerns about the computational efficiency and the assumption of approximate-DP. The paper's contributions are primarily theoretical, and it remains to be seen how the results can be applied in practice.\n\nAs a meta-reviewer, I would recommend accepting the paper with minor revisions. The authors could address the concern about approximate-DP by providing additional discussion on the implications of their results in the context of Renyi-DP. Additionally, the authors could provide more intuition on the use of Tu"
    },
    "H1ldzA4tPr": {
        "generation": "\n\nThis paper proposes a novel method for modeling dynamical systems over graphs by combining graph neural networks with approximate Koopman embedding. The key contribution is reducing parameters by assuming that the interactions between different objects in the Koopman space are limited to some fixed number of types, which allows for a block matrix structure with a small number of parameters. The model is extended to add a control matrix to model external control. The authors claim that their method is 20 times faster than baselines, but no evidence is provided.\n\nThe paper is well-written, and the proposed idea is novel and builds on a sound theoretical framework of Koopman operator theory. The experiments are well-designed, and the results shown in the paper indicate the effectiveness and efficiency of the algorithm in both simulation and control. However, the evaluation is limited to synthetic datasets, and it would be useful to evaluate on similar tasks, such as strings, to draw comparisons.\n\nOne reviewer suggests that the proposed approach could be extended to objects with varying properties, such as objects with connected springs of different stiffness. Another reviewer asks for more details on the training process and whether the model is trained in an end-to-end fashion or in parts.\n\nOverall, the reviewers find the paper interesting and worth publishing, but they raise concerns about the limited evaluation and the lack of evidence for the claimed speedup. They suggest that the authors provide more details on the training process and consider evaluating their method on more diverse and realistic datasets."
    },
    "BJl6bANtwH": {
        "generation": "\n\nMeta-review\n\nThis paper presents a novel method for detecting underdetermination in neural networks and quantifying uncertainty in their predictions. The proposed method, called Local Ensemble, is based on the idea of approximating the ensemble subspace of the Hessian matrix, which is the subspace of parameters that have a small impact on the training loss. The authors show that the Local Ensemble method can be used to detect underdetermination and quantify uncertainty in neural networks, and demonstrate its effectiveness through experiments on synthetic and real-world data.\n\nThe reviewers have provided valuable feedback on the paper, highlighting its strengths and weaknesses. Some of the reviewers have expressed concerns about the novelty of the method and its comparison to other recent works in the field. However, most reviewers agree that the paper presents a well-motivated and well-executed research work that contributes to the growing body of literature on loss-surface inference.\n\nOne of the main concerns raised by the reviewers is the computational efficiency of the proposed method. Some reviewers have noted that the method requires a forward and backward pass for each sample to estimate the gradient, which may be computationally expensive for large datasets. The authors have addressed this concern by pointing out that their method can be performed using mini-batches, which can significantly reduce the computational cost. However, some reviewers have suggested that the authors should provide a more detailed analysis of the efficiency of their method, including a comparison with other approximate ensemble methods.\n\nAnother concern raised by the reviewers is the choice of the extrapolation score, which is based on the norm of the component of the test point's gradient that aligns well with the low curvature directions of the Hessian. Some reviewers have suggested that the authors should provide a more thorough justification for the choice of this score, and explore other possible choices that may be more effective in certain situations.\n\nFinally, some reviewers have raised minor concerns about the presentation of the paper, such as the lack of axis labels in some plots, the choice of AUC as a metric, and the absence of experimental details in the main body of the paper.\n\nOverall, the reviewers have provided constructive feedback that can help improve the paper and enhance its contributions to the field. The authors are encouraged to address the concerns and suggestions raised by the reviewers, and to provide a re"
    },
    "BJeapjA5FX": {
        "generation": "\n\nThis manuscript proposes a novel approach to building robust neural networks using Bayesian Non-Parametric Mixture Models (BNP-MFA) for feature extraction and a kernel density estimate (KDE) for classification. The authors claim that their approach is more robust than traditional neural networks and provide some experiments to support their claim.\n\nThe first reviewer raises some questions about the intuition behind the approach and suggests that more technical details and experiments are needed to support the authors' arguments. They also mention that the problem of scalability needs to be addressed.\n\nThe second reviewer suggests that the proposed model is not substantially different from existing works such as deep kNN and simple cache model, and questions the contribution of the soft label encoding. They also mention that the experiments are incomplete and lacking in variety.\n\nThe third reviewer is more critical, stating that the work lacks evidence to support its claims of robustness and that the experiments are not thorough enough. They suggest that direct attacks such as gradient-based, score-based or decision-based attacks should be used to evaluate robustness.\n\nOverall, the reviews suggest that the manuscript has some interesting ideas, but that more work needs to be done to support the claims of robustness and to demonstrate the advantages of the proposed approach over existing works. The authors should address the concerns raised by the reviewers, provide more technical details and experiments, and better evaluate the robustness of their approach."
    },
    "8pOPKfibVN": {
        "generation": "\n\nThis paper proposes a new method called tailoring, which aims to adapt neural networks at inference time to the given input such that the model minimizes some given unsupervised loss. The paper also proposes meta-tailoring, which trains the model using tailoring. The main contribution of this paper is the introduction of tailoring and meta-tailoring, which provide a different perspective on understanding the \"generalization\" gap and different inductive biases. The paper also provides extensive examples of inductive biases, and the experiment results demonstrate the effectiveness of both tailoring and meta-tailoring.\n\nHowever, there are some concerns and limitations of this paper. First, the method is related to the field of semi-supervised learning, and the authors should provide more discussions on the related works. Second, the method is highly related to the field of semi-supervised learning, and the authors should provide more discussions on the related works. Third, the authors should provide more discussions on how to avoid catastrophic forgetting when conducting tailoring. Fourth, the experiments are quite limited, and the authors should provide more experiments to show the effectiveness of the method in different domains. Fifth, the authors should provide means to overcome the practical problems with implementation in popular deep learning frameworks. Sixth, the authors should provide more discussions on the scalability of the solution. Seventh, the authors should provide more discussions on the negative impact of the method on privacy.\n\nOverall, this paper is a good addition to the ML community, and the proposed method is interesting and novel. The authors have adequately addressed the limitations of their work and societal impact. However, the paper needs some improvements in terms of writing, experimentation, and discussions on related works and limitations."
    },
    "qRDQi3ocgR3": {
        "generation": "\n\nA well-written paper that addresses an important problem in deep learning, the tendency of deep neural networks to prefer certain cues over others when multiple cues are equally likely. The authors propose a framework for studying this phenomenon and conduct empirical studies on several datasets. They introduce a set of metrics to analyze cue preferences from a loss landscape perspective.\n\nThe reviewers generally agree that the paper is well-motivated and well-written, with a clear and concise introduction, a well-defined problem statement, and appropriate methodology. They also appreciate the contributions of the paper, which provide insights into the preference of visual models towards low-complexity visual cues.\n\nHowever, some reviewers raise concerns and suggestions for improvement. One reviewer questions the authors' choice of cues for analysis, suggesting that the selected cues have intrinsically different extraction demands from the stimuli, which may affect the results. Another reviewer suggests that the experiments on the UTKFace dataset are not convincing, as there may be other \"shortcut cues\" outside of the selected ethnicity and age cues.\n\nAdditionally, one reviewer raises a methodological concern, suggesting that the problem statement needs minor clarification, and the main weakness of the paper is that it does not provide surprising or counterintuitive answers. They also suggest that the authors could rephrase the contributions more clearly towards the benefits and provide more impactful contributions.\n\nAnother reviewer suggests that the paper could benefit from additional analyses. They propose an interesting analysis to measure how much one would need to deviate from the diagonal dataset construct to obtain an \"averted solution.\" They also suggest that it would be interesting to see if the model preference for more complex cues would also increase as the parameter count increases.\n\nIn conclusion, the paper provides valuable insights into the preference of visual models towards low-complexity visual cues. However, some reviewers raise concerns and suggestions for improvement, such as clarifying the problem statement, providing more impactful contributions, and conducting additional analyses. Addressing these concerns and suggestions could further strengthen the paper and enhance its contributions to the field of deep learning."
    },
    "SkgVRiC9Km": {
        "generation": "\n\nThis paper proposes a new defense to adversarial examples based on the 'fortification' of hidden layers using a denoising autoencoder. The authors evaluate their defense using FGSM and PGD attacks on CIFAR and report accuracies to a range of epsilon values. However, the evaluation is limited as the authors only use a small range of epsilon values for the PGD attack, and do not benchmark against state-of-the-art defenses.\n\nThe authors motivate the use of denoising autoencoders by suggesting that adversarial examples lie off the data manifold, and that the denoising module can help to bring the adversarial examples back onto the manifold. However, the authors do not compare their approach to a simple regularization loss based on the error between hidden layer representations of natural examples and their corresponding adversarial examples.\n\nOne reviewer notes that the results are hard to parse due to the arrangement of figures and tables, and asks for clarification on which approach is denoted as \u2018baseline adv. Train\u2019 in the tables.\n\nAnother reviewer raises a concern that the method may not effectively defend against more specifically designed algorithms, and questions whether the objective function is harder to find adversarial examples for.\n\nA third reviewer suggests that the paper provides interesting and complete experimentations, but that the major issue of when and where to use fortified layers is left as an open question.\n\nMinor issues include grammar errors in the text and a confusing sentence in the description of the method.\n\nOverall, the paper presents an interesting approach to defending against adversarial examples, but the evaluation is limited, and the authors do not adequately address the concerns raised by the reviewers. The paper could benefit from additional experiments and comparisons to other defenses, as well as further discussion of the limitations and potential applications of the proposed approach."
    },
    "rC3zu-OqnII": {
        "generation": "\n\nReviewers have provided valuable feedback on the paper, \"Asymptotics of the Upper Confidence Bound Algorithm in Multi-Armed Bandits.\" While there are some limitations to the work, the majority of reviewers agree that it makes significant contributions to the field of bandit algorithms and provides new insights into the behavior of the UCB algorithm.\n\nReviewer 1 raises a concern about the paper's focus on the two-armed bandit problem, which may limit its practical applicability. However, Reviewer 3 notes that the paper's results can be applied to more standard bandit algorithms, and Reviewer 5 suggests that the paper sets up seminal directions in bandits literature and could benefit subsequent researchers.\n\nReviewer 2 highlights the incomplete learning aspect of Thompson sampling, which may be a concern for the paper's practical implications. However, Reviewer 4 notes that the paper provides a thorough study of the performance of bandit algorithms and deserves an accept.\n\nReviewer 5 raises some concerns about notation and the extension of results to the multi-armed case. However, they also note that the paper is well-written, and the math is easy to read and verify.\n\nOverall, the reviews suggest that the paper makes significant contributions to the field of bandit algorithms and provides new insights into the behavior of the UCB algorithm. While there are some limitations to the work, the majority of reviewers agree that it is a valuable and well-written paper that deserves to be accepted.\n\nAs a meta-reviewer, I recommend that the authors address the concerns raised by Reviewer 5 regarding notation and the extension of results to the multi-armed case. Additionally, the authors may want to consider providing more discussion on the practical implications of their results and how they could be applied in real-world scenarios.\n\nIn conclusion, the paper \"Asymptotics of the Upper Confidence Bound Algorithm in Multi-Armed Bandits\" makes significant contributions to the field of bandit algorithms and provides new insights into the behavior of the UCB algorithm. While there are some limitations to the work, the majority of reviewers agree that it is a valuable and well-written paper that deserves to be accepted."
    },
    "HJgkx2Aqt7": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a novel approach to optimize the parameters of a simulator using reinforcement learning to improve the performance of a downstream task. The approach aims to maximize the performance of a model trained using the simulated data on real-world data. The paper presents a thorough evaluation of the proposed method and shows promising results.\n\nThe reviewers have raised several valid points regarding the quality, clarity, originality, and significance of the paper.\n\nQuality:\nThe proposed method is sound and well-motivated, but the experimental evaluation is underwhelming. The choice of problems and parameters to tune could have been better, and the comparison with related work is limited.\n\nClarity:\nThe writing of the paper is clear, but the experimental section could be clearer. The authors could have provided more detail on model/hyperparameter selection and the selection of the validation set.\n\nOriginality:\nThe proposed method is novel, but the related work is missing important previous papers that have proposed similar algorithms for fitting simulator parameters.\n\nSignificance:\nThe significance of the paper is moderate given some similar previous works, but the significance of the method itself is important.\n\nOverall, the paper presents a valuable contribution to the field of simulator optimization using reinforcement learning. However, the experimental evaluation could be improved, and the related work could be expanded to include more relevant previous research. The authors could also consider providing more detail on the validation set and the selection of parameters for the outer loop."
    },
    "BJx040EFvH": {
        "generation": "The manuscript is a research article titled \"A Simple and Efficient Method for Training Robust Neural Networks against Adversarial Attacks\" by Xu et al.\n\nThe meta-review should summarize the main points of the reviews and provide an overall assessment of the manuscript. It should also highlight any major concerns or disagreements among the reviewers and suggest ways to address them.\n\nThe meta-review should be written in a professional and objective tone, and should not express any personal opinions or biases.\n\nPlease use the following template for the meta-review:\n\nMeta-Review:\n\nTitle: A Simple and Efficient Method for Training Robust Neural Networks against Adversarial Attacks\n\nAuthors: Xu et al.\n\nReviews: 3\n\nOverall, the manuscript presents a novel approach to training robust neural networks against adversarial attacks, which has been a long-standing problem in the field of machine learning. The proposed method combines randomization and fast gradient sign method (FGSM) adversarial training with optimization tricks like cyclic learning rate and mixed precision. The authors claim that this approach can train models that are robust to strong PGD attacks while significantly reducing the training time.\n\nThe reviewers generally agree that the manuscript presents a surprising and interesting finding, and that the proposed method is simple and easy to implement. However, there are some concerns and disagreements among the reviewers regarding the originality and technical contributions of the manuscript, as well as the fairness and reproducibility of the experiments.\n\nReviewer 1 raises several concerns about the experimental design and implementation, including the choice of step size, the lack of projection onto the feasible set, and the potential for label leakage. Reviewer 2 also questions the originality of the method and suggests that it is similar to previous methods, although the authors claim that their implementation is different. Reviewer 3 raises concerns about the fairness of the experimental comparison, including the use of early stopping and the lack of update time for the CIFAR-10 results.\n\nTo address these concerns, the authors could provide more details about their implementation and the differences between their method and previous approaches. They could also provide additional experimental results to demonstrate the robustness and efficiency of their method, and to compare it with other state-of-the-art methods.\n\nIn summary, the manuscript"
    },
    "bJz3cFePTna": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a novel method for mixture proportion estimation and positive-unlabeled (PU) learning. The proposed method, called TED, combines two novel algorithms: Best Bin Estimation (BBE) and Conditional Value Under Optimism (CVuO). BBE is a method for estimating the fraction of positives among unlabeled examples, while CVuO is a method for PU learning that assumes the fraction of positive examples to be known. The authors provide a finite sample guarantee for BBE and empirically verify its performance. They also propose an algorithm, $TED^n$, that combines BBE and CVuO. The methods are tested on several semisynthetic problems created from CIFAR, MNIST, and IMDB datasets and are shown to outperform several recent methods.\n\nThe reviews generally agree that the paper is technically sound and the proposed methods are novel and interesting. However, there are some concerns and suggestions for improvement. Some reviewers feel that the paper could benefit from a higher-level explanation of the core idea behind BBE, and that the limits of the proposed method are not emphasized enough. Additionally, some reviewers suggest that the authors provide more detailed analyses for experimental results and conduct an ablation study to discuss the influence of hyperparameters.\n\nOverall, the reviews suggest that the paper makes important contributions to the field of PU learning and mixture proportion estimation, and that the proposed methods are effective and novel. However, some improvements could be made to the paper to make it more clear and comprehensive.\n\nRecommendations:\n\n1. Provide a higher-level explanation of the core idea behind BBE to help readers understand it better.\n2. Emphasize the limits of the proposed method and discuss the conditions under which it is likely to perform well.\n3. Provide more detailed analyses for experimental results to help readers understand the performance of the proposed method better.\n4. Conduct an ablation study to discuss the influence of hyperparameters on the performance of the proposed method.\n5. Consider adding more discussion on the related work and how the proposed method compares to existing methods in the field.\n\nAfter addressing these concerns and suggestions, the paper is likely to be even more solid and well-received by the community."
    },
    "iBBcRUlOAPR": {
        "generation": "\n\nMeta-Review:\n\nThis paper presents an empirical study that explores the scaling law for training transformer-based language models. The study investigates the trade-off between model size and training tokens, and finds that the optimal scaling way is to increase the size of training data equally while enlarging the model size. The authors claim that the current LLMs are under-trained and their findings suggest that the model size and training data should be scaled equally. They train a 70B parameter model called Chinchilla, which outperforms the Gopher 280B model with the same compute budget.\n\nStrengths:\n\n* The paper provides valuable insights into the scaling law for training LLMs, which can help researchers and practitioners in the field to optimize their models more effectively.\n* The study is well-designed and well-executed, with extensive experiments that cover a wide range of model sizes and training tokens.\n* The authors provide strong results that demonstrate the effectiveness of their proposed scaling law, and their findings have the potential to impact the future development of LLMs.\n\nWeaknesses:\n\n* The study only focuses on autoregressive models, and it is unclear whether the findings can be applied to BERT-like models.\n* The study does not consider the impact of data quality on the scaling law, which could be an important factor to consider.\n* The authors do not provide code or models for reproducibility, which is a limitation of the study.\n\nSocietal Impact:\n\nThe paper has the potential to positively impact society by providing insights that can help improve the development of LLMs, which can be used for a wide range of applications such as language translation, text summarization, and chatbots. However, there is also the potential for negative impact if the models are used for malicious purposes such as generating fake news or propaganda.\n\nOverall, the paper provides valuable insights into the scaling law for training LLMs and has the potential to impact the future development of these models. However, the study has some limitations, and the authors should consider addressing these limitations in future work. Additionally, the authors should provide code and models for reproducibility and consider the potential societal impact of their work."
    },
    "rRFIni1CYmy": {
        "generation": "\n\n1. The paper is innovative and has the potential to be influential in the field.\n2. The proposed method is well-motivated and has a clear contribution.\n3. The experimental evaluation is well-designed and well-executed.\n4. The results are convincing and support the claims made in the paper.\n5. The paper is well-written and easy to follow.\n6. The paper addresses a significant problem in the field and provides a meaningful solution.\n7. The paper compares the proposed method with relevant baselines and state-of-the-art methods.\n8. The paper provides sufficient detail for replication, and the authors have made their code and data available.\n\nHowever, some reviewers have raised concerns about the paper's clarity and organization. Specifically, one reviewer found the method section confusing and difficult to follow, with errors in notation and variables that are not properly explained. Another reviewer suggested that the paper's experimental evaluation could be improved by better explaining the experimental setup and providing more baselines for comparison.\n\nOverall, the reviewers have identified both strengths and weaknesses in the paper. While the proposed method is innovative and has the potential to be influential, the paper could benefit from improvements in clarity and organization. Additionally, more baselines and experimental setup details would help strengthen the paper's conclusions.\n\nBased on the reviews and discussions, I recommend that the authors address the concerns raised by the reviewers and revise the paper accordingly. Specifically, the authors should clarify and organize the method section better, provide more detail about the experimental setup, and include more baselines for comparison. Once these revisions are made, the paper will be well-positioned to make a meaningful contribution to the field."
    },
    "rylDzTEKwr": {
        "generation": "\n\nThe manuscript proposes a new method for hashing-based collaborative filtering, which is a well-studied problem in recommender systems. The proposed method, called variational hashing-based collaborative filtering with self-masking (VaHSM-CF), aims to improve the efficiency of the previous hashing-based methods without sacrificing much accuracy. The authors introduce a personalized self-masking technique to learn binary vector representations of the users' and items' latent representations. The proposed method is trained using a variational encoder and a straight-through estimator. The authors conduct experiments on four public datasets and show that the proposed method outperforms other state-of-the-art methods.\n\nThe reviewers have raised several concerns and suggestions about the manuscript. Reviewer 1 suggests including more accurate algorithms than the basic MF algorithm used in the empirical studies. Reviewer 2 suggests providing a brief explanation of choosing the baseline methods in the context of other hashing-based CF methods. Reviewer 3 suggests providing more details on the number of latent dimensions in MF and clarifying the presentation of the parameter setting.\n\nReviewer 4 raises concerns about using VAE to model the ratings of each user and item pair. They suggest that VAE needs to learn from a lot of data, not just one data point, and that the model may not have learned anything useful. They also suggest that the idea of using discrete VAE for hashing tasks has been explored before, and that the novelty of the model is limited.\n\nReviewer 5 raises concerns about the experimental results, suggesting that the reported results of MF and its variants are lower than those of DCF, which is unreasonable. They also suggest that the results of CCCF are inconsistent with the results in the original paper. They doubt that there is much accuracy improvement compared to state-of-the-art methods, and suggest that the motivations of the designed hashing method are vague if there are no advantages on recommendation accuracy and efficiency.\n\nOverall, the reviewers have raised several valid concerns and suggestions about the manuscript. The authors should address these concerns and provide more convincing evidence to support their claims. The manuscript has the potential to be a valuable contribution to the field of recommender systems, but it needs further improvement before it can be accepted for publication."
    },
    "BkSDMA36Z": {
        "generation": "\n\nMeta-Review\n\nThis manuscript presents a simple and efficient method for text classification using a novel technique called \"region embeddings\". The proposed method has achieved state-of-the-art results on several benchmark datasets, outperforming existing methods including d-LSTMs and BoWs. The key idea of the method is to learn vector representations of fixed-size text regions of up to a few words by using a \"context unit\" of size d x K, where d is the embedding size and K the region size. The authors propose two different models, word-context and context-word region embeddings, which combine the word embeddings and context units in different ways. The models are trained jointly on a supervised text classification task, and the authors also perform ablation studies and analysis of the influence of parameters.\n\nThe reviewers generally agree that the proposed method is sound and well justified, and the empirical evaluations and comparisons to existing methods are well executed. The authors are also praised for the simplicity and efficiency of the proposed model. However, some reviewers have raised concerns about the writing quality, notation, and qualitative analysis, which could be improved. Additionally, the authors are asked to provide more details on the training process and computational efficiency, and to discuss the relationship between their method and the \"attention is all you need\" paper by Vaswani et al.\n\nOverall, the reviews suggest that the manuscript is a strong contribution to the field of text classification, and the proposed method has the potential to be widely adopted. However, some improvements in the writing and presentation could further strengthen the paper.\n\nRecommendations:\n\n* Improve the writing quality and notation to make the paper more clear and concise.\n* Provide more details on the training process and computational efficiency.\n* Discuss the relationship between the proposed method and the \"attention is all you need\" paper by Vaswani et al.\n* Consider adding more analyses to further understand the properties of the learned embeddings and their relationship to semantic meaning.\n* Provide more detailed explanations and examples of the \"context units\" and how they capture the importance of relative position.\n* Consider adding more baselines and comparisons to other recent methods in the field."
    },
    "vuFJO_W85VU": {
        "generation": "\n\nThis paper proposes a novel approach to policy optimization in reinforcement learning (RL) called iterative amortization. The approach leverages the connection between amortized variational inference and policy-based RL with function approximation. The authors claim that their approach can improve performance, accuracy, and flexibility in the optimization process. The paper presents experimental results to support their claims and discusses the benefits and limitations of their approach.  \n\nThe reviewers generally agree that the paper is well-written and easy to follow, and that the proposed approach is novel and original. They also agree that the experimental results are comprehensive and demonstrate the benefits of the proposed approach. However, they also raise some concerns and suggestions for improvement.  \n\nOne reviewer notes that the added complexity of the proposed method is not discussed, and suggests that the authors should provide a detailed analysis of the tradeoffs between performance improvement and computational expense. Another reviewer asks for more discussion on the properties of an environment that make iterative amortization more or less useful, and how these properties vary among the tasks used in the paper.  \n\nAnother reviewer raises a minor point about the formatting of a figure, and suggests that the authors should mark the exact section of the appendix in which supplementary results appear.  \n\nFinally, one reviewer notes that while the raw performance improvement due to the proposed approach is not generally notable, the main significance of this work lies in its novelty and potential for follow-up work. They suggest that some theoretical analysis would be helpful in developing a deeper understanding of the proposed approach.  \n\nOverall, the reviewers agree that the paper is well-written, presents novel and interesting ideas, and provides comprehensive experimental results. However, they also suggest that the authors should address some of the limitations and concerns raised in the reviews to further improve the paper. \n\nBased on the above reviews and discussions, I recommend accepting the paper with minor revisions to address the concerns and suggestions raised by the reviewers."
    },
    "XQu7UFSbzd2": {
        "generation": "\n\nThis paper presents a new approach to event sequence modeling that addresses the problem of distribution shift by incorporating causal inference and variational inference. The proposed method, called CaseQ, combines a causal \"backdoor adjustment\" to correct for confounding latent context, variational inference to infer the distribution over the latent context, and a neural network with a \"hierarchical branching structure\" for the inference. The approach is evaluated on several benchmark datasets, and the results show that CaseQ can mitigate accuracy drops when the gap between the training and test sets grows over time.\n\nThe reviewers generally found the paper to be well-written and the proposed approach innovative. They appreciated the attention to addressing the distribution shift problem, which is a critical issue in many real-world applications. However, some reviewers raised concerns about the complexity of the proposed method and questioned whether it is necessary to combine so many different ideas to achieve the desired results.\n\nOne reviewer suggested that the authors consider simplifications of the proposed framework to better understand its effectiveness. Another reviewer pointed out that the problem formulation may not be well-motivated for real-world applications, as the goal of predicting events in the remote future may not be appropriate when the event to be predicted will most likely not be the \"next\" event.\n\nSome reviewers also raised questions about the novelty of the specific model instantiations and the lack of discussion on the limitations of the proposed method. One reviewer suggested that the authors discuss other potential reasons for temporal distribution shift and how their current model may try to capture these other reasons.\n\nAnother concern raised by the reviewers was the lack of evaluation on a benchmark where changes in context are known to test if the learned contexts map to domain knowledge. Additionally, reviewers questioned how the model deals with novel or derivative contexts and how sensitive the model is to the number of contexts when dealing with datasets of differing scales and changes in distributions.\n\nOverall, the reviewers found the paper to be interesting and innovative, but they raised several concerns that need to be addressed before it can be considered for publication. The authors are encouraged to revise the paper and address the concerns raised by the reviewers."
    },
    "yqPnIRhHtZv": {
        "generation": "to 'Kusano et\n  al. proposed/stated/...'\n\n- The authors are using the phrase 'the latter' a few times in the\n  paper. While it is clear what is meant, it is sometimes not entirely\n  clear to which statement or part of the previous sentence the\n  authors are referring. I would suggest to use pronouns to refer to\n  statements or phrases instead of using 'latter'; this would\n  increase clarity and readability.\n\n- The authors cite their own work in the main text. While I am aware\n  that this is not uncommon, it can come across as slightly\n  self-aggrandizing. I would suggest to move the citation to the\n  supplements or to omit it altogether and point the reader to the\n  supplements, where the interested reader can find a more detailed\n  explanation.\n\n- In the caption of Fig. 1, I would replace 'Persistence diagrams' with\n  'Topological features'; this would make the caption more concise and\n  descriptive.\n\n- In the caption of Fig. 4, I would replace 'Ablation study' with\n  'Effect of hyperbolic embedding'; this would help the reader better\n  understand the purpose of this figure.\n\n- Finally, I would suggest to use more descriptive labels for the axes\n  in the diagrams. For example, in Fig. 2, the authors could\n  relabel the axes as follows:\n\n   x-axis: 'Hyperbolic distance' (or 'Distance to hyperbolic center'.)\n   y-axis: 'Feature similarity' (or 'Persistent feature similarity'.)\n\n   This would make the figure more accessible and informative to a\n   casual reader.\n\n- In the main text, I would suggest to use more descriptive section\n   headings. For example, instead of 'Evaluation', one could use\n   'Experiments with graph classification and image classification'.\n   The authors could also add a new section describing the method in\n   more detail, 'Ablation study', 'Effect of hyperbolic embedding', or\n   'Choices of the hyperbolic embedding'.\n\n- I would suggest to replace 'Appendix A' with 'Supplementary material' or"
    },
    "eNB4WXnNczJ": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a new method for distributed optimization, called CANITA, which combines acceleration and compression techniques. The method is proven to achieve a faster convergence rate than previous methods for non-strongly convex problems. The theoretical analysis is solid, and the paper is well-written and clear.\n\nHowever, some reviewers have raised concerns about the practicality of the studied setting, as it requires each node to compute and communicate its full gradient, which may not be practical in many real-world scenarios. Additionally, some reviewers have suggested including numerical experiments to compare CANITA with other distributed communication-efficient optimization methods.\n\nOverall, the paper presents a novel contribution to the field of distributed optimization, and the proposed method has the potential to improve the efficiency of distributed optimization algorithms. However, further research is needed to address the practicality concerns and to compare the performance of CANITA with other state-of-the-art methods.\n\nRecommendations:\n\n1. Include numerical experiments to compare the performance of CANITA with other distributed communication-efficient optimization methods.\n2. Consider exploring the use of stochastic gradients or other approximate gradient methods to reduce the computational complexity of the method.\n3. Discuss the practicality concerns and potential limitations of the method in the manuscript.\n4. Provide a more detailed comparison of the proposed method with existing methods, including a discussion of the trade-offs between convergence rate, communication efficiency, and computational complexity."
    },
    "-geBFMKGlkq": {
        "generation": "\n\nThis paper proposes a novel density function for density-based clustering methods, such as DBSCAN and DPC, based on a diffusion process. The proposed density function, called FKD, is computationally efficient and has been shown to have promising results in experiments. However, some concerns have been raised regarding the theoretical justification and experimental validation.\n\nOne of the reviewers questions whether the proposed method is able to capture the local characteristics of the dataset, and another reviewer points out that the computational complexity of computing \\rho_{KD} is not discussed. Additionally, the authors have not provided a thorough comparison of the FKD method with other clustering methods, such as spectral clustering, which could help to position their method in the existing literature.\n\nAnother concern is related to the experimental setup. The authors have used hyperparameter tuning, which is not practical for real-life clustering applications. Furthermore, the authors have not provided NMI values for the datasets used in their experiments, which is a common measure for evaluating clustering performance.\n\nOverall, the reviewers agree that the proposed idea is interesting, but further work is needed to address the concerns regarding theoretical justification and experimental validation. The authors are encouraged to provide a more thorough comparison of their method with other clustering methods, to better justify their choice of diffusion process, and to provide more detailed experimental results.\n\nFurthermore, the authors are encouraged to address the following specific points raised by the reviewers:\n\n* Provide a theoretical justification for why the proposed method should work well in practice.\n* Discuss the computational complexity of computing \\rho_{KD}.\n* Provide a thorough comparison of the FKD method with other clustering methods, such as spectral clustering.\n* Address the issue of boundary bias in kernel density estimation.\n* Provide NMI values for the datasets used in the experiments.\n* Include a discussion on the choice of the hyperparameter h and how it affects the performance of the method.\n* Provide more detailed experimental results, including the performance of the method on different types of datasets and with different parameter settings.\n\nBy addressing these points, the authors can improve the quality of their paper and provide a stronger contribution to the field of density-based clustering."
    },
    "Ehhk6jyas6v": {
        "generation": "m not sure if this is an aggregate over many concepts or what\n- in general, the discussion of the metrics here feels a bit light - it would be good to see some more analysis of why we want to use these metrics, how they relate to the underlying properties of the data, and how they can help us better understand the limitations of the methods in question\n\nThis paper proposes two new metrics for evaluating the quality of learned concepts in the context of concept-based representation learning (CL) and disentanglement learning (DGL). The first metric, Oracle Impurity Score (OIS), measures how well the learned concepts predict the ground truth concepts, while the second metric, Niching Score (NS), assesses the degree to which the learned concepts can be grouped into useful and useless categories for predicting a particular output dimension. The authors evaluate the proposed metrics on several state-of-the-art methods in both CL and DGL.\n\nThe paper is well-structured and easy to follow, and the proposed metrics seem to be well-motivated and meaningful. However, there are a few areas where the paper could be improved. Firstly, the introduction could be more engaging, providing more background on the importance of concept-based representation learning and disentanglement learning, and how the proposed metrics can help in this regard.\n\nSecondly, the authors could have provided more discussion on the limitations and potential biases of the proposed metrics, as well as how they relate to other metrics that have been proposed in the literature. Additionally, the authors could have included more analysis of the results, particularly in the context of the downstream task, to help readers better understand the implications of the proposed metrics.\n\nFinally, the paper could benefit from additional clarity on the relationship between the proposed metrics and the underlying factors that they are designed to capture. Overall, this paper provides valuable contributions to the field of concept-based representation learning and disentanglement learning, and the proposed metrics, OIS and NS, seem like important additions to the toolkit of evaluation methods for these areas.\nThe authors introduce two new metrics, Oracle Impurity Score (OIS) and Niching Score (NS), to evaluate the quality of learned concepts in concept-based representation learning (CL) and disentanglement learning (DGL) in this paper. The OIS measures how well the learned concepts predict the ground truth concepts, while the"
    },
    "ZDaSIkWT-AP": {
        "generation": "\n\nThis paper presents a method for using case-based reasoning (CBR) to improve the performance of reinforcement learning (RL) agents on text-based games. The method involves representing states as knowledge graphs and using a vector-quantized encoding scheme to enable efficient retrieval of relevant contexts. The paper includes experiments that show the effectiveness of the proposed approach, including improved performance on out-of-distribution environments.\n\nThe reviewers generally found the paper to be well-written and the proposed approach to be interesting and potentially valuable. However, some reviewers raised concerns about the novelty of the approach and the lack of comparison to other recent methods that use transformers for text-based RL. Additionally, some reviewers had questions about the design choices made in the retrieval and retain modules, and the use of a learned discretization scheme.\n\nOverall, the reviewers seem to agree that the paper presents a solid approach to combining CBR and RL, and that the experiments are thorough and convincing. However, some reviewers have raised important questions about the novelty and design choices of the approach, and the need for further analysis and ablations to fully understand the contributions of the paper.\n\nBased on the above reviews, I would recommend that the authors address the following points in their revision:\n\n1. Provide a more thorough comparison to other recent methods that use transformers for text-based RL, and discuss how the proposed approach differs and potentially benefits from the use of CBR.\n2. Provide more justification and analysis of the design choices made in the retrieval and retain modules, and consider performing ablations to better understand the contributions of these components.\n3. Address the concern about the use of a learned discretization scheme, and consider alternative methods such as locality-sensitive hashing.\n4. Clarify the threshold selection for the retriever module and the context selectors used in the algorithm.\n5. Provide more detail on how the model chooses between two actions with the same template, and consider including an example in the paper to illustrate this process.\n\nBy addressing these points, the authors can strengthen the paper and provide a more comprehensive understanding of the proposed approach and its contributions to the field of text-based RL."
    },
    "GjWDguPZRmr": {
        "generation": "\n\nThe manuscript under review is a research article titled \"Density-Gap VAEs\" by [Author's Name]. The article proposes a new approach to training Variational Autoencoders (VAEs) that aims to address two common problems in deep generative modeling: the \"hole problem\" and \"posterior collapse.\" The authors claim that their method, called Density-Gap VAEs (DG-VAE), effectively addresses these issues and provides better performance in terms of mutual information and latent space visualizations compared to several baselines.\n\nThe paper has received positive feedback from reviewers, who appreciate the clear explanation of the hole problem and posterior collapse, as well as the proposed solution. The authors have also provided convincing empirical results that demonstrate the effectiveness of their method.\n\nHowever, some reviewers have raised concerns about the clarity of the method's explanation, noting that some parts of the paper are difficult to follow. Additionally, some reviewers have questioned the evaluation methodology, requesting more details on the experimental setup and suggesting that the interpolation task used to measure latent-guided generation may not be the most convincing.\n\nOne reviewer has also raised an important question regarding the limitations of the proposed method, specifically whether it is still suitable for tasks such as density estimation and out-of-distribution detection, given that the training objective is no longer a lower bound on the true log likelihood of the data.\n\nOverall, the manuscript has received positive feedback, but there are some areas where clarity and evaluation methodology could be improved. The authors are encouraged to address these concerns and provide a more comprehensive evaluation of their method's performance and limitations."
    },
    "_A4-JP8d_f": {
        "generation": "\n\nVariational inference is a powerful tool for approximate Bayesian inference, but the choices that need to be made when using it can significantly impact the quality of the approximation. This paper proposes a conceptual framework and design guidelines for variational inference based on the pre-asymptotic behavior of the unnormalized importance weight, using Pareto smoothed importance sampling (PSIS). The authors evaluate the behavior of commonly used variational families and f-divergences in mid-to-high dimensions and find that mass-covering divergences, while often superior theoretically and for low-dimensional problems, are hard to optimize in mid-to-high dimensions compared to the mode-seeking exclusive KL-divergence. They also observe that the tail index of the best-fit generalized Pareto distribution can be used to predict the behavior of variational inference methods. The paper includes actionable advice for practitioners, such as using flows with exclusive KL and PSIS as a good default.\n\nThe reviewers generally found the paper to be well-written and significant, with one reviewer stating that it represents a useful and needed direction for the field. However, some reviewers raised concerns about the lack of a proper review of prior work and the need for more rigorous proofs or justifications to generalize the conclusions to other application scenarios. One reviewer also suggested providing a table to summarize the conclusions in the paper and including a discussion of the limitations of the work.\n\nThe reviewers also raised some minor technical issues, such as a problem with the notation (D is used for divergence and dimension) and a missing factor in the denominator of the normalized importance weight. One reviewer also asked for clarification on how to actually use the PSIS correction to improve the estimator and requested additional results for the other divergences in the appendix.\n\nOverall, the reviewers found the paper to be valuable and useful for practitioners, but also identified areas for improvement and further research. The authors are encouraged to address these concerns in a revised version of the manuscript."
    },
    "zgMPc_48Zb": {
        "generation": "\n\nThe submitted manuscript proposes a differentially private method for training a generative model using the Sinkhorn divergence. The method is based on the idea of training a generative model with Sinkhorn divergence and adding Gaussian noise to the gradient of the output w.r.t. generated samples. The authors claim that their method is robust against the choice of hyperparameters, but they do not provide any evidence to support this claim.\n\nThe reviews are mixed, with some reviewers finding the paper interesting and others having reservations. Reviewer 1 is generally positive, highlighting the novelty of the approach and its potential for improving the state-of-the-art in differentially private generative models. Reviewer 2, on the other hand, is more critical, pointing out that the method has low originality and is not well motivated. They also question the robustness of the method against the choice of hyperparameters and suggest that the proposed approach may not be the best way to achieve high accuracy in downstream classification tasks. Reviewer 3 raises similar concerns, suggesting that the method may not be novel and that the authors do not adequately address the relationship between their work and previous research in the field.\n\nOverall, the reviews suggest that the manuscript has some interesting ideas but may benefit from further development and refinement. The authors may want to address the concerns raised by the reviewers, particularly with regard to the originality and motivation of the method, the robustness of the method against hyperparameter choice, and the relationship to previous research in the field. Additionally, the authors may want to consider providing more experimental results to support their claims and to better understand the performance of their method in different settings."
    },
    "SkxxIs0qY7": {
        "generation": "\n\nMeta-Review:\n\nThis paper presents a novel approach to generative modeling of sequence data, called CoT, which replaces the discriminator of a GAN with a \"mediator\" that estimates the mixture of the data and generator distributions. The mediator is used to compute the Jensen-Shannon divergence, which is then optimized by the generator. The paper presents theoretical convergence results and promising experiments on a synthetic Turing test and text generation from EMNLP's 2017 news dataset.\n\nThe reviewers generally found the paper to be clear and interesting, with some minor grammatical errors and odd formulations. They also noted that the paper's approach is novel and has potential advantages over existing methods for discrete sequential data. However, some reviewers expressed concerns about the limitations of the approach, such as its reliance on the Jensen-Shannon divergence and its potential inability to extend to other types of divergences.\n\nOne reviewer asked for more information about the experimental setup and suggested providing other metrics to evaluate diversity, such as self-BLEU. Another reviewer suggested providing more detail on how the number of samples affects CoT and how the approach can be applied to continuous data.\n\nOverall, the reviewers found the paper to be a valuable contribution to the field of generative modeling, but with some minor flaws and limitations that can be addressed in future work."
    },
    "5dHQyEcYDgA": {
        "generation": "\n\nThis paper proposes a novel approach to improve the interpretability of attention-based multiple instance learning (MIL) models in digital pathology. The authors introduce an additive predictor function, which computes the prediction for each instance and then combines them additively to obtain the final prediction. This approach allows for straightforward patch credit assignment and improves the interpretability of the model. The authors demonstrate the effectiveness of their method on three histology datasets and show that it does not degrade the performance of the state-of-the-art MIL models.\n\nThe reviewers generally agree that the paper presents a valuable contribution to the field of MIL and digital pathology. They praise the clarity and quality of the paper, the significance of the method, and the thorough experimental study. The reviewers also appreciate the effort made by the authors to annotate cancer regions on one dataset to quantify the improvement in explainability.\n\nHowever, some reviewers raise some concerns and suggestions. One reviewer notes that the true novelty of the paper is somewhat limited, as it combines two existing methods, attention-based MIL and additive attribution models. Another reviewer suggests that the authors should include results on the use of Additive MIL without an attention module to disentangle the importance of attention and additive MIL for predictive performance.\n\nAdditionally, one reviewer raises a concern about the theoretical guarantee provided by the authors, which states that the additive MIL method does not degrade the performance of the state-of-the-art MIL methods. The reviewer suggests that this guarantee may not hold true for all datasets and MIL methods.\n\nFinally, one reviewer suggests that the authors should obtain pathologist segmentation of tumor subtypes to evaluate better if the model heatmap aligns with human expectation. The reviewer also suggests that the figures could be labeled more clearly, and that the authors should provide more information about the computational cost of their method compared to the state-of-the-art.\n\nOverall, the reviewers find the paper to be well-written and well-researched, but suggest some improvements to further strengthen the contributions and impact of the paper."
    },
    "0RDcd5Axok": {
        "generation": "\n\nRevised score: 4.4 out of 5.\n\nThis paper presents a unified framework for parameter-efficient transfer learning in NLP, which encompasses various recent approaches such as adapters, prefix tuning, and LoRA. The authors analyze the design dimensions of these methods and propose new variants, including parallel adapters, multi-head adapters, and scaled adapters. The paper also conducts a thorough evaluation of these methods on four NLP tasks, shedding light on their performance differences and limitations.\n\nThe strengths of this paper include its comprehensive analysis of parameter-efficient methods, the introduction of new variants that improve upon existing approaches, and the well-designed experiments that provide valuable insights into the performance of these methods. The paper is well-organized, well-motivated, and well-written, making it accessible to a broad audience in NLP.\n\nHowever, some reviewers have raised concerns regarding the lack of standard deviations in the results, which makes it difficult to assess the significance of the performance gaps between different methods. Additionally, the focus on generative tasks and encoder-decoder models limits the generalizability of the findings to other types of NLP tasks and architectures.\n\nOverall, this paper provides valuable contributions to the field of parameter-efficient NLP and paves the way for future research in this area. The proposed methods and analyses will be useful for researchers and practitioners who work on deploying NLP models at scale. With some minor revisions to address the concerns raised by the reviewers, this paper is ready for publication.\n\nRecommendation:\n\nAccept with minor revisions.\n\nNote: This meta-review is based on the reviews and discussions provided, and it represents a summary of the strengths and weaknesses of the paper. The recommended revision is minor and focused on addressing the concerns related to the lack of standard deviations and the limited generalizability of the findings."
    },
    "4azYdmhHCG": {
        "generation": "\n\nThe paper discusses the limitations of existing ensemble-based debiasing (EBD) methods, which rely on a bias-only model to estimate the uncertainty of the biased model. The paper shows, both theoretically and empirically, that the bias-only model often overestimates its uncertainty, leading to suboptimal debiasing performance. The authors propose a simple modification to EBD methods, called MoCaD, that calibrates the bias-only model to improve its uncertainty estimation. The paper provides thorough theoretical and empirical analysis of MoCaD and shows that it consistently improves debiasing performance across multiple datasets and methods.\n\nThe reviewers generally found the paper well-written, well-structured, and well-motivated. They appreciated the novelty of the paper's contributions, particularly the connection between uncertainty calibration and EBD. The reviewers also acknowledged the paper's limitations, such as the inconsistent improvement on image tasks and the focus on only two NLP tasks.\n\nOne reviewer suggested considering more expressive calibration methods, such as the more expressive temperature scaling, and nonparametric calibration. They also recommended discussing the difference between the two plots in Figure 1 and the choice of tasks.\n\nAnother reviewer asked for more discussion on improving the bias-only model and the advantage of the joint bias and debiased model approach that was not studied in the paper. They also suggested exploring and explaining the inconsistent improvement on image tasks.\n\nOverall, the reviewers found the paper's contributions novel and valuable, and they provided constructive suggestions for improvement. The authors have addressed the limitations of the paper and provided additional experiments and discussions in their rebuttal. The paper has the potential to inspire further research in ensemble-based debiasing methods and to improve the performance of debiased models in various applications."
    },
    "2zCRcTafea": {
        "generation": "\n\nThe manuscript presents a novel attention mechanism for vision transformer called focal attention, which enables efficient long-range interactions. The authors demonstrate its performance on image classification and object detection tasks and propose a new focal transformer by plugging the proposed module into a multi-scale vision transformer model. The paper is well-written, and the experiments are conducted on various benchmarks with extensive ablation studies. \n\nHowever, there are some concerns and suggestions from the reviewers. Reviewer 1 suggests that the paper lacks a discussion on model efficiency and requests additional experimental comparisons with other efficient transformer methods. Reviewer 2 wonders about the inference speed of the method and suggests reporting the speed and comparing it with other methods, especially on object detection and segmentation tasks. Reviewer 3 raises a question about the number of levels in the actual model configuration and suggests doing ablation studies on this aspect. Reviewer 4 points out a potential issue in handling resolution mismatches between the attention mechanism and input resolution. \n\nTo address these concerns, the authors could consider adding more discussions on model efficiency and comparing the proposed method with other efficient transformer methods. They could also report the inference speed and compare it with other methods on object detection and segmentation tasks. Moreover, the authors could conduct ablation studies on the number of levels in the attention mechanism and explain how they handle resolution mismatches between the attention mechanism and input resolution. \n\nOverall, the manuscript presents a novel attention mechanism for vision transformer and demonstrates its performance on various benchmarks. However, the authors should address the concerns and suggestions raised by the reviewers to further improve the paper's quality and significance."
    },
    "TlS3LBoDj3Z": {
        "generation": "\n\nThis manuscript presents an improved version of QTRAN, a theoretically justified algorithm for cooperative multiagent learning with centralised learning and decentralised execution. The proposed improvements, QTRAN++, aim to address the limited performance of the original algorithm. The paper presents strong empirical results on the SMAC benchmark, outperforming several baselines.\n\nReviewers have raised several concerns and suggestions for improvement. One reviewer questions the stability and convergence of the algorithm, as well as the choice of architecture and hyperparameters. Another reviewer raises concerns about the performance on other benchmarks and the lack of comparison to other related works.\n\nAfter discussions and updates, the authors have addressed some of the concerns and provided additional experiments and explanations. However, some concerns remain, particularly regarding the performance on other benchmarks and the comparison to other related works.\n\nBased on the reviews and discussions, I recommend a marginal acceptance of the manuscript. While the proposed improvements to QTRAN are interesting and show promising results on the SMAC benchmark, further investigation and comparison to other related works are needed to fully establish the contributions and limitations of the proposed algorithm.\n\nThe authors should consider addressing the remaining concerns and suggestions for improvement, and provide a more comprehensive evaluation of the algorithm's performance on different benchmarks and in comparison to other related works.\n\nThis manuscript presents an improved version of QTRAN, a theoretically justified algorithm for cooperative multiagent learning with centralised learning and decentralised execution. The proposed improvements, QTRAN++, aim to address the limited performance of the original algorithm. The paper presents strong empirical results on the SMAC benchmark, outperforming several baselines.\n\nReviewers have raised several concerns and suggestions for improvement. One reviewer questions the stability and convergence of the algorithm, as well as the choice of architecture and hyperparameters. Another reviewer raises concerns about the performance on other benchmarks and the lack of comparison to other related works.\n\nAfter discussions and updates, the authors have addressed some of the concerns and provided additional experiments and explanations. However, some concerns remain, particularly regarding the performance on other benchmarks and the comparison to other related works.\n\nBased on the reviews and discussions, I recommend a marginal acceptance of the manuscript. While the proposed improvements to QTRAN are interesting and show promising results on the SMAC benchmark, further investigation and"
    },
    "SJw03ceRW": {
        "generation": "\n\nThis paper addresses a fundamental problem in few-shot learning, which is how to adapt a pre-trained model to new classes while avoiding catastrophic forgetting of the old classes. The proposed method, called hard distillation, is a simple yet powerful method that adds new weights to the base network to classify the novel classes while keeping the weights of the base network unchanged. The paper also proposes a new benchmark for low-shot network expansion and compares the proposed method with other state-of-the-art methods.\n\nThe reviewers generally find the paper well-written and well-organized, and they appreciate the novelty of the proposed method. However, they also raise several concerns and suggestions for improvement.\n\nOne reviewer notes that there is still a drop in accuracy on the base classes after adding new classes, and this drop may increase as more classes are added. This suggests that the method may not be able to perfectly avoid catastrophic forgetting. Another reviewer suggests that the experimental setting is unrealistic, with only 5 base classes and 2 novel classes, and that the method should be tested on more realistic datasets with more classes.\n\nAnother reviewer suggests that the paper should explore more fully the ideas proposed, such as the influence of forgetting on base classes, the number of components used in the GMM, and the influence of the low-shot. They also suggest that the paper should compare the proposed method with other few-shot learning methods that use a fixed representation.\n\nOverall, the reviewers find the paper interesting and novel, but they suggest that more work is needed to fully explore the proposed method and to validate it on more realistic datasets. They also suggest that the authors should consider other few-shot learning methods that use a fixed representation and compare their method with these.\n\nAs a meta-reviewer, I agree with the reviewers that the paper proposes a novel and interesting method for few-shot learning, but more work is needed to fully explore and validate the method. I suggest that the authors address the concerns and suggestions raised by the reviewers and provide more comprehensive experimental results to demonstrate the effectiveness of the proposed method."
    },
    "r111KtCp-": {
        "generation": "\n\nThe manuscript discusses the problem of learning a one-dimensional manifold using an autoencoder, specifically the problem of image radius. The authors propose a solution to the problem by including a bias term and L2 regularization of the encoder weights. The manuscript also discusses the effect of different choices of activation functions and regularization schemes on the performance of the autoencoder.\n\nThe reviewers generally agree that the problem addressed in the manuscript is interesting and relevant, but they have some concerns about the approach used and the conclusions drawn. Some reviewers feel that the approach used is too simplistic and does not adequately address the complexity of the problem. Others point out that the results are heavily dependent on the choice of activation function, and that other regularization schemes have been proposed in the literature that could be more effective.\n\nOne reviewer suggests that the manuscript could be improved by including a more comprehensive analysis of the input data space, including visualizations of the training and test data, as well as a more detailed analysis of the effect of different regularization schemes on the performance of the autoencoder. Another reviewer suggests that the manuscript could benefit from a more detailed comparison with previous work in the field, such as the Denoising Autoencoders proposed by Pascal et al.\n\nOverall, the reviewers suggest that the manuscript could be improved by addressing these concerns and providing a more comprehensive and rigorous analysis of the problem and proposed solution.\n\nReviewer 1:\n\nThe manuscript addresses an interesting problem in machine learning, specifically the problem of learning a one-dimensional manifold using an autoencoder. The authors propose a solution to the problem by including a bias term and L2 regularization of the encoder weights. The manuscript also discusses the effect of different choices of activation functions and regularization schemes on the performance of the autoencoder.\n\nHowever, I have some concerns about the approach used in the manuscript. Firstly, the approach used is too simplistic and does not adequately address the complexity of the problem. The authors assume that the bias term and L2 regularization are sufficient to solve the problem, but they do not provide a thorough analysis of the effect of these factors on the performance of the autoencoder.\n\nFurthermore, the results presented in the manuscript are heavily dependent on the choice of activation function. The authors show that the sigmoid function leads to better performance than the ReL"
    },
    "uFk038O5wZ": {
        "generation": "\n\nThis paper proposes a novel framework, Knowledge Graph Enhanced Dual-Copy network (KGEDC) for abstractive dialogue summarization. The framework incorporates conversational structure and factual knowledge via graph networks to address long-distance cross-sentence dependencies and faithfulness issues. The proposed method outperforms several baselines on two datasets, and human evaluation shows that it leads to increased relevance and readability.\n\nReviewers generally find the paper to be well-written and the motivation to be clear. They appreciate the incorporation of factual knowledge into the model and the proposed method's performance gains over previous baselines. However, some reviewers raise concerns about the lack of analysis of the method, the design choices, and the experimental section. They suggest multiple ablation studies to validate the design choices and to further demonstrate the validity of their model.\n\nOne reviewer points out that the constructed factual knowledge graph seems to be sparse, and the model may not handle this issue well. Another reviewer questions the exclusion of the graph encoder in predicting the next word at the decoder stage. They also suggest including more content on ablation and case studies, such as distinguishing between different edges and examining cases where the proposed method does not perform well.\n\nIn response to the reviews, the authors could consider addressing these concerns by performing additional ablation studies and including more analysis in the paper. They could also provide more details on how the biLSTM encoding and GE encoding are combined and the ratio of h_G and h_S in gating fusion. Furthermore, they could explain the contribution of the sequential encoder and why it is necessary when there is already sequential context dependency in GE.\n\nOverall, the paper presents a promising approach to abstractive dialogue summarization by incorporating factual knowledge into the model. However, further analysis and ablation studies are needed to validate the design choices and fully demonstrate the effectiveness of the proposed method."
    },
    "24-DxeAe2af": {
        "generation": "\n\nMeta-Review: \n\nThis paper describes a deep learning-based approach for detecting copy number variations (CNVs) from DNA sequencing data. The authors propose a method called CNV-Net, which encodes mapped DNA sequences into a pileup image and uses a convolutional neural network (CNN) to classify it into deletions, duplications, or non-breakpoints. They evaluate CNV-Net on two whole-genome sequencing datasets and claim to obtain more accurate and efficient results than current tools. \n\nThe reviewers have raised several concerns and suggestions regarding the paper. One reviewer pointed out that the authors' method is not the first to use a CNN for CNV detection, and they should properly cite and compare previous works. Another reviewer noted that the novelty of the paper is limited, as the pileup image encoding and CNNs have already been used in previous works for SNV detection. \n\nAdditionally, reviewers have questioned the experimental settings, suggesting that the authors should handle the issue of candidate CNV positions rather than leaving it as a limitation of the work. They also recommended providing more details on how they used other tools for the experiments and how they obtained the metrics for the multi-class problem. \n\nFinally, one reviewer suggested that the authors should perform additional experiments to validate their method, such as training and testing on more genomes and recovering known canonical copy number variations. \n\nOverall, the reviewers have identified several areas where the paper could be improved, and the authors should address these concerns before the paper can be considered for publication. \n\nRecommendation: \n\n* The authors should properly cite and compare previous works that have used CNNs for CNV detection. \n* The authors should address the concern regarding the experimental settings and candidate CNV positions. \n* The authors should provide more details on how they used other tools for the experiments and how they obtained the metrics for the multi-class problem. \n* The authors should perform additional experiments to validate their method, such as training and testing on more genomes and recovering known canonical copy number variations. \n* The authors should revise the paper to address the concerns and suggestions raised by the reviewers. \n\nNote to the authors: \n\n* Please carefully address the concerns and suggestions raised by the reviewers and"
    },
    "HyezmlBKwr": {
        "generation": "\n\nMeta-Review:\nThis paper proposes a method for adapting model parameters at test time, called test-time training, which uses an auxiliary task to provide a kind of loose supervision during test time. The method is evaluated on several image classification tasks, and the results show striking improvements in out-of-domain performance while preserving in-domain performance.\n\nThe reviewers have raised several points about the paper. One reviewer notes that the method is not truly about learning with out-of-distribution instances, but rather about improving robustness to domain shifts. Another reviewer points out that the method is not clear about the problem of catastrophic forgetting phenomena in online setting, and the advantage of test-time self-supervision over training-time joint self-supervised approach is not well-explained.\n\nOne reviewer suggests that the method is fundamentally better suited for adjusting for corruptions and modest dataset shifts, rather than large shifts. They also note that the method relies on the assumption of correlation between the loss gradients, which may not hold in the case of large shifts.\n\nThe reviewers have also raised minor comments, such as typos and unclear writing.\n\nOverall, the paper presents a promising approach to improving the robustness of machine learning models, but the reviewers have raised several concerns about the method and its limitations. The authors may want to address these concerns in a revised version of the paper."
    },
    "qwjrO7Rewqy": {
        "generation": "\n\nMeta-review\n\nThis paper proposes a method to approximate extended persistence diagrams (EPD) using a graph neural network (GNN) architecture. The method is efficient and scalable for large and dense graphs and outperforms other methods in terms of computation time. The authors also show that the approximation error of the proposed method is low, which indicates that the method is effective in estimating EPD. However, some reviewers have raised concerns about the choice of filter functions and the threshold value of average N/E or degree to decide which method is the fastest to compute/estimate the EPD. Additionally, there is a lack of discussion on whether there are other research on EPD acceleration and why other speed-up methods for TDAs cannot be used for EPD.\n\nStrengths:\n\n* The method is efficient and scalable for large and dense graphs.\n* The approximation error of the proposed method is low, indicating that the method is effective in estimating EPD.\n* The authors provide a thorough analysis of the computational complexity of the proposed method.\n\nWeaknesses:\n\n* The choice of filter functions is not well-justified.\n* The threshold value of average N/E or degree to decide which method is the fastest to compute/estimate the EPD is not discussed.\n* There is a lack of discussion on whether there are other research on EPD acceleration and why other speed-up methods for TDAs cannot be used for EPD.\n\nSuggestions:\n\n* Provide more justification for the choice of filter functions.\n* Discuss the threshold value of average N/E or degree to decide which method is the fastest to compute/estimate the EPD.\n* Discuss whether there are other research on EPD acceleration and why other speed-up methods for TDAs cannot be used for EPD.\n* Provide more experimental results to support the effectiveness and efficiency of the proposed method."
    },
    "cZAi1yWpiXQ": {
        "generation": "\nThis paper presents a causal perspective on adversarial attacks and defenses, providing a new view on a long-standing problem in machine learning. The proposed method is novel, well-motivated, and empirically validated on several attacks and datasets. However, some questions and concerns have been raised by the reviewers.\n\nThe first reviewer raises questions about the scope of the problem domain and the causal graph, suggesting that the graph may not be applicable to all image problems and that certain assumptions, such as the anti-causal learning assumption, should be motivated. They also suggest that the authors should test their hypothesis that spurious correlation between labels and style features is the reason for adversarial vulnerability.\n\nThe second reviewer raises questions about the objective in Madry et al. 2017 and the term nuisance factors, suggesting that the term should be defined earlier. They also suggest that the authors should provide an example of an integrated representation s(X).\n\nThe third reviewer raises questions about the choices made in Appendix A and the evaluation of the method, suggesting that the authors should provide more evidence for their hypothesis on the reasons for adversarial vulnerability and test their method against stronger baselines.\n\nThe fourth reviewer raises questions about the selection of hyperparameters, the sensitivity of the method, and the consistency of the results across different datasets. They also suggest that the authors should address their concerns on the experiments to be more convinced.\n\nThe fifth reviewer raises questions about the theoretical justification of the method and the empirical examples of the kind of adversarial examples that the ADA method is robust to.\n\nThe sixth reviewer raises questions about the technical details, such as the objective function and the connection between g and h. They also suggest that the authors should clarify how $\\hat{s}(X)$ can approximate $s(X)$ and whether $\\hat{c}(X)$ and $\\hat{s}(X)$ are supposed to be independent when $x$ is given.\n\nIn summary, the reviewers have raised important questions and concerns about the scope, motivation, and technical details of the proposed method. The authors should address these concerns and provide more evidence and clarification to strengthen their claims and improve the quality of the paper."
    },
    "B1grSREtDH": {
        "generation": "\n\nA manuscript on Bayesian Residual Policy Optimization (BRPO) for Bayesian reinforcement learning problems has been reviewed by several experts. The proposed algorithm aims to improve an ensemble of expert policies by learning to reduce uncertainty. The reviews have raised several concerns and suggestions for improvement.\n\nOne reviewer has questioned the motivation for using residual policy learning and the lack of theoretical support for the proposed method. They suggest comparing the proposed method with other exploration methods to empirically show its advantages. Another reviewer has also raised similar concerns and suggested providing more experiment results in different games to demonstrate the generality of the algorithm.\n\nAnother reviewer has asked for clarification on the task formulation, specifically regarding the agent's sensing capabilities and the reward structure. They also suggest providing more intuition about the algorithm's exploration mechanism.\n\nIn response to these concerns, the authors have added a comparison with a state-of-the-art exploration method (PSRL) and provided more analysis on the relationship between the proposed method and other exploration methods. They have also addressed some of the questions regarding the task formulation and provided more details on the reward structure.\n\nDespite these improvements, some concerns remain. The main theoretical contribution of the paper, Theorem 1, is still considered hasty and lacking detailed analysis. The authors are encouraged to provide a more thorough analysis of the relationship between the proposed method and other exploration methods, and to demonstrate the algorithm's performance in a wider range of tasks.\n\nOverall, the manuscript has potential but requires further work to address the concerns raised by the reviewers. The authors are encouraged to continue refining their algorithm and providing more comprehensive experimental and theoretical analysis to support their claims."
    },
    "rkQuFUmUOg3": {
        "generation": "The manuscript has a novel framework named MetaD2A, which performs neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios. The authors claim that the proposed framework can fast adapt NAS from one image dataset to others and achieve SOTA performance.\n\nMeta-Review:\n\nThis paper proposes a novel framework, MetaD2A, for neural architecture search (NAS) that leverages a \"dataset-to-neural-network-architecture\" transformation learned from a database of datasets and architectures. The framework consists of a set encoder, a graph decoder, and a meta-performance predictor. The authors claim that MetaD2A can fast adapt NAS from one image dataset to others and achieve state-of-the-art (SOTA) performance.\n\nThe reviewers have identified several strengths and weaknesses of the paper. On the positive side, the results presented in the paper look solid and indicate improvements over existing NAS approaches in terms of time and prediction performance. The paper is well-organized, and the authors provide additional details in the appendix.\n\nHowever, some reviewers have raised concerns about the novelty of the framework's components. The set encoder, graph decoder, and meta-performance predictor are all proposed before in different NAS scenarios. Moreover, the authors claim that NAS with meta-learning has only been done with small datasets in the past, but the datasets used in their experiments are also small, which may limit the generalization ability of the whole algorithm.\n\nAnother reviewer points out that the predictor consists of two linear layers, which may not be able to take architecture with different nodes as input, thus limiting the generalization ability of the whole algorithm. They also suggest comparing MetaD2A with other MetaNAS methods, such as [2], and performing ablation studies to better understand the importance of different components.\n\nOverall, while the paper presents a promising approach to NAS, the reviewers have identified some limitations and areas for improvement. The authors may need to address these concerns and provide further justification for their framework's novelty and generalization ability. Additionally, further experimentation and comparison with other state-of-the-art methods may be necessary to fully establish the effectiveness of MetaD2A."
    },
    "sEIl_stzQyB": {
        "generation": "no explanation how to derive this equation. The reviewer doesn't understand how the author derives the equation (9) from the utility function. The author should provide more explanations on how to derive the equation (9).\n\nThe authors address an important problem in multi-agent reinforcement learning (MARL) of ensuring that the joint action selection is consistent with the individual agent's greedy action selection. They propose a new method called Greedy-Value based Representation (GVR) that ensures both Individual-Global-Max (IGM) and True-Global-Max (TGM) conditions without requiring a completely expressive (CEC) value function. The method consists of two parts: Inferior Target Sampling (ITS) and Prioritized Experience Replay (PER). ITS ensures that the greedy joint action is stable, and PER assigns higher priority to non-greedy actions that have higher values than a state-based critic. The paper presents theoretical and experimental results to validate the proposed method.\n Strengths:\n\n* The problem addressed is important and relevant to the community.\n* The method proposed is novel and has a clear advantage over existing methods.\n* The paper presents both theoretical and experimental results to support the claims.\n* The experimental results on the Starcraft II benchmark map 6h_vs_8z look impressive.\n\nWeaknesses:\n\n* The paper is difficult to follow, especially in Section 5.1.\n* The reviewer is not convinced that the proposed superior experience replay can destabilize the non-optimal greedy actions.\n* The paper is missing some important baselines in the experimental evaluations.\n* The performance of WQMIX reported on the StarCraft II benchmark map 6h_vs_8z is not consistent with the performance reported in the WQMIX paper.\n* The code is missing from the submission for reproducibility.\n\nThe paper considers the problem of value decomposition in multi-agent settings. The author identifies a new problem, the True-Global-Max (TGM) condition, and suggests a method to satisfy it. The method, called Greedy-Value based Representation (GVR), consists of two parts: Inferior Target Sampling (ITS) and Prioritized Experience Replay (PER). The paper presents theoretical and experimental"
    },
    "RmcPm9m3tnk": {
        "generation": "overfitting.\n\n**Additional comments**:\n\n1. The work shares similarities with UCSG-NET. While GSGN uses a variational framework and is trained unsupervised, UCSG-NET is a supervised work that uses a contrastive loss to learn a disentangled representation of the scene. Perhaps the author can discuss the connection in the rebuttal.\n\n2. The work is compared to SPACE, which is not a generative model, but a discriminative model. It'd be more fair to compare GSGN with other generative models like VAEs or GANs.\n\n3. I find it interesting that the author tried to show the quality of the learned part/object decomposition by visualizing them in Fig. 3 and Fig. 4. However, the visualization looks a bit misleading to me. For example, in Fig. 3, the pose of the object (rotation, translation) is not obvious from the 2D rendering, and the same goes for the part decomposition in Fig. 4. Perhaps the author can clarify why the visualization is not convincing and/or provide more visualizations that demonstrate the quality of the learned scene graph.\n\n**Revised rating**: 7.5. While my concerns have been partially addressed, I believe the paper can be much stronger with more convincing experiments on more complex objects, and clear demonstrations of the usefulness of the learned scene graph."
    },
    "WXwg_9eRQ0T": {
        "generation": "\n\nMeta-Review:\n\nThis paper presents a deep learning approach for resolving program merge conflicts in software development. The approach uses a combination of token-level differencing and a BERT-based classifier to predict the correct resolution for a given conflict. The authors evaluate their approach on several datasets and compare it to other state-of-the-art methods, showing that it outperforms them in most cases.\n\nThe reviewers generally agree that the problem addressed in the paper is important and that the proposed approach is promising. However, there are some concerns and suggestions for improvement.\n\nOne reviewer notes that the approach is somewhat incremental and builds upon previous work, but does not provide a thorough comparison with the previous state-of-the-art method. Another reviewer suggests that the authors should have used a more extensive ablation study to better understand the contribution of different components of their approach.\n\nSeveral reviewers have questions about the technical details of the approach, such as how the BERT model is fine-tuned and how the edit type embeddings are used. One reviewer also notes that the classification approach used in the paper is not novel and has been used in other works.\n\nFinally, one reviewer suggests that the paper could benefit from a more detailed discussion of the limitations of the approach and how it could be improved in future work.\n\nOverall, the reviewers agree that the paper presents a valuable contribution to the field of software development, but could benefit from more thorough evaluation and technical detail."
    },
    "NgwrhCBPTVk": {
        "generation": "\n\nThis paper considers the online bipartite matching problem with predicted degrees of offline vertices. The author proposes a simple algorithm MPD which matches the vertices with low degrees first to exploit the predicted degree. This algorithm has a sub-optimal 1/2 competitive ratio in the worst case. But it performs well on some real data set and CLV random graphs, as the experiments in this paper show. The author proves that this algorithm is optimal on CLV-B random graphs when the prediction is perfect. The author also estimates the lower bound of the competitive ratio of MPD on symmetry CLV-B random graphs through solutions of some derivative equations. \n\nStrengths:\n1.\tOnline bipartite matching problems are very important and have many applications in the real world. The prediction model in the paper is simple and well-motivated. It\u2019s reasonable that we can obtain predicted degrees in real-world applications.\n2.\tThe experimental results are well done and detailed. The author compares MPD and Ranking on real-world data sets, symmetry CLV-B graph, and known i.i.d setting. The MPD outperforms Ranking well. And it seems that even when the degrees are estimated very poor, MPD still outperforms Ranking on CLV-B random graphs.\n3.\tThough MPD is sub-optimal in the worst case, the author shows that it is optimal on CLV-B random graphs. This is a nice theoretical result.\n\nWeaknesses:\n1.\tThe theoretical results are not general enough. They only cover the CLV random graphs where the expected degrees obey zipf\u2019s law. \n2.\tMain result on the optimality of MPD assumes that the predictions are perfect. If \u201cprediction\u201d is the main motivation of this paper, the error of the prediction is an important factor to study. However, the paper does not discuss the performance under imperfect prediction thoroughly. This makes the contribution of the paper limited.  \n 1.\tIn the paragraph \u201cOn prediction error\u201d, the author uses the quantity \u201cthe minimum number of offline nodes that needs to be deleted such that \u03c0 and \u03c0_0 induce the same order on the remaining nodes\u201d to measure the prediction error. But in Appendix D, the author uses the quantity \u201cLargest Increasing Subsequence\u201d. It"
    },
    "H1gax6VtDB": {
        "generation": "\n\nReviews of the manuscript were mixed, with some reviewers recommending acceptance and others suggesting revisions before acceptance. The majority of reviewers agreed that the paper presents a well-motivated method for learning a structured latent space for images, and that the results are promising, especially in simple synthetic environments. However, some reviewers raised concerns about the sensitivity of the model to the number of object slots (K) and the lack of evaluation in more complex environments.\n\nOne reviewer noted that the contrastive loss used in the paper is not well-formulated, as it only considers the positive energy term and not the negative energy term. Another reviewer suggested that the model could benefit from using more features per object, and that the dimensionality of the abstract representation (D) should be explored further.\n\nReviewers also suggested that the GraphNet could be improved by allowing the action to condition the network, and that the mapping between slot k in z_t and z_t should be clarified. Additionally, one reviewer suggested that the results could be improved by relating the work to other papers in the field, such as COBRA and Spriteworld.\n\nOverall, the reviewers believe that the paper presents an interesting approach to learning a structured latent space, but that further work is needed to address the concerns raised above."
    },
    "S0UdquAnr9k": {
        "generation": "\n\nThis paper proposes a new approach to network width search using a locally free weight sharing strategy called CafeNet. The authors claim that their method can reduce the search space while maintaining performance. They also propose a new way of measuring FLOPs sensitivity and use it to reduce the search space further. The paper includes experiments on various datasets and compares the proposed method with other state-of-the-art methods. \n\nThe reviewers generally find the paper well-motivated and the proposed approach interesting. They also find the experiments sufficient and the results promising. However, some reviewers raise concerns about the relation between the proposed approach and other existing methods, the scalability of the search space, and the influence of the supernetwork. \n\nOne reviewer suggests that the authors should provide more discussion on the relation between the proposed approach and completely free weight and fixed weight patterns. Another reviewer points out that the search space of the proposed method scales at O(n), which is considerably larger than the single network width. The authors should provide some discussion on this and analyze the influence of the supernetwork. \n\nAnother reviewer suggests that the writing can be enhanced by going over the manuscript and correcting grammar errors. \n\nOverall, the reviewers find the paper interesting and worth considering for publication. However, they also raise some concerns that the authors should address before publication."
    },
    "iaqgio-pOv": {
        "generation": "the automatic evaluation metric used is the generalized infidelity, which is defined as the maximum distance between the predicted distance and the true distance. However, I'm not sure what the implication of a low generalized infidelity score is. \n\nOverall, while I do appreciate the novelty of the proposed method, I believe that the experimental evaluation is not convincing, and the authors should be aware of the potential issues with the user study. \n\n Minor comments: \n\n* In the main paper, the similarity prediction task is introduced as a \"black-box\" model, and then the explanation for the similarity prediction task is introduced. It would be more natural to introduce the explanation problem of the similarity prediction task first, and then introduce the \"black-box\" model as a motivation for the explanation problem. \n\n* In Sec. 5.1, the authors assert that \"we also observed that the analogy-based explanation is more interpretable and easier to understand for non-technical users.\". This statement is not backed up by any user study or experiments, and should be removed or be supported by proper evaluation. \n\n* In Sec. 5.1, the authors introduced the \"perturbation-based\" explanation method, but it is not used in the evaluation. It should be either removed or properly evaluated. \n\n* The feature attribution method in the paper is similar to LIME. However, the authors did not provide a thorough comparison between the two methods. The authors should either provide a thorough comparison, or provide a clear explanation of why no thorough comparison is provided. \n\n* In Sec. 4.1, the authors mentioned that the analogy-based explanation method can be used to generate explanations for the feature attribution method. This statement is not clear, and the authors should provide more explanation of the relationship between the two methods. \n\n* The authors did not provide a thorough analysis of the computational complexity of the proposed method, and the authors should provide such analysis. \n\n* In Sec. 4.1, the authors mentioned that the analogy-based explanation method can be used to generate explanations for the feature attribution method. This statement is not clear, and the authors should provide more explanation of the relationship between the two methods. \n\n* In Sec. 5.1, the authors asserted that \"we also observed that the analogy-based explanation is"
    },
    "B1eWOJHKvB": {
        "generation": "\n\nMeta-review:\n\nThis paper discusses the cycle GAN loss and its susceptibility to symmetries, which can result in the presence of multiple solutions. The authors claim that adding a regularizer that utilizes the identity loss is insufficient to address this problem. They also experimentally demonstrate the effect of skip connections in F and G on the performance of cycle GAN.\n\nThe notations and formalism used in the paper are criticized for being heavy and cumbersome, and the experiments are considered limited. However, the paper's theoretical results are considered interesting and valuable, particularly proposition 2.1, which provides insights into the invariance of the kernel space.\n\nThe authors' response to the reviewers' comments is also considered helpful in addressing some of the concerns. One reviewer suggests expanding the experimental evaluation to have more significance. Another reviewer suggests an information-theoretic analysis of cycle GAN using the objective of \"MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal Image-to-Image Translation\" or a radically new approach such as the Gromov Wasserstein distance.\n\nOverall, the paper provides valuable theoretical insights into the behavior of cycle GAN and its limitations. However, the experimental evaluation is considered limited, and the paper's scope is thought to be narrow. The authors' response is helpful in addressing some concerns, but further experimental evaluation is needed to establish the paper's significance."
    },
    "r1xMnCNYvB": {
        "generation": "\n\nThe manuscript presents a new software package for simulating molecular dynamics, called JAX MD, which integrates closely with a neural network/machine learning library. The package aims to simplify the process of incorporating machine learning models in the molecular dynamics workflow and provides automatic derivations. The manuscript is well-written and clearly presents the features and capabilities of JAX MD.\n\nThe reviewers generally agree that the package is useful and well-designed, but there is some debate about its suitability for the ICLR conference. Some reviewers suggest that the paper would be better placed in a physics/chemistry venue, as it is primarily focused on the application of machine learning in molecular dynamics, rather than advancing the field of machine learning.\n\nOne reviewer raises several concerns, including the lack of a detailed description of the design of JAX MD and its limitations, as well as the absence of performance comparisons with other molecular dynamics libraries. Another reviewer suggests that the paper could benefit from a more academic tone and a clearer statement of the contribution of the work to the field of machine learning.\n\nOverall, the reviewers agree that the package is valuable and could be of great use to the physics community, but there are some doubts about its suitability for the ICLR conference. It may be beneficial to consider resubmitting the paper to a more specialized venue, such as a physics or chemistry journal, where it can receive more targeted feedback and exposure to the relevant community."
    },
    "uFORMPcA_b": {
        "generation": "\n\nThis paper proposes Lale, a system for gradual AutoML that is concise, modular, and easy-to-use. The system uses combinators to create pipelines and a translation scheme to translate pipelines into search spaces for various backend AutoML optimizers. The paper also includes a user study and experiments to evaluate the effectiveness of Lale.\n\nThe reviewers generally find the paper well-written and well-organized, and appreciate the novelty of the unified translation schema. However, there are some concerns about the significance and impact of the work, as well as the scientific contribution and future work.\n\nOne reviewer suggests that the paper has scientific contributions by identifying three combinators for AutoML and enabling gradual AutoML, but questions the usefulness of Lale for experts in ML and AutoML. Another reviewer thinks that the paper's contributions are more in the domain of software engineering, and wonders how this scientifically contributes to the NeurIPS community.\n\nThere are also some concerns about the presentation of the empirical study, such as the limited number of participants and the lack of statistical analysis. Additionally, one reviewer suggests that the paper could benefit from more analysis on the performance gain and how it relates to the proposed design.\n\nOverall, the reviewers recommend that the authors address these concerns and provide more evidence on the usefulness and impact of Lale, as well as clarify the scientific contribution and future work.\n\nMeta-Review\n\nStrengths:\n\n* Well-written and well-organized paper\n* Novel unified translation schema\n* User study and experiments to evaluate effectiveness\n\nWeaknesses:\n\n* Limited significance and impact\n* Unclear scientific contribution and future work\n* Concerns about presentation of empirical study\n\nRecommendations:\n\n* Address concerns about significance and impact\n* Provide more evidence on usefulness and impact of Lale\n* Clarify scientific contribution and future work\n* Improve presentation of empirical study\n\nOverall, the paper presents a system for gradual AutoML that is concise, modular, and easy-to-use. While the reviewers appreciate the novelty of the unified translation schema, there are concerns about the significance and impact of the work, as well as the scientific contribution and future work. The paper could benefit from addressing these"
    },
    "BJlowyHYPr": {
        "generation": "\n\nMeta-review\n\nThe paper \"D-Conv: Efficient Spatiotemporal Point Cloud Processing with Dynamic Convolution\" presents a new approach to processing point cloud data that combines the strengths of both convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The proposed method, called D-Conv, is designed to handle spatiotemporal point cloud data, which is a set of n points with features and localization coordinates that can evolve over time.\n\nThe D-Conv operator is the core of the approach, which uses the k-nearest neighbor structure to determine the new location and feature values of a point. The operator is embedded in a LSTM architecture to enable spatiotemporal modeling of point cloud data. The authors claim that the D-Conv operator is computationally efficient and does not reduce the dimensionality of the input data.\n\nThe paper includes experiments on four benchmark datasets, comparing the proposed method with several baselines and alternatives. The results show that CloudLSTM, which combines the D-Conv operator with an LSTM architecture, achieves lower prediction error than other methods in most cases.\n\nThe reviewers have raised several concerns and suggestions. One reviewer pointed out that the notation in the paper is a bit confusing, and the authors should clarify the definition of the D-Conv operator. Another reviewer suggested using different markers for the different lines in Figure 4 to make it easier to distinguish.\n\nOne reviewer raised a concern about the vulnerability of the D-Conv operator to outliers, as it relies heavily on the nearest neighbors. The reviewer suggested that the authors should consider adding a regularization term to the weights of the D-Conv operator to reduce the impact of outliers.\n\nAnother reviewer suggested that the authors should provide more analysis of the results, particularly in terms of the contribution of the different components of the CloudLSTM model. The reviewer also suggested that the authors should study the time complexity of the different models and provide a more detailed analysis of the effect of the choice of K on the performance.\n\nFinally, one reviewer suggested that the authors should consider using graph embeddings as feature representations for point cloud data, as it could be more effective in capturing the spatial relationships between points.\n\nOverall, the reviewers have provided valuable feedback that"
    },
    "HkldyTNYwH": {
        "generation": "\n\nThis paper presents a novel approach to address the issues of mode collapse and mode mixture in Generative Adversarial Networks (GANs) using optimal transport theory. The authors propose a two-stage approach, first using an autoencoder to map the input data to a latent space, and then finding a discontinuous transportation map using convex optimization. The proposed method is evaluated on several benchmark datasets, and the results show that it performs well compared to state-of-the-art methods.\n\nThe reviewers generally agree that the paper presents a novel and interesting approach to address the issues of mode collapse and mode mixture in GANs. However, some reviewers have concerns about the complexity of the proposed method and its reliance on heavy linear programming. One reviewer suggests a simpler approach using Gaussian mixture models, which could be more efficient and easier to implement.\n\nAnother reviewer raises a question about the relevance of Theorem 3 in the paper, which is specific to Newton's method, while the authors use gradient descent in their implementation. The reviewer suggests that the authors should provide more explanation or justification for their choice of algorithm.\n\nFinally, one reviewer points out some minor issues with the paper, such as typos and missing definitions, and suggests that the authors should proofread the draft more carefully.\n\nOverall, the reviews suggest that the paper presents a novel and interesting approach to address the issues of mode collapse and mode mixture in GANs, but the proposed method may be overly complex and require further justification. The authors should also consider alternative approaches and provide more explanation for their choices."
    },
    "BJzuKiC9KX": {
        "generation": "\n\nMeta-Review:\n\nThis manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS.\n\nThe reviewers have pointed out some issues with the manuscript. One reviewer has raised questions about the clarity of some of the terminology used in the paper, such as \"branching\" and its various forms. Another reviewer has suggested that the paper lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. The same reviewer has also suggested that the paper should include stronger baselines, such as RBM, DVAE, DVAE++, VQ-VAE etc.\n\nThe reviewers have also suggested that the paper should provide a better introduction to the failure mode of delta-WW and its relationship to the branching phenomenon. Moreover, the reviewers have asked for more explanation on how to estimate KL(q(z|x) || p(z|x)) in Fig 1 (right).\n\nIn addition, the reviewers have suggested that the paper should provide more analysis on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. They have also suggested that the paper could dive deeper into the interesting findings discovered in the experiments.\n\nOverall, the reviewers have found the manuscript to be interesting and important, but have suggested that it needs more rigorous analysis and explanation of the observations made in the paper. They have also suggested that the paper should include stronger baselines and provide more discussion on the shortcomings of RWS and how it can be improved.\n\nBased on the above reviews and discussions, I would recommend that the authors address the following points in their revised manuscript:\n\n1. Clarify the terminology used in the paper, especially \"branching\" and its various forms.\n2. Provide a more rigorous analysis of why the RWS performance"
    },
    "3R--2TdxMps": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a method for identifying and correcting regions on the data manifold where a trained classifier fails. The method consists of three phases: identification, distillation, and correction. In the identification phase, the authors use a Variational Autoencoder (VAE) to generate adversarial examples, which are then clustered using a Gaussian Mixture Model (GMM) to identify failure scenarios. In the distillation phase, the authors use a non-parametric model to fine-tune the classifier with additional synthetic samples from the GMM. Finally, in the correction phase, the authors use the fine-tuned classifier to correct the labels of the original adversarial examples.\n\nThe reviewers have pointed out several strengths and weaknesses of the paper. One reviewer noted that the method is strongly based on Zhao et al. (2018), a method for generating on-manifold black-box adversarial examples using a GAN. Another reviewer found the writing quality to be good, but suggested additional details and justifications for certain choices made in the paper.\n\nHowever, several reviewers have raised significant concerns about the paper. One reviewer questioned the contribution of the paper over Zhao et al. (2018), as the main contribution seems to be the clustering of adversarial examples and fine-tuning the classifier, which is not significantly different from the original method. Another reviewer pointed out that the experimental results are not sufficient to establish the effectiveness of the proposed method, as the improvement in accuracy on the test set is only marginal. Additionally, the reviewer noted that the method is not able to improve the accuracy on natural, unseen test sets, which raises questions about its ability to generalize.\n\nOne reviewer also pointed out that the objective of the proposed method is not clear, as it seems to be focused on defense against on-manifold adversarial attacks rather than debugging and correcting the classifier. Another reviewer suggested that the authors should provide more details on the experimental results, such as the accuracy values on the train, test, and adversarial sets before and after the fine-tuning.\n\nOverall, the reviews suggest that the paper has potential but needs significant revisions to establish its contribution and effectiveness. The authors should address the concerns raised by the reviewers, such"
    },
    "HJe6uANtwH": {
        "generation": "\n\nMeta-review:\n\nThis manuscript presents a novel routing algorithm for capsule networks, which improves upon the previous dynamic routing algorithm by removing the squash function and utilizing layer normalization instead. The proposed method is evaluated on several benchmark datasets, including Cifar10 and Cifar100, and achieves competitive performance with ResNet. The authors also perform an ablation study to compare their method with the original dynamic routing algorithm and demonstrate its superiority.\n\nHowever, there are some concerns and suggestions raised by the reviewers. Firstly, reviewer 1 points out that the \"inverted dot-product attention\" is not new and has been previously proposed in Sabour et al. 2017. Moreover, the claim that the proposed method can \"reconstruct the layer below\" is not entirely accurate, as it only normalizes the input capsules and does not truly reconstruct the layer.\n\nSecondly, reviewer 2 questions the choice of baseline models for comparison. Specifically, the authors compare their method with a ResNet backbone, but replace the activation function with a classifier on top, which may not be the best way to compare viewpoint generalization. The reviewer suggests that a more appropriate baseline would be a dynamic routing capsnet.\n\nThirdly, reviewer 3 raises concerns about the learning rate schedule used in the experiments. The authors claim that the optimization hyperparameters are tuned individually, but the learning rate schedule appears to be the same for all three models in Figure 4. The reviewer suggests that the authors should provide more information about the learning rate schedule and how it was tuned.\n\nFourthly, reviewer 3 also notes that the proposed method requires initialized poses for the capsules in L+1, which may greatly affect the performance of the model. The authors set this value to 0 and do not discuss it further, but the reviewer suggests that randomizing or learning a bias for this value may be worth exploring.\n\nFinally, reviewer 3 mentions that the proposed method shows sudden huge decreases in test accuracy while training, which may be worth discussing and explaining.\n\nOverall, the manuscript presents a novel and effective routing algorithm for capsule networks, but there are some concerns and suggestions raised by the reviewers that the authors should address before publication."
    },
    "r1lUl6NFDH": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes a novel method for neural network quantization based on the Mirror Descent (MD) framework. The authors claim that their method allows for more general non-Euclidean space, which is not achievable by traditional proximal gradient descent. They also propose a stable implementation of MD by storing an additional set of auxiliary dual variables. The experimental results show the effectiveness of the proposed method on several benchmark datasets and architectures.\n\nThe reviewers have raised several concerns and suggestions regarding the novelty, theoretical contributions, and empirical comparisons of the paper. Reviewer 1 suggests a weak rejection of the paper due to the lack of theoretical analysis on nonconvex objective functions and the effect of annealing. Reviewer 2 also raises concerns about the novelty of the paper, suggesting that it is limited to applying MD from convex optimization to NN quantization. Reviewer 3 suggests that the authors should provide more experimental comparisons, including comparisons with state-of-the-art (STOA) networks and performances on ImageNet.\n\nOverall, the reviewers have provided constructive feedback on the paper, and the authors should address these concerns to improve the manuscript. The authors should provide more theoretical analysis on the convergence of MD with nonconvex objective functions in NN quantization and discuss how to choose the projection for mirror mapping construction. They should also provide more empirical comparisons to demonstrate the effectiveness of their method and clarify the novelty of their approach.\n\nBased on the feedback from the reviewers, I would suggest the authors to revise and resubmit the paper, addressing the concerns and suggestions raised by the reviewers."
    },
    "0cn6LSqwjUv": {
        "generation": "\n\nThis paper presents a large-scale dataset for spatial precipitation downscaling, which contains over 62,400 pairs of high-quality low/high-resolution precipitation maps for over 17 years. The paper also proposes a novel implicit dynamics estimation-driven model and evaluates its performance along with several state-of-the-art methods using task-specific metrics.\n\nThe reviewers generally agree that the dataset is valuable and the task is important, but they have some concerns regarding the necessity of low-resolution data, the evaluation metrics, and the training details of the baseline models.\n\nOne reviewer suggests that obtaining low-resolution data by simulated degradation may not be necessary, and the authors should compare this approach with obtaining low-resolution data by downsampling the high-resolution data. Another reviewer suggests that the proposed metrics PEM and PDEM need more empirical evidence to demonstrate their effectiveness.\n\nThe reviewers also suggest that the baseline models used for comparison are not designed for precipitation data and need to be modified and tuned for fair comparison. However, the authors do not provide sufficient details about the training process of these models.\n\nFinally, one reviewer points out some formatting errors in the paper, such as missing brackets and misleading notations.\n\nOverall, the reviewers suggest that the paper needs to address these concerns and provide more empirical evidence to support its claims.\n\nAs a meta-reviewer, I suggest that the authors address the following points in their revision:\n\n1. Provide more evidence to support the necessity of using low-resolution data obtained by simulated degradation instead of downsampling the high-resolution data.\n2. Conduct empirical studies to demonstrate the effectiveness of the proposed metrics PEM and PDEM.\n3. Provide more details about the training process of the baseline models and modify them appropriately for fair comparison.\n4. Address the formatting errors and improve the readability of the paper.\n\nIf the authors address these concerns, the paper has the potential to be a valuable contribution to the field of spatial precipitation downscaling."
    },
    "ygWoT6hOc28": {
        "generation": "\n\nMeta-Review:\n\nThis paper extends Prior Networks models, previously introduced for classification, to regression problems. The authors propose a method for modelling uncertainty in regression tasks by emulating an ensemble using a single model. The proposed approach returns the parameters of a Normal-Wishart distribution, which defines a probability distribution over Normal distributions, and, in turn, each Normal distribution defines a probability distribution over the value of the target variable.\n\nThe reviewers generally find the paper well-written and clearly structured. However, they have concerns regarding the novelty of the proposed approach. One reviewer notes that the extension to regression tasks follows closely the work previously carried out for classification, with the only difference being the use of a Normal Wishart distribution instead of a Dirichlet distribution. Another reviewer comments that the paper lacks novelty as it builds heavily on previous work.\n\nThe reviewers also raise questions about the sensitivity of the training parameters and the lack of comparison with alternative methods to estimate prediction uncertainty. One reviewer suggests comparing the proposed method with Bayesian neural networks using variational inference or dropout.\n\nThe reviewers also have concerns regarding the experiments presented in the paper. One reviewer notes that the OOD loss term in the loss function (8) is a bit artificial, and the choice of beta is not systematic. Another reviewer suggests using a \"bad GAN\" to obtain synthetic inputs that are far from true inputs.\n\nOverall, the reviewers find the paper technically sound, but with limited novelty and concerns regarding the experiments presented. They suggest improvements to the paper, including comparing the proposed method with alternative methods and providing more clarity in the presentation of the experimental protocol."
    },
    "9U4gLR_lRP": {
        "generation": "\n\nThis paper proposes a novel and effective method to improve the transferability of adversarial attacks. It increases the logit margins between targeted and non-targeted classes, which can quickly become saturated in cross-entropy loss. The proposed method has various settings and hyperparameters, and comprehensive experiments are presented. The combining logit calibrations have significantly better performance than previous methods. The findings are very interesting and the motivation is well-explained. \n\nHowever, there are also some weaknesses in the paper. First, the proposed method has various settings and hyperparameters, which can make it less attractive to the community. Second, there is no theoretical analysis to support the empirical findings. Third, the presentation of the results needs to be improved, and all tables contain many numbers, which makes it hard for the reader to get the point in a short time. \n\nThis paper is very similar to another paper, \"On Success and Simplicity: A Second Look at Transferable Targeted Attacks\" by Zhengyu Zhao, Zhuoran Liu, and Martha Larson. The equations and rewriting sentences are almost the same. The improvement of the proposed method over the previous work is marginal. The contribution of this work is limited, and it did not provide extra useful insights into this research area. \n\nThe authors propose a novel and effective method to improve the transferability of adversarial attacks by increasing the logit margins between targeted and non-targeted classes, which can quickly become saturated in cross-entropy loss. The proposed method has various settings and hyperparameters, and comprehensive experiments are presented. The combining logit calibrations have significantly better performance than previous methods. The findings are very interesting and the motivation is well-explained. \n\nHowever, there are also some weaknesses in the paper. First, the proposed method has various settings and hyperparameters, which can make it less attractive to the community. Second, there is no theoretical analysis to support the empirical findings. Third, the presentation of the results needs to be improved, and all tables contain many numbers, which makes it hard for the reader to get the point in a short time. \n\nThis paper is very similar to another paper, \"On Success and Simplicity: A Second Look at Transferable Targeted Attacks\" by Zh"
    },
    "3h1iwXmYVVJ": {
        "generation": "\n\nMeta-Review: \n\nThis manuscript has received four reviews. The reviewers have a mixed opinion of the paper. Two of the reviewers have raised some concerns about the contribution and the practicality of the proposed method. Another reviewer has provided some suggestions for improvement. \n\nOne reviewer has provided a very detailed review and has pointed out some minor mistakes in the paper. The reviewer has also suggested some additional references that may be relevant to the paper. \n\nOverall, the reviews suggest that the paper has some interesting results, but there are also some limitations and areas for improvement. The authors may want to address the concerns and suggestions raised by the reviewers before resubmitting the paper.\n\nRecommendation:\n\nBased on the reviews, I would recommend that the authors revise and resubmit the paper. The authors should address the concerns and suggestions raised by the reviewers and provide a more detailed analysis of the proposed method. Additionally, the authors should also provide a clearer explanation of the contributions and the practical implications of the paper. \n\nOnce the revisions are complete, the paper can be resubmitted, and it will be sent out for another round of peer review."
    },
    "mOO-LfEVZK": {
        "generation": "\n\nManuscript: \"Manifold-Aware Training (MAT) for Adversarial Robustness\"\n\nReviews: 4\n\nOverall, the reviewers have raised several concerns and suggestions regarding the manuscript. While some reviewers have appreciated the novel approach and interesting intuition, others have raised concerns about the lack of comparison with similar approaches and the lack of conclusive remarks. One reviewer has also pointed out that the results seem to show no clear loss-dependent trend, and it is not clear whether the approach provides any guarantees in practice.\n\nThe reviewers have also raised some minor typos and suggested improvements in the presentation of the paper.\n\nBased on the above reviews, it is recommended that the authors address the following points before the manuscript can be considered for publication:\n\n1. Provide a thorough comparison with similar approaches, including a discussion of the advantages and disadvantages of the proposed approach.\n2. Provide conclusive remarks and actionable points that can guide practitioners in using the proposed approach in real-world scenarios.\n3. Address the concerns regarding the lack of a clear trend in the results and provide a theoretical analysis of the approach's performance.\n4. Provide more experimental results to demonstrate the generalization ability and scalability of the proposed approach.\n5. Address the minor typos and improve the presentation of the paper.\n\nOnce these concerns are addressed, the manuscript can be considered for publication."
    },
    "bmGLlsX_iJl": {
        "generation": "\n\nThe paper presents a novel method for missing data imputation called EMFlow, which uses a normalizing flow to map input data to a multivariate Gaussian distribution and then uses an online version of the Expectation-Maximization (EM) algorithm to impute missing data. The authors evaluate the performance of EMFlow on several datasets, including UCI regression tasks, MNIST, and CIFAR-10. The results show that EMFlow outperforms several baseline methods, including MCFlow, and achieves competitive performance on image classification tasks.\n\nThe reviewers generally agree that the paper is well-organized, and the proposed method is sound and easy to implement. However, some reviewers raise concerns about the novelty of the paper, as it builds closely upon MCFlow, and the assumptions made about the relationships between the dependencies in the latent space and the observed data space.\n\nOne reviewer suggests that the authors should provide more analysis of the convergence of the covariance matrix estimation during optimization and explore the possibility of adapting EMFlow to work with low-rank covariance estimators. Another reviewer suggests that the authors should provide more discussions on the interpretability of the proposed model and its potential applications.\n\nOverall, the reviewers agree that the paper presents a well-designed and well-executed study, but the novelty of the proposed method may be limited. The authors may need to further justify the assumptions made and provide more comparisons with related works to better evaluate the proposed method's performance."
    },
    "NbaEmFm2mUW": {
        "generation": "\n\nOne potential improvement would be to evaluate the proposed algorithm on a wider range of tasks, including those with dense rewards, to better demonstrate its versatility. \n\nAnother potential improvement would be to provide more discussion and analysis of the features and target features selected during pre-training, and how they relate to the learned skills. Additionally, it may be worth considering alternative methods for defining the target features, such as using demonstrations or mocap data.\n\nFinally, it would be beneficial to include more videos of the resulting skills and policies to provide a more comprehensive understanding of the algorithm's performance.\n\nOverall, the paper presents a novel approach to hierarchical skill learning that effectively trades off generality and learning speed, and demonstrates its effectiveness on a variety of challenging tasks.\n\nOriginality: 8/10\n\nQuality: 8/10\n\nClarity: 9/10\n\nSignificance: 7/10\n\nThe proposed algorithm outperforms reasonable baselines. The experimental results with the humanoid are limited in terms of the goal spaces and the full goal space baseline appears to work just as well if not better. One potential improvement would be to evaluate the proposed algorithm on a wider range of tasks, including those with dense rewards, to better demonstrate its versatility. Another potential improvement would be to provide more discussion and analysis of the features and target features selected during pre-training, and how they relate to the learned skills. Additionally, it may be worth considering alternative methods for defining the target features, such as using demonstrations or mocap data. Finally, it would be beneficial to include more videos of the resulting skills and policies to provide a more comprehensive understanding of the algorithm's performance.\n\nOverall, the paper presents a novel approach to hierarchical skill learning that effectively trades off generality and learning speed, and demonstrates its effectiveness on a variety of challenging tasks.\n\nOriginality: 8/10\n\nQuality: 8/10\n\nClarity: 9/10\n\nSignificance: 7/10\n\nThe proposed algorithm outperforms reasonable baselines. The experimental results with the humanoid are limited in terms of the goal spaces and the full goal space baseline appears to work just as well if not better. One potential"
    },
    "YYHXJOawkPb": {
        "generation": "\n\nMeta-Review:\n\nThe paper \"The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning\" by [Author List] presents an interesting investigation into the concept of effective robustness during fine-tuning. The paper's findings show that pre-trained models exhibit better effective robustness during training, which vanishes upon convergence. However, the paper's contributions and conclusions have been questioned by several reviewers.\n\nReviewer 1 raises concerns about the paper's contributions and claims that the findings may not be useful. They also point out that the paper does not provide analysis for the observations, and the observations may not be general.\n\nReviewer 2 highlights the strengths of the paper, including its clear writing, thorough experimental section, and selection of experiments. However, they also point out that the paper does not properly define fine-tuning and that an analysis of the relation between the fine-tuning dataset and the OOD test set would be useful.\n\nReviewer 3 states that the conclusions seem 'remarkably obvious' and that the paper does not establish the novelty of the insights reached from the numerical experiments. They also suggest that the paper could be useful as a reference for simple examples demonstrating general principles of machine learning.\n\nReviewer 4 raises several concerns about the paper's findings and justifications. They suggest that the paper lacks depth and justifications to a majority of findings, and that the paper relies heavily on Taori et al. (2020), which has unresolved concerns. They also suggest that the paper should show the same findings hold for at least one more domain.\n\nOverall, the reviewers suggest that the paper has some strengths, such as its clear writing and thorough experimental section, but also have concerns about the paper's contributions, novelty, and justifications of the findings. The reviewers suggest that the paper could benefit from further analysis and justifications of the proposed concepts, and that the findings should be demonstrated to hold for at least one more domain."
    },
    "HkeuD34KPH": {
        "generation": "\n\nMeta-Review:\n\nThis paper proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SSE) regularization. The authors claim that SSE-PT outperforms several baseline methods on five datasets. However, the reviews have raised several concerns about the novelty, experimental results, and technical contribution of the paper.\n\nReviewer 1 questions the novelty of the work, stating that it is based on existing techniques such as SASREC and transformer, and that the addition of user embedding and SSE regularization is not sufficient to justify a new paper. Reviewer 2 also expresses similar concerns, suggesting that the technical novelty is limited and that the paper is an extension of previous works.\n\nReviewer 3 raises concerns about the experimental results, stating that the results are not convincing and that the authors have not provided sufficient ablation studies or discussion about the effectiveness of personalized transformer. Reviewer 3 also suggests that the authors should include more baseline methods to make the results more convincing.\n\nReviewer 4 provides a more positive review, stating that the paper is well-presented and that the authors have conducted extensive empirical studies. However, Reviewer 4 also suggests that the paper is a bit incremental in terms of technical contribution and that the authors could have made clearer what the major contribution of the paper is.\n\nOverall, the reviews suggest that the paper has some strengths, such as its well-presented introduction and discussion of related works, but that it also has weaknesses, such as limited novelty and experimental results that are not convincing. The reviewers have provided several suggestions for improvement, including including more baseline methods, conducting ablation studies, and clarifying the major contribution of the paper. Based on these reviews, I would recommend that the authors revise and resubmit the paper, addressing the concerns raised by the reviewers."
    },
    "XL9DWRG7mJn": {
        "generation": "Summarize the strengths, weaknesses, and suggested improvements. Provide an overall evaluation of the manuscript and its contributions to the field.\n\nA meta-review for the manuscript \"On the Optimality of Hard-Threshold Sparsifiers for Distributed SGD\"\n\nStrengths:\n\n* The manuscript presents new theoretical results on the convergence of distributed SGD with error feedback and sparsification.\n* The authors provide a thorough analysis of the communication complexity model and propose a new sparsifier, the hard-threshold sparsifier, which is proven to be optimal for this model.\n* The paper includes a thorough comparison of the proposed hard-threshold sparsifier with the top-k sparsifier in terms of convergence rates and communication complexity.\n* The authors provide solid experimental results to support their theoretical findings.\n\nWeaknesses:\n\n* The paper's claims on optimality are somewhat misleading, as they rely on a specific choice of sparsification and may not hold for other sparsification methods.\n* The paper does not adequately discuss the limitations of the hard-threshold sparsifier, such as the potential for high communication peaks.\n* The motivation behind considering error-feedback mechanisms is not clearly explained.\n* Some quantities, such as $\\gamma_t$ and $v$, are not explicitly defined or introduced.\n* The convergence guarantees for absolute compressors do not depend explicitly on the compression factor $v$, which may be unexpected for some readers.\n\nSuggestions for Improvement:\n\n* Tone down the claims on optimality and provide a more nuanced discussion of the limitations of the hard-threshold sparsifier.\n* Provide a clearer explanation of the motivation behind considering error-feedback mechanisms.\n* Define and introduce all quantities used in the paper, including $\\gamma_t$ and $v$.\n* Provide further discussion on the discrepancy between the convergence guarantees for absolute compressors and $\\delta$-contraction operators.\n* Discuss the limitations of the hard-threshold sparsifier in terms of communication complexity and provide a more comprehensive comparison with other sparsification methods.\n\nOverall Evaluation:\n\nThe manuscript presents valuable theoretical contributions to the field of distributed optimization and provides new insights into the optimization errors of distributed SGD with"
    },
    "r1exVhActQ": {
        "generation": "\n\nThe current version of the manuscript has received mixed feedback from the reviewers. While some reviewers have praised the work for its novelty and potential impact, others have raised several concerns regarding the technical details and the lack of solid experiments.\n\nOne of the main concerns is that the manuscript lacks a clear contribution and novelty compared to previous works. The authors' approach is seen as too similar to existing methods, and the claimed improvements are not clearly demonstrated. Reviewers have suggested that the authors should differentiate their approach from previous works and show a clear improvement in experimental results.\n\nAnother concern is the limited scope of the experiments. The authors have only tested their method on a small dataset and a single model architecture, which raises questions about the method's generalizability. Reviewers have suggested that the authors should test their method on a wider range of datasets and architectures to demonstrate its effectiveness.\n\nAdditionally, some reviewers have pointed out minor issues with the manuscript's notation, terminology, and presentation. These issues should be addressed to improve the clarity and readability of the manuscript.\n\nOverall, while the manuscript has some potential, it needs significant revisions to address the concerns raised by the reviewers. The authors should provide a clear contribution and novelty, improve the experimental scope, and address the minor issues to make the manuscript more convincing and well-rounded."
    },
    "a0yodLze7gs": {
        "generation": "bit confused and not well organized. The paper might be rejected for major revision.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "DGIXvEAJVd": {
        "generation": "\n\nMeta-review\n\nThis paper presents a novel approach to learning the rules of chess by training a transformer-based language model on a large corpus of chess games represented in UCI notation. The authors propose a new evaluation metric, called \"legal move prediction,\" to measure the model's ability to predict the next move in a chess game. They also introduce a multi-view training strategy that incorporates additional supervision during training to improve the model's performance. The results show that the proposed approach achieves high accuracy on both legal move prediction and piece location tasks, outperforming a baseline model that relies solely on the UCI notation.\n\nThe reviewers generally found the paper to be well-written and the results intriguing, with one reviewer stating that the paper presents a \"creative application of transformers to a non-traditional textual inference task.\" However, some reviewers raised concerns about the limited scope of the paper and the lack of connections made to wider research issues in grounded language learning and implicit world state tracking. One reviewer also pointed out that the performance of the Oracle Baseline in Table 3 is outperformed by trained models with less information, which raises questions about the upper bound of model performance.\n\nTo address these concerns, the authors could consider expanding the scope of their work to explore the applicability of their approach to other domains and to establish connections with related research in grounded language learning and world state tracking. Additionally, the authors could provide more analysis to understand the reasons behind the performance of the Oracle Baseline and the trained models.\n\nOverall, the paper presents an interesting and innovative approach to learning the rules of chess, and the results show promising performance. With further exploration and analysis, the authors could strengthen their contribution and demonstrate the applicability of their approach to other domains.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    "B4OTsjq63T5": {
        "generation": "\n\nThe paper \"Bayesian Inference via Sparse Hamiltonian Flows\" presents a novel approach to Bayesian inference that combines the use of coresets, Hamiltonian flows, and quasi-refreshments to improve the efficiency and accuracy of Bayesian inference. The paper provides both theoretical and empirical evidence for the effectiveness of the proposed approach, which is demonstrated to outperform other state-of-the-art methods in several experiments.\n\nThe reviewers generally agree that the paper is well-written, clear, and presents a significant contribution to the field of Bayesian inference. They also appreciate the thorough review of related work and the novel insights provided by the authors. However, some reviewers raise concerns about the limitations of the proposed method, such as the sensitivity of the algorithm to the choice of parameters and the potential difficulty in applying the method to more complex models and data.\n\nOne reviewer suggests including simpler Bayesian inference baselines, such as Laplace and simple mean-field VI, to provide a better comparison with conventional approaches. Another reviewer questions the choice of baselines used in some of the experiments and suggests including a dedicated Related Work section to elaborate on the connections to earlier work.\n\nThe authors are encouraged to address these concerns and provide a more detailed discussion of the limitations of their method. Additionally, they may want to consider including more complex experiments to demonstrate the applicability of their approach to a wider range of scenarios. Overall, the paper is considered to be of high quality and has the potential to make a significant impact in the field of Bayesian inference."
    },
    "1vusesyN7E": {
        "generation": "\n\nMeta-Review\n\nThe paper \"Unlearnable Examples via Autoregressive Noise\" presents a novel approach to data poisoning, which uses autoregressive (AR) noise to prevent data scraping. The proposed method is efficient, technically sound, and generic across different model architectures and datasets. The paper also provides insights into why the proposed method is effective.\n\nThe paper's strengths include the following:\n\n1. The proposed method is novel and has good transferability across different model architectures and datasets.\n2. The method is efficient and does not rely on optimizations, which is a significant advantage over existing methods.\n3. The proposed method generates unlearnable examples that are more robust towards augmentations.\n\nHowever, there are also some limitations and concerns:\n\n1. The proposed method may not be effective in the adaptive case, where the defender can retroactively apply new models/methods.\n2. The method may not be effective against adversarial training, and the performance is similar to other baselines under this setting.\n3. The generated perturbations are only evaluated in the $\\ell_2$ norm, and it is unclear whether the method is applicable to other norms.\n4. The proposed attack requires a high poison rate to be effective, and it is unclear how well it would perform in scenarios where the data is collected from multiple sources.\n5. The theoretical analysis in Section 3.3 is not sufficient, and the relation between the lemma and the effectiveness of the proposed method in poisoning attacks is not obvious.\n\nOverall, the paper presents a novel and interesting approach to data poisoning, but further research is needed to address the limitations and concerns mentioned above."
    },
    "6UtOXn1LwNE": {
        "generation": "\n\nThe paper proposes a new approach to learning the reward function from preference datasets. The proposed approach is based on the concept of regret, which is the difference in the expected return between the chosen and non-chosen actions. The authors argue that this approach is more robust than the traditional approach of using partial returns, as it can handle situations where the reward function is not fully observed.\n\nThe paper presents a theoretical analysis of the proposed approach, showing that it has desirable properties such as identifiability and consistency. The authors also provide an algorithm for learning the reward function from preference datasets using the regret approach.\n\nThe paper also includes experimental results on a simulated delivery problem, where the proposed approach is compared with a partial return baseline. The results show that the proposed approach performs better than the baseline in terms of both accuracy and efficiency.\n\nHowever, the reviewers have raised some concerns about the paper. One reviewer notes that the proposed approach is limited to deterministic policies, and it is unclear how it can be extended to stochastic policies. Another reviewer points out that the computational complexity of the proposed approach is higher than that of the partial return approach, and it is unclear whether it is scalable to more practical problems.\n\nThe reviewers also note that the theoretical analysis is based on a simplifying assumption that the dataset covers all possible pairs of segments, which may not be realistic in practice. Additionally, the paper does not provide a comparison with other baselines, such as IRL methods, which are commonly used in this area.\n\nOverall, the paper presents a promising approach to learning the reward function from preference datasets. However, the reviewers have raised some concerns that need to be addressed before the paper can be accepted for publication. The authors should provide a more detailed analysis of the computational complexity of the proposed approach and its scalability to more practical problems. Additionally, the authors should provide a comparison with other baselines, such as IRL methods, to demonstrate the advantages and limitations of the proposed approach.\n\nRevise the manuscript according to the feedback from the reviewers and resubmit it.\n\nDear Editor,\n\nThank you for the opportunity to revise our manuscript based on the feedback from the reviewers. We have carefully considered the comments and suggestions provided by the reviewers and have made significant revisions to our paper.\n\nFirstly, we address the concern raised by Reviewer 1 regarding the limitation of our"
    },
    "7BlQMwp_44p": {
        "generation": "A meta-review should provide an overview of the reviews, identify areas of agreement and disagreement, and provide an overall assessment of the manuscript's strengths and weaknesses.\n\nThis manuscript has received five reviews, each of which provides a thoughtful and detailed assessment of the work. The reviews highlight both strengths and weaknesses of the manuscript, and offer suggestions for improvement.\n\nOne area of agreement among the reviewers is the significance of the work. All reviewers acknowledge the importance of studying linear and ReLU regression in the presence of a bounded adversarial Massart noise model. They also appreciate the novelty of the approach, which considers the unknown corrupted samples as outliers and leverages existing robust estimators to remove the effect of outliers and acquire the true parameters.\n\nAnother area of agreement is the well-organized and well-written nature of the manuscript. The reviewers praise the clear and intuitive explanations, rigorous proofs, and proper citations. They also appreciate the careful evaluation of the strengths and weaknesses of the work.\n\nHowever, there are some areas of disagreement and concerns among the reviewers. One reviewer raises a concern about the assumption of a completely clean and noise-free model except for the adversarial part, which they feel is worrisome in practice. Another reviewer questions the computational cost of obtaining a radial-isotropic transformation matrix A, which they feel is a crucial algorithmic component but is not adequately discussed.\n\nAdditionally, some reviewers have minor comments and suggestions, such as providing more discussion on the transition from ReLU and linear regression to $\\ell_0$ or $\\ell_1$-regression, clarifying the adversary model, and discussing the bit complexity of the parameters and samples.\n\nOverall, the reviews suggest that the manuscript is a valuable contribution to the field of linear and ReLU regression in the presence of a bounded adversarial Massart noise model. The work is well-written, well-organized, and novel. However, there are some areas of concern and suggestions for improvement, such as addressing the computational cost of obtaining a radial-isotropic transformation matrix A and providing more discussion on the transition from ReLU and linear regression to $\\ell_0$ or $\\ell_1$-regression.\n\nBased on the reviews, I"
    },
    "aKZeBGUJXlH": {
        "generation": "\n\nThis paper proposes a defense mechanism against backdoor attacks on pre-trained language models. The proposed method, called Gradient Broadcast Adaptation (GBA), aims to update word embeddings in a way that mitigates the effect of backdoor attacks. The authors evaluate the effectiveness of GBA against four state-of-the-art backdoor attacks and show that it can effectively defend against these attacks without significantly affecting the model's generalization ability.\n\nThe reviewers generally find the paper well-motivated and novel. They appreciate the authors' effort to address a previously under-explored challenge in pre-trained model security. However, some reviewers have concerns about the empirical results, which they find marginally significant. They also have questions about the technical details of the proposed method.\n\nOne reviewer suggests that the authors consider their approach a \"firewall\" to standard adaptation operations, which could broaden the usage scenarios of the proposed approach. Another reviewer raises a question about the applicability of the method to more complicated tasks, as the experiments only consider tasks with few classes.\n\nThe reviewers also suggest several improvements to the paper, such as providing more detailed explanations and definitions, improving the writing quality, and addressing the concerns about the empirical results.\n\nOverall, the reviewers recognize the paper's novelty and potential contribution to the field of pre-trained model security. They suggest that the authors address the concerns and improve the paper to further strengthen its impact."
    },
    "6lH8nkwKRXV": {
        "generation": "\n\nThis paper focuses on improving pooling functions in graph neural networks for the purpose of better addressing graph classification problems. The proposed method, StructAgg, groups node representations and learns a softmax-based representation parameterized by downstream supervision signals. The paper presents an analysis of the effectiveness and impact of the proposed technique and compares it with existing approaches. The paper also discusses the connection between the proposed method and existing literature and highlights the unique perspective and value in the proposed technique. The paper provides empirical evidences to confirm the effectiveness and impact from StructAgg, and the results show that StructAgg outperforms other methods in some cases. \n\nHowever, some reviewers have raised several concerns about the paper. One reviewer notes that the idea of \"grouping node representations using softmax for graph classification\" may have been investigated in existing works, and the authors may need to discuss its connection with existing literature and highlight the unique perspective. Another reviewer raises concerns about the technical impact of the paper, which may be limited, and the authors may need to provide more theoretical or empirical evidence to motivate the problem, highlight the uniqueness in the technique, and justify their design choices. \n\nAdditionally, some reviewers have raised questions about the paper's experimental setup and results. One reviewer notes that the proposed method only works with a small number of structural classes and may not be efficient in both storage and computation for complex graphs. Another reviewer notes that the method assumes that similar node embeddings should have similar structural roles, which may not always be true. Some reviewers have also raised concerns about the lack of robustness in the method and the fact that it does not provide a complete solution to the problem. \n\nOverall, the paper presents an interesting approach to improving pooling functions in graph neural networks, and the proposed method shows promising results in some cases. However, the paper may need further work to address the concerns and questions raised by the reviewers, particularly regarding the method's connection to existing literature, its technical impact, and its limitations. The authors may also need to provide more empirical evidence and theoretical analysis to support their claims."
    },
    "VAeAUWHNrty": {
        "generation": "\n\nMeta-Review\n\nThis manuscript presents a novel approach to inverse rendering, combining neural fields, Monte Carlo integration, and denoising. The proposed method has been shown to produce high-quality results in recovering geometry, material, and lighting of a scene from a set of multi-view images. The strengths of the manuscript include the original idea of denoising Monte Carlo rendering, the convincing results, and the thorough discussion of limitations.\n\nHowever, some reviewers have raised concerns about the evaluation of individual intrinsic components, such as 3D geometry and environment maps, and the lack of comparison with recent Monte Carlo inverse rendering methods. Additionally, there are questions about the scalability of the method and its applicability to handle inter-reflections and light refraction in translucent objects.\n\nOverall, the reviewers find the manuscript to be well-written and well-structured, with a clear introduction and proper citations. The proposed method has the potential to significantly improve the state-of-the-art in inverse rendering, and the manuscript provides a valuable contribution to the field. However, some revisions are suggested to further strengthen the manuscript, including a more comprehensive evaluation and comparison with related work.\n\nBased on the feedback from the reviewers, we recommend the following actions:\n\n1. Provide a more comprehensive evaluation of the proposed method, including the evaluation of individual intrinsic components such as 3D geometry and environment maps.\n2. Include a comparison with recent Monte Carlo inverse rendering methods, particularly [31,37], to demonstrate the advantages and limitations of the proposed method.\n3. Address the concerns about scalability and applicability to handle inter-reflections and light refraction in translucent objects.\n4. Consider revising the manuscript to better align with the scope of NeurIPS, as suggested by one of the reviewers.\n\nWe believe that addressing these concerns and suggestions will further strengthen the manuscript and help it reach its full potential. We look forward to seeing the revised version of the manuscript."
    }
}