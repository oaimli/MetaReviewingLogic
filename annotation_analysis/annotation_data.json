{
    "a0yodLze7gs": {
        "contradict": 1,
        "review_score_variance": 2.0,
        "paper_acceptance": "withdrawn-rejected-submissions",
        "label": "train",
        "documents": [
            {
                "document_title": "Final Decision",
                "document_content": "The initial round of reviews showed a consensus among the reviewers that the presentation of the paper was poor, the novelty was unclear, claims were not properly justified, and the experimental evaluation and discussion were quite insufficient. The authors provided a rebuttal and an updated version of the paper. Although the updated paper demonstrated that the proposed approach indeed provides some benefits, it appears that the authors were not successful to address the numerous but constructive reviewers' comments.\n\nThe paper is not ready for publication in ICLR 2021 and can benefit from major revisions and careful proofreading. "
            },
            {
                "document_title": "Abstract",
                "document_content": "Disentanglement is a highly desirable property of representation due to its similarity with human\u2019s understanding and reasoning. This improves interpretability, enables the performance of down-stream tasks, and enables controllable generative models.However, this domain is challenged by the abstract notion and incomplete theories to support unsupervised disentanglement learning. We demonstrate the data itself, such as the orientation of images, plays a crucial role in disentanglement and instead of the factors, and the disentangled representations align the latent variables with the action sequences. We further introduce the concept of disentangling action sequences which facilitates the description of the behaviours of the existing disentangling approaches. An analogy for this process is to discover the commonality between the things and categorizing them. \n      \n      Furthermore, we analyze the inductive biases on the data and find that the latent information thresholds are correlated with the significance of the actions. For the supervised and unsupervised settings, we respectively introduce two methods to measure the thresholds. We further propose a novel framework, fractional variational autoencoder (FVAE), to disentangle the action sequences with different significance step-by-step. Experimental results on dSprites and 3D Chairs show that FVAE improves the stability of disentanglement."
            },
            {
                "document_title": "Interesting model idea w.r.t. annealing capacity of latent representation, but relation to competing SOTA approaches requires more clarity",
                "document_content": "### Summary:\nIn this submission, a common modelling assumption for unsupervised disentanglement is challenged: that the disentangled representation follows the independence structure of the underlying (data generating) factors. Instead, the paper proposes to consider *action sequences* which describe how datapoints are interrelated. The paper provides evidence that the capacity of the latent representation (controlled by Lagrange parameter beta in beta-VAE related models) is related to the significance of particular action sequence for disentanglement. To leverage this insight, the fractional VAE (FVAE) is proposed, consisting of several sub-encoders and different training stages. The disentangling properties of the FVAE is demonstrated on the dSprites and 3D chairs datasets, with the FVAE performing favourably to the beta-VAE w.r.t. the Mutual Information Gap (MIG) disentanglement metric on dSprites.\n\n### Strengths:\n- Novelty / relevance: The submission addresses the important topics of inductive biases and disentangling factors in learning disentangled representations and suggest the interesting and novel concept of action sequences which seems to be related to the general idea of uncovering symmetries with deep latent variable models. In particular, the annealing approach with respect to the KL-divergence Lagrange parameter beta in the FVAE setting to separate \u201csignificant modes\u201d might pose a relevant insight useful in other related approaches and to a more broader audience. \n\n### Weaknesses:\n- Technical quality / significance: The submission mentions the similarities to approaches like AnnealedVAE by Burgess et al. and qualitatively discusses differences and relates some results to this competing approach, but an empirical evaluation of the proposed approach to the competing method is missing. This is quite important, as the technical details of annealing the capacity of the latent representation seem very much alike. Also comparing the disentangling scores to more state-of-the-art approaches like FactorVAE (Kim and Minh) would be important. The evaluation is solely done with respect to the beta-VAE which might not be the most relevant competitor here. For instance, figure 3 in Burgess et al. reports a similar finding as provided in figure 7a in the submission, i.e. controlling the information capacity disentangles first positional / translational factors, then scale and then orientation / shape. Therefore, it is difficult to assess the validity of the claims of the proposed approach and whether a significantly different contribution than in Burgess et al. is made.\n- Figure 5a suggests for dSprites that position/translation, scale and shape are the relevant actions in that order. However, the result in figure 7a suggest, that first translation, then scale and lastly rotation are gradually disentangled which seems to contradict the first result in figure 5a. Shouldn\u2019t these be the same?\n- Figure 6a and 6b are not explained or discussed and their interpretation is not clear. A reader might be familiar with similar plots e.g. in the paper by Higgins et al., but still the key insight should be stated somewhat more clearly in the paper.\n- Clarity: At times it is difficult to follow the presentation of the content in the paper and in some cases I find it hard to follow the statements and conclusions. For instance:\n- Toy example in section 3.1, especially last paragraph: I believe the interpretation of the results in figure 1 requires a little bit more explanation. As I understand it, the ground-truth factors here are the positions X, Y of the rectangles. The dataset provides the variables (i) orientation of the rectangle, (ii) coordinates in either Cartesian or polar coordinate system. I do not quite follow the statement that *\u201c[\u2026] learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).\u201d* In case (A1, A3) I would say the latent representation is the same up to permutation of coordinate axes / rotations, which is inherent to VAE / PCA approaches. I.e. the meaning of the axes would be still the same (up to these transformations). Therefore, I am also not quite sure about the statement: *\u201cAs we have shown in Sec. 3.1, the orientation of the rectangle can affect the direction of the disentangled representation\u201d* (p. 5). The \u201cdirection\u201d of the representation is less relevant, as the interpretation of the axes is still the same. However, I might miss the point which is tried to be made here. Could the authors comment on that?\n- Definition of action sequence, section 3.2: The paper tries to motivate \u201caction sequences\u201d but in my opinion the notion remains somewhat unclear in an abstract setting. A \u201cmeaningful action sequence\u201d is defined as *\u201ca sequence / ordered permutation of elements from a subset of the dataset, which reveals the relationship among the elements\u201d*, with elements being images here. In a simple example as scaling or translating objects, this notion and the distinction to \u201cground-truth factors\u201d might be clearer. However, in more complex / less structured examples, say images of faces, the difference between \u201caction sequence\u201d and \u201cground-truth factors\u201d is not very clear to me. The paper suggests for a more formal definition to consider Higgins et al. but, in order to be self-contained and clear, a more explicit and formal definition of this notion is required in the paper, in my opinion. Could the authors maybe provide a more formal definition? \n\n### Additional Feedback:\n- Page 6 and figure 3: *\u201cPlease note that the maximum for both are reached when theta=90 and L is at its maximum.\u201d* Figure 3 suggests that the maximum (yellow region) is reached for large L and theta close to 0 or about 180. It seems that there is a discrepancy between the description and the figure.\n- Figure 5, page 7: In 7b the legend specifies integers, but it is not clear, what these integers encode. And is it maybe *\u201cKL divergence vs beta\u201d* (-> *\u201dy against x\u201d*) in the caption?\n\n- Abstract (p. 1): I would suggest rephrasing the following sentence:  *\u201cWe demonstrate the data itself, such as the orientation of images, plays a crucial role in disentanglement and instead of the factors, and the disentangled representations align the latent variables with the action sequences.\u201d*\nMaybe get rid of the first *\u201cand\u201d* as well as making clear what *\u201cfactors\u201d* (maybe rather *\u201cground-truth / separating factors\u201d*?) are meant. On the first read, this sentence was quite confusing to me.\n- Introduction (p. 1): Second sentence, *\u201cthinking\u201d* -> *\u201cthink\u201d*.\n- Introduction (p. 1): Third sentence, *\u201c[\u2026] single glance this is because [\u2026]\u201d* -> *\u201c[\u2026] single glance. This is [\u2026]\u201d*.\n- Introduction (p. 1): Notion paragraph first word, *\u201cthe\u201d* -> *\u201cThe\u201d*.\n- Introduction (p. 1): Notion paragraph, *\u201c[\u2026] a question arise here is [\u2026]\u201d* -> *\u201c[\u2026] a question which arises here is: [\u2026]\u201d*.\n- Figure 1, caption: *\u201cleaned\u201d* -> *\u201clearned\u201d*.\n- Section 3.3, incomplete sentence after equation 5 or unnecessary *\u201c,\u201d*.\n- Section 4, page 6: *\u201c[\u2026] leading to the disentangling process decays [\u2026]\u201d* -> *\u201d[\u2026] decaying [\u2026]\u201d*\n- Section 4, page 6: *\u201c[\u2026] targeted action into the leaned codes.\u201c* -> *\u201c[\u2026] learned [\u2026]\u201d*\n- Figure 7, page 8: Full stop *\u201c.\u201d* missing in the last sentence of the caption.\n\n### Recommendation:\nIn general, the paper deals with relevant issues in learning disentangled representations and provides interesting tools to address some of these aspects. In particular, the annealing procedure in the FVAE is potentially a relevant contribution. However, the relation to similar approaches is not evaluate adequately, in my opinion, which makes it difficult to assess the justification of some claims. Also, a careful revision of the submission seems advisable which might clarify some of the aspects raised above. In the current form, I believe that the paper is not ready for publication and I would rather see this submission rejected. Nevertheless, I am willing to reconsider my rating if the authors are able to address some of the concerns and questions raised above.\n\n### Post-Rebuttal:\nI want to thank the authors for their responses and clarifications. I think the revision already improved the quality of the submission quite a bit. However, I still believe that there are some aspects which need a better presentation and clearer discussion. \n\nFor example, a more direct discussion and (empirical) comparison to other approaches like AnnealedVAE is necessary, as also other reviewers pointed out, to justify the points made (qualitatively) in the paper. The added results in figure 6c already provide results in that direction.  \n\nI appreciate the clarifications in the notions of action and action sequence. Although I agree that the notions are comprehensible in the toy example and dSprites setting, I still think that the point I raised in my initial review applies. In order to provide a well-defined notion a more formal definition is required. To me it is still unclear what an action sequence in the case of e.g. images of faces should be.\n\nI genuinely believe that the proposed approach might pose a relevant contribution but the paper lacks an adequate presentation at the moment, in my opinion. Therefore, I stand with my initial recommendation that this submission is not ready for publication and I endorse rejecting the paper. However, I would like to encourage the authors to do a major revision taking the issues raised by the reviewers into consideration and to submit again.\n\n\n### References: \n- Higgins et al., \u201cbeta-VAE: Learning basic visual concepts with a constrained\nvariational framework\u201d, ICLR 2017.\n- Kim and Mnih, \u201cDisentangling by factorising\u201d, ICML 2018.\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018."
            },
            {
                "document_title": "Response to Reviewer1",
                "document_content": "Thanks for your detailed feedback and the insightful reviews!\n\n> What's the KL divergence here? \n\nThat's right. The KL divergence is the regularization term of the VAE objective.\n\n> KL divergence is consistent with that of entropy.\n\nBasically, the relationship between the KL term and entropy has not yet been verified. We admit that mainly is a hypothesis, though we conduct two experiments in Fig. 3 and 11. However, the current works, such as AnnealedVAE and PCA Directions indicate similar results that different actions have different thresholds. Indeed, such a conclusion can hardly induce; hence, abundant experiments are required. However, the available datasets are insufficient to verify it.\n\n> What's definition of the label here?\n\nThe dSprites is an artificial dataset and contains the factors information. The labels are ground-truth factors. Although our method is an unsupervised approach, the calculation of action's entropy needs label information.\n\n> The authors arranged three stages for dSprites. \n\nThe direct answer is prior to this dataset. We already know it consists of five factors, and two are similar (posX, posY); the shape is not a good action. Besides, we also provide an annealing test in Sec 4.6 to no prior information case. Increasing the number of stages has little influence on disentanglement, but it does waste the computational resource.\n\n> training objective function\n\nThe objective function is beta-VAE. The only difference is the training process.\n\n> How are the curves in Fig. 5 derived?\n\nEach line denotes the dimensional KL diverge over \u03b2 increasing. For the supervised case, we can name the dimension by its most informative factor. For the unsupervised case, we show their index of dimension.\n\n> Thus, the following three-stage training process is questionable.\n\nFor now, FVAE needs the participation of humans. It may not be a bad idea because unsupervised disentanglement learning without inductive biases is impossible. The main purpose of this work is not to propose a competent disentangling method. We focus on the interpretation of why VAEs can disentangle. In this work, we try to provide an insight into combining the data and the representation, and the thresholds could help disentanglement.\n\nReference:\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.\n- Michal Rolinek; Dominik Zietlow; Georg Martius: Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019."
            },
            {
                "document_title": "Response to Reviewer4",
                "document_content": "Thanks for your detailed feedback and the insightful reviews! \n\n**Ambiguous definitions**\n\n1. Action : *the continuous set of images over a certain direction.*\n2. Action sequence: the discrete action or a sequence of sampled images from the action.\n3. Generating action sequence: the action sequences traversing the ground-truth factors.\n4. Learned action sequence: the action sequences traversing the latent variables.\n5. Entropy of action:\n\n$$H(S') = - \\frac{1}{N}\\sum_{x_i \\in S'} \\mathrm{log} \n        (\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp^{-\\frac{(x_i-\\bar{X})^2}{2\\sigma^2} })$$\nwhere $S'$ is the set of an action, $x$ is the sampled images form this action, $\\bar{X}$ is the mean of the action.\n\n"
            },
            {
                "document_title": "Response to reviewer 3.",
                "document_content": "Thanks for your detailed feedback and the insightful reviews!\n\n**The difference between Annealed VAE**\nThe interpretation of disentanglement: AnnealedVAE argues that the information bottleneck enforces the model to encode \"the most significant improvement in data log-likelihood,\" which leads to disentanglement.  In contrast, we claim that a high regularization penalty on KL divergence prevents the insignificant action sequences from being encoded. In other words, the key to disentanglement is learning information solely.  \n\n> Would this help disentangle position at first then orientation of rectangles?\n\nWe are sorry for the confusing description that A1-3 have the same two-dimension factors denoting images' position. The orientation of images is a fixed property of the dataset. \"The most significant improvement\" should have the largest variation, which can also understand from the PCA theory. In fact, the current theories (information bottleneck, PCA Directions) support the results of A1-3. The action sequence moving along the rectangle's short side has the largest variation and improves the log-likelihood the most significantly. Hence, we say disentangling action sequences is a proper description of disentanglement.\n\n> a formal definition of the inductive bias is still unavailable.\n\nThough Burgess and Rol\u0131nek indicate the inductive bias, they don't propose a calculation for that. \n\n> existing models disentangle the ground-truth factors by accident.\n\nRol\u0131nek used a similar expression, \"Variational Autoencoders Pursue PCA Directions (by Accident).\" That means the success of the current approaches (beta-VAE, TC-VAE, AnnealedVAE, DIP-VAE, FactorVAE) mainly contributes to the well-designed dataset. For instance, they fail to disentangle in the cases of A1-3. If the disentangled representation depends on the data, these approaches don't guarantee the disentanglement when facing an unknown dataset.\n\n"
            },
            {
                "document_title": "Response to Reviewer 2",
                "document_content": "Thanks for your detailed feedback and the insightful reviews! We feel sorry about failing to show the comprehensive results of our work. We hope our responses correctly answer your concerns.\n\n**The different contributions** \n1. The interpretation of disentanglement: AnnealedVAE argues that the information bottleneck enforces the model to encode \"the most significant improvement in data log-likelihood,\" which leads to disentanglement.  In contrast, we claim that a high regularization penalty on KL divergence prevents the insignificant action sequences from being encoded. In other words, the key to disentanglement is learning information solely. \n2. As far as we know, we are the first to define the inductive biases on the data and associate it with disentanglement. Though Burgess and Rol\u0131nek indicate something similar, we give a former and explicit definition. \n3. We improve disentanglement by solving re-entanglement.\n\n**Explanation of toy examples** We want to show some evidence of inductive biases on the data in this part. The learned action sequences should match the generating action sequence precisely for the popular view of disentanglement learning. However, the experimental results of A2 and A3 reveal that the current disentangling approach learns a significant action sequence or the principal component on the data.  We will update this figure for easy understanding. A1 and A3 have the same generating action, but the model learns two different action sequences. In contrast, A1 and A2 have different generating actions, but the model learns similar action sequences.\n\n**Contradiction** The orders should be the same if they are all actions.  However, there are only three types of shape, eclipse, square, and heart.  We don't feel surprised by this result because *shape* is not an action like others having internal frames. An action should consist of a series of continuous images. \n\n**Notion** Action: The continuous set of images over a certain direction. \n\nThis notion denotes the real action in reality, i.e., a ball falls. However, the action is infeasible for the machine, and we have to sample the discrete action as *an action sequence*. Therefore, a subset of the dataset varying one factor is an action sequence. We call the action sequences generated by the ground-truth factors *generating actions* or just *actions*, and the reconstructed sequences by the decoder *learned action sequences* or just *action sequences*.\n\n\nReference:\n- Higgins et al., \u201cbeta-VAE: Learning basic visual concepts with a constrained variational framework\u201d, ICLR 2017.\n- Burgess et al., \u201dUnderstanding disentangling in beta-VAE\u201d, NeurIPS 2018.\n- Michal Rolinek; Dominik Zietlow; Georg Martius: Variational Autoencoders Pursue PCA Directions (by Accident), CVPR 2019.\n- Francesco et al., Challenging common assumptions in the unsupervised learning of disentangled representations. In 36th International Conference on Machine Learning, ICML 2019.\n\n"
            },
            {
                "document_title": "Response to All Reviewers",
                "document_content": "I apologize for my poor writing skills and all the defects of our paper. It seems necessary to reclaim the motivation of this work. This paper's main purpose is to emphasize the importance of the data itself on disentanglement learning. We argue that a proper definition of disentanglement is disentangling explanatory action sequences because an isolating sample and its representation are insufficient for disentanglement or interpretability.  The internal relationships between the samples are the key to understanding and disentanglement.\n\nThe current works mainly try to interpret disentanglement from the model perspective.\n1. AnnealedVAE claims that an information bottleneck enforces the model to find a local minimum for the objective, and \"which are aligned with factors of variation.\" It suggests that increasing information on latent leads disentanglement.\n2. Rol\u0131nek shows the similarity between PCA and VAE about \"the local behavior of promoting both reconstruction and orthogonality.\"\nHowever, we believe that the data plays a primary role, and the model is secondary. Therefore, we examine the effects of the data by four cases in Sec. 3. The special cases show little correlation between the learned representation and the ground-truth. Though PCA-like behaviors can interpret the A1-A3, it needs more explanations to interpret A4. The other difficulty is measuring the principal component quantitatively. Dividing the dataset into action sequences also helps us calculate the entropy of actions or the variation of components.\n\nOur method is similar to AnnealedVAE both in results and the method; nevertheless, the interpretation is the main difference. We argue that the step values of beta instead of gradually modifying are vital to disentanglement. The other reason is the phenomenon of re-entanglement. The disentanglement metric reaches the highest in the middle phase, and it falls on the last phases. Here is the MIG score of one trail (beta-vae):\n\n| Step | discrete_mig        |\n|------|---------------------|\n| 230  | 0.3756360504736129  |\n| 461  | 0.37883395407382375 |\n| 693  | 0.399240366607318*  |\n| 924  | 0.3772557544010944  |\n| 1156 | 0.3333029456260484  |\n| 1387 | 0.3523813802097221  |\n| 1618 | 0.3801707492631984  |\n| 1850 | 0.349180837978194   |\n| 2081 | 0.3663198905598549  |\n| 2313 | 0.34369199263536326 |\n| 2544 | 0.3663410570974127  |\n| 2775 | 0.353156190268057   |\n\n"
            },
            {
                "document_title": "Algorithm is not clearly explained and more experiments are needed.",
                "document_content": "Summary:\n\nThe authors proposed fractional variational autoencoder (FVAE) for the learning of disentangled representation where the action sequences can be extracted step-by-step. Experiments are shown to illustrate how the algorithm works.\n\n#################\n\n\n. The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. Is it the same as the objective of \\beta-VAE?\n\n. Fig.3: 1) What's the KL divergence here? Is it between the posterior and the prior? 2) It's claimed that the trend of KL divergence is consistent with that of entropy. But it is hard to see from Fig. 3. 3) It is claimed that the significance of action is related to the capacity of learned latent information. Based on Fig. 3, this conclusion is not convincing. Also, Fig. 3 is obtained based on a toy dataset. To claim it as a main contribution, the conclusion needs to be verified on other datasets as well.  \n\n. Section 4.1: What's definition of the label here? It's not clear. Is it like the types of shapes on dSprites?\n\n. Section 4.1: The training on dSprites includes two phases: find thresholds and then train different stages. 1) The authors arranged  three stages for dSprites. This seems arbitrary. Why not four or five stages? 2) What's the training objective function of each stage? 3) How are the curves in Fig. 5 derived? More explanation is required.\n\n. Section 4.2: It is claimed that ``One can recognize three points where the latent information suddenly increases: 60, 20, 4.'' This is hard to see from Fig. 5b) as all curves look smooth. Thus, the following three-stage training process is questionable. The training for unlabeled task needs more study.\n\n. The experiments are limited. There are a lot of papers regarding disentangled representation, and the authors only compared with \\beta-VAE. \n\n\n"
            },
            {
                "document_title": "Disentangling action sequences is interesting but more details and experimental results are needed.",
                "document_content": "Summary: \nThis paper addresses the problem of disentangling representations using Variational Autoencoders. In particular, the authors introduce the concept of disentangling action sequences and propose the fractional variational autoencoder framework to disentangle them step-by-step. To this end, they analyze the inductive biases on the data and define latent information thresholds which are correlated with the significance of the actions.\n\n##################################################################\n\nStrengths:\n- The paper tackles the important problem of disentangling representations.\n- Overall, the paper is well structured. In particular, the introduction section clearly motivates the problem and summarises existing approaches.\n- The idea of disentangling action sequences is interesting as it allows to analyse the inductive bias on the data.\n\n##################################################################\n\nWeaknesses:\n- Fractional variational autoencoder (FVAE) proposed in this paper is closely related to the work of Burgess et al. (2018). Although authors include a brief discussion comparing both methods, the main novelty of FVAE (i.e. explicitly avoid mixing the factors and defining thresholds to prevent re-entanglement for extremely high capacity) is still not sufficiently emphasised throughout the paper. Moreover, it would be good to include experimental comparisons to Annealed VAE (for instance in Figure 6) to give more insights on the relevance of the proposed approach.\n- Description of the toy dataset family is not easily understood and it would be good to clarify annotations in Figure 1(a). Since Figure 9 of Appendix is clear enough, it might be nice to include it in the main paper to help the reader follow the analysis of the corresponding experiment. In the latter, it is shown that the disentangled representations are not invariant to orientation of rectangles (A1, A3). Here, one can assume that positions x and y and orientation of rectangles contribute differently to reconstruction. Hence, it would be interesting to see the effect of progressively increasing the bottleneck capacity on the obtained representations, as proposed in Burgess et al. (2018). Would this help disentangle position at first then orientation of rectangles?\n- While a quantitative analysis has been provided in Figure 6 using the MIG metric to compare FVAE and Beta-VAE, it is still insufficient to make clear conclusions on the performance of the proposed method. Several metrics (e.g. Mutual Information Gap, Modularity, etc.) and evaluation benchmarks have been integrated in DisLib (Locatello et al. (2019)) allowing easy evaluations of disentangling approaches. I recommend using it for further quantitative analysis. \n- In Section 2, authors present the concept of disentangling representations and describe the work of Locatello et al. (2019) which shows the necessity of inductive bias to unsupervisedly disentangle the underlying factors. After mentioning that \u201ca formal definition of the inductive bias is still unavailable\u201d, this point would be more clear with some examples, for instance the assumption in Burgess et al. (2018) that Beta-VAE aligns latent dimensions with components that make different contributions to reconstruction.\n- In Section 3.1, authors mention that existing models disentangle the ground-truth factors by accident. This would be a little misleading to previous claims on the role of inductive bias on the data (or the model) which allows to achieve disentanglement  Locatello et al. (2019). I suggest more clarification to this point.\n"
            },
            {
                "document_title": "This paper gives a new kind of comprehension to disentangled representation learning. In this paper, existing unsupervised disentangled methods are trying to obtain disentangled action sequences instead of independent factors. Through the proposed FVAE model, action sequences with different levels of significance can be obtained step by step, and experiment results show that the weight \beta has positive correlation with the significance.",
                "document_content": "Pros:\n1. This paper gives a new comprehension of existing unsupervised disentangled representation learning method, regarding it as finding commonality of input data and disentangling action sequence information in factors.\n2.  This paper gives a new idea that \\beta has positive relationship with action significance and conduct an experiment to validate it.\n3.  This paper proposes a new variant of VAE called FVAE which learns the disentangled action sequence step by step.\nCons:\n1.  This paper mainly gives descriptions of insights while lacking some formulations to explain the settings and methods better.\n2.  Experimental results are not well organized, some axis lack corresponding labels, like Figure 5. Figures are not clear, like numbers in Figure 1 (a). \n3.  Some definitions are ambiguous, like x in equation 5. \n4.  Some descriptions in the paper are confusing, e.g. \u201cWe argue that the factors are not the key to disentanglement since the learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).\u201d This experiment, from my perspective, shows that the learned factors are disentangled in a particular form which is not consistent with the preset ground truth. And different action sequences are also different factors. Section 3.1 might be described in a more considerate way to show what the experiment results really indicate. \n5. There exist some typos in this paper, like \u201cleaned\u201d for \u201clearned\u201d.\n\nOverall review:\nThis paper gives a new comprehension of existing disentangled representation learning by regarding it as finding disentangled action sequences, which is interesting and has some good insights. However, some ideas should be supported by clearer formulations and some conclusions of experiments are not valid. Moreover, the logic of this paper is a little unclear, and experimental figures are incomplete. With some modifications, this paper could be an excellent paper..\n"
            },
            {
                "document_title": "Review 5",
                "document_content": "Manuscript Summary\n====\n\nThis paper constructs a disentanglement problem from a temporally causal view, where data are observed in sequences, and where actions cause those observations to change as the sequence progresses in time. Their stated objective is to recover the specific actions and their parameters (e.g. rotation and translation, and their magnitudes/signs).\n\nThe authors thus construct a \"Fractional VAE\" (FVAE), and then construct sequences from Dsprites and Chairs based on their statement of the problem.\n\nInitial Decision (from this reviewer), Review, and Reasoning\n====\n\nI think this paper should be rejected; were this a journal, I would suggest at least major revision.\n\nOverall the concept of bringing temporal causality (which for some cases *is* valid as the causal diagram/frame) into the disentanglement problem statement is a good idea. However, after that point in the manuscript, I cannot understand what has done. For example, section 2 is a restatement of previous work, and section 3 begins with an explanation of the dataset construction. Section 3.3, section 4, and Figure 4A I think describe the paired asymmetric autoencoder method, but a sparse few paragraphs are given at this point. What they _do_ describe is a set of \"sub-encoders\" with varying compression rates $\\beta$. However, beyond varying the rates, it's not clear how particular \"ground truth\" factors (e.g. $\\theta, L$) can be selected for and locked in to specific latent factors in an unsupervised manner, or even if this should happen.\n\nBy varying $\\beta$ we receive different amounts of information in the representation, but how can we ensure across \"learning phases\" disentanglement? Further, if these KL divergences are set to different $\\beta$, this means we don't have a divergence for the joint representation? (the concatenation of the sub-encoders) So how can we ensure that these are disentangled themselves? While successively learned encodings would optimally not include previously encoded data, why would these encoders learn separate concepts instead of coarse grained representation with all concepts to successively finer representations (or refinements to those coarse grained representations) with each successive sub-encoder.  Or are these separate phases repeated?\n\nPerhaps these questions have answers in the positive, but they should be answered by the manuscript.\n\nI further cannot make a connection between the actions sequences and the training methods/arch. I think I have understood both (...save for the above highlighted problems), but I cannot understand where the sequences come in practically speaking, even modulo the aforementioned issues. How does the FVAE or its training scheme use this information? Does it use this information?\n\nI think the positive experimental results in Figure 6c mean that there is something here. However, I cannot tell given section 4 what is actually being done.\n\nSuggestions\n====\n\nI suggest a clear procedure section with numbered steps. It is of vital importance that the reader understand what has been done. If it is already there, it should be made much more obvious/clear.\n\nI think the connection between sequences of images under actions and the proposed method needs to be made, or, if I missed this connection, should be made clear.\n\nThe initial portion of section 3 concerning dataset construction might also be moved to much later.\n\nThere are philosophically challenging sections which I did not comment on in the review portion. I think these are approximately orthogonal to the method due to the scope of the problem: rotation/translation may be disentangled. Can dog breeds *be* disentangled, even in theory (example from Section 3)? Disentanglement of simple mechanisms/\"actions\" are perfectly acceptable at least in my opinion for the state of the field at this moment. Using more complex examples may not be helpful. Similarly, the discussion in section 1 raises questions that are unrelated to the later method. Since a literature review is undertaken in Section 2, the paper could have started at \"In this paper, we first demonstrate that instead of the ground-truth factors the disentangling approaches [should] learn [disentangled] actions.\", with an update for phrasing.\n\nI would also give the paper another read through for grammar."
            }
        ]
    },
    "7BlQMwp_44p": {
        "contradict": 0,
        "review_score_variance": 0.25,
        "paper_acceptance": "accept",
        "label": "train",
        "documents": [
            {
                "document_title": "Paper Decision",
                "document_content": "This paper considers the problem of ReLU regression under the Massart noise model that has recently been studied extensively for classification problems. The main result of the paper is an algorithm that does exact parameter learning under certain distributional assumptions. \n\nAll the reviewers appreciated the results of the paper. While it builds on prior techniques in the area, the technical novelty in the work is high enough. Certain technical questions raised by the reviewers were subsequently resolved by the authors' response. Overall this is a solid theory paper and I recommend acceptance."
            },
            {
                "document_title": "Abstract",
                "document_content": "Ilias Diakonikolas, Jong Ho Park, Christos Tzamos"
            },
            {
                "document_title": "Post-rebuttal",
                "document_content": " Thanks for the feedback. I keep my initial score and recommend this paper for acceptance. "
            },
            {
                "document_title": "Final comments to authors",
                "document_content": " Thanks for your response,\n\nMost of my concerns are obviated. \nHowever, I keep my score since (at least IMO) the writing of the paper can still be greatly improved and the contents could become more accessible by the general audience. Moreover, some of the transitions between topics are not smooth enough.\n\nOther than that, this work seems to be a valuable technical contribution to the NeurIPS community."
            },
            {
                "document_title": "reviewer comments",
                "document_content": " Thank you for the response. It addressed all my concerns."
            },
            {
                "document_title": "Author response for Reviewer 8NsY",
                "document_content": " We thank the reviewer for the helpful suggestions and comments. Below we respond to the main concerns/questions from the reviewer. \n\n**Sample Complexity:** As the reviewer has mentioned, the sample complexity of our algorithm for linear regression with Massart noise is quantitatively higher, compared to the noiseless case (corresponding to $\\eta=0$). While it is an interesting question to improve on the sample complexity of our algorithm, we emphasize that our work gives the *first polynomial sample and time* algorithm with strong recovery guarantees for ReLUs under minimal distributional assumptions. \n\n**Bit complexity Dependence in Runtime:** As the reviewer has noted, the runtime of our algorithm depends on the bit complexity $b$ of the points. This is a common theme in *any* algorithm that uses linear programming as a subroutine. For example, even for the basic problem of learning a linear separator *without noise*, all known algorithms require such a bit complexity dependence. In fact, such a dependence cannot be removed without a strongly polynomial time algorithm for arbitrary linear programs -- a major open problem in computer science. In our specific setting, we solve a linear program (or use the ellipsoid method) to perform exact recovery, which incurs a runtime polynomial in $\\log(1/\\epsilon)$, not $1/\\epsilon$. Given our learning task, we cannot remove the dependence on $b$ without a strongly polynomial algorithm for LPs. However, it is important to note that the sample complexity of our algorithms does not depend on the bit complexity, as a consequence of using radial isotropy. In fact, spectral outlier removal procedures incur a b dependence in the sample complexity, as demonstrated in our PAC learning results (Appendix D). Therefore, we highlight that the bit complexity dependence on the runtime is not unusual and rather that the lack of $b$ in the sample complexity is an achievement.\n\n**Distributional Assumptions and Examples:** The distributional assumption we require, i.e., that $\\Pr[w \\cdot x \\geq 0] \\geq \\lambda$, means that there exists at least some non-trivial probability mass on any homogeneous halfspace. For example, the parameter $\\lambda$ would be $1/2$ for any distribution symmetric around the origin. This is an extremely mild condition that is satisfied by a wide range of distributions. For example, it is satisfied by any mean-zero distribution with non-degenerate covariance matrix. In contrast, virtually all prior results on linear or ReLU regression require at least some non-trivial concentration (tail bounds), anti-concentration, and anti-anti-concentration properties.\n\n**Technical Novelty of the use of Radial-isotropic Transformations:** The central question of our paper is whether there exists realistic label noise models in which efficient learning is possible *without strong distributional assumptions*. We know that stronger adversarial noise models, such as the strong contamination model, have computational hardness results as described in the introduction (Lines 27-34). Moreover, for the Massart noise model, there has been recent work (see references 9, 12 in our paper) that learn halfspaces in a distribution-independent fashion. Analogously, we provide ReLU regression results with only mild assumptions that do not require any strong tail bounds or concentration bounds.\n\nIndeed, as the reviewer points out, if the data is uniformly distributed on the unit sphere, then we do not need to perform radial-isotropic transformations since the data is already well-behaved. However, this is not the case for most distributions, even after normalization. Thus, the use of radial-isotropic transformations allows us to generalize our regression method to apply beyond such well-behaved distributions.\n\n**Computational Cost of Radial-Isotropic Transformation:** Computing a radial-isotropic transformation $A$ can be done in polynomial time as specified in Lemma 2.1. Because we do not need an exact transformation but only an approximate one where $\\gamma = 1/2$, we can use previously established algorithmic results in computing this approximate transform. We describe how to compute $A$ using the algorithmic results of Artstein-Avidan et al. and prove this lemma in Appendix A.\n"
            },
            {
                "document_title": "Author response for Reviewer Dpt6",
                "document_content": " We thank the reviewer for the appreciation of our paper and the helpful suggestions.\n\nFor Definition 1.1, the adversary may change the labels of the (potentially) corrupted samples to arbitrary values. We will also update the paper with clear definitions and discussions with regard to the bit complexity of the parameters and samples."
            },
            {
                "document_title": "Author response for Reviewer vpXo",
                "document_content": " We thank the reviewer for the helpful suggestions and comments. Below we address the specific questions from the reviewer. \n\n**Robustness under Model Misspecification:** It is indeed an interesting open question and a promising future direction to extend our work to cases where the true function is not actually a ReLU and allow for some form of model misspecification or additive noise as the reviewer suggests. Our theoretical results focus on the realizable model for learning ReLUs (and linear functions) with Massart noise. This is an important and well-studied setting even for the noiseless case (corresponding to $\\eta = 0$), and has been the focus of a number of prior works, including recent papers appearing in top venues (see references [23, 31, 45, 49] and Lines 66-73).\n\n**Description and Definition of Adversary:** We define the Massart adversary in Definition 1.1 and use this definition throughout the paper. In fact, this definition is equivalent to the informal description given in Lines 117-119. This is because the adversary of Lines 117-119 does not have to change all the labels of the randomly selected $\\eta$-fraction. By keeping some of the labels clean, this adversary is equivalent to that of Definition 1.1.\n\n**Intuition Behind the Massart Model:** As the reviewer has mentioned, the Massart noise model is stronger than purely random noise and weaker than the strong contamination model, which has computational hardness results even for well-behaved distributions, as described in Lines 32-34. It aims to capture cases where the samples that are (potentially) corrupted are not correlated, i.e., a uniformly random subset of examples is corrupted. The adversary may corrupt the values of these samples arbitrarily in correlated ways, but may not choose which points to corrupt in advance. This model allows us to escape the computational hardness results that apply when the adversary is all-too-powerful, and obtain efficient and robust regression algorithms under minimal assumptions about how the values are corrupted.\n\n**PAC Learning ReLUs in Line 107:** PAC learning ReLUs is the task of learning a hypothesis $h$ such that $\\Pr_{x \\sim \\mathcal{D}_x}[h(x) \\neq \\mathrm{ReLU}(w^* \\cdot x)] \\leq \\epsilon$ with probability at least $1-\\delta$ *without* any distributional assumptions on $\\mathcal{D}_x$. So it suffices to learn a hypothesis that outputs similar labels as $w^*$ without the hypothesis having to be exactly $w^*$. For instance, we can PAC learn linear functions even in the case that it is information-theoretically impossible to recover $w^*$ exactly, as shown in Appendix D. In the case of Theorem 1.3, we make a mild assumption that $\\Pr(w\\cdot x \\geq 0) \\geq \\lambda$ to *exactly* recover $w^*$, while PAC learning would not make any assumptions and output a close hypothesis $h$.\n\n**Comment regarding Line 144:** Yes, we did not include the transpose since $A$ is symmetric, but we agree that this may be confusing. We will fix the typo and keep our notation consistent. \n\n**Definition of $\\mathcal{S}^{d-1}$:** Yes, the reviewer is correct. We will add this definition for clarity."
            },
            {
                "document_title": "Author response for Reviewer 463t",
                "document_content": " We thank the reviewer for the appreciation of our paper and the helpful suggestions. \n\nRegarding the reviewer\u2019s first comment, we note that our results in the main body of the submission are self-contained and do not depend on Hopkins et al.; only our extended results in the appendix require the facts proved in Hopkins et al. However, we agree that their work provides relevant insight and techniques. We will discuss their work when discussing radial isotropy in the Technical Overview section. \nRegarding the second comment, we note that ReLU regression is significantly more challenging than linear regression. A lot of recent literature aims to address the intricacies of ReLU regression stemming from the non-convexity of $\\ell_1$ and $\\ell_2$ regression. We have already highlighted these difficulties in Section 1.2 (Lines 156-174).\n\nAt a high-level, if we knew a priori which points lie on the positive side of the ReLU, ReLU regression would indeed be a straightforward application of linear regression --  since we can learn the parameter vector $w$ with these points. In the presence of noise, however, it is unclear which region corresponds to the positive part and we must learn it simultaneously. This is one of the main challenges that we overcome. Our algorithm builds on the ideas developed in our linear regression warmup, to iteratively improve the guesses about the positive ReLU region yielding a separation oracle. We will incorporate this additional intuition in the revised version of the paper.\n"
            },
            {
                "document_title": "Joint Response",
                "document_content": " We thank the reviewers for their time and effort in providing feedback. We are encouraged by the positive feedback, and that the reviewers appreciated the paper for the following: (i) significant and/or interesting results (463t, vpXo, Dpt6), (ii) technical contribution (463t, Dpt6), and (iii) organization/clarity (vpXo, Dpt6, 8NsY). We address the individual questions and comments by the reviewers separately."
            },
            {
                "document_title": "Official Review of Paper7733 by Reviewer vpXo",
                "document_content": "The aim of this work is to propose a computationally efficient algorithm for linear and ReLU regression in the presence of a bounded adversarial Massart noise model. It has been tried to make the assumptions on the underlying data distribution as mild as possible, not to mention that (as claimed by the author(s)) some assumptions are necessary from an information-theoretic point of view.\n\nThe core idea is to consider the unknown corrupted samples as outliers which do not follow the simple linear or ReLU rules between their features and corresponding labels. This should be added to the fact that (based on the paper's assumption) the fraction of corrupted samples is less than 1/2, so the clean data points have the majority. This way, author(s) have proposed a linear transformation on the samples in order the make them \"Radially Isotropic\", so (with high probability) the ratio of outliers to clean samples remain as low as possible in *every direction* of the space. Then, some existing robust estimators such as sum of $\\ell_0$ or $\\ell_1$ models have been shown to remove the effect of outliers and acquire the true parameters. I haven't completely checked the proofs, however, the overall idea makes sense to me and is, in fact, quite interesting. In any case, I am not completely familiar with this line of research so I wait to see other reviews in order to assess the novelty of techniques that have been utilized in this work.\n\nWith respect to weaknesses, paper lacks proper discussion at some points which I have explained in the \"Main review\" section. Also, author(s) have assumed a completely clean and noise-free model except for the adversarial part which still includes more than half of the samples. This assumption is a little worrisome in practice, where some minimum levels of, for example, additive noise are *always* present. How robust or sensitive is the proposed method and its theoretical guarantees when the presumed ideal noise-free environment is minimally perturbed?\n\nOverall a well-motivated and fairly well-written paper with some interesting theoretical achievements (as far as I am aware). My vote at this stage is weak-accept.\n  \nMy main concerns are as follows:\n\nAuthor(s) seem to have a good grasp of existing literature in this area, but it hasn't been completely reflected in the Introduction section. Some discussions and comparisons are still vague and need clarification. I believe the classification of prior advancements in terms of: achieved guarantees, distributional assumptions and etc. can be improved by some reorganization, which also helps the reader to position the current work w.r.t. others, more effectively.\n\nThe adversary described in Definition 1.1 does not match with the one that has been explained in Lines (117:119). In Def 1.1, adversary is allowed to alter (at will) each point $\\left(x_i,f\\left(x_i\\right)\\right)$, with probability $\\eta\\left(x_i\\right)\\leq\\eta$. No information regarding $\\eta\\left(\\cdot\\right)$ has been given except that it is bounded by the constant $\\eta$. That means the function $\\eta\\left(\\cdot\\right)$ can also be chosen by an adversary. That is different from the explanation given in Lines (117:119): \"we consider a more restricted adversary that is presented with a uniformly random $\\eta$ fraction of the points, which can be corrupted arbitrarily at will\". So a natural question would be which of them is finally considered in this work?\n\nAlso, more discussion w.r.t. the adversarial perturbation model considered in this work would be helpful. It is not as strong as the \"contamination model\", but obviously hurt more than a purely random noise. What is the intuition behind this particular noise model?\n\nIn Section 1.2, the transition from ReLU and linear regression in the presence of Massart noise to $\\ell_0$ or $\\ell_1$-regression is not smooth. More explanations are needed here, otherwise the core idea behind the results become somehow ambiguous and hard to understand.\n\n\n------------------------------------------------\nMinor comments:\n\n-(Line 107): What author(s) mean by \"It remains an interesting open problem whether similar PAC learning guarantees can be obtained for the case of ReLU regression.\"? Aren't the results from Theorem 1.3 associated with the most general case? Or author(s) are referring to a more general adversarial noise model rather than Massart noise here? \n\n-(Line 144): I guess it should be $w=A^Tw'$, right? It might not be important since $A$ is later assumed to be symmetric. However, it may confuse the readers.\n\n-(Definition 1.4): What is $\\mathcal{S}^{d-1}$? Is it the surface of $d$-dimensional unit sphere? No problem here."
            },
            {
                "document_title": "Official Review of Paper7733 by Reviewer 463t",
                "document_content": "The authors study exact parameter recovery in ReLU regression under a generalization of the semi-random Massart noise model. Assuming certain distributional anti-concentration, the authors propose and analyze an algorithm that runs in polynomial time and recovers the exact parameters with high probability, given a sample of size polynomial in the input dimension and the (inverse of) the Massart parameter. Along the way, they also provide similar guarantees for the simpler case of linear regression.  The paper is a joy to read. The study is important and relevant to the NeurIPS community. To the best of my knowledge, the results are novel and significant. I checked the proofs only at a high level, but the claims seem sound to me.\n\nA couple of comments/questions:\n1. The algorithm design as well as the proofs -- both in linear/ReLU regression -- leverage insights and techniques from the work of Hopkins et al. 2020. While this paper is cited in the appendix, there is no reference to it in the main text. I urge the authors to discuss this reference in the main text.\n2. Given suitable anti-concentration assumptions, what are the main challenges in extending the results from linear regression to ReLU regression? The guarantees in Theorems 1.2 and 1.3 are very similar, and under the anti-concentration assumption for ReLU regression, proof of 1.3 seems like a straightforward extension of 1.2.\n Yes"
            },
            {
                "document_title": "Official Review of Paper7733 by Reviewer Dpt6",
                "document_content": "The paper considers the problem of linear and ReLu regression in the Massart noise model, where an adversary is allowed to change the label to an arbitrarily value with some probability at most $\\eta <1/2$. The work develops an efficient algorithm that achieves exact parameter recovery in this model under mild assumptions on the underlying distribution.   **Quality and Clarity**: \n\n-  One of the strong points if the paper is that it is very well-written and well organized. The ideas are presented clearly with intuitive explanations, and the proofs are rigorous. \n\n- The authors are careful in evaluating the strengths/weaknesses of their work. The work seems to be built upon an established literature and the review of related works are informative. \n\n- Minor comments: \n  - Definition 1.1: \"and after inspecting which samples can be corrupted, it may change the label to an arbitrary value\" --> I interpreted this as \"change each label to an arbitrary value\" or \"change the labels to arbitrary values\". Please clarify.\n  - bit complexity of the parameters and samples: this is mentioned a few time but are not defined\n  - the sentence starting at Line 140 is very difficult to read\n\n**Originality**: \n\n-  The paper consider a slightly weakened model (in comparison to the the contamination model), but leads to more generality in the underlying data distribution.\n\n- The result of the work is significantly different (stronger) from the closest comparisons ([32] and [10]), and the methods are designed specifically to address the limitation of those works.\n\n- The generalization of the approach from linear to ReLU regressions are non-trivial in both technical analysis and algorithmic designs. \n\n\n**Significance**:\n\n- As stated in the Originality section, the result of the work is significantly different (stronger) from the closest comparisons ([32] and [10]) and seems to extend existing works in a demonstrable way.\n\n- On the other hand, I'm unfamiliar with some pieces of related work and don't have a strong conviction in how it will impact the field, and would leave the judgement up to other reviewers. \n n/a"
            },
            {
                "document_title": "Official Review of Paper7733 by Reviewer 8NsY",
                "document_content": "This paper studies regression problems in presence of Massart (bounded) noise, where for each sample x_i, the adversary is allowed to change its label/response f(x_i) to an arbitrary value with probability less or equal to eta. Specifically, the authors focus on two problems: linear regression and ReLU regression. For both cases, they make a ``mild yet necessary assumption on the data, that the distribution is not concentrated entirely on any linear subspace; and provide algorithms which exactly recover the true function in polynomial time with constant probability.   I feel the paper is well-written but I have a few concerns on the theoretical guarantee.\n\n- In Theorem 1.2, it turns out that the sample complexity for robust linear regression is suboptimal. In particular, even when the noise rate eta = 0, it is given by O(d^3) which is worse than that of noiseless regression. It also seems unpleasant to have bit complexity dependence in running time.\n\n- In Theorem 1.3, I cannot follow the assumption that $P( w \\cdot x \\geq 0 ) \\geq \\lambda$ for all w.\n\n- What is the computational cost to obtain a radial-isotropic transformation matrix A? This turns out to be a crucial algorithmic component but I did not find a concrete discussion. Note that since you require unit data norm and identity covariance simultaneously, finding such transformation is nontrivial.\n\n- The distributional assumption is informally stated throughout. Can you provide a fews examples that satisify the condition, say Gaussian distributions or uniform distributions?\n\n- I may miss some important messages, but if the data distribution were uniform, then would it be necessary to perform radial-isotropic transformation? My understanding is not being necessary, in which case the technical novelty appears vacuous.  yes"
            }
        ]
    },
    "XL9DWRG7mJn": {
        "contradict": 0,
        "review_score_variance": 0.0,
        "paper_acceptance": "accept",
        "label": "train",
        "documents": [
            {
                "document_title": "Paper Decision",
                "document_content": "This paper reformulates an existing problem (how to sparsify gradients in distributed training) and proposes to minimize a new objective (the total compression error subject to communication constraint as opposed to per-iteration compression error). This change of viewpoint leads to a new algorithm (hard threshold algorithm with variable sparsity). The authors show the effectiveness of the proposed algorithm through theoretical bounds and experiments. All reviewers agree that this is a valuable contribution. Comments from previous submission to ICML are adequately addressed."
            },
            {
                "document_title": "Abstract",
                "document_content": "Atal Sahu, Aritra Dutta, Ahmed M. Abdelmoniem, Trambak Banerjee, Marco Canini, Panos Kalnis"
            },
            {
                "document_title": "Thank you for your positive reassessment",
                "document_content": " We thank the reviewer for the positive feedback, and for pointing out a meaningful direction for future research. We will clearly elaborate our use of the word optimal, as mentioned in the response to reviewer o9PJ."
            },
            {
                "document_title": "Comments after rebuttal",
                "document_content": " Many thanks to the authors' detailed replies and other reviewers' insights into this paper. I do appreciate the great value of this work, but I am still worried about the claim of optimality, even though I understand that this optimality is restricted to the model the authors proposed. My understanding is that from the upper bounds in Theorem 1, no matter what optimization trajectories, the upper bound only depends on the total compression errors, and the hard-threshod is optimal for any fixed optimization trajectories as long as the right threshold is chosen, and thus the optimality (please let me know if I misunderstood this). However, Theorem 1 only provides an upper bound, if we are minimizing an upper bound, claiming optimal seems to me kind of strong. I understand this upper bound motivates the hard-threshold compression methods, and the paper presents very solid theoretical and experimental results for this method. However, in terms of communication efficiency, i.e. bits over guaranteed optimization errors, there is no clear theoretical improvements shown, so maybe this can be some future works. I agree with other reviewers' votes and raise my score to 7."
            },
            {
                "document_title": "Official Review of Paper10919 by Reviewer TZ2h",
                "document_content": "This paper analyzes the hard-threshold sparsifier in distributed SGD with convergence analysis and extensive experiments. Main contributions include: 1. Provides upper bounds of the optimization errors for the hard-threshold sparsified distributed SGD, which improves from top-k compressor with linear speedup and compressor operator parameter dependence. 2. Conducts extensive experiments to demonstrate the benefits of hard-threshold sparsifier, achieving much better performance given the same average compression density (# of used coordinates / # of coordinates of DNN weights).   1. I am trying to understand why the proposed communication-complexity model is new, it seems to me it is an adaptive compression operator allowing different # of coordinates to be sent for each iteration. And the section 4.4 and Lemma 3 seems incorrect to me, since the optimization over B is sequential, the choice of first block will change the second block of A, so I didn\u2019t quite follow why the proposed sparisfier is optimal for this communication-complexity model. \n2. On the confusion of total error minimization. In the paragraph starting from line 44, it seems to the authors are trying to present a new perspective that is not minimizing per iteration compression error but total compression errors, but the last sentence seems to me conveying the message of considering fixed total communication budget, which is exactly what I think other papers have discussed, how to tradeoff optimization error with total communication budgets. So there may be some confusion in this paragraph.\n3. The authors provide upper bounds of optimization errors for the proposed spasifier, in strongly convex, convex, and nonconvex settings respectively. Those upper bounds improve from the results using top-k sparsifier in terms of linear speedup and compressor parameter dependence. \n4. The provided upper bounds, however, seem to me not clearly they are more communication-efficient than the top-k sparsifier, since there is no characterization of the total number of coordinates being used in theory. Ideally, if we set the threshold be very small, most coordinates will be used. In experiments, the authors provide solid results showing that the hard-threshold sparsifier does use less coordinates in total than the top-k. \n5. The total error minimization perspective seems to me more like an observation of the consequence using a hard-threshold sparsifier, it may need further arguments to show it is the reason for better performance. \n6. The paper is in general well written and very clear, the extensive experiments are helpful for the understanding of the practical benefits of the hard-threshold sparsifier.  Yes."
            },
            {
                "document_title": "Official Review of Paper10919 by Reviewer VFA3",
                "document_content": "The paper studies the role of compression operators on the convergence of Distributed SGD with Error Feedback. In particular, via simple observations, the authors conclude that a hard-threshold sparsifier with a carefully tuned threshold parameter minimizes the total error appearing in the analysis because of the presence of compression. Moreover, they show empirically the connection between poor behavior of EF-SGD with Top-k compression and the severe error accumulation.\n\nMotivated by these observations, the authors derive new convergence guarantees for EF-SGD with absolute compressors. This class of compressors covers hard-threshold sparsifier. The derived bounds show that the compression does not affect the slowest terms in the bound. Moreover, the authors derived the first complexity result in the non-convex case for EF-SGD with $\\delta$-cnotraction operators without bounded gradient assumption and $n > 1$ (though, under bounded data dissimilarity). Finally, the paper contains a good empirical study of the performance of EF-SGD with the hard-threshold sparsifier. The authors also provide an insight on how to tune the threshold parameter in order to outperform EF-SGD with the Top-k operator.  ## Strengths\n\n1. **Simple but important observations about total error minimization.** The paper provides a closer look at the convergence of EF-SGD and identifies what quantity should be minimized in order to get better results. That is, via the sequence of simple observations, authors show that a hard-thresholding sparsifier (with fine-tuned threshold parameter) is the optimal choice in terms of the total error minimization. Moreover, the authors properly explain all the details and support their theoretical observations with empirical findings (e.g., see Figures 1 and 3).\n\n2. **Clarity and proofs.** The paper is clearly written. The proofs are easy to follow and contain only a couple of typos.\n\n3. **New results for EF-SGD.** The authors derived new results for the convergence of EF-SGD with absolute compression for strongly convex, convex, and non-convex objectives. The slowest terms in the derived bounds have a linear speedup and are not affected by compression-dependent parameters. This is a good property since the derived bounds match the ones for SGD without compression if the target accuracy is small enough / the number of communication rounds is large enough. Moreover, the authors derived the first convergence result for EF-SGD for $\\delta$-contraction operators without assuming boundedness of the gradients, but under Assumption 4, that bounds dissimilarity between local loss functions. Although the proofs substantially rely on the known techniques, the obtained results are quite good.\n\n4. **Numerical experiments** show a connection between the behavior of EF-SGD with Top-k and hard-thresholding sparsifier and \"error buildup\". Therefore, these numerical results justify the insights provided in Section 4.\n\n## Weaknesses\n\n1. **No analysis for the arbitrary heterogeneous case for non-convex objectives (minor).** The derived bounds in the non-convex case substantially rely on Assumption 4 that bounds the dissimilarity between local loss functions. Although this is a significant limitation, previous works on EF-SGD rely on even stronger assumptions. Therefore, this is a minor drawback.\n\n## Questions and Comments\n\n1. **Rates for EF-SGD with $\\delta$-contraction operator.** The rates shown in Remarks 5 and 7 can be significantly improved via the results from [19]. Although, in [19], Assumption 3 is not considered it can be easily cast in the general framework from [19] via the following derivation: $\\frac{M}{n}\\sum_{i=1}^n\\|\\nabla f_i(x^k)\\|^2 + \\sigma^2 \\leq 2LM(f(x^k) - f(x^*)) + \\frac{M}{n}\\sum_{i=1}^n \\|\\nabla f_i(x^*)\\|^2 + \\sigma^2$. Using the results from [19] one can actually show the linear speedup even for $\\delta$-contraction operators in the $\\mathcal{O}(1/T)$ and $\\mathcal{O}(1/\\sqrt{T})$ decaying terms for $\\mu > 0$ and $\\mu = 0$ respectively. Therefore, the conclusion from lines 270-272 is not correct.\n\n2. **equation after line 654:** the enumerator in the second term should be $\\mu L^2(1+M/n)^2R_0 + L\\kappa^2 \\ln(T)$.\n\n3. **line 247, $\\nu = \\gamma_t \\kappa$:** It is better not to use $\\kappa$ in the definition of $\\nu$ because $\\kappa$ usually denotes the condition number of the problem in the optimization literature.\n\n4. **lines 283-284:** This is done for $\\delta$-contraction operators in [19] and in Qian, X., Dong, H., Richt\u00e1rik, P., & Zhang, T. Error Compensated Loopless SVRG for Distributed Optimization, Qian, X., Richt\u00e1rik, P., & Zhang, T. (2020). Error compensated distributed SGD can be accelerated. arXiv preprint arXiv:2010.00091.\n\n5. **Lemmas 6 and 7.** First of all, one should add that $\\gamma = \\min\\left(\\frac{1}{d}, \\sqrt{\\frac{r_0}{cT}}\\right)$. Next, Lemma 7 can be tightened when $c$ is small, see Lemma D.3 from [19].\n\n6. **Lemma 9.** When $c = 0$ this result is incorrect since the logarithmic factor becomes infinity. See Lemma D.2 from [19] for the correct version.\n\n7. **Lemma 11.** It is better to cite the assumptions of the lemma in the statement (or at the beginning of the subsection).\n\n8. **lines 623-627:** The discussion in these lines should be rewritten after applying the corrections suggested in comment 1. Moreover, one should also say that $\\lambda$ can be large.\n\n## Comment after rebuttal\nI thank the authors for their response. I have read other reviews as well. Overall, my evaluation of the work remains the same. Therefore, I recommend the paper for acceptance and hope that the authors will apply all necessary corrections mentioned in the reviews. The authors adequately addressed the limitations and potential negative societal impact of their work."
            },
            {
                "document_title": "Official Review of Paper10919 by Reviewer o9PJ",
                "document_content": "The paper considers gradient sparsification for learning in distributed setup, and advocates using a hard-threshold sparsifier combined with error-feedback mechanism. The paper shows that such algorithm is _optimal_ in a certain sense, and give several convergence guarantees for the error-feedback algorithms using absolute compressors and relative compressors. The empirical performance of HT and top-k compressors are also compared.  Post-rebuttal: Thank you for the reply. My concerns have been addressed well. Raising the score to 7.\n\n---\n\n__TL;DR.__ I think this paper presents several meaningful theoretical results as a contribution. However, I think some of the paper's claims are being quite oversold.\n\n__Strengths.__ Some of the theoretical results are definitely very cool to have. I believe that the convergence results in Section 5 (Theorems 2--5) is a nice contribution, and would be of interest to the distributed learning society, especially to those who study error-feedback mechanisms. Also, the proof technique going through the perturbed iteration analysis (via Lemma 10) is quite neat. Finally, the manuscript seems to discuss the related work relatively well.\n\n__Claims on optimality.__ I am very worried about the paper's claims about the optimality. The word \"optimal\" is a very bold, and should be used with a great care (in my opinion), as they could be quite misleading without delivering the assumptions and conditions it relies on. For instance, the abstract states that top-k is \"communication-optimal given a per-iteration $k$-element budget.\" But, what exactly does the word \"communication-optimal\" here mean? It is very easy to understand the statement as saying that such algorithm gives the hypothesis with a smallest loss---either in expected or high-probability sense. If I understood correctly, I think the paper is pointing to lemma 2, where authors state that Top-k gives the sparsified version of the error-feedback (or actually any signal) that has the smallest squared distortion from the original gradient signal. This also relies on the assumption that the choice of sparsification does not affect the subsequent gradient signals. This discrepancy gets more significant for the claims on the optimality of hard-threshold methods, where this \"independency assumption\" is critical for the proof. I believe that these ill-specified claims on the (possibly vacuous notions of) optimality should either be toned down to a certain degree to help readers better understand what the paper is contributing. Also, I think presenting the optimality claims in a form of lemmata without proofs is unnatural, no matter how straightforward the proofs are. I recommend either stating the claims in plain words without a formalization, or provide the result-specific assumptions clearly in the lemma statement---so that it is self-contained---and give at least a formal proof.\n\n__Stating the limitations.___ If I understood correctly, such hard-threshold sparsifiers may need a very high communication throughput at some epochs (mostly earlier). However, there are many setups such high peak communication rate is undesirable, due to the limited capacity of the communication channel (but when the delay is crucial). In such cases, having a constant communication rate could be beneficial. I think authors should discuss such scenarios to appropriately deliver the cases where the considered hard-threshold methods are desirable, and the cases they are not (sorry if I missed these parts).\n\n__Clarity: Why error-feedback?__ I am not entirely sure what is the big motivation behind considering the error-feedback mechanisms for this paper. Line 51 says: \"Consequently, we consider sparsification using the error-feedback mechanism, ...\" but I couldn't really locate the part that necessitates considering error-feedback. Could you please further clarify?\n\n__Clarity: $\\gamma_t$.__ The quantity $\\gamma_t$ appears at line 133, but I do not think this quantity is defined or introduced properly in the text.\n\n__Clarity: Assumption 2.__ I do not think Assumption 2 is explicitly assumed in any theorem appearing in the main text. But if I am correct, it is implicitly used for every theorems that use $R_0 := \\lVert x_T - x^\\star \\rVert$ in its bounds.\n\n__Question: $v$-dependency__ It was unexpected to me that the convergence guarantees for the absolute compressors $C_v ( \\cdot ) $ does not depend explicitly on the compression factor $v$, especially considering the fact that the guarantees for the $\\delta$-contraction operators contain a term that is inversely proportional to $\\delta$ (see Theorem 2 & Remark 5, for instance). Any further discussion explaining why such discrepancy happens (especially for the case $\\delta \\to 0, v \\to \\infty$) would be very nice to have; does this suggest the existence of a tighter bound for $\\delta$-contraction operators under additional assumptions the size of $p_{i,t}$?\n\n__Suggestion: Discussing $P_T$.__ While the quantity $P_T$ appears in many optimization literature, the meaning of the quantity may not be very straightforward for the readers who are relatively new to the field (like myself), especially because it gives the performance bound for $\\bar{x}_T$ generated by some weights $w_t$. Giving more ideas about the quantity may help the readers a lot, including whether one can choose $w_t$ arbitrarily or not, whether it implies that we should use an averaging scheme... I think authors should additionally mention when can such variable-communication-load methods may not useful or usable."
            },
            {
                "document_title": "Author Response to Reviewer TZ2h",
                "document_content": " We sincerely thank the reviewer for the positive assessment of our paper and for constructive feedback. Below we address the points mentioned by the reviewer:\n\n\n**1.Communication-complexity model and total error minimization.** We thank the reviewer for pointing this out. We accept that there is a slight misunderstanding about the simplification in our communication-complexity model. As mentioned in line 192, we assume a simplified model---Instead of the error-corrected update, $\\gamma g_t +e_t$, we consider a sequence of *fixed* vectors $(a_t)_{t \\in T}$  and formalized the optimization problems (5) and (6) in Section 4.4. Hence as the reviewer asserted, the optimization in Section 4.4 is not sequential. We will clarify this. \n\nWe note that the existing communication-optimal strategies [18, 38, 13, 4] minimize the compression factor (see Footnote 2) under a budget for each vector; please see lines 221-227. In contrast, the novelty of our communication-complexity model is that it better captures the total error. We owe this insight to Theorem 1 that accounts for the effect of sparsification in the entire training process. Please refer to the statement of Theorem 1, which presents the convergence of distributed error-feedback SGD with compressed communication (here, sparsification). The third term on the right hand side of the inequality captures the effect of compression between the error-corrected update, $\\gamma g_t +e_t$, and its compressed form, $C(\\gamma g_t +e_t)$ over all iterations, $t=0,1,\\cdots, T-1$. This is a well-accepted theoretical result and appears abundantly in the literature; please see [29,46]. Motivated by this result, we consider the total error perspective as a communication complexity model----where simplified total error is minimized under a total communication budget. Hard-threshold sparsifier comes out as the communication-optimal compressor under this communication complexity model. Therefore, the total-error minimization is not the consequence of using the hard-threshold sparsifier; it is the reason behind the hard-threshold as a communication-optimal sparsifier. Moreover, we substantiate this with insights from our experiments in Figures 1, 3, 5, and 6. \n\nNext, we respectfully note that we use the overall communication budget to denote the total communication throughout the training.\n\n\n**2. Convergence of hard-threshold.** We thank the reviewer for this question. The reviewer is correct---Our convergence results do not show that the hard-threshold is more communication-efficient than Top-$k$. Because we do not characterize the average data transmission for a threshold. However, we have demonstrated that hard-threshold is communication-optimal in our communication-complexity model. Our communication-complexity model is motivated by the EF-SGD non-convex convergence result, and it provides us insight into why hard-threshold has better convergence than Top-$k$ in practice."
            },
            {
                "document_title": "Author Response to Reviewer o9PJ",
                "document_content": " We are grateful to the reviewer for the constructive feedback and for providing a positive assessment of our paper. The reviewer has raised some valid questions and provided many mindful suggestions. The following are our responses to the reviewer\u2019s comments: \n\n**Claims on optimality:** We thank the reviewer for this comment. Our use of the word *optimal* is motivated by recent literature in compressed distributed optimization [ 39, 18, 13, 4] that focus on the compression-error (see Footnote 2). Any compressed optimization convergence analysis captures the effect of compression via the compression-error, and this effect is always inverse---the lower the compression error, the better the optimization upper bound is. Please refer to one-step descent Lemmas 12 and 15. To derive a convergence rate from these lemmas, we need to use the worst-case compression error/factor . For instance, to derive a convergence rate for $\\delta$-contraction operators in Remark 5 and Remark 7, one uses the worst-case compression factor in equation (7) and lines 211-212. Due to this, [39, 18, 13, 4] directly optimize for this worst-case compression factor and call their compressors as *optimal*. \n\nHowever, for gradient sparsification with a fixed $k$ element communication per iteration, this worst-case bound is not insightful. For example, to sparsify a $d$-dimensional vector, both Random-$k$, and Top-$k$ have the same compression factor, $k/d$, although Top-$k$ performs significantly better in practice than Random-$k$; please see further discussion in [8]. But we know that for a given signal, Top-$k$ attains the lowest compression-error among all sparsifiers with $k$ element communication budget. And therefore, we state Top-$k$ as the communication-optimal sparsifier under a fixed $k$-element communication budget. Precisely, this is the message behind Lemma 2 as the reviewer has correctly identified. We will elaborate on this in detail in the final version.\n\nNext, we will mitigate the confusion regarding Lemma 3. We agree that we should provide proof for Lemma 3, and will do so in the Appendix of the revised paper. Please note that we have been careful to stress throughout the text that hard-threshold is communication optimal *in our communication complexity model*, and have stated that the total-error cannot be directly minimized (Lines 175-177). We coined the term *total-error* because this term captures the compression error in the entire training process. We formalize our communication complexity model in (5) after simplifying the total error; please see Lines 191-193. \n\n**Stating the limitations.** We thank the reviewer for pointing this out and we agree with the comment. Indeed there are scenarios where a predetermined compression ratio for an iteration is desirable. Examples include dynamic network environments such as a public cloud or a shared cluster with colocated jobs [Abdelmoniem et al., 2021]. In such a setting, one may want to adjust the compression knobs according to the current network bandwidth so that training finishes within a time budget, and therefore hard-threshold is not a good candidate for this setting. \n\nIn a standard distributed cluster setting with a dedicated network, if communication is a bottleneck for Top-$k$, i.e., there is not a complete overlap between communication and computation, so a hard-threshold with the same total communication volume will have non-overlapped communication in the iterations with high data transmission, but may also have completely overlapped communication in iterations with low data transmission. Thus, hard-threshold can have less non-overlapped communication time than Top-$k$ in this case. This happens in large-scale CRT models such as DeepLight which has 90% of non-overlapping communication. Considering the opposite, if there is complete overlap between computation and communication for Top-$k$, then a hard-threshold with the same total communication volume may have non-overlapped communication time in some iterations with high data transmission. Here, we ignored two important aspects of hard-threshold: (i) Hard-threshold has better statistical efficiency than Top-$k$, thus one may require smaller iterations to a target accuracy. (ii) Hard-threshold has negligible compression overhead in comparison to Top-$k$ (lines 73-79). \n\n[Abdelmoniem et al., 2021] DC2: Delay-aware compression control for distributed machine learning. IEEE INFOCOMM 2021.\n\n**Why error-feedback?** This is a critical question and we thank the reviewer for asking this. Please allow us to discuss the development of the error-feedback theory in this context. Error-feedback was first empirically introduced by Seide et al. [40] in 2014, to alleviate the convergence of 1-bit low-precision SGD in training language models. But, for the next 5 years, the community was unaware about why error-feedback is an important technique. In 2018, Stich et al. [45] were the first to theoretically establish the convergence of SGD using $\\delta$-contraction operators with error-feedback, which was extended to the distributed setting by Zheng et al. in [55]. More interestingly, in subsequent work, Karimireddy et al. [29] theoretically showed that error-feedback can remedy the convergence issues of aggressive quantizers, such as 1-bit/Sign SGD, as well as biased $\\delta$-sparsifiers, such as Top-$k$, Random-$k$, etc. We also refer to [46] for a detailed discussion on the error-feedback framework. In a nutshell, without error-feedback, most sparsifiers (which also belong to the class of $\\delta$-contraction operators) diverge [29,46]. Moreover, the best compression ratios for gradient sparsification are achieved when we use error-feedback. Please refer to Table 1 in the comprehensive survey [54], where all implemented sparsifiers use error-feedback. This makes error-feedback an \"indispensable and essential\" technique for gradient sparsification. And this was the \"big motivation\" of using error-feedback in our work as it is focussed on gradient sparsification. However, we also thank the reviewer for pointing out \"Line 51 says: \"Consequently, we consider sparsification using the error-feedback mechanism, ....\".\" We will rewrite this line to justify why we use the error-feedback. \n\n**Clarity:$\\gamma_t$**. Thank you for this important observation. We will clearly mention that $\\gamma_t>0$ is the stepsize sequence. \n*Assumption 2.* We thank the reviewer for catching this typo and rectifying us. We will mention Assumption 2 in the theorems where it is supposed to appear. \n\n**$\\upsilon$-dependency.** We apologize for the confusion. Absolute compressors\u2019 convergence is in terms of $\\kappa$; please see lines 244-250. We will make this clearer by directly compressing the error-compensated gradients in Algorithm 1 instead of the error-compensated updates as done presently. \n\n**Discussing $P_T$.** The quantity $P_T$ in our paper denotes the expected suboptimality gap, $E[f(\\bar{x}_T)]-f^\\star$ at averaged iterate, $\\bar{x}_T$, where $f^*$ is the global minimum, defined in Assumption 2. We understand and appreciate the reviewer's concern in clarifying the meaning behind this quantity. Here $w_t$ is a carefully chosen set of weights such that we achieve the convergence result. We will highlight its meaning with references at the beginning of Section 5.1 where it appears for the first time. \n"
            },
            {
                "document_title": "Author Response to Reviewer eex4",
                "document_content": " We thank the reviewer for the effort in reviewing our paper. We also thank the reviewer for providing a positive assessment of our work and for appreciating our communication-complexity model which is indeed a key contribution of the paper. "
            },
            {
                "document_title": "Author Response to Reviewer VFA3",
                "document_content": " We thank the reviewer for the positive review of our paper. We are glad that the reviewer considers total error minimization as an important contribution to our paper. Indeed, this is the heart of our paper. Below we discuss the questions and the comments of the reviewer.\n\n**No analysis for the arbitrary heterogeneous case for non-convex objectives (minor).** We sincerely thank the reviewer for pointing out this interesting aspect. We will address the arbitrary heterogeneous case in our future work.\n\n## Questions/Comments\n\n**1. Rates for EF-SGD with $\\delta$-contraction operator.** We sincerely thank the reviewer for bringing the tighter rates in [19] to our notice. We will rewrite this discussion accordingly.\n\n**2. Typo after line 654.** We thank the reviewer for indicating this typo. We will correct this. \n\n**3. Notation $\\kappa$.** We thank the reviewer for this comment; and yes, you are right. We will use a better notation in the final version of the paper.\n\n**4. Related works.** We thank the reviewer for pointing out these works. We will indeed mention them and include more discussions in a proper context in the final version of the paper. \n\n**5. Lemmas 6 and 7.** We thank the reviewer for giving us this insight. We will use the tighter result from [19].\n\n**6. Lemma 9.** We thank the reviewer for spotting this. We will correct it.\n\n**7. Assumption in Lemma 11.** We thank the reviewer for this insightful comment. Indeed we will state the assumptions in the main statement of Lemma 11. \n\n**8. Lines 623-627.** As mentioned in comment 5, we will now compare absolute compressors and $\\delta$-contraction operators using [19]. We will also state that $\\lambda$ can be arbitrarily large.\n"
            },
            {
                "document_title": "Official Review of Paper10919 by Reviewer eex4",
                "document_content": "In this paper, the authors demonstrate that, in the context of distributed optimization problems with $n$ workers, the hard-threshold sparsifier is the optimal sparsifier for a proposed communication complexity model which where the goal is to minimize the total error for a sequence of responses. This allows the authors to compare the sum of compression errors for various algorithms and to demonstrate that while for the per-iteration $k$-element budget the Top-$k$ sparsifier is optimal, when it comes to total error the hard threshold sparsifier is better. \n\nThe authors also compare the convergence rates of the top-$k$ sparsifier vs the hard threshold sparsifer for image classification, language modeling, and recommendation tasks.   I find this paper interesting and vote to accept it. I think it is an interesting result but unfortunately I am not an expert in the field and so am not sure about it's significance in the context of the field. I do like the communication complexity model.  This is a theoretical paper and has limited societal impact. "
            }
        ]
    }
}